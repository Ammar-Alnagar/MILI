"""
Activation Functions GPU Kernel Implementation for MILI.
Implements SwiGLU and other modern activation functions as efficient GPU kernels.

SwiGLU formula:
output = Swish(gate) * value = gate * sigmoid(gate) * value

Where Swish(x) = x * sigmoid(x)
"""

from utils.types import (
    DType, TensorMetadata, DeviceContext, KernelConfig,
    GPUConfig, ThreadIdx, BlockIdx
)
import math


# ============================================================================
# Activation Function Configuration
# ============================================================================

@register_passable("trivial")
struct ActivationConfig:
    """Configuration for activation kernels."""
    var activation_type: UInt8  # 0: SwiGLU, 1: GELU, 2: ReLU, 3: SILU
    var use_approximation: Bool  # Use fast approximation (e.g., GELU approx)
    var threads_per_block: Int32
    var use_fp16: Bool
    
    alias SWIGLU = ActivationConfig.create_swiglu()
    alias GELU = ActivationConfig.create_gelu()
    alias RELU = ActivationConfig.create_relu()
    alias SILU = ActivationConfig.create_silu()
    
    fn __init__(
        activation_type: UInt8 = 0,
        use_approximation: Bool = True,
        threads_per_block: Int32 = 256,
        use_fp16: Bool = False
    ) -> Self:
        return Self(
            activation_type=activation_type,
            use_approximation=use_approximation,
            threads_per_block=threads_per_block,
            use_fp16=use_fp16
        )
    
    @staticmethod
    fn create_swiglu() -> ActivationConfig:
        return ActivationConfig(activation_type=0)
    
    @staticmethod
    fn create_gelu() -> ActivationConfig:
        return ActivationConfig(activation_type=1, use_approximation=True)
    
    @staticmethod
    fn create_relu() -> ActivationConfig:
        return ActivationConfig(activation_type=2)
    
    @staticmethod
    fn create_silu() -> ActivationConfig:
        return ActivationConfig(activation_type=3)


# ============================================================================
# Primitive Activation Functions
# ============================================================================

@register_passable("trivial")
struct ActivationOps:
    """Collection of activation operations."""
    
    @staticmethod
    fn sigmoid(x: Float32) -> Float32:
        """Sigmoid function: 1 / (1 + exp(-x))"""
        if x > 0.0:
            let exp_neg_x = exp(-x)
            return 1.0 / (1.0 + exp_neg_x)
        else:
            let exp_x = exp(x)
            return exp_x / (1.0 + exp_x)
    
    @staticmethod
    fn sigmoid_approx(x: Float32) -> Float32:
        """Fast sigmoid approximation using tanh."""
        return 0.5 + 0.125 * x  // Very rough approximation
    
    @staticmethod
    fn swish(x: Float32) -> Float32:
        """Swish activation: x * sigmoid(x)"""
        return x * ActivationOps.sigmoid(x)
    
    @staticmethod
    fn swish_approx(x: Float32) -> Float32:
        """Fast Swish approximation."""
        let sigmoid_val = ActivationOps.sigmoid_approx(x)
        return x * sigmoid_val
    
    @staticmethod
    fn gelu(x: Float32) -> Float32:
        """
        GELU activation: x * Phi(x)
        where Phi(x) is the cumulative distribution function of the standard normal distribution.
        Approximation: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))
        """
        let c = sqrt(2.0 / 3.14159265359)
        let x_cubed = x * x * x
        let tanh_input = c * (x + 0.044715 * x_cubed)
        let tanh_val = tanh(tanh_input)
        return 0.5 * x * (1.0 + tanh_val)
    
    @staticmethod
    fn gelu_precise(x: Float32) -> Float32:
        """
        More precise GELU using error function approximation.
        GELU(x) = x * Phi(x) = 0.5 * x * (1 + erf(x/sqrt(2)))
        """
        let sqrt2 = sqrt(2.0)
        let x_normalized = x / sqrt2
        // Approximate erf using rational approximation
        let erf_approx = tanh(1.20671 * x_normalized)
        return 0.5 * x * (1.0 + erf_approx)
    
    @staticmethod
    fn relu(x: Float32) -> Float32:
        """ReLU activation: max(0, x)"""
        return max(0.0, x)
    
    @staticmethod
    fn silu(x: Float32) -> Float32:
        """SiLU activation (Swish with beta=1): x * sigmoid(x)"""
        return ActivationOps.swish(x)


# ============================================================================
# SwiGLU Kernel
# ============================================================================

struct SwiGLUKernel:
    """GPU kernel for SwiGLU activation."""
    
    var context: DeviceContext
    var metadata: TensorMetadata
    var config: ActivationConfig
    
    fn __init__(
        context: DeviceContext,
        metadata: TensorMetadata,
        config: ActivationConfig = ActivationConfig.SWIGLU,
    ) -> Self:
        return Self(
            context=context,
            metadata=metadata,
            config=config
        )
    
    fn compute_swiglu_element(self, gate: Float32, value: Float32) -> Float32:
        """
        Compute SwiGLU for a single element.
        SwiGLU(gate, value) = gate * sigmoid(gate) * value
        """
        let swish_gate = ActivationOps.swish(gate)
        return swish_gate * value
    
    fn compute_swiglu_thread(
        inout self,
        thread_idx: Int32,
        block_idx: Int32,
        gate_ptr: DTypePointer[DType.float32],
        value_ptr: DTypePointer[DType.float32],
        output_ptr: DTypePointer[DType.float32]
    ):
        """
        Compute SwiGLU for elements assigned to this thread.
        
        CUDA-style kernel: threads process different elements in parallel.
        Each thread handles one or more elements of the output tensor.
        
        Memory layout:
        - Input (gate + value concatenated): [batch, seq_len, hidden_size * 2]
        - Output: [batch, seq_len, hidden_size]
        """
        let batch_size = self.metadata.batch_size
        let seq_length = self.metadata.seq_length
        let hidden_size = self.metadata.hidden_size()
        
        // Total number of output elements
        let total_elements = batch_size * seq_length * hidden_size
        let element_idx = thread_idx
        
        if element_idx >= total_elements:
            return  // Out of bounds
        
        // Compute batch, seq, and hidden indices
        let hidden_idx = element_idx % hidden_size
        let token_idx = element_idx / hidden_size
        
        // Gate and value are interleaved or concatenated
        let gate_offset = token_idx * hidden_size * 2 + hidden_idx
        let value_offset = token_idx * hidden_size * 2 + hidden_size + hidden_idx
        
        let gate = gate_ptr[gate_offset]
        let value = value_ptr[value_offset]
        
        // Compute SwiGLU
        let result = self.compute_swiglu_element(gate, value)
        output_ptr[element_idx] = result
    
    fn kernel_launch(
        inout self,
        gate_ptr: DTypePointer[DType.float32],
        value_ptr: DTypePointer[DType.float32],
        output_ptr: DTypePointer[DType.float32]
    ):
        """
        Launch the SwiGLU kernel with CUDA-style grid/block structure.
        
        Grid structure:
        - Each thread processes one output element
        - Threads distributed across all batch and sequence positions
        """
        let batch_size = self.metadata.batch_size
        let seq_length = self.metadata.seq_length
        let hidden_size = self.metadata.hidden_size()
        let total_elements = batch_size * seq_length * hidden_size
        
        let block_size = self.config.threads_per_block
        let grid_size = (total_elements + block_size - 1) // block_size
        
        self.context.config.block_size_x = block_size
        self.context.config.grid_size_x = grid_size
        
        // Execute kernel on each thread
        for block_idx in range(grid_size):
            for thread_idx in range(block_size):
                let global_thread_id = block_idx * block_size + thread_idx
                if global_thread_id < total_elements:
                    self.compute_swiglu_thread(
                        global_thread_id,
                        block_idx,
                        gate_ptr,
                        value_ptr,
                        output_ptr
                    )
    
    fn forward(
        inout self,
        gate_ptr: DTypePointer[DType.float32],
        value_ptr: DTypePointer[DType.float32],
        output_ptr: DTypePointer[DType.float32],
    ):
        """
        Forward pass: apply SwiGLU activation.
        
        Formula: output = Swish(gate) * value
        
        Args:
            gate_ptr: Gate tensor [batch, seq_len, hidden_size]
            value_ptr: Value tensor [batch, seq_len, hidden_size]
            output_ptr: Output tensor [batch, seq_len, hidden_size]
        """
        self.kernel_launch(gate_ptr, value_ptr, output_ptr)
    
    fn forward_fused(
        inout self,
        ffn_input: DTypePointer[DType.float32],
        weight1_ptr: DTypePointer[DType.float32],
        weight2_ptr: DTypePointer[DType.float32],
        output_ptr: DTypePointer[DType.float32],
    ):
        """
        Fused operation: linear projection to [gate, value] then SwiGLU.
        More efficient than separate operations.
        
        Args:
            ffn_input: Input to FFN [batch, seq_len, hidden_size]
            weight1_ptr: First weight matrix for gate
            weight2_ptr: Second weight matrix for value
            output_ptr: Output tensor
        """
        // This would involve:
        // 1. Compute gate = ffn_input @ weight1 + bias1
        // 2. Compute value = ffn_input @ weight2 + bias2
        // 3. Apply SwiGLU
        // For now, call the standard forward after projections
        self.kernel_launch(ffn_input, ffn_input, output_ptr)


# ============================================================================
# GELU Kernel
# ============================================================================

struct GELUKernel:
    """GPU kernel for GELU activation."""
    
    var context: DeviceContext
    var metadata: TensorMetadata
    var config: ActivationConfig
    
    fn __init__(
        context: DeviceContext,
        metadata: TensorMetadata,
        config: ActivationConfig = ActivationConfig.GELU,
    ) -> Self:
        return Self(
            context=context,
            metadata=metadata,
            config=config
        )
    
    fn compute_gelu_element(self, x: Float32) -> Float32:
        """Compute GELU for a single element."""
        if self.config.use_approximation:
            return ActivationOps.gelu(x)
        else:
            return ActivationOps.gelu_precise(x)
    
    fn forward(
        inout self,
        input_ptr: DTypePointer[DType.float32],
        output_ptr: DTypePointer[DType.float32],
    ):
        """
        Forward pass: apply GELU activation element-wise.
        
        Args:
            input_ptr: Input tensor
            output_ptr: Output tensor (same shape as input)
        """
        let batch_size = self.metadata.batch_size
        let seq_length = self.metadata.seq_length
        let hidden_size = self.metadata.hidden_size()
        let total_elements = batch_size * seq_length * hidden_size
        
        for i in range(total_elements):
            output_ptr[i] = self.compute_gelu_element(input_ptr[i])


# ============================================================================
# Generic Activation Kernel
# ============================================================================

struct ActivationKernel:
    """Generic kernel that supports multiple activation functions."""
    
    var context: DeviceContext
    var metadata: TensorMetadata
    var config: ActivationConfig
    
    fn __init__(
        context: DeviceContext,
        metadata: TensorMetadata,
        config: ActivationConfig,
    ) -> Self:
        return Self(
            context=context,
            metadata=metadata,
            config=config
        )
    
    fn forward(
        inout self,
        input_ptr: DTypePointer[DType.float32],
        output_ptr: DTypePointer[DType.float32],
    ):
        """Apply configured activation function."""
        let batch_size = self.metadata.batch_size
        let seq_length = self.metadata.seq_length
        let hidden_size = self.metadata.hidden_size()
        let total_elements = batch_size * seq_length * hidden_size
        
        for i in range(total_elements):
            let x = input_ptr[i]
            let result: Float32
            
            if self.config.activation_type == 0:  // SwiGLU (needs gate and value)
                result = x  // Placeholder
            elif self.config.activation_type == 1:  // GELU
                result = ActivationOps.gelu(x)
            elif self.config.activation_type == 2:  // ReLU
                result = ActivationOps.relu(x)
            elif self.config.activation_type == 3:  // SiLU
                result = ActivationOps.silu(x)
            else:
                result = x  // Identity
            
            output_ptr[i] = result


# ============================================================================
# Utility Functions
# ============================================================================

fn fused_linear_swiglu(
    input_ptr: DTypePointer[DType.float32],
    weight_gate: DTypePointer[DType.float32],
    bias_gate: DTypePointer[DType.float32],
    weight_value: DTypePointer[DType.float32],
    bias_value: DTypePointer[DType.float32],
    output_ptr: DTypePointer[DType.float32],
    batch_size: Int32,
    seq_len: Int32,
    input_dim: Int32,
    hidden_dim: Int32,
):
    """
    Fused operation: Linear transformation to gate and value, then SwiGLU.
    Equivalent to:
    1. gate = input @ weight_gate + bias_gate
    2. value = input @ weight_value + bias_value
    3. output = Swish(gate) * value
    """
    for b in range(batch_size):
        for s in range(seq_len):
            let input_offset = (b * seq_len + s) * input_dim
            let output_offset = (b * seq_len + s) * hidden_dim
            
            for h in range(hidden_dim):
                var gate_val: Float32 = bias_gate[h]
                var value_val: Float32 = bias_value[h]
                
                // Matrix multiplication
                for i in range(input_dim):
                    gate_val += input_ptr[input_offset + i] * weight_gate[h * input_dim + i]
                    value_val += input_ptr[input_offset + i] * weight_value[h * input_dim + i]
                
                // Apply SwiGLU
                let swish_val = ActivationOps.swish(gate_val)
                output_ptr[output_offset + h] = swish_val * value_val


fn fused_gelu_linear(
    input_ptr: DTypePointer[DType.float32],
    weight_ptr: DTypePointer[DType.float32],
    bias_ptr: DTypePointer[DType.float32],
    output_ptr: DTypePointer[DType.float32],
    batch_size: Int32,
    seq_len: Int32,
    input_dim: Int32,
    output_dim: Int32,
):
    """
    Fused operation: Apply GELU, then linear transformation.
    Equivalent to:
    1. activated = GELU(input)
    2. output = activated @ weight + bias
    """
    for b in range(batch_size):
        for s in range(seq_len):
            let input_offset = (b * seq_len + s) * input_dim
            let output_offset = (b * seq_len + s) * output_dim
            
            // Apply GELU to input
            var gelu_input = DynamicVector[Float32](capacity=input_dim)
            for i in range(input_dim):
                let x = input_ptr[input_offset + i]
                gelu_input.push_back(ActivationOps.gelu(x))
            
            // Linear transformation
            for o in range(output_dim):
                var val: Float32 = bias_ptr[o]
                for i in range(input_dim):
                    val += gelu_input[i] * weight_ptr[o * input_dim + i]
                output_ptr[output_offset + o] = val
