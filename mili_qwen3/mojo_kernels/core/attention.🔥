"""
FlashAttention and Optimized Attention Kernels for MILO.
High-performance GPU kernels for both prefill and decode phases of attention.

FlashAttention Formula (Prefill):
output = softmax(Q @ K^T / sqrt(d_k)) @ V

Decode Attention Formula:
output = softmax(Q_new @ K_cache^T / sqrt(d_k)) @ V_cache
"""

from utils.types import (
    DType, TensorMetadata, DeviceContext, KernelConfig,
    GPUConfig, ThreadIdx, BlockIdx
)
import math


# ============================================================================
# Attention Kernel Configuration
# ============================================================================

@register_passable("trivial")
struct AttentionConfig:
    """Configuration for attention kernels."""
    var block_size_m: Int32  # Block size for M dimension (sequence)
    var block_size_n: Int32  # Block size for N dimension (key length)
    var block_size_k: Int32  # Block size for K dimension (head_dim)
    var use_shared_memory: Bool
    var use_causal_mask: Bool
    var attention_dropout: Float32
    var num_heads: Int32
    var num_kv_heads: Int32
    
    fn __init__(
        block_size_m: Int32 = 64,
        block_size_n: Int32 = 64,
        block_size_k: Int32 = 32,
        use_causal_mask: Bool = True,
        attention_dropout: Float32 = 0.0,
        num_heads: Int32 = 32,
        num_kv_heads: Int32 = 8,
    ) -> Self:
        return Self(
            block_size_m=block_size_m,
            block_size_n=block_size_n,
            block_size_k=block_size_k,
            use_shared_memory=True,
            use_causal_mask=use_causal_mask,
            attention_dropout=attention_dropout,
            num_heads=num_heads,
            num_kv_heads=num_kv_heads
        )
    
    fn tokens_per_block(self) -> Int32:
        """Total tokens processed per block."""
        return self.block_size_m * self.block_size_n


# ============================================================================
# Attention Score Computation
# ============================================================================

struct AttentionScoreCompute:
    """Computes attention scores (Q @ K^T)."""
    
    var scale: Float32
    var use_fp16: Bool
    
    fn __init__(head_dim: Int32, use_fp16: Bool = False) -> Self:
        let scale = 1.0 / sqrt(Float32(head_dim))
        return Self(scale=scale, use_fp16=use_fp16)
    
    fn compute_score(
        self,
        q_ptr: DTypePointer[DType.float32],
        k_ptr: DTypePointer[DType.float32],
        q_idx: Int32,
        k_idx: Int32,
        head_dim: Int32
    ) -> Float32:
        """
        Compute dot product between Q and K vectors.
        Q @ K^T / sqrt(d_k)
        """
        var score: Float32 = 0.0
        
        for d in range(head_dim):
            score += q_ptr[q_idx + d] * k_ptr[k_idx + d]
        
        return score * self.scale
    
    fn compute_score_block(
        self,
        q_ptr: DTypePointer[DType.float32],
        k_ptr: DTypePointer[DType.float32],
        scores_ptr: DTypePointer[DType.float32],
        q_start: Int32,
        k_start: Int32,
        block_m: Int32,
        block_n: Int32,
        head_dim: Int32,
        stride_q: Int32,
        stride_k: Int32,
        stride_scores: Int32
    ):
        """
        Compute attention scores for a block of tokens.
        Scores: [block_m, block_n]
        """
        for m in range(block_m):
            for n in range(block_n):
                let q_idx = q_start + m * stride_q
                let k_idx = k_start + n * stride_k
                let score = self.compute_score(q_ptr, k_ptr, q_idx, k_idx, head_dim)
                
                let scores_idx = m * stride_scores + n
                scores_ptr[scores_idx] = score


# ============================================================================
# Online Softmax Computation
# ============================================================================

struct OnlineSoftmax:
    """
    Online softmax computation using the two-pass algorithm.
    Maintains numerically stable computation without materializing full matrix.
    """
    
    var max_val: Float32
    var sum_exp: Float32
    
    fn __init__() -> Self:
        return Self(max_val=-1e9, sum_exp=0.0)
    
    fn update(inout self, scores: DynamicVector[Float32]):
        """
        Update online softmax with new scores.
        First pass: find max, second pass: compute sum of exponentials.
        """
        // First pass: find maximum
        var local_max = self.max_val
        for score in scores:
            local_max = max(local_max, score)
        
        // Update global max
        let old_max = self.max_val
        self.max_val = local_max
        
        // Adjust previous sum_exp for new max
        if old_max != -1e9:
            self.sum_exp *= exp(old_max - self.max_val)
        
        // Second pass: accumulate exponentials
        for score in scores:
            self.sum_exp += exp(score - self.max_val)
    
    fn get_softmax_weight(self, score: Float32) -> Float32:
        """Get softmax weight for a single score."""
        if self.sum_exp == 0.0:
            return 0.0
        return exp(score - self.max_val) / self.sum_exp


# ============================================================================
# FlashAttention Kernel
# ============================================================================

struct FlashAttentionKernel:
    """High-performance FlashAttention kernel for prefill phase."""
    
    var context: DeviceContext
    var metadata: TensorMetadata
    var config: AttentionConfig
    var score_compute: AttentionScoreCompute
    
    fn __init__(
        context: DeviceContext,
        metadata: TensorMetadata,
        config: AttentionConfig,
    ) -> Self:
        let score_compute = AttentionScoreCompute(
            head_dim=metadata.head_dim,
            use_fp16=False
        )
        return Self(
            context=context,
            metadata=metadata,
            config=config,
            score_compute=score_compute
        )
    
    fn compute_attention_block(
        inout self,
        q_ptr: DTypePointer[DType.float32],
        k_ptr: DTypePointer[DType.float32],
        v_ptr: DTypePointer[DType.float32],
        output_ptr: DTypePointer[DType.float32],
        batch_idx: Int32,
        head_idx: Int32,
        pos_start: Int32,
        pos_end: Int32,
        seq_len: Int32
    ):
        """
        Compute attention for a block of positions.
        Implements tiled computation for memory efficiency.
        """
        let head_dim = self.metadata.head_dim
        let num_heads = self.metadata.num_heads
        
        let block_size = self.config.block_size_m
        let num_blocks = (seq_len + block_size - 1) // block_size
        
        for block_idx in range(num_blocks):
            let block_start = block_idx * block_size
            let block_end = min(block_start + block_size, seq_len)
            let block_len = block_end - block_start
            
            // Compute scores for this block: [positions, block_len]
            var scores = DynamicVector[Float32](capacity=block_len)
            var softmax = OnlineSoftmax()
            
            // Process each position
            for pos in range(pos_start, pos_end):
                // Compute scores against block keys
                for k_pos in range(block_start, block_end):
                    let q_offset = batch_idx * seq_len * num_heads * head_dim 
                                 + pos * num_heads * head_dim
                                 + head_idx * head_dim
                    let k_offset = batch_idx * seq_len * num_heads * head_dim
                                 + k_pos * num_heads * head_dim
                                 + head_idx * head_dim
                    
                    let score = self.score_compute.compute_score(
                        q_ptr, k_ptr,
                        q_offset, k_offset,
                        head_dim
                    )
                    
                    // Apply causal mask if needed
                    if self.config.use_causal_mask and k_pos > pos:
                        scores.push_back(-1e9)
                    else:
                        scores.push_back(score)
                
                // Update online softmax
                softmax.update(scores)
                
                // Compute output for this position
                var output_val: Float32 = 0.0
                for k_idx in range(len(scores)):
                    let k_pos = block_start + k_idx
                    let v_offset = batch_idx * seq_len * num_heads * head_dim
                                 + k_pos * num_heads * head_dim
                                 + head_idx * head_dim
                    
                    let weight = softmax.get_softmax_weight(scores[k_idx])
                    
                    // Accumulate weighted value
                    for d in range(head_dim):
                        output_val += weight * v_ptr[v_offset + d]
                
                // Store output
                let output_offset = batch_idx * seq_len * num_heads * head_dim
                                  + pos * num_heads * head_dim
                                  + head_idx * head_dim
                output_ptr[output_offset] = output_val
                
                // Clear scores for next position
                scores.clear()
    
    fn forward(
        inout self,
        q_ptr: DTypePointer[DType.float32],
        k_ptr: DTypePointer[DType.float32],
        v_ptr: DTypePointer[DType.float32],
        output_ptr: DTypePointer[DType.float32],
    ):
        """
        Forward pass: compute attention for all tokens.
        
        Args:
            q_ptr: Query tensor [batch, seq_len, num_heads, head_dim]
            k_ptr: Key tensor [batch, seq_len, num_heads, head_dim]
            v_ptr: Value tensor [batch, seq_len, num_heads, head_dim]
            output_ptr: Output tensor [batch, seq_len, num_heads, head_dim]
        """
        let batch_size = self.metadata.batch_size
        let seq_len = self.metadata.seq_length
        let num_heads = self.metadata.num_heads
        
        // Process each batch and head
        for batch in range(batch_size):
            for head in range(num_heads):
                self.compute_attention_block(
                    q_ptr, k_ptr, v_ptr, output_ptr,
                    batch, head,
                    0, seq_len,
                    seq_len
                )


# ============================================================================
# Decode Attention Kernel (Optimized for single token)
# ============================================================================

struct DecodeAttentionKernel:
    """Optimized attention kernel for decode phase (single token generation)."""
    
    var context: DeviceContext
    var metadata: TensorMetadata
    var config: AttentionConfig
    var score_compute: AttentionScoreCompute
    
    fn __init__(
        context: DeviceContext,
        metadata: TensorMetadata,
        config: AttentionConfig,
    ) -> Self:
        let score_compute = AttentionScoreCompute(
            head_dim=metadata.head_dim,
            use_fp16=False
        )
        return Self(
            context=context,
            metadata=metadata,
            config=config,
            score_compute=score_compute
        )
    
    fn forward(
        inout self,
        q_ptr: DTypePointer[DType.float32],
        k_cache_ptr: DTypePointer[DType.float32],
        v_cache_ptr: DTypePointer[DType.float32],
        output_ptr: DTypePointer[DType.float32],
        seq_len: Int32
    ):
        """
        Forward pass: compute attention for single new token.
        
        Optimized for:
        - Single query token (1, num_heads, head_dim)
        - Full KV cache (seq_len, num_heads, head_dim)
        - Output: (1, num_heads, head_dim)
        
        Args:
            q_ptr: Query for new token [1, num_heads, head_dim]
            k_cache_ptr: Cached keys [seq_len, num_heads, head_dim]
            v_cache_ptr: Cached values [seq_len, num_heads, head_dim]
            output_ptr: Output [1, num_heads, head_dim]
            seq_len: Length of cached sequence
        """
        let head_dim = self.metadata.head_dim
        let num_heads = self.metadata.num_heads
        
        for head in range(num_heads):
            // Compute scores: q @ k^T
            var scores = DynamicVector[Float32](capacity=seq_len)
            
            for k_pos in range(seq_len):
                let q_offset = head * head_dim
                let k_offset = k_pos * num_heads * head_dim + head * head_dim
                
                var score: Float32 = 0.0
                for d in range(head_dim):
                    score += q_ptr[q_offset + d] * k_cache_ptr[k_offset + d]
                
                score *= self.score_compute.scale
                scores.push_back(score)
            
            // Compute softmax
            var max_score: Float32 = -1e9
            for score in scores:
                max_score = max(max_score, score)
            
            var sum_exp: Float32 = 0.0
            for i in range(len(scores)):
                scores[i] = exp(scores[i] - max_score)
                sum_exp += scores[i]
            
            let inv_sum = 1.0 / sum_exp
            for i in range(len(scores)):
                scores[i] *= inv_sum
            
            // Compute output: softmax(scores) @ v
            var output_vec = DynamicVector[Float32](capacity=head_dim)
            for d in range(head_dim):
                var val: Float32 = 0.0
                for k_pos in range(seq_len):
                    let v_offset = k_pos * num_heads * head_dim + head * head_dim
                    val += scores[k_pos] * v_cache_ptr[v_offset + d]
                output_vec.push_back(val)
            
            // Store output
            let output_offset = head * head_dim
            for d in range(head_dim):
                output_ptr[output_offset + d] = output_vec[d]


# ============================================================================
# Grouped Query Attention (GQA) Kernel
# ============================================================================

struct GroupedQueryAttentionKernel:
    """
    Attention kernel supporting Grouped Query Attention (GQA).
    Multiple query heads share key-value pairs.
    """
    
    var base_kernel: FlashAttentionKernel
    var num_query_groups: Int32
    
    fn __init__(
        base_kernel: FlashAttentionKernel,
        num_query_heads: Int32,
        num_kv_heads: Int32
    ) -> Self:
        let num_query_groups = num_query_heads / num_kv_heads
        return Self(
            base_kernel=base_kernel,
            num_query_groups=num_query_groups
        )
    
    fn forward(
        inout self,
        q_ptr: DTypePointer[DType.float32],
        k_ptr: DTypePointer[DType.float32],
        v_ptr: DTypePointer[DType.float32],
        output_ptr: DTypePointer[DType.float32],
    ):
        """
        Apply GQA: expand KV heads to match query heads.
        
        Q shape: [batch, seq_len, num_query_heads, head_dim]
        K shape: [batch, seq_len, num_kv_heads, head_dim]
        V shape: [batch, seq_len, num_kv_heads, head_dim]
        Output shape: [batch, seq_len, num_query_heads, head_dim]
        """
        let batch_size = self.base_kernel.metadata.batch_size
        let seq_len = self.base_kernel.metadata.seq_length
        let head_dim = self.base_kernel.metadata.head_dim
        let num_query_heads = self.base_kernel.metadata.num_heads
        let num_kv_heads = self.base_kernel.metadata.num_kv_heads
        
        for batch in range(batch_size):
            for seq in range(seq_len):
                for q_head in range(num_query_heads):
                    // Map query head to kv head
                    let kv_head = q_head / self.num_query_groups
                    
                    // Compute attention using the mapped kv_head
                    // (Actual implementation would compute scores and output)
                    
                    let output_offset = batch * seq_len * num_query_heads * head_dim
                                      + seq * num_query_heads * head_dim
                                      + q_head * head_dim
                    
                    // Store result (simplified)
                    for d in range(head_dim):
                        output_ptr[output_offset + d] = 0.0


# ============================================================================
# Utility Functions
# ============================================================================

fn compute_attention_context_length(
    batch_size: Int32,
    seq_len: Int32,
    num_heads: Int32,
    head_dim: Int32
) -> Int32:
    """Calculate total context length needed for attention computation."""
    return batch_size * seq_len * num_heads * head_dim


fn estimate_shared_memory_needed(
    block_size_m: Int32,
    block_size_n: Int32,
    head_dim: Int32,
    dtype_bytes: Int32 = 4
) -> Int32:
    """
    Estimate shared memory needed for attention block computation.
    Includes Q, K blocks and scores matrix.
    """
    let q_mem = block_size_m * head_dim * dtype_bytes
    let k_mem = block_size_n * head_dim * dtype_bytes
    let scores_mem = block_size_m * block_size_n * dtype_bytes
    
    return q_mem + k_mem + scores_mem
