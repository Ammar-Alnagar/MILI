# Mojo API Documentation

> The Mojo API reference.

This file contains all documentation content in a single document following the llmstxt.org standard.

## allgather

<section class='mojo-docs'>

`allgather[dtype: DType, rank: Int, ngpus: Int](input_buffers: InlineArray[NDBuffer[dtype, rank, MutAnyOrigin], ngpus], output_buffers: InlineArray[NDBuffer[dtype, rank, MutAnyOrigin], (ngpus * ngpus)], rank_sigs: InlineArray[LegacyUnsafePointer[Signal], 8], ctxs: List[DeviceContext], _max_num_blocks: Optional[Int] = None)`

Performs all-gather across GPUs with variadic output.

Each device receives individual copies of all input buffers.

The implementation automatically selects between P2P and non-P2P paths
based on hardware capabilities.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType - The data type of tensor elements.
* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): Int - Number of dimensions in input tensors.
* â€‹ngpus ([`Int`](/mojo/std/builtin/int/Int)): Int - Number of GPUs participating in all-gather.

**Args:**

* â€‹input\_buffers ([`InlineArray`](/mojo/std/collections/inline_array/InlineArray)): Input buffers from each GPU.
* â€‹output\_buffers ([`InlineArray`](/mojo/std/collections/inline_array/InlineArray)): Flat array of ngpus \* ngpus output buffers.
  Layout: output\_buffers\[device\_idx \* ngpus + input\_idx]
  contains device\_idx's copy of input\_idx's data.
* â€‹rank\_sigs ([`InlineArray`](/mojo/std/collections/inline_array/InlineArray)): Signal pointers for P2P synchronization.
* â€‹ctxs ([`List`](/mojo/std/collections/list/List)): List of device contexts for participating GPUs.
* â€‹\_max\_num\_blocks ([`Optional`](/mojo/std/collections/optional/Optional)): Maximum number of blocks for kernel launch (optional).

`allgather[dtype: DType, rank: Int, ngpus: Int](input_buffers: InlineArray[NDBuffer[dtype, rank, MutAnyOrigin], ngpus], output_buffers: InlineArray[NDBuffer[dtype, rank, MutAnyOrigin], (ngpus * ngpus)], ctxs: List[DeviceContext])`

Backward compatible version without rank\_sigs parameter.

This version uses the naive implementation since we can't allocate signal
buffers with proper lifetime in this function.

**Deprecated:** Use the `signal_buffers` overload of `allgather` instead.

</section>

---

## allgather (Allgather)

<section class='mojo-docs'>

Multi-GPU allgather implementation that gathers values from multiple GPUs into an output buffer.

This module provides an optimized implementation of allgather operations across
multiple GPUs, supporting both peer-to-peer (P2P) and non-P2P communication
patterns. The implementation automatically selects between approaches based on
hardware capabilities:

1. P2P-based implementation (when P2P access is available):
   * Uses direct GPU-to-GPU memory access for better performance.
   * Optimized for NVLink and xGMI bandwidth utilization.
   * Uses vectorized memory access.

2. Non-P2P fallback implementation:
   * Copies data through device memory when direct GPU access isn't possible.
   * Simple but functional approach for systems without P2P support.

## Functions

* [â€‹`allgather`](./allgather): Performs all-gather across GPUs with variadic output.

</section>

---

## TuningConfigAllreduce

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TuningConfigAllreduce`

Parameters:     ngpus: Number of GPUs for running allreduce.     num\_bytes: Total number of input bytes supported by the config.     sm\_version: SM version (as string).     num\_blocks: Number of thread blocks for running allreduce.

## Fields

* â€‹ngpus (`Int`):
* â€‹num\_bytes (`Int`):
* â€‹sm\_version (`StaticString`):
* â€‹num\_blocks (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`TuningConfig`](/mojo/kernels/internal_utils/dispatch_utils/TuningConfig)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__str__`

`__str__(self) -> String`

**Returns:**

`String`

</section>

---

## allreduce

<section class='mojo-docs'>

`allreduce[dtype: DType, rank: Int, ngpus: Int, output_lambda: OptionalReg[fn[dtype: DType, rank: Int, width: Int, *, alignment: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None] = None, pdl_level: PDLLevel = PDLLevel(), *, use_multimem: Bool = False, use_quickreduce: Bool = False](input_buffers: InlineArray[NDBuffer[dtype, rank, MutAnyOrigin], 1 if use_multimem else ngpus], output_buffer: NDBuffer[dtype, rank, MutAnyOrigin], rank_sigs: InlineArray[LegacyUnsafePointer[Signal], 8], ctx: DeviceContext, _max_num_blocks: Optional[Int] = None, iteration: Int = 0)`

Per-device allreduce: one instance per GPU builds its own output.

High-level model

* Each GPU runs one instance of this function in parallel with the others.
* Every instance reads all inputs but writes only its own output buffer.
* A Python-level fence is inserted across the outputs to prevent reordering.

Two execution paths

1. P2P fast path (when peer access is available)
   * 1-stage kernel (latency-bound): each thread vector-loads from all GPUs,
     accumulates in higher precision, and writes directly to the result.
   * 2-stage kernel (bandwidth-bound): reduce-scatter then all-gather.
     Uses each GPU's `rank_sigs[*]` payload as a staging area for partitions.

     Diagram (per GPU r, 2-stage):

     * Stage 1: write reduced partition r into payload of `rank_sigs[r]`.
     * Stage 2: gather partitions from all peers' payloads into `out_r`.

2. Naive fallback (no P2P)
   * For GPU r: create local accumulator A\_r, allocate a temporary buffer S\_r,
     copy each peer input into S\_r and accumulate into A\_r, then apply the epilogue
     into `out_r`.

     Diagram (per GPU r, naive):
     in\_r â†’ A\_r += in\_r; for iâ‰ r: in\_i â†’ tmp\_r â†’ A\_r += tmp\_r; A\_r â†’ out\_r

Notes:

* Inputs must have identical shape/dtype across GPUs.
* Signal buffers must be sized at least `size_of(Signal) + payload_bytes` for the P2P 2-stage path,
  where `payload_bytes` equals the input tensor bytecount.
* The naive path is automatically selected if P2P cannot be enabled.
* The `use_multimem` parameter requires P2P access between GPUs to be enabled.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the tensor elements.
* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): Number of dimensions in the tensors.
* â€‹ngpus ([`Int`](/mojo/std/builtin/int/Int)): Number of GPUs participating in the allreduce.
* â€‹output\_lambda ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Elementwise epilogue applied on the device result.
* â€‹pdl\_level ([`PDLLevel`](/mojo/std/gpu/primitives/grid_controls/PDLLevel)): Controls PDL behavior for P2P kernels.
* â€‹use\_multimem ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to use multimem mode for improved performance.
* â€‹use\_quickreduce ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, prefer the quickreduce 2-stage path when eligible.

**Args:**

* â€‹input\_buffers ([`InlineArray`](/mojo/std/collections/inline_array/InlineArray)): Inputs from ALL GPUs (for P2P, these must be peer accessible).
* â€‹output\_buffer ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): Output for THIS GPU.
* â€‹rank\_sigs ([`InlineArray`](/mojo/std/collections/inline_array/InlineArray)): Per-GPU Signal; header plus payload. Payload is used as scratch
  for the P2P 2-stage path.
* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): Device context for THIS GPU (device id â†’ rank).
* â€‹\_max\_num\_blocks ([`Optional`](/mojo/std/collections/optional/Optional)): Optional grid limit (dispatch selects a default otherwise).
* â€‹iteration ([`Int`](/mojo/std/builtin/int/Int)): Monotonic per-call counter used to color quickreduce flags.
  Increment each launch; ensures barrier flags are unique across
  iterations to prevent reuse hazards when reusing the same signal buffers.

</section>

---

## allreduce_2stage_quickreduce

<section class='mojo-docs'>

`allreduce_2stage_quickreduce[dtype: DType, rank: Int, ngpus: Int, *, BLOCK_SIZE: Int, output_lambda: elementwise_epilogue_type, atom_size: Int](result: NDBuffer[dtype, rank, MutAnyOrigin], local_src: LegacyUnsafePointer[Scalar[dtype]], rank_sigs: InlineArray[LegacyUnsafePointer[Signal], 8], num_elements: Int, my_rank: Int, iteration: Int, num_tiles_total: Int)`

</section>

---

## allreduce_2stage_quickreduce_tile

<section class='mojo-docs'>

`allreduce_2stage_quickreduce_tile[dtype: DType, rank: Int, ngpus: Int, *, BLOCK_SIZE: Int, output_lambda: elementwise_epilogue_type, atom_size: Int, use_bufferio: Bool](result: NDBuffer[dtype, rank, MutAnyOrigin], local_src: LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.GLOBAL if is_amd_gpu() else AddressSpace.GENERIC], rank_sigs: InlineArray[LegacyUnsafePointer[Signal], 8], num_elements: Int, my_rank: Int, tile: Int, num_tiles: Int, iteration: Int)`

</section>

---

## get_sm_version

<section class='mojo-docs'>

`get_sm_version() -> StaticString`

**Returns:**

`StaticString`

</section>

---

## allreduce (Allreduce)

<section class='mojo-docs'>

Multi-GPU allreduce implementation for efficient tensor reduction across GPUs.

This module provides an optimized implementation of allreduce operations across multiple GPUs,
supporting both peer-to-peer (P2P) and non-P2P communication patterns. The implementation
automatically selects between two approaches based on hardware capabilities:

1. P2P-based implementation (when P2P access is available):
   * Uses direct GPU-to-GPU memory access for better performance
   * Implements both single-stage and two-stage algorithms:
     * Single-stage for latency-bound transfers (small tensors)
     * Two-stage (reduce-scatter + all-gather) for bandwidth-bound transfers (large tensors)
   * Optimized for NVLink bandwidth utilization
   * Uses vectorized memory access and higher precision accumulation

2. Non-P2P fallback implementation:
   * Copies data through host memory when direct GPU access isn't possible
   * Simple but functional approach for systems without P2P support

The implementation is tuned for common GPU architectures (A100, H100) and includes
parameters that can be adjusted for different hardware configurations.

## Per-Device Architecture

The allreduce operation follows a per-device execution model:

1. **Single-Device Instances**: Each GPU runs its own instance of the allreduce
   operation.

2. **Parallel Execution**: The Python/Graph API layer is responsible for:
   * Creating one allreduce op instance per participating GPU.
   * Ensuring all instances execute in parallel.
   * Ensuring correctness by staging mo.fence.

3. **Device Affinity**: Each allreduce instance:
   * Executes on its assigned GPU (specified via device context).
   * Reads from all GPUs' input buffers (requires P2P access).
   * Writes only to its own output buffer.
   * Uses the same synchronization signals as other instances.

4. **Requirements**:
   * Peer-to-peer access must be enabled between all participating GPUs.
   * All instances must launch before any can complete (for synchronization).
   * The device context determines which GPU executes each instance.

Limitations:

* Number of elements must be a multiple of SIMD width.
* Maximum of 8 GPUs supported.
* All input/output buffers must have identical shapes.

## Visual Overview

1. 1-Stage P2P (latency-bound)

   Each GPU r reads its portion from every peer buffer directly (via P2P),
   accumulates, then writes to its result using the epilogue:

   ```
   GPU r (result_r)
   src_ptrs[0] â”€â”
   src_ptrs[1] â”€â”¼â”€â”€â–º Î£ (high-precision accum) â”€â”€â–º output_lambda â”€â”€â–º result_r
   ...         â”€â”˜
   ```

   Notes:

   * Vectorized loads from global memory on each GPU.
   * Good for small/latency-bound tensors.

2. 2-Stage P2P (bandwidth-bound)

   Stage 1 (reduce-scatter): Each GPU r reduces its assigned partition and writes
   into its own signal payload (the bytes after the Signal header).

   ```
   src_ptrs[*]  â”€â”€â–º  reduce(partition r)  â”€â”€â–º  rank_sigs[r].payload  (per-GPU)
   ```

   Stage 2 (all-gather): Each GPU r gathers all partitions from peers' payloads
   and writes them to its result using the epilogue.

   ```
   [payload_0], [payload_1], ..., [payload_{ngpus-1}]  â”€â”€â–º  result_r (via output_lambda)
   ```

For the naive allreduce (no P2P) per-device flow and staging details, see the
`_allreduce_naive_single` docstring in this file.

## `comptime` values

### `allreduce_table`

`comptime allreduce_table = Table[TuningConfigAllreduce](List[TuningConfigAllreduce](TuningConfigAllreduce(-1, -1, "sm_90a", 216), TuningConfigAllreduce(4, 134217728, "sm_90a", 232), TuningConfigAllreduce(-1, -1, "sm_100a", 512), TuningConfigAllreduce(2, 8388608, "sm_100a", 512), TuningConfigAllreduce(2, 16777216, "sm_100a", 512), TuningConfigAllreduce(2, 33554432, "sm_100a", 512), TuningConfigAllreduce(2, 67108864, "sm_100a", 512), TuningConfigAllreduce(2, 134217728, "sm_100a", 512), TuningConfigAllreduce(4, 8388608, "sm_100a", 512), TuningConfigAllreduce(4, 16777216, "sm_100a", 512), TuningConfigAllreduce(4, 33554432, "sm_100a", 512), TuningConfigAllreduce(4, 67108864, "sm_100a", 512), TuningConfigAllreduce(4, 134217728, "sm_100a", 512), Tuple[]()), "allreduce_table")`

### `elementwise_epilogue_type`

`comptime elementwise_epilogue_type = fn[dtype: DType, rank: Int, width: Int, *, alignment: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None`

## Structs

* [â€‹`TuningConfigAllreduce`](./TuningConfigAllreduce): Parameters:     ngpus: Number of GPUs for running allreduce.     num\_bytes: Total number of input bytes supported by the config.     sm\_version: SM version (as string).     num\_blocks: Number of thread blocks for running allreduce.

## Functions

* [â€‹`allreduce`](./allreduce): Per-device allreduce: one instance per GPU builds its own output.
* [â€‹`allreduce_2stage_quickreduce`](./allreduce_2stage_quickreduce):
* [â€‹`allreduce_2stage_quickreduce_tile`](./allreduce_2stage_quickreduce_tile):
* [â€‹`get_sm_version`](./get_sm_version):

</section>

---

## comm

<section class='mojo-docs'>

Provides communication primitives for GPUs.

This package includes functions for sending and receiving data between GPUs,
as well as for synchronizing threads across GPUs.

## Packages

* [â€‹`vendor`](./vendor/):

## Modules

* [â€‹`allgather`](./allgather/): Multi-GPU allgather implementation that gathers values from multiple GPUs into an output buffer.
* [â€‹`allreduce`](./allreduce/): Multi-GPU allreduce implementation for efficient tensor reduction across GPUs.
* [â€‹`sync`](./sync/):

</section>

---

## Signal

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct Signal`

A synchronization primitive for coordinating GPU thread blocks across multiple devices.

This struct provides counter-based synchronization between thread blocks on different GPUs.
It maintains two sets of counters:

1. self\_counter: Used by blocks on the current GPU to signal their progress
2. peer\_counter: Used to track progress of blocks on other GPUs

Note:
The counters use unsigned integers that may overflow, but this is safe since
unsigned integer overflow has well-defined behavior.

## Fields

* â€‹self\_counter (`StaticTuple[StaticTuple[UInt32, 8], 512]`): A 2D array of counters with shape (MAX\_NUM\_BLOCKS\_UPPER\_BOUND, MAX\_GPUS). Each counter tracks the progress of a specific thread block on the current GPU. Thread blocks increment their corresponding counter to signal completion of a phase, allowing other GPUs to detect when synchronization points are reached. The counters use atomic operations to ensure proper synchronization across devices.
* â€‹peer\_counter (`StaticTuple[StaticTuple[StaticTuple[UInt32, 8], 512], 2]`): A 3D array of counters with shape (2, MAX\_NUM\_BLOCKS\_UPPER\_BOUND, MAX\_GPUS). Contains two sets of counters to handle two synchronization points safely. The dual counter design prevents race conditions where a peer block arrives at the second sync point before the current block passes the first sync point.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `flag_t`

`comptime flag_t = DType.uint32`

</section>

---

## can_enable_p2p

<section class='mojo-docs'>

`can_enable_p2p() -> Bool`

If peer-to-peer access is supported, enables it between all GPU pairs.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if P2P access is possible between all GPU pairs, False otherwise.

</section>

---

## group_end

<section class='mojo-docs'>

`group_end()`

</section>

---

## group_start

<section class='mojo-docs'>

`group_start()`

</section>

---

## sync

<section class='mojo-docs'>

## `comptime` values

### `MAX_GPUS`

`comptime MAX_GPUS = 8`

Maximum number of GPUs supported in the allreduce implementation.

This constant sets the upper bound for the number of GPUS supported in this algorithm.

### `MAX_NUM_BLOCKS_UPPER_BOUND`

`comptime MAX_NUM_BLOCKS_UPPER_BOUND = 512`

Maximum number of thread blocks to use for reduction kernels.

This value has been empirically optimized through grid search across different GPU architectures.
While this value is optimal for A100 GPUs, H100 GPUs may benefit from more blocks to fully
saturate NVLink bandwidth.

## Structs

* [â€‹`Signal`](./Signal): A synchronization primitive for coordinating GPU thread blocks across multiple devices.

## Functions

* [â€‹`can_enable_p2p`](./can_enable_p2p): If peer-to-peer access is supported, enables it between all GPU pairs.
* [â€‹`group_end`](./group_end):
* [â€‹`group_start`](./group_start):

</section>

---

## Communicators

<section class='mojo-docs'>

`struct Communicators`

## Fields

* â€‹ngpus (`Int`):
* â€‹comms (`InlineArray[ncclComm_t, 8]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

</section>

---

## allgather (Ccl)

<section class='mojo-docs'>

`allgather[dtype: DType, rank: Int, ngpus: Int](inputs: InlineArray[NDBuffer[dtype, rank, MutAnyOrigin], ngpus], outputs: InlineArray[NDBuffer[dtype, rank, MutAnyOrigin], (ngpus * ngpus)], list_of_ctx: List[DeviceContext])`

</section>

---

## allreduce (Ccl)

<section class='mojo-docs'>

`allreduce[dtype: DType, rank: Int, ngpus: Int, output_lambda: OptionalReg[fn[dtype: DType, rank: Int, width: Int, *, alignment: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None] = None, pdl_level: PDLLevel = PDLLevel(), *, use_multimem: Bool = False, use_quickreduce: Bool = False](input_buffers: InlineArray[NDBuffer[dtype, rank, MutAnyOrigin], 1 if use_multimem else ngpus], output_buffer: NDBuffer[dtype, rank, MutAnyOrigin], rank_sigs: InlineArray[LegacyUnsafePointer[Signal], 8], ctx: DeviceContext, _max_num_blocks: Optional[Int] = None, iteration: Int = 0)`

Per-GPU allreduce for use in multi-threaded contexts.

Currently requires prior single-threaded call to init\_comms, as thread-safe
version not yet implemented.

</section>

---

## group

<section class='mojo-docs'>

`group() -> _Group`

**Returns:**

`_Group`

</section>

---

## ccl

<section class='mojo-docs'>

## `comptime` values

### `CCL_LIBRARY`

`comptime CCL_LIBRARY = _Global["CCL_LIBRARY", _init_ccl_dylib]`

### `CCLAllGatherFn`

`comptime CCLAllGatherFn = fn(LegacyUnsafePointer[NoneType], LegacyUnsafePointer[NoneType], Int, ncclDataType_t, LegacyUnsafePointer[NoneType], LegacyUnsafePointer[NoneType]) -> ncclResult_t`

### `CCLAllReduceFn`

`comptime CCLAllReduceFn = fn(LegacyUnsafePointer[NoneType], LegacyUnsafePointer[NoneType], Int, ncclDataType_t, ncclRedOp_t, LegacyUnsafePointer[NoneType], LegacyUnsafePointer[NoneType]) -> ncclResult_t`

### `NCCL_LIBRARY_PATHS`

`comptime NCCL_LIBRARY_PATHS = List[Path]("libnccl.so", "libnccl.so.2", "/usr/lib/x86_64-linux-gnu/libnccl.so", "/usr/lib/x86_64-linux-gnu/libnccl.so.2", Tuple[]())`

### `ncclComm_t`

`comptime ncclComm_t = LegacyOpaquePointer`

### `RCCL_LIBRARY_PATHS`

`comptime RCCL_LIBRARY_PATHS = List[Path]("librccl.so", "librccl.so.1", "/opt/rocm/lib/librccl.so", "/opt/rocm/lib/librccl.so.1", Tuple[]())`

## Structs

* [â€‹`Communicators`](./Communicators):
* [â€‹`ncclDataType_t`](./ncclDataType_t):
* [â€‹`ncclRedOp_t`](./ncclRedOp_t):
* [â€‹`ncclResult_t`](./ncclResult_t):

## Functions

* [â€‹`allgather`](./allgather):
* [â€‹`allreduce`](./allreduce): Per-GPU allreduce for use in multi-threaded contexts.
* [â€‹`group`](./group):
* [â€‹`init_comms`](./init_comms): Pre-initialize NCCL/RCCL communicators.
* [â€‹`is_allgather_available`](./is_allgather_available):
* [â€‹`is_allreduce_available`](./is_allreduce_available):
* [â€‹`ncclCommInitAll`](./ncclCommInitAll):

</section>

---

## init_comms

<section class='mojo-docs'>

`init_comms(ngpus: Int)`

Pre-initialize NCCL/RCCL communicators.

Must be called from a single thread before using allreduce
from multiple threads. This ensures thread-safe initialization since
ncclCommInitAll is not designed for concurrent calls.

</section>

---

## is_allgather_available

<section class='mojo-docs'>

`is_allgather_available() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## is_allreduce_available

<section class='mojo-docs'>

`is_allreduce_available() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## ncclCommInitAll

<section class='mojo-docs'>

`ncclCommInitAll(comms: LegacyUnsafePointer[ncclComm_t], ndev: Int, devlist: LegacyUnsafePointer[Int32]) -> ncclResult_t`

**Returns:**

`ncclResult_t`

</section>

---

## ncclDataType_t

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ncclDataType_t`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ncclBfloat16`

`comptime ncclBfloat16 = ncclDataType_t(9)`

### `ncclFloat16`

`comptime ncclFloat16 = ncclDataType_t(6)`

### `ncclFloat32`

`comptime ncclFloat32 = ncclDataType_t(7)`

## Methods

### `__init__`

`__init__(value: Int) -> Self`

</section>

---

## ncclRedOp_t

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ncclRedOp_t`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ncclSum`

`comptime ncclSum = ncclRedOp_t(0)`

## Methods

### `__init__`

`__init__(value: Int) -> Self`

</section>

---

## ncclResult_t

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ncclResult_t`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Equatable`](/mojo/std/builtin/comparable/Equatable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ncclSuccess`

`comptime ncclSuccess = ncclResult_t(0)`

## Methods

### `__init__`

`__init__(value: Int) -> Self`

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `write_to`

`write_to(self, mut writer: T)`

</section>

---

## vendor

<section class='mojo-docs'>

## Modules

* [â€‹`ccl`](./ccl/):

</section>

---

## extensibility

<section class='mojo-docs'>

Includes the tensor package.

## Packages

* [â€‹`tensor`](./tensor/): APIs to create and manage tensors in a graph.

</section>

---

## tensor

<section class='mojo-docs'>

APIs to create and manage tensors in a graph.

## Modules

* [â€‹`io_spec`](./io_spec/):
* [â€‹`managed_tensor_slice`](./managed_tensor_slice/): Implements the `ManagedTensorSlice` type - a view of a tensor that doesn't own the underlying data. This type is used to build custom graph operations.
* [â€‹`operation_traits`](./operation_traits/):
* [â€‹`tensor_spec`](./tensor_spec/): You can import these APIs from the `max.tensor` package. For example:
* [â€‹`transitional`](./transitional/): Utilities for transitional period during NDBuffer deprecation.

</section>

---

## IO

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct IO`

## Fields

* â€‹value (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `FusedInput`

`comptime FusedInput = IO(2)`

### `FusedOutput`

`comptime FusedOutput = IO(3)`

### `Input`

`comptime Input = IO(1)`

### `Output`

`comptime Output = IO(0)`

### `Unknown`

`comptime Unknown = IO(-1)`

## Methods

### `__init__`

`__init__(value: Int) -> Self`

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## IOSpec

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct IOSpec[mut: Bool, input: IO]`

Parameter used to encode whether a particular tensor argument to a DPS kernel is an output, input, or mutable input.

```mojo
Input == IOSpec[False, IO.Input]()
Output == IOSpec[True, IO.Output]()
MutableInput == IOSpec[True, IO.Input]()
FusedInput == IOSpec[False, IO.FusedInput]()
FusedOutput == IOSpec[True, IO.FusedOutput]()
```

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

</section>

---

## io_spec

<section class='mojo-docs'>

## `comptime` values

### `FusedInput`

`comptime FusedInput = IOSpec[False, IO.FusedInput]()`

### `FusedOutput`

`comptime FusedOutput = IOSpec[True, IO.FusedOutput]()`

### `Input`

`comptime Input = IOSpec[False, IO.Input]()`

### `IOUnknown`

`comptime IOUnknown = IOSpec[True, IO.Unknown]()`

### `MutableInput`

`comptime MutableInput = IOSpec[True, IO.Input]()`

### `Output`

`comptime Output = IOSpec[True, IO.Output]()`

## Structs

* [â€‹`IO`](./IO):
* [â€‹`IOSpec`](./IOSpec): Parameter used to encode whether a particular tensor argument to a DPS kernel is an output, input, or mutable input.

</section>

---

## ManagedTensorSlice

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ManagedTensorSlice[mut: Bool, input: IO, dtype: DType, rank: Int, //, io_spec: IOSpec[mut, input], *, static_spec: StaticTensorSpec[dtype, rank]]`

A view of a tensor that does not own the underlying allocated pointer. When the object lifetime ends it does not free the underlying pointer. Conversely, if a `ManagedTensorSlice` is created, it will not extend the life of the underlying pointer.

Therefore, the user must take care to keep the pointer alive until the last
use of a `ManagedTensorSlice` instance. This class is useful for writing
custom operations where memory is managed by an external runtime like in
MAX's inference stack.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `address_space`

`comptime address_space = static_spec.address_space`

### `alignment`

`comptime alignment = static_spec.alignment`

### `device_type`

`comptime device_type = LayoutTensor[dtype, static_spec.to_layout[dtype, rank](), MutAnyOrigin]`

### `exclusive`

`comptime exclusive = static_spec.exclusive`

## Methods

### `__init__`

`__init__(ptr: LegacyUnsafePointer[Scalar[dtype]], slices: InlineArray[Slice, rank], slicer_spec: RuntimeTensorSpec[dtype, rank]) -> Self`

Initializes a ManagedTensorSlice from a pointer, array of slices and tensor spec.

In general, custom operations should not create `ManagedTensorSlice`
instances, but instead use the ones provided by the MAX inference
engine.

`__init__(ptr: LegacyUnsafePointer[Scalar[dtype]], shape: IndexList[rank]) -> Self`

Initializes a ManagedTensorSlice from a pointer and shape.

In general, custom operations should not create `ManagedTensorSlice`
instances, but instead use the ones provided by the MAX inference
engine.

`__init__(ptr: LegacyUnsafePointer[Scalar[dtype]], shape: IndexList[rank], strides: IndexList[rank]) -> Self`

Initializes a ManagedTensorSlice from a pointer, shape, and strides.

In general, custom operations should not create `ManagedTensorSlice`
instances, but instead use the ones provided by the MAX inference
engine.

### `__getitem__`

`__getitem__(self, indices: IndexList[rank]) -> Scalar[dtype]`

Gets the value at the specified indices.

**Args:**

* â€‹indices ([`IndexList`](/mojo/std/utils/index_/IndexList)): The indices of the value to retrieve.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The value at the specified indices.

`__getitem__(self, *indices: Int) -> Scalar[dtype]`

Gets the value at the specified indices.

**Args:**

* â€‹\*indices ([`Int`](/mojo/std/builtin/int/Int)): The indices of the value to retrieve.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The value at the specified indices.

### `__setitem__`

`__setitem__(self, *indices: Int, *, val: Scalar[dtype])`

Stores the value at the specified indices.

**Args:**

* â€‹\*indices ([`Int`](/mojo/std/builtin/int/Int)): The indices of the value to store.
* â€‹val ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The value to store.

`__setitem__(self, indices: IndexList[rank], val: Scalar[dtype])`

Stores the value at the specified indices.

**Args:**

* â€‹indices ([`IndexList`](/mojo/std/utils/index_/IndexList)): The indices of the value to store.
* â€‹val ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The value to store.

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `spec`

`spec(self) -> RuntimeTensorSpec[dtype, rank]`

Gets the `TensorSpec` of this tensor slice, which provides meta-data about the tensor slice.

**Returns:**

[`RuntimeTensorSpec`](/mojo/tensor/tensor_spec/RuntimeTensorSpec): The static `TensorSpec` for this tensor slice.

### `shape`

`shape(self) -> IndexList[rank]`

Gets the shape of this tensor slice, as an `IndexList`.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The shape of this tensor slice.

### `dim_size`

`dim_size(self, index: Int) -> Int`

Gets the size of a given dimension of this tensor slice using a run time value.

**Args:**

* â€‹index ([`Int`](/mojo/std/builtin/int/Int)): The zero-based index of the dimension.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The size of the tensor slice in the given dimension.

`dim_size[index: Int](self) -> Int`

Gets the size of a given dimension of this tensor slice using a compile time value.

**Parameters:**

* â€‹index ([`Int`](/mojo/std/builtin/int/Int)): The zero-based index of the dimension.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The size of the tensor slice in the given dimension.

### `strides`

`strides(self) -> IndexList[rank]`

Gets the strides of this tensor slice, as an `IndexList`.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The strides of this tensor slice.

### `stride_length`

`stride_length(self, index: Int) -> Int`

Gets the length of the stride of a given dimension of this tensor slice using a run time value.

**Args:**

* â€‹index ([`Int`](/mojo/std/builtin/int/Int)): The zero-based index of the dimension.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The size of the tensor slice in the given dimension.

`stride_length[index: Int](self) -> Int`

Gets the length of the stride of a given dimension of this tensor slice using a compile time value.

**Parameters:**

* â€‹index ([`Int`](/mojo/std/builtin/int/Int)): The zero-based index of the dimension.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The size of the tensor slice in the given dimension.

### `size`

`size(self) -> Int`

Computes the tensor slice's number of elements.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The total number of elements in the tensor slice.

### `unsafe_ptr`

`unsafe_ptr[_dtype: DType = dtype](self) -> LegacyUnsafePointer[Scalar[_dtype]]`

Get the pointer stored in this tensor slice.

Since this method obtains the pointer stored in this tensor slice, it
can modify the invariants of this tensor slice and lead to unexpected
behavior. It should be used with caution.

**Parameters:**

* â€‹\_dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The type of the `UnsafePointer` in this tensor slice.

**Returns:**

`LegacyUnsafePointer`: The `UnsafePointer` which contains the data for this tensor slice.

### `load`

`load[width: Int, _rank: Int, element_alignment: Int = 1](self, index: IndexList[_rank]) -> SIMD[dtype, width]`

Gets data from this tensor slice as a `SIMD`.

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): The width of the `SIMD` value. This must be large enough to contain the data from this tensor slice.
* â€‹\_rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the tensor slice.
* â€‹element\_alignment ([`Int`](/mojo/std/builtin/int/Int)): Indicate the alignment of the pointer stored to memory. This is needed to issue vector load for GPUs with strict alignment requirements.

**Args:**

* â€‹index ([`IndexList`](/mojo/std/utils/index_/IndexList)): An `IndexList` of size `_rank` to indicate the dimension of the tensor slice to obtain data from.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): Data from this tensor slice at dimension `index`.

### `store`

`store[width: Int, _rank: Int, element_alignment: Int = 1](self: ManagedTensorSlice[io_spec, static_spec=static_spec], index: IndexList[_rank], val: SIMD[dtype, width])`

Sets data in this tensor slice from a `SIMD`.

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): The width of the `SIMD` value.
* â€‹\_rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the tensor slice.
* â€‹element\_alignment ([`Int`](/mojo/std/builtin/int/Int)): Indicate the alignment of the pointer stored to memory. This is needed to issue vector store for GPUs with strict alignment requirements.

**Args:**

* â€‹index ([`IndexList`](/mojo/std/utils/index_/IndexList)): An `IndexList` of size `_rank` to indicate the dimension of the tensor slice to set data in.
* â€‹val ([`SIMD`](/mojo/std/builtin/simd/SIMD)): The data to set into this tensor slice.

### `with_layout`

`with_layout[new_rank: Int, //, new_static_shape: DimList, new_static_strides: DimList](self, new_runtime_shape: IndexList[new_rank], new_runtime_strides: IndexList[new_rank], offset_ptr: OptionalReg[LegacyUnsafePointer[Scalar[dtype]]] = None) -> ManagedTensorSlice[io_spec, static_spec=static_spec.with_layout[dtype, rank, new_rank](new_static_shape, new_static_strides)]`

**Returns:**

[`ManagedTensorSlice`](/mojo/tensor/managed_tensor_slice/ManagedTensorSlice)

### `to_layout_tensor`

`to_layout_tensor(self) -> LayoutTensor[dtype, static_spec.to_layout[dtype, rank](), MutAnyOrigin]`

**Returns:**

`LayoutTensor`

### `write_to`

`write_to(self, mut writer: T)`

Formats this buffer to the provided Writer.

**Args:**

* â€‹writer (`T`): The object to write to.

### `__repr__`

`__repr__(self) -> String`

Gets the buffer as a string.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): A compact string representation of the buffer.

### `__str__`

`__str__(self) -> String`

Gets the buffer as a string.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): A compact string of the buffer.

</section>

---

## VariadicTensors

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct VariadicTensors[mut: Bool, input: IO, //, dtype: DType, rank: Int, size: Int, io_spec: IOSpec[mut, input], *, static_specs: StaticTuple[StaticTensorSpec[dtype, rank], size]]`

A tuple-like container of tensors representing variadic arguments from the graph compiler.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Sized`](/mojo/std/builtin/len/Sized)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__getitem__`

`__getitem__[index: Int](self) -> ManagedTensorSlice[io_spec, static_spec=static_specs.__getitem__[size, Int](index)]`

Returns the tensor at the given position in the variadic argument argument pack.

**Parameters:**

* â€‹index ([`Int`](/mojo/std/builtin/int/Int)): The index into the variadic tensor arguments.

**Returns:**

[`ManagedTensorSlice`](/mojo/tensor/managed_tensor_slice/ManagedTensorSlice): The tensor at the specified index.

### `__len__`

`__len__(self) -> Int`

Returns the number of variadic arguments in the pack.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The number of variadic arguments.

</section>

---

## foreach

<section class='mojo-docs'>

`foreach[dtype: DType, rank: Int, //, func: fn[width: Int, element_alignment: Int](IndexList[rank]) capturing -> SIMD[dtype, width], *, target: StringSlice[StaticConstantOrigin] = "cpu", simd_width: Int = get_kernel_simd_width[dtype, target](), _trace_name: StringSlice[StaticConstantOrigin] = "mogg.for_each"](tensor: ManagedTensorSlice[io_spec, static_spec=static_spec], ctx: DeviceContextPtr = DeviceContextPtr())`

Apply the function `func` to each element of the tensor slice.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the elements in the tensor slice.
* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the tensor slice.
* â€‹func (`fn[width: Int, element_alignment: Int](IndexList[rank]) capturing -> SIMD[dtype, width]`): The function to apply to each element of the tensor slice.
* â€‹target (`StringSlice`): Indicates the type of the target device (e.g. "cpu", "gpu").
* â€‹simd\_width ([`Int`](/mojo/std/builtin/int/Int)): The SIMD width for the target (usually leave this as its default value).
* â€‹\_trace\_name (`StringSlice`): Name of the executed operation displayed in the trace\_description.

**Args:**

* â€‹tensor ([`ManagedTensorSlice`](/mojo/tensor/managed_tensor_slice/ManagedTensorSlice)): The output tensor slice which receives the return values from `func`.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The call context (forward this from the custom operation).

`foreach[dtype: DType, rank: Int, //, func: fn[width: Int](IndexList[rank]) capturing -> SIMD[dtype, width], out_func: fn[width: Int](IndexList[rank]) capturing -> None, *, target: StringSlice[StaticConstantOrigin] = "cpu", simd_width: Int = get_kernel_simd_width[dtype, target](), _trace_name: StringSlice[StaticConstantOrigin] = "mogg.for_each"](tensor: ManagedTensorSlice[io_spec, static_spec=static_spec], ctx: DeviceContextPtr = DeviceContextPtr())`

Apply the function `func` to each element of the tensor slice.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the elements in the tensor slice.
* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the tensor slice.
* â€‹func (`fn[width: Int](IndexList[rank]) capturing -> SIMD[dtype, width]`): The function to apply to each element of the tensor slice.
* â€‹out\_func (`fn[width: Int](IndexList[rank]) capturing -> None`): The function to apply on each output element.
* â€‹target (`StringSlice`): Indicates the type of the target device (e.g. "cpu", "gpu").
* â€‹simd\_width ([`Int`](/mojo/std/builtin/int/Int)): The SIMD width for the target (usually leave this as its default value).
* â€‹\_trace\_name (`StringSlice`): Name of the executed operation displayed in the trace\_description.

**Args:**

* â€‹tensor ([`ManagedTensorSlice`](/mojo/tensor/managed_tensor_slice/ManagedTensorSlice)): The input tensor slice which the consumed values.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The call context (forward this from the custom operation).

`foreach[dtype: DType, rank: Int, //, func: fn[width: Int](IndexList[rank]) capturing -> SIMD[dtype, width], *, target: StringSlice[StaticConstantOrigin] = "cpu", simd_width: Int = get_kernel_simd_width[dtype, target](), _trace_name: StringSlice[StaticConstantOrigin] = "mogg.for_each"](tensor: ManagedTensorSlice[io_spec, static_spec=static_spec], ctx: DeviceContextPtr = DeviceContextPtr())`

Apply the function `func` to each element of the tensor slice.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the elements in the tensor slice.
* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the tensor slice.
* â€‹func (`fn[width: Int](IndexList[rank]) capturing -> SIMD[dtype, width]`): The function to apply to each element of the tensor slice.
* â€‹target (`StringSlice`): Indicates the type of the target device (e.g. "cpu", "gpu").
* â€‹simd\_width ([`Int`](/mojo/std/builtin/int/Int)): The SIMD width for the target (usually leave this as its default value).
* â€‹\_trace\_name (`StringSlice`): Name of the executed operation displayed in the trace\_description.

**Args:**

* â€‹tensor ([`ManagedTensorSlice`](/mojo/tensor/managed_tensor_slice/ManagedTensorSlice)): The output tensor slice which receives the return values from `func`.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The call context (forward this from the custom operation).

</section>

---

## managed_tensor_slice

<section class='mojo-docs'>

Implements the `ManagedTensorSlice` type - a view of a tensor that doesn't own the underlying data. This type is used to build custom graph operations.

## `comptime` values

### `DynamicTensor`

`comptime DynamicTensor[dtype: DType, rank: Int] = ManagedTensorSlice[IOUnknown, static_spec=StaticTensorSpec.create_unknown[dtype, rank]()]`

#### Parameters

* â€‹dtype (`DType`):
* â€‹rank ([`Int`](/std/builtin/int/Int)):

### `InputTensor`

`comptime InputTensor = ManagedTensorSlice[Input, static_spec=?]`

### `InputVariadicTensors`

`comptime InputVariadicTensors = VariadicTensors[?, ?, ?, Input, static_specs=?]`

### `OutputTensor`

`comptime OutputTensor = ManagedTensorSlice[Output, static_spec=?]`

### `OutputVariadicTensors`

`comptime OutputVariadicTensors = VariadicTensors[?, ?, ?, Output, static_specs=?]`

## Structs

* [â€‹`ManagedTensorSlice`](./ManagedTensorSlice): A view of a tensor that does not own the underlying allocated pointer. When the object lifetime ends it does not free the underlying pointer. Conversely, if a `ManagedTensorSlice` is created, it will not extend the life of the underlying pointer.
* [â€‹`VariadicTensors`](./VariadicTensors): A tuple-like container of tensors representing variadic arguments from the graph compiler.

## Functions

* [â€‹`foreach`](./foreach): Apply the function `func` to each element of the tensor slice.
* [â€‹`rebuild_mix_precision_static_tensor_specs_with_input_lambda`](./rebuild_mix_precision_static_tensor_specs_with_input_lambda):
* [â€‹`rebuild_static_tensor_specs_with_compute_output_lambda`](./rebuild_static_tensor_specs_with_compute_output_lambda):
* [â€‹`rebuild_static_tensor_specs_with_input_lambda`](./rebuild_static_tensor_specs_with_input_lambda):
* [â€‹`rebuild_static_tensor_specs_with_output_lambda`](./rebuild_static_tensor_specs_with_output_lambda):
* [â€‹`trace_slice_arg`](./trace_slice_arg): Helper to stringify the type and shape of a kernel argument for tracing.

</section>

---

## rebuild_mix_precision_static_tensor_specs_with_input_lambda

<section class='mojo-docs'>

`rebuild_mix_precision_static_tensor_specs_with_input_lambda[func_type: AnyTrivialRegType, //, src_dtype: DType, dst_dtype: DType, rank: Int](spec: StaticTensorSpec[src_dtype, rank], in_lambda: func_type) -> StaticTensorSpec[dst_dtype, rank]`

**Returns:**

[`StaticTensorSpec`](/mojo/compiler_internal/directives/StaticTensorSpec)

</section>

---

## rebuild_static_tensor_specs_with_compute_output_lambda

<section class='mojo-docs'>

`rebuild_static_tensor_specs_with_compute_output_lambda[func_type: AnyTrivialRegType, //, dtype: DType, rank: Int](spec: StaticTensorSpec[dtype, rank], out_compute_lambda: func_type) -> StaticTensorSpec[dtype, rank]`

**Returns:**

[`StaticTensorSpec`](/mojo/compiler_internal/directives/StaticTensorSpec)

</section>

---

## rebuild_static_tensor_specs_with_input_lambda

<section class='mojo-docs'>

`rebuild_static_tensor_specs_with_input_lambda[func_type: AnyTrivialRegType, //, dtype: DType, rank: Int](spec: StaticTensorSpec[dtype, rank], in_lambda: func_type) -> StaticTensorSpec[dtype, rank]`

**Returns:**

[`StaticTensorSpec`](/mojo/compiler_internal/directives/StaticTensorSpec)

</section>

---

## rebuild_static_tensor_specs_with_output_lambda

<section class='mojo-docs'>

`rebuild_static_tensor_specs_with_output_lambda[func_type: AnyTrivialRegType, //, dtype: DType, rank: Int](spec: StaticTensorSpec[dtype, rank], out_lambda: func_type) -> StaticTensorSpec[dtype, rank]`

**Returns:**

[`StaticTensorSpec`](/mojo/compiler_internal/directives/StaticTensorSpec)

</section>

---

## trace_slice_arg

<section class='mojo-docs'>

`trace_slice_arg(name: String, buf: ManagedTensorSlice[io_spec, static_spec=static_spec]) -> String`

Helper to stringify the type and shape of a kernel argument for tracing.

**Args:**

* â€‹name ([`String`](/mojo/std/collections/string/string/String)): The name of the argument.
* â€‹buf ([`ManagedTensorSlice`](/mojo/tensor/managed_tensor_slice/ManagedTensorSlice)): The NDBuffer to trace.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): A string representation of the buffer with its shape and data type.

</section>

---

## ElementwiseBinaryComparisonOp

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

## Required methods

### `elementwise`

`static elementwise[dtype: DType, width: Int](lhs: SIMD[dtype, width], rhs: SIMD[dtype, width]) -> SIMD[DType.bool, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## ElementwiseBinaryOp

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

## Required methods

### `elementwise`

`static elementwise[dtype: DType, width: Int](lhs: SIMD[dtype, width], rhs: SIMD[dtype, width]) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## ElementwiseUnaryMixedOp

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

## Required methods

### `elementwise`

`static elementwise[dtype: DType, out_dtype: DType, width: Int](x: SIMD[dtype, width]) -> SIMD[out_dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## ElementwiseUnaryOp

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

## Required methods

### `elementwise`

`static elementwise[dtype: DType, width: Int](x: SIMD[dtype, width]) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## operation_traits

<section class='mojo-docs'>

## Traits

* [â€‹`ElementwiseBinaryComparisonOp`](./ElementwiseBinaryComparisonOp):
* [â€‹`ElementwiseBinaryOp`](./ElementwiseBinaryOp):
* [â€‹`ElementwiseUnaryMixedOp`](./ElementwiseUnaryMixedOp):
* [â€‹`ElementwiseUnaryOp`](./ElementwiseUnaryOp):

</section>

---

## RuntimeTensorSpec

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct RuntimeTensorSpec[dtype: DType, rank: Int]`

## Fields

* â€‹shape (`IndexList[rank]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__getitem__`

`__getitem__(self, idx: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `bytecount`

`bytecount(self) -> Int`

Gets the total byte count.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The total byte count.

</section>

---

## tensor_spec

<section class='mojo-docs'>

You can import these APIs from the `max.tensor` package. For example:

```mojo
from max.tensor import RuntimeTensorSpec
```

## Structs

* [â€‹`RuntimeTensorSpec`](./RuntimeTensorSpec):

</section>

---

## transitional

<section class='mojo-docs'>

Utilities for transitional period during NDBuffer deprecation.

## Functions

* [â€‹`managed_tensor_slice_to_ndbuffer`](./managed_tensor_slice_to_ndbuffer):

</section>

---

## managed_tensor_slice_to_ndbuffer

<section class='mojo-docs'>

`managed_tensor_slice_to_ndbuffer[spec: StaticTensorSpec[dtype, rank], //](tensor: ManagedTensorSlice[io_spec, static_spec=spec]) -> NDBuffer[dtype, rank, MutAnyOrigin, spec.shape, spec.strides, address_space=spec.address_space]`

**Returns:**

`NDBuffer`

</section>

---

## kv_cache

<section class='mojo-docs'>

Contains implementations for several types of key-value caches.

[KV caches](/glossary/ai/kv-cache) are used in transformer models to store
key-value tensors output from self-attention layers.

These APIs are used in the higher-level functions in the
[`nn`](/mojo/kernels/nn) package.

## Modules

* [â€‹`types`](./types/): This module contains the types for the key-value cache APIs.

</section>

---

## ContinuousBatchingKVCache

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ContinuousBatchingKVCache[dtype_: DType, kv_params_: KVCacheStaticParams]`

Wrapper for the ContinuousKVCache of a given layer in the transformer model.

This abstracts the Pointer indirection for accessing the ContinuousKVCache
for a given batch entry.

THIS IS THE TYPE THAT IS PASSED TO KV PROJECTION AND FLASH ATTENTION
KERNELS.

## Parameters

* â€‹dtype\_ ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of the kv-cache.
* â€‹kv\_params\_ ([`KVCacheStaticParams`](/mojo/kernels/kv_cache/types/KVCacheStaticParams)): The kv-cache static parameters.

## Fields

* â€‹blocks (`ContinuousBatchingKVCache[dtype_, kv_params_].blocks_type`):
* â€‹cache\_lengths (`LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin]`):
* â€‹lookup\_table (`LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin]`):
* â€‹max\_seq\_length (`UInt32`):
* â€‹max\_cache\_length (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`KVCacheT`](/mojo/kernels/kv_cache/types/KVCacheT),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `blocks_layout`

`comptime blocks_layout = Layout.row_major(ContinuousBatchingKVCache[dtype_, kv_params_].blocks_shape)`

### `blocks_shape`

`comptime blocks_shape = IntTuple(-1, -1, Int(ContinuousBatchingKVCache[dtype_, kv_params_].kv_params), Int(ContinuousBatchingKVCache[dtype_, kv_params_].kv_params))`

### `blocks_type`

`comptime blocks_type = LayoutTensor[ContinuousBatchingKVCache[dtype_, kv_params_].dtype, ContinuousBatchingKVCache[dtype_, kv_params_].blocks_layout, MutAnyOrigin]`

### `device_type`

`comptime device_type = ContinuousBatchingKVCache[dtype_, kv_params_]`

### `dtype`

`comptime dtype = dtype_`

### `kv_params`

`comptime kv_params = kv_params_`

### `page_size_`

`comptime page_size_ = 0`

## Methods

### `__init__`

`__init__(blocks: LayoutTensor[ContinuousBatchingKVCache[dtype_, kv_params_].dtype, ContinuousBatchingKVCache[dtype_, kv_params_].blocks_layout, MutAnyOrigin], cache_lengths: LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin], lookup_table: LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin], max_seq_length: UInt32, max_cache_length: UInt32) -> Self`

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

`String`

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

`String`

### `max_tile_size`

`static max_tile_size() -> Int`

Returns the maximum tile size for the KVCache.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `cache_lengths_nd`

`cache_lengths_nd(self) -> LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `cache_length`

`cache_length(self, batch_idx: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `load`

`load[width: Int](self, bs: Int, head_idx: Int, tok_idx: Int, head_dim_idx: Int) -> SIMD[ContinuousBatchingKVCache[dtype_, kv_params_].dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

### `store`

`store(self, bs: Int, head_idx: Int, tok_idx: Int, head_dim_idx: Int, val: SIMD[ContinuousBatchingKVCache[dtype_, kv_params_].dtype, size])`

### `empty_cache`

`empty_cache(self) -> Bool`

Returns true if the cache\_lengths for all requests is 0, false otherwise.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `max_prompt_length`

`max_prompt_length(self) -> UInt32`

Returns the maximum sequence length across all batches of the current request.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `max_context_length`

`max_context_length(self) -> UInt32`

Returns the maximum cache length used across all batches of the current request.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `row_idx`

`row_idx(self, batch_idx: UInt32, tok_idx: UInt32) -> UInt32`

Returns the row idx when viewing the memory as a matrix.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `create_tma_tile`

`create_tma_tile[BN: Int, swizzle_mode: TensorMapSwizzle](self, ctx: DeviceContext) -> TMATensorTile[ContinuousBatchingKVCache[dtype_, kv_params_].dtype, _split_last_layout[ContinuousBatchingKVCache[dtype_, kv_params_].dtype](IndexList[3, DType.int64](BN, 1, Int(ContinuousBatchingKVCache[dtype_, kv_params_].kv_params), Tuple[]()), swizzle_mode, True), _ragged_desc_layout[ContinuousBatchingKVCache[dtype_, kv_params_].dtype](IndexList[3, DType.int64](BN, 1, Int(ContinuousBatchingKVCache[dtype_, kv_params_].kv_params), Tuple[]()), swizzle_mode)]`

Creates a TMA tile for this KV cache.

**Returns:**

`TMATensorTile`

### `block_paged_ptr`

`block_paged_ptr[tile_size: Int](self, batch_idx: Int, start_tok_idx: Int, head_idx: Int, head_dim_idx: Int = 0) -> LegacyUnsafePointer[Scalar[ContinuousBatchingKVCache[dtype_, kv_params_].dtype]]`

**Returns:**

`LegacyUnsafePointer`

</section>

---

## ContinuousBatchingKVCacheCollection

<section class='mojo-docs'>

`struct ContinuousBatchingKVCacheCollection[dtype_: DType, kv_params_: KVCacheStaticParams]`

This is a "view" of the cache for the given sequences in the batch.

This object does not own the underlying buffers in k\_cache and v\_cache,
it's borrowing them from the BlockWrappers in our KVCacheManager.
It does own the Pointer\[NDBuffer\[dtype, 3]] and valid\_lengths buffer

## Parameters

* â€‹dtype\_ ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of the kv-cache.
* â€‹kv\_params\_ ([`KVCacheStaticParams`](/mojo/kernels/kv_cache/types/KVCacheStaticParams)): The kv-cache static parameters.

## Fields

* â€‹cache\_lengths (`LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin]`):
* â€‹lookup\_table (`LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin]`):
* â€‹blocks (`ContinuousBatchingKVCacheCollection[dtype_, kv_params_].blocks_type`):
* â€‹max\_seq\_length (`UInt32`):
* â€‹max\_cache\_length (`UInt32`):
* â€‹kv\_cache\_dynamic\_shape (`IndexList[4]`):
* â€‹kv\_cache\_dynamic\_strides (`IndexList[4]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`KVCollectionT`](/mojo/kernels/kv_cache/types/KVCollectionT),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `blocks_layout`

`comptime blocks_layout = Layout.row_major(ContinuousBatchingKVCacheCollection[dtype_, kv_params_].blocks_shape)`

### `blocks_shape`

`comptime blocks_shape = IntTuple(-1, -1, -1, -1, Int(ContinuousBatchingKVCacheCollection[dtype_, kv_params_].kv_params), Int(ContinuousBatchingKVCacheCollection[dtype_, kv_params_].kv_params))`

### `blocks_type`

`comptime blocks_type = LayoutTensor[ContinuousBatchingKVCacheCollection[dtype_, kv_params_].dtype, ContinuousBatchingKVCacheCollection[dtype_, kv_params_].blocks_layout, MutAnyOrigin]`

### `CacheType`

`comptime CacheType = ContinuousBatchingKVCache[ContinuousBatchingKVCacheCollection[dtype_, kv_params_].dtype, ContinuousBatchingKVCacheCollection[dtype_, kv_params_].kv_params]`

### `dtype`

`comptime dtype = dtype_`

### `kv_params`

`comptime kv_params = kv_params_`

### `name_str`

`comptime name_str = "continuous_batching"`

## Methods

### `__init__`

`__init__(out self, blocks: LayoutTensor[ContinuousBatchingKVCacheCollection[dtype_, kv_params_].dtype, Layout.row_major[6](), MutAnyOrigin], cache_lengths: LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin], lookup_table: LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin], max_seq_length: UInt32, max_cache_length: UInt32)`

### `get_key_cache`

`get_key_cache(self, layer_idx: Int) -> ContinuousBatchingKVCacheCollection[dtype_, kv_params_].CacheType`

**Returns:**

`ContinuousBatchingKVCacheCollection`

### `get_value_cache`

`get_value_cache(self, layer_idx: Int) -> ContinuousBatchingKVCacheCollection[dtype_, kv_params_].CacheType`

**Returns:**

`ContinuousBatchingKVCacheCollection`

### `cache_length`

`cache_length(self, bs_idx: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## KVCacheStaticParams

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct KVCacheStaticParams`

## Fields

* â€‹num\_heads (`UInt`):
* â€‹head\_size (`UInt`):
* â€‹is\_mla (`Bool`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Equatable`](/mojo/std/builtin/comparable/Equatable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(num_heads: UInt, head_size: UInt, is_mla: Bool = False) -> Self`

Initialize KVCacheStaticParams. Args:     num\_heads (UInt): Number of attention heads.     head\_size (UInt): Size of each attention head.     is\_mla (Bool, optional): Whether to use Multi-Linear Attention (MLA) mode.         If true, we only store k cache. If False, we store k and v cache.         Defaults to False.

### `__eq__`

`__eq__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## KVCacheT

<section class='mojo-docs'>

Trait for different KVCache types and implementations.

Represents a single (key or value) cache.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__copyinit__` is trivial.

The implementation of `__copyinit__` is considered to be trivial if:

* The struct has a compiler-generated trivial `__copyinit__` and all its fields
  have a trivial `__copyinit__` method.

In practice, it means the value can be copied by copying the bits from
one location to another without side effects.

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__moveinit__` is trivial.

The implementation of `__moveinit__` is considered to be trivial if:

* The struct has a compiler-generated `__moveinit__` and all its fields
  have a trivial `__moveinit__` method.

In practice, it means the value can be moved by moving the bits from
one location to another without side effects.

### `device_type`

`comptime device_type`

Indicate the type being used on accelerator devices.

### `dtype`

`comptime dtype`

### `kv_params`

`comptime kv_params`

### `page_size_`

`comptime page_size_`

## Required methods

### `__copyinit__`

`__copyinit__(out self: _Self, existing: _Self, /)`

Create a new instance of the value by copying an existing one.

**Args:**

* â€‹existing (`_Self`): The value to copy.

**Returns:**

`_Self`

### `__moveinit__`

`__moveinit__(out self: _Self, deinit existing: _Self, /)`

Create a new instance of the value by moving the value of another.

**Args:**

* â€‹existing (`_Self`): The value to move.

**Returns:**

`_Self`

### `cache_lengths_nd`

`cache_lengths_nd(self: _Self) -> LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin]`

Returns the cache lengths as a NDBuffer.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `cache_length`

`cache_length(self: _Self, batch_idx: Int) -> Int`

Returns the length of the cache for a given batch index.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `load`

`load[width: Int](self: _Self, bs: Int, head_idx: Int, tok_idx: Int, head_dim_idx: Int) -> SIMD[_Self.dtype, width]`

Loads an element from the given index.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

### `store`

`store(self: _Self, bs: Int, head_idx: Int, tok_idx: Int, head_dim_idx: Int, val: SIMD[_Self.dtype, size])`

Stores an element at the given index.

### `empty_cache`

`empty_cache(self: _Self) -> Bool`

Returns true if the cache\_lengths for all requests is 0, false otherwise.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `max_prompt_length`

`max_prompt_length(self: _Self) -> UInt32`

Returns the maximum sequence length across all batches of the current request.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `max_context_length`

`max_context_length(self: _Self) -> UInt32`

Returns the maximum cache length used across all batches of the current request.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `block_paged_ptr`

`block_paged_ptr[tile_size: Int](self: _Self, batch_idx: Int, start_tok_idx: Int, head_idx: Int, head_dim_idx: Int = 0) -> LegacyUnsafePointer[Scalar[_Self.dtype]]`

Returns a LayoutTensor pointing to the KVCache block at the given index.

Paged KVCache implementations must have a block\_size which is a multiple of the
and greater than the layout's first dimension.

**Returns:**

`LegacyUnsafePointer`

### `max_tile_size`

`static max_tile_size() -> Int`

Returns the maximum tile size for the KVCache.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `row_idx`

`row_idx(self: _Self, batch_idx: UInt32, start_tok_idx: UInt32) -> UInt32`

Returns the row idx when viewing the memory as a matrix.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `create_tma_tile`

`create_tma_tile[BN: Int, swizzle_mode: TensorMapSwizzle](self: _Self, ctx: DeviceContext) -> TMATensorTile[_Self.dtype, _split_last_layout[_Self.dtype](IndexList[3, DType.int64](BN, 1, Int(_Self.kv_params), Tuple[]()), swizzle_mode, True), _ragged_desc_layout[_Self.dtype](IndexList[3, DType.int64](BN, 1, Int(_Self.kv_params), Tuple[]()), swizzle_mode)]`

Creates a TMA tile for this KV cache.

**Returns:**

`TMATensorTile`

### `get_type_name`

`static get_type_name() -> String`

Gets the name of the host type (the one implementing this trait). For example, Int would return "Int", DeviceBuffer\[DType.float32] would return "DeviceBuffer\[DType.float32]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

`String`: The host type's name.

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name. For example, because DeviceBuffer's device\_type is UnsafePointer, DeviceBuffer\[DType.float32]'s get\_device\_type\_name() should return something like "UnsafePointer\[Scalar\[DType.float32]]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

`String`: The device type's name.

## Provided methods

### `copy`

`copy(self: _Self) -> _Self`

Explicitly construct a copy of self.

**Returns:**

`_Self`: A copy of this value.

</section>

---

## KVCollectionT

<section class='mojo-docs'>

Trait for a pair of caches (keys and values).

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__copyinit__` is trivial.

The implementation of `__copyinit__` is considered to be trivial if:

* The struct has a compiler-generated trivial `__copyinit__` and all its fields
  have a trivial `__copyinit__` method.

In practice, it means the value can be copied by copying the bits from
one location to another without side effects.

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__moveinit__` is trivial.

The implementation of `__moveinit__` is considered to be trivial if:

* The struct has a compiler-generated `__moveinit__` and all its fields
  have a trivial `__moveinit__` method.

In practice, it means the value can be moved by moving the bits from
one location to another without side effects.

### `CacheType`

`comptime CacheType`

### `dtype`

`comptime dtype`

### `kv_params`

`comptime kv_params`

### `name_str`

`comptime name_str`

## Required methods

### `__copyinit__`

`__copyinit__(out self: _Self, existing: _Self, /)`

Create a new instance of the value by copying an existing one.

**Args:**

* â€‹existing (`_Self`): The value to copy.

**Returns:**

`_Self`

### `__moveinit__`

`__moveinit__(out self: _Self, deinit existing: _Self, /)`

Create a new instance of the value by moving the value of another.

**Args:**

* â€‹existing (`_Self`): The value to move.

**Returns:**

`_Self`

### `get_key_cache`

`get_key_cache(self: _Self, layer_idx: Int) -> _Self.CacheType`

**Returns:**

`_Self.CacheType`

### `get_value_cache`

`get_value_cache(self: _Self, layer_idx: Int) -> _Self.CacheType`

**Returns:**

`_Self.CacheType`

### `cache_length`

`cache_length(self: _Self, bs_idx: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

## Provided methods

### `copy`

`copy(self: _Self) -> _Self`

Explicitly construct a copy of self.

**Returns:**

`_Self`: A copy of this value.

</section>

---

## PagedKVCache

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct PagedKVCache[dtype_: DType, kv_params_: KVCacheStaticParams, page_size: Int]`

The PagedKVCache is a wrapper around the KVCache blocks for a given layer. It is used to access the KVCache blocks for PagedAttention.

## Parameters

* â€‹dtype\_ ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of the kv-cache.
* â€‹kv\_params\_ ([`KVCacheStaticParams`](/mojo/kernels/kv_cache/types/KVCacheStaticParams)): The kv-cache static parameters.
* â€‹page\_size ([`Int`](/mojo/std/builtin/int/Int)): The size of the page.

## Fields

* â€‹blocks (`PagedKVCache[dtype_, kv_params_, page_size].blocks_type`):
* â€‹cache\_lengths (`LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin]`):
* â€‹lookup\_table (`LayoutTensor[DType.uint32, Layout.row_major[2](), ImmutAnyOrigin]`):
* â€‹max\_seq\_length (`UInt32`):
* â€‹max\_cache\_length (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`KVCacheT`](/mojo/kernels/kv_cache/types/KVCacheT),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `blocks_layout`

`comptime blocks_layout = Layout.row_major(PagedKVCache[dtype_, kv_params_, page_size].blocks_shape)`

### `blocks_shape`

`comptime blocks_shape = IntTuple(-1, page_size, Int(PagedKVCache[dtype_, kv_params_, page_size].kv_params), Int(PagedKVCache[dtype_, kv_params_, page_size].kv_params))`

### `blocks_type`

`comptime blocks_type = LayoutTensor[PagedKVCache[dtype_, kv_params_, page_size].dtype, PagedKVCache[dtype_, kv_params_, page_size].blocks_layout, MutAnyOrigin]`

### `device_type`

`comptime device_type = PagedKVCache[dtype_, kv_params_, page_size]`

### `dtype`

`comptime dtype = dtype_`

### `kv_params`

`comptime kv_params = kv_params_`

### `page_size_`

`comptime page_size_ = page_size`

## Methods

### `__init__`

`__init__(blocks: LayoutTensor[PagedKVCache[dtype_, kv_params_, page_size].dtype, PagedKVCache[dtype_, kv_params_, page_size].blocks_layout, MutAnyOrigin], cache_lengths: LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin], lookup_table: LayoutTensor[DType.uint32, Layout.row_major[2](), ImmutAnyOrigin], max_seq_length: UInt32, max_cache_length: UInt32) -> Self`

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

`String`

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

`String`

### `max_tile_size`

`static max_tile_size() -> Int`

Returns the maximum tile size for the KVCache.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `cache_lengths_nd`

`cache_lengths_nd(self) -> LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `cache_length`

`cache_length(self, batch_idx: Int) -> Int`

Returns the length of the cache for a given batch index.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `row_idx`

`row_idx(self, batch_idx: UInt32, tok_idx: UInt32) -> UInt32`

Returns the row idx when viewing the memory as a matrix.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `create_tma_tile`

`create_tma_tile[BN: Int, swizzle_mode: TensorMapSwizzle](self, ctx: DeviceContext) -> TMATensorTile[PagedKVCache[dtype_, kv_params_, page_size].dtype, _split_last_layout[PagedKVCache[dtype_, kv_params_, page_size].dtype](IndexList[3, DType.int64](BN, 1, Int(PagedKVCache[dtype_, kv_params_, page_size].kv_params), Tuple[]()), swizzle_mode, True), _ragged_desc_layout[PagedKVCache[dtype_, kv_params_, page_size].dtype](IndexList[3, DType.int64](BN, 1, Int(PagedKVCache[dtype_, kv_params_, page_size].kv_params), Tuple[]()), swizzle_mode)]`

Creates a TMA tile for this KV cache.

**Returns:**

`TMATensorTile`

### `load`

`load[width: Int](self, bs: Int, head_idx: Int, tok_idx: Int, head_dim_idx: Int) -> SIMD[PagedKVCache[dtype_, kv_params_, page_size].dtype, width]`

Loads an element from the given index.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

### `store`

`store(self, bs: Int, head_idx: Int, tok_idx: Int, head_dim_idx: Int, val: SIMD[PagedKVCache[dtype_, kv_params_, page_size].dtype, size])`

Stores an element at the given index.

### `empty_cache`

`empty_cache(self) -> Bool`

Returns true if the cache\_lengths for all requests is 0, false otherwise.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `max_prompt_length`

`max_prompt_length(self) -> UInt32`

Returns the maximum sequence length across all batches of the current request.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `max_context_length`

`max_context_length(self) -> UInt32`

Returns the maximum cache length used across all batches of the current request.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `block_paged_ptr`

`block_paged_ptr[tile_size: Int](self, batch_idx: Int, start_tok_idx: Int, head_idx: Int, head_dim_idx: Int = 0) -> LegacyUnsafePointer[Scalar[PagedKVCache[dtype_, kv_params_, page_size].dtype]]`

**Returns:**

`LegacyUnsafePointer`

</section>

---

## PagedKVCacheCollection

<section class='mojo-docs'>

`struct PagedKVCacheCollection[dtype_: DType, kv_params_: KVCacheStaticParams, page_size: Int]`

## Fields

* â€‹blocks (`PagedKVCacheCollection[dtype_, kv_params_, page_size].blocks_type`):
* â€‹cache\_lengths (`PagedKVCacheCollection[dtype_, kv_params_, page_size].cache_lengths_type`):
* â€‹lookup\_table (`PagedKVCacheCollection[dtype_, kv_params_, page_size].lookup_table_type`):
* â€‹max\_seq\_length (`UInt32`):
* â€‹max\_cache\_length (`UInt32`):
* â€‹kv\_cache\_dynamic\_shape (`IndexList[4]`):
* â€‹kv\_cache\_dynamic\_strides (`IndexList[4]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`KVCollectionT`](/mojo/kernels/kv_cache/types/KVCollectionT),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `blocks_layout`

`comptime blocks_layout = Layout.row_major(PagedKVCacheCollection[dtype_, kv_params_, page_size].blocks_shape)`

### `blocks_shape`

`comptime blocks_shape = IntTuple(-1, 2 if (xor kv_params_.is_mla._mlir_value, True) else 1, -1, page_size, Int(PagedKVCacheCollection[dtype_, kv_params_, page_size].kv_params), Int(PagedKVCacheCollection[dtype_, kv_params_, page_size].kv_params))`

### `blocks_type`

`comptime blocks_type = LayoutTensor[PagedKVCacheCollection[dtype_, kv_params_, page_size].dtype, PagedKVCacheCollection[dtype_, kv_params_, page_size].blocks_layout, MutAnyOrigin]`

### `cache_lengths_type`

`comptime cache_lengths_type = LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin]`

### `CacheType`

`comptime CacheType = PagedKVCache[PagedKVCacheCollection[dtype_, kv_params_, page_size].dtype, PagedKVCacheCollection[dtype_, kv_params_, page_size].kv_params, page_size]`

### `dtype`

`comptime dtype = dtype_`

### `kv_params`

`comptime kv_params = kv_params_`

### `lookup_table_type`

`comptime lookup_table_type = LayoutTensor[DType.uint32, Layout.row_major[2](), ImmutAnyOrigin]`

### `name_str`

`comptime name_str = "paged"`

## Methods

### `__init__`

`__init__(out self, blocks: LayoutTensor[PagedKVCacheCollection[dtype_, kv_params_, page_size].dtype, Layout.row_major[6](), MutAnyOrigin], cache_lengths: LayoutTensor[DType.uint32, Layout(IntTuple(-1)), ImmutAnyOrigin], lookup_table: LayoutTensor[DType.uint32, Layout.row_major[2](), ImmutAnyOrigin], max_seq_length: UInt32, max_cache_length: UInt32)`

### `get_key_cache`

`get_key_cache(self, layer_idx: Int) -> PagedKVCacheCollection[dtype_, kv_params_, page_size].CacheType`

**Returns:**

`PagedKVCacheCollection`

### `get_value_cache`

`get_value_cache(self, layer_idx: Int) -> PagedKVCacheCollection[dtype_, kv_params_, page_size].CacheType`

**Returns:**

`PagedKVCacheCollection`

### `cache_length`

`cache_length(self, bs_idx: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## types

<section class='mojo-docs'>

This module contains the types for the key-value cache APIs.

The module includes structs implementing several different types of
[KV caches](/glossary/ai/kv-cache).

This module defines two traits that define the roles of the different structs

* `KVCacheT`: Defines the interface for a single (key or value) cache.
* `KVCollectionT`: Defines the interface for a pair of caches (keys and values).

## Structs

* [â€‹`ContinuousBatchingKVCache`](./ContinuousBatchingKVCache): Wrapper for the ContinuousKVCache of a given layer in the transformer model.
* [â€‹`ContinuousBatchingKVCacheCollection`](./ContinuousBatchingKVCacheCollection): This is a "view" of the cache for the given sequences in the batch.
* [â€‹`KVCacheStaticParams`](./KVCacheStaticParams):
* [â€‹`PagedKVCache`](./PagedKVCache): The PagedKVCache is a wrapper around the KVCache blocks for a given layer. It is used to access the KVCache blocks for PagedAttention.
* [â€‹`PagedKVCacheCollection`](./PagedKVCacheCollection):

## Traits

* [â€‹`KVCacheT`](./KVCacheT): Trait for different KVCache types and implementations.
* [â€‹`KVCollectionT`](./KVCollectionT): Trait for a pair of caches (keys and values).

</section>

---

## CopyPolicy

<section class='mojo-docs'>

The CopyPolicy trait defines requirements needed for a tensor to be copied.

These requirements check the compatibility of the source and destination tensors.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__copyinit__` is trivial.

The implementation of `__copyinit__` is considered to be trivial if:

* The struct has a compiler-generated trivial `__copyinit__` and all its fields
  have a trivial `__copyinit__` method.

In practice, it means the value can be copied by copying the bits from
one location to another without side effects.

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__moveinit__` is trivial.

The implementation of `__moveinit__` is considered to be trivial if:

* The struct has a compiler-generated `__moveinit__` and all its fields
  have a trivial `__moveinit__` method.

In practice, it means the value can be moved by moving the bits from
one location to another without side effects.

### `device_type`

`comptime device_type`

Indicate the type being used on accelerator devices.

## Required methods

### `__copyinit__`

`__copyinit__(out self: _Self, existing: _Self, /)`

Create a new instance of the value by copying an existing one.

**Args:**

* â€‹existing (`_Self`): The value to copy.

**Returns:**

`_Self`

### `__moveinit__`

`__moveinit__(out self: _Self, deinit existing: _Self, /)`

Create a new instance of the value by moving the value of another.

**Args:**

* â€‹existing (`_Self`): The value to move.

**Returns:**

`_Self`

### `verify_source_tensor`

`static verify_source_tensor(src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

A static function that verifies the source tensor is compatible with the copy operation. If the tensor is not valid compilation will fail.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor that will be copied from.

### `verify_destination_tensor`

`static verify_destination_tensor(dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

A static function that verifies the destination tensor is compatible with the copy operation. If the tensor is not valid compilation will fail.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor that will be copied to.

### `get_type_name`

`static get_type_name() -> String`

Gets the name of the host type (the one implementing this trait). For example, Int would return "Int", DeviceBuffer\[DType.float32] would return "DeviceBuffer\[DType.float32]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

`String`: The host type's name.

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name. For example, because DeviceBuffer's device\_type is UnsafePointer, DeviceBuffer\[DType.float32]'s get\_device\_type\_name() should return something like "UnsafePointer\[Scalar\[DType.float32]]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

`String`: The device type's name.

## Provided methods

### `copy`

`copy(self: _Self) -> _Self`

Explicitly construct a copy of self.

**Returns:**

`_Self`: A copy of this value.

</section>

---

## copy

<section class='mojo-docs'>

## Traits

* [â€‹`CopyPolicy`](./CopyPolicy): The CopyPolicy trait defines requirements needed for a tensor to be copied.

</section>

---

## Element

<section class='mojo-docs'>

`struct Element[dtype: DType, layout: Layout, /, index_type: DType = _get_index_type(layout)]`

A wrapper around SIMD types that provides layout-driven vectorized operations.

The `Element` struct extends SIMD types with layout-aware load and store
operations, enabling efficient vectorized access to multi-dimensional data.
It maps between logical tensor coordinates and physical memory locations
according to the specified layout.

## Parameters

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the elements.
* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The memory layout describing how elements are organized.
* â€‹index\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The integer type of the index pointing to each element.

## Fields

* â€‹element\_data (`Element[dtype, layout, index_type].element_data_type`): The actual SIMD data stored in this element.
  This field contains the vectorized data values that can be processed
  efficiently using SIMD operations.
* â€‹runtime\_layout (`RuntimeLayout[layout, element_type=DType.int32, linear_idx_type=index_type]`): The runtime layout information for memory access patterns.
  This field stores the layout information needed to map between logical tensor
  coordinates and physical memory locations, supporting both compile-time and
  runtime-determined access patterns.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `element_data_type`

`comptime element_data_type = SIMD[dtype, layout.size()]`

The SIMD type used to store and process the element data.

This type alias defines a SIMD vector with the specified data type and size
matching the layout's total element count, enabling efficient vectorized operations.

## Methods

### `__init__`

`__init__(out self, element_data: SIMD[dtype, layout.size()])`

Initializes an Element with the given SIMD data.

**Args:**

* â€‹element\_data ([`SIMD`](/mojo/std/builtin/simd/SIMD)): The SIMD data to initialize the element with.

`__init__(out self, element_data: SIMD[dtype, layout.size()], runtime_layout: RuntimeLayout[layout, element_type=DType.int32, linear_idx_type=index_type])`

Initializes an Element with the given SIMD data and runtime layout.

**Args:**

* â€‹element\_data ([`SIMD`](/mojo/std/builtin/simd/SIMD)): The SIMD data to initialize the element with.
* â€‹runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout to use for memory access.

### `load`

`static load(ptr: UnsafePointer[Scalar[dtype], origin, address_space=address_space], runtime_layout: RuntimeLayout[layout, element_type=DType.int32, linear_idx_type=index_type] = RuntimeLayout[layout, DType.int32, index_type]()) -> Self`

Loads data from memory according to the specified layout.

This method loads data from memory using the layout information to determine
the memory access pattern. It supports both rank-1 and rank-2 layouts with
various stride patterns, optimizing for contiguous memory access when
possible.

**Args:**

* â€‹ptr ([`UnsafePointer`](/mojo/std/memory/unsafe_pointer/UnsafePointer)): Pointer to the memory location to load from.
* â€‹runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout to use for memory access.

**Returns:**

`Self`: A new `Element` containing the loaded data.

### `masked_load`

`static masked_load(ptr: UnsafePointer[Scalar[dtype], origin, address_space=address_space], runtime_layout: RuntimeLayout[layout, element_type=DType.int32, linear_idx_type=index_type] = RuntimeLayout[layout, DType.int32, index_type]()) -> Self`

Loads data from memory with masking for partial loads.

This method loads data from memory using the layout information, but also
handles cases where the runtime dimensions are smaller than the static
layout dimensions. It ensures that only valid memory locations are accessed.

**Args:**

* â€‹ptr ([`UnsafePointer`](/mojo/std/memory/unsafe_pointer/UnsafePointer)): Pointer to the memory location to load from.
* â€‹runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout to use for memory access.

**Returns:**

`Self`: A new `Element` containing the loaded data, with zeros in positions
beyond the runtime dimensions.

### `store`

`store(self, ptr: UnsafePointer[Scalar[dtype], origin, address_space=address_space])`

Stores element data to memory according to the specified layout.

This method performs a layout-aware store operation, writing data to memory
following the access patterns defined by the layout. It optimizes memory
writes based on the layout's stride patterns to maximize performance.

The method handles different memory layout patterns:

* For rank-1 tensors with contiguous memory (stride=1), it uses vectorized stores
* For rank-2 tensors with contiguous rows or columns, it uses optimized slice-based stores
* For non-contiguous memory layouts, it performs element-by-element stores

Unlike `masked_store()`, this method assumes the full static dimensions will be written
and does not perform runtime dimension boundary checking.

Note:
This method is constrained to layouts with rank <= 2. For higher-rank
tensors, consider decomposing the operation.

**Args:**

* â€‹ptr ([`UnsafePointer`](/mojo/std/memory/unsafe_pointer/UnsafePointer)): Mutable pointer to the memory location where data will be stored.

### `masked_store`

`masked_store(self, ptr: UnsafePointer[Scalar[dtype], origin, address_space=address_space])`

Stores element data to memory with masking for partial stores.

This method performs a layout-aware store operation with boundary checking.
It ensures that only valid memory locations are written to when the runtime
dimensions are smaller than the static layout dimensions, preventing out-of-bounds
memory access.

The method optimizes for different memory layouts:

* For contiguous memory (stride=1), it uses vectorized stores when possible
* For non-contiguous memory, it performs element-by-element stores
* For all patterns, it respects runtime dimension bounds

Note:
This method is constrained to layouts with rank <= 2. For higher-rank
tensors, consider decomposing the operation.

**Args:**

* â€‹ptr ([`UnsafePointer`](/mojo/std/memory/unsafe_pointer/UnsafePointer)): Pointer to the memory location where data will be stored.

### `__str__`

`__str__(self) -> String`

Returns a string representation of the element.

**Returns:**

`String`: A string representation of the element's data.

### `write_to`

`write_to(self, mut writer: T)`

Writes the element to the specified writer.

**Args:**

* â€‹writer (`T`): The writer to output the element representation to.

</section>

---

## MemoryElement

<section class='mojo-docs'>

`struct MemoryElement[mut: Bool, //, dtype: DType, layout: Layout, origin: Origin[mut=mut], /, address_space: AddressSpace, *, index_type: DType = _get_index_type(layout, address_space)]`

Represents data in memory organized according to a specific layout.

The `MemoryElement` struct provides a high-level interface for accessing data
in memory with a specific layout. It encapsulates a pointer to the memory
location and the runtime layout information needed to access the data correctly.

This abstraction enables efficient memory operations that respect the underlying
memory organization, supporting vectorized loads and stores while handling
different memory layouts transparently.

## Parameters

* â€‹mut ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether the memory element is mutable.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the elements.
* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The memory layout describing how elements are organized.
* â€‹origin ([`Origin`](/mojo/std/builtin/type_aliases/Origin)): The origin of the memory element.
* â€‹address\_space ([`AddressSpace`](/mojo/std/memory/pointer/AddressSpace)): The memory address space where the data is located.
* â€‹index\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The integer type of the index pointing to each memory element.

## Fields

* â€‹ptr (`UnsafePointer[Scalar[dtype], origin, address_space=address_space]`): Pointer to the memory location where the data is stored.
  This pointer provides access to the underlying memory with the specified
  address space and alignment requirements. It points to the first element
  of the data structure in memory.
* â€‹runtime\_layout (`RuntimeLayout[layout, element_type=DType.int32, linear_idx_type=index_type]`): Runtime layout information used for memory access calculations.
  This field stores the runtime layout information needed to compute memory
  offsets for accessing elements according to the specified layout pattern.
  It handles both compile-time known dimensions and runtime-determined dimensions.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

## Methods

### `__init__`

`__init__(out self, ptr: UnsafePointer[Scalar[dtype], origin, address_space=address_space], runtime_layout: RuntimeLayout[layout, element_type=DType.int32, linear_idx_type=index_type])`

Initializes a `MemoryElement` with the given pointer and runtime layout.

**Args:**

* â€‹ptr ([`UnsafePointer`](/mojo/std/memory/unsafe_pointer/UnsafePointer)): Pointer to the memory location of the element.
* â€‹runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout to use for memory access.

### `load`

`load(self, out result: Element[dtype, layout, index_type])`

Loads data from memory according to the specified layout.

This method performs a layout-aware load operation, reading data from memory
following the access patterns defined by the layout. It optimizes memory
reads based on the layout's stride patterns to maximize performance.

The method leverages the underlying `Element.load` implementation which handles
different memory layout patterns including contiguous and strided access.

**Returns:**

[`Element`](/mojo/kernels/layout/element/Element): An `Element` containing the loaded data organized according to the layout.

### `store`

`store(self: MemoryElement[dtype, layout, mut_origin, address_space, index_type=index_type], src: Element[dtype, layout, index_type])`

Stores element data to the memory location of this MemoryElement.

This method performs a layout-aware store operation, writing data to memory
following the access patterns defined by the layout. It optimizes memory
writes based on the layout's stride patterns to maximize performance.

The method delegates to the `Element.store` implementation which handles
different memory layout patterns including vectorized stores for contiguous memory
and element-by-element stores for non-contiguous layouts.

**Args:**

* â€‹src ([`Element`](/mojo/kernels/layout/element/Element)): The `Element` containing the data to store.

### `transfer`

`transfer(self: MemoryElement[dtype, layout, mut_origin, address_space, index_type=index_type], src: MemoryElement[dtype, layout, origin, address_space, index_type=index_type])`

Transfers data from another `MemoryElement` to this one.

This method efficiently transfers data between memory locations with potentially
different layouts and data types. It performs the following operations:

1. Loads data from the source `MemoryElement` using its layout
2. Converts the data to the destination data type if necessary
3. Stores the converted data to the destination memory location using its layout

This provides a high-performance way to copy and convert data between different
memory representations while respecting both source and destination memory layouts.

**Args:**

* â€‹src ([`MemoryElement`](/mojo/kernels/layout/element/MemoryElement)): The source `MemoryElement` to transfer data from.

</section>

---

## element (Element)

<section class='mojo-docs'>

Provides element-based access to memory using layout-driven vectorization.

This module implements efficient memory access patterns for multi-dimensional data
using the layout system. It provides abstractions for loading and storing data with
specific memory layouts, enabling vectorized operations that respect the underlying
memory organization.

Key components:

* `Element`: A wrapper around SIMD types that provides layout-driven vectorized
  operations
* `MemoryElement`: Represents data in memory organized according to a specific layout

These components enable efficient tensor operations by ensuring memory accesses
follow optimal patterns defined by the layout system.

## Structs

* [â€‹`Element`](./Element): A wrapper around SIMD types that provides layout-driven vectorized operations.
* [â€‹`MemoryElement`](./MemoryElement): Represents data in memory organized according to a specific layout.

</section>

---

## layout

<section class='mojo-docs'>

Provides layout and layout tensor types, which abstract memory layout for multidimensional data.

* The [`Layout`](/mojo/kernels/layout/layout/Layout) type represents a mapping
  between a set of logical coordinates and a linear index. It can be used, for
  example, to map logical tensor coordinates to a memory address, or to map GPU
  threads to tiles of data.

* The [`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor) type is a
  high-performance tensor with explicit memory layout via a `Layout`.

## Modules

* [â€‹`copy`](./copy/):
* [â€‹`element`](./element/): Provides element-based access to memory using layout-driven vectorization.
* [â€‹`int_tuple`](./int_tuple/): Hierarchical integer tuple data structures for high-performance tensor operations.
* [â€‹`layout`](./layout/): Provides a high-performance tensor layout system for memory mapping and indexing.
* [â€‹`layout_tensor`](./layout_tensor/): Provides the `LayoutTensor` type for representing multidimensional data.
* [â€‹`math`](./math/): Implements math methods that work on layout tensors.
* [â€‹`runtime_layout`](./runtime_layout/): Provides the `RuntimeLayout` type and functions for working with it. You can use `RuntimeLayout` to define a layout where the dimensions are not known at compile time.
* [â€‹`runtime_tuple`](./runtime_tuple/): Provides the `RuntimeTuple` data structure and related utility functions for handling tuple-like data with both compile-time and runtime elements. `RuntimeTuple` is designed for high-performance tensor operations, supporting efficient manipulation of multi-dimensional data structures like shapes, indices, and coordinates.
* [â€‹`swizzle`](./swizzle/): Defines swizzle layouts for optimizing memory access patterns.
* [â€‹`tensor_core`](./tensor_core/): Tensor Core Module for High-Performance Matrix Operations
* [â€‹`tensor_core_async`](./tensor_core_async/): Tensor Core Async Module
* [â€‹`tma_async`](./tma_async/): Tensor Memory Accelerator (TMA) Asynchronous Operations Module

</section>

---

## IntArray

<section class='mojo-docs'>

`@register_passable`
`struct IntArray`

A memory-efficient, register-passable array of integers.

`IntArray` provides a low-level implementation of a dynamically-sized integer array
with direct memory management.

This struct serves as the underlying storage mechanism for `IntTuple` and related
data structures, optimized for high-performance tensor operations.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = False`

### `__del__is_trivial`

`comptime __del__is_trivial = False`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(size: Int = 0) -> Self`

Initialize a new owned `IntArray` with the specified size.

**Args:**

* â€‹size ([`Int`](/mojo/std/builtin/int/Int)): Number of integers to allocate space for. Defaults to 0.

### `__copyinit__`

`__copyinit__(existing: Self) -> Self`

Initialize by copying an existing `IntArray`.

For owned arrays, this performs a deep copy of the data.

**Args:**

* â€‹existing (`Self`): The source array to copy from.

### `__del__`

`__del__(deinit self)`

Destroy the `IntArray` and free its memory if owned.

Only frees memory for owned arrays (positive \_size) to prevent
double-free errors with views.

### `__getitem__`

`__getitem__(self, idx: Int) -> Int`

Access an element at the specified index.

Note:
Bounds checking is performed when assertions are enabled (e.g., -D ASSERT=all).

**Args:**

* â€‹idx ([`Int`](/mojo/std/builtin/int/Int)): Zero-based index of the element to access.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The integer value at the specified index.

### `__setitem__`

`__setitem__(mut self, idx: Int, value: Int)`

Set the value at the specified index.

Note:
Bounds checking is performed when assertions are enabled (e.g., -D ASSERT=all).

**Args:**

* â€‹idx ([`Int`](/mojo/std/builtin/int/Int)): Zero-based index of the element to modify.
* â€‹value ([`Int`](/mojo/std/builtin/int/Int)): The integer value to store at the specified index.

### `owning`

`owning(self) -> Bool`

Check if this `IntArray` owns its memory.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if this array owns its memory (positive \_size),
False if it's a view (negative \_size).

### `size`

`size(self) -> Int`

Get the number of elements in the array.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The number of elements in the array, regardless of ownership status.

### `copy_from`

`copy_from(mut self, offset: Int, source: Self, size: Int)`

Copy elements from another `IntArray`.

**Args:**

* â€‹offset ([`Int`](/mojo/std/builtin/int/Int)): Destination offset in this array.
* â€‹source (`Self`): Source array to copy from.
* â€‹size ([`Int`](/mojo/std/builtin/int/Int)): Number of elements to copy.

`copy_from(mut self, dst_offset: Int, source: Self, src_offset: Int, size: Int)`

Copy elements from another IntArray with source offset.

**Args:**

* â€‹dst\_offset ([`Int`](/mojo/std/builtin/int/Int)): Destination offset in this array.
* â€‹source (`Self`): Source array to copy from.
* â€‹src\_offset ([`Int`](/mojo/std/builtin/int/Int)): Source offset in the source array.
* â€‹size ([`Int`](/mojo/std/builtin/int/Int)): Number of elements to copy.

</section>

---

## IntTuple

<section class='mojo-docs'>

`struct IntTuple`

A hierarchical, nested tuple of integers with efficient memory management.

IntTuple provides a flexible data structure for representing multi-dimensional
shapes, indices, and other nested integer collections. It supports both flat
and hierarchical representations with efficient memory sharing.

This structure is fundamental for tensor operations, layout specifications,
and dimension handling in high-performance computing contexts.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`Equatable`](/mojo/std/builtin/comparable/Equatable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Intable`](/mojo/std/builtin/int/Intable),
[`Iterable`](/mojo/std/iter/Iterable),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Sized`](/mojo/std/builtin/len/Sized),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = False`

### `__del__is_trivial`

`comptime __del__is_trivial = False`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `IteratorType`

`comptime IteratorType[iterable_mut: Bool, //, iterable_origin: Origin[mut=iterable_mut]] = _IntTupleIter[origin_of((muttoimm iterable_origin._mlir_origin))]`

The iterator type for IntTuple iteration.

#### Parameters

* â€‹iterable\_mut ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether the iterable is mutable.
* â€‹iterable\_origin ([`Origin`](/mojo/std/builtin/type_aliases/Origin)): The origin of the iterable.

### `MinimumValue`

`comptime MinimumValue = -65534`

Minimum allowed value for integers in an `IntTuple`.

This constant defines the lower bound for integer values that can be stored
directly in an `IntTuple`. Values below this threshold are reserved for internal
use to represent structural information like sub-tuple offsets.

## Methods

### `__init__`

`__init__(out self)`

Initialize an empty IntTuple.

Creates an `IntTuple` with zero elements, which can be used as a starting
point for building tuples incrementally with `append` or `extend`.

Performance:

* Minimal allocation (just a single element for length).
* Structure validation performed when assertions are enabled.

`__init__(out self, *, num_elems: Int)`

Initialize an `IntTuple` with a specified number of uninitialized elements.

Creates an `IntTuple` with space for the specified number of elements,
but does not initialize the elements themselves.

Note:
Structure validation performed when assertions are enabled.

**Args:**

* â€‹num\_elems ([`Int`](/mojo/std/builtin/int/Int)): The number of elements to allocate space for.

`__init__(out self, *elements: Int)`

Initialize an `IntTuple` with a variadic list of integers.

Creates an `IntTuple` containing the provided integer values.

**Args:**

* â€‹\*elements ([`Int`](/mojo/std/builtin/int/Int)): Variable number of integer values to store in the tuple.

`__init__(out self, elements: VariadicList[Int])`

Initialize an `IntTuple` with a list of integers.

Creates an `IntTuple` containing the provided integer values.

Notes:

* Pre-allocates exact memory needed for efficiency.
* Validates that all values are above `MinimumValue`. If any value is
  less than `MinimumValue`, assertion fails with an error message.
* Structure validation performed when assertions are enabled.

**Args:**

* â€‹elements ([`VariadicList`](/mojo/std/builtin/variadics/VariadicList)): List of integer values to store in the tuple.

`@implicit`
`__init__(out self, value: Int)`

Initialize an `IntTuple` with a single integer value.

Creates an `IntTuple` containing a single integer element.

**Args:**

* â€‹value ([`Int`](/mojo/std/builtin/int/Int)): The integer value to store in the tuple.

`__init__(out self, *elements: Self, *, __list_literal__: Tuple[] = Tuple[]())`

Initialize an `IntTuple` with nested IntTuples.

Creates a hierarchical `IntTuple` containing the provided `IntTuple` elements,
preserving their nested structure.

**Args:**

* â€‹\*elements (`Self`): Variable number of `IntTuple` values to store in the tuple.
* â€‹**list\_literal** ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Specifies that this constructor can be used for
  list literals.

`__init__(out self, *, var _owned: IntArray)`

Initialize an `IntTuple` taking the values of an `IntArray`.

**Args:**

* â€‹\_owned ([`IntArray`](/mojo/kernels/layout/int_tuple/IntArray)): The `IntArray` to use as storage.

`__init__(out self, existing: Self, rng: _StridedRange)`

Initialize an `IntTuple` as a slice of an existing `IntTuple`.

Creates a new `IntTuple` containing only the elements from the existing
`IntTuple` that are specified by the range.

Notes:

* Preserves nested structure of elements in the slice.
* Structure validation performed when assertions are enabled.

**Args:**

* â€‹existing (`Self`): The source `IntTuple` to slice from.
* â€‹rng ([`_StridedRange`](/mojo/std/builtin/range/_StridedRange)): The range of indices to include in the new `IntTuple`.

`__init__(out self, dimlist: DimList)`

Initialize an `IntTuple` from a DimList.

Creates an `IntTuple` containing the dimensions from a DimList, handling
both defined and undefined dimensions appropriately.

Notes:

* Converts undefined dimensions to `UNKNOWN_VALUE`.
* Validates that all values are above `MinimumValue`. If any value is
  less than `MinimumValue`, assertion fails with an error message.

**Args:**

* â€‹dimlist ([`DimList`](/mojo/kernels/buffer/dimlist/DimList)): The DimList containing dimension information.

`__init__[IterableType: Iterable](out self, iterable: IterableType)`

Initialize an `IntTuple` from a zip iterator.

Creates an `IntTuple` by appending each element from the zip iterator.

Note:
This implementation is not optimized and may be improved in future versions.

**Parameters:**

* â€‹IterableType ([`Iterable`](/mojo/std/iter/Iterable)): The type of the iterable.

**Args:**

* â€‹iterable (`IterableType`): An iterable containing pairs of elements to append.

### `__copyinit__`

`__copyinit__(out self, existing: Self)`

Initialize by copying an existing `IntTuple`.

Creates a deep copy of the provided `IntTuple`, copying all its data
into newly allocated memory.

Note:
There is a Mojo bug where this method unnecessarily propagates
the origin of self to the new copy.

**Args:**

* â€‹existing (`Self`): The `IntTuple` to copy from.

### `__getitem__`

`__getitem__(self, _idx: Int) -> Self`

Retrieves an element at the specified index from the `IntTuple`.

Supports negative indexing (e.g., `-1` for the last element).

Notes:
If index is out of bounds, assertion fails with an error message.

**Args:**

* â€‹\_idx ([`Int`](/mojo/std/builtin/int/Int)): The index of the element to retrieve.

**Returns:**

`Self`: An `IntTuple` containing either a single value or a sub-tuple.

`__getitem__(self, span: Slice) -> Self`

Retrieves a slice of elements from the `IntTuple`.

Creates a new `IntTuple` containing the elements specified by the slice.

**Args:**

* â€‹span ([`Slice`](/mojo/std/builtin/builtin_slice/Slice)): A slice object specifying the range of elements to retrieve.

**Returns:**

`Self`: A new `IntTuple` containing the specified elements.

### `__lt__`

`__lt__(self, rhs: Self) -> Bool`

Compare two `IntTuple`s lexicographically.

This function performs element-wise comparison of two `IntTuple`s and determines
if the first is lexicographically less than the second. It compares corresponding
elements until it finds a pair where the elements differ.

Example:

```mojo
from layout.int_tuple import IntTuple

var tuple1 = IntTuple(1, 2, 3)
var tuple2 = IntTuple(1, 2, 4)

var result = tuple1 < tuple2  # Returns True because 3 < 4
```

**Args:**

* â€‹rhs (`Self`): The other `IntTuple` to compare.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if `self` is lexicographically less than `rhs`, False otherwise.

### `__eq__`

`__eq__(self, other: Self) -> Bool`

Equality operator for `IntTuple`.

**Args:**

* â€‹other (`Self`): The `IntTuple` to compare with.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the `IntTuple`s are equal, False otherwise.

### `__ne__`

`__ne__(self, other: Self) -> Bool`

Inequality operator for `IntTuple`.

**Args:**

* â€‹other (`Self`): The `IntTuple` to compare with.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the `IntTuple`s are not equal, False otherwise.

### `elements_size`

`static elements_size(elements: VariadicListMem[IntTuple, is_owned]) -> Int`

Calculate the total storage size needed for a list of IntTuples.

Computes the sum of sizes for all elements, accounting for both direct
integer values and nested sub-tuples.

**Args:**

* â€‹elements ([`VariadicListMem`](/mojo/std/builtin/variadics/VariadicListMem)): List of `IntTuple` elements to measure.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The total storage size required for all elements.

`static elements_size[_origin: ImmutOrigin, n: Int](elements: InlineArray[Pointer[IntTuple, _origin], n], idx: Int) -> Int`

Calculate the total storage size needed for IntTuples at a specific index.

Computes the sum of sizes for all elements at the given index in an array
of `IntTuple` pointers.

**Parameters:**

* â€‹\_origin ([`ImmutOrigin`](/mojo/std/builtin/type_aliases/#immutorigin)): Origin tracking for memory safety.
* â€‹n ([`Int`](/mojo/std/builtin/int/Int)): Size of the inline array.

**Args:**

* â€‹elements ([`InlineArray`](/mojo/std/collections/inline_array/InlineArray)): Array of pointers to `IntTuple`s.
* â€‹idx ([`Int`](/mojo/std/builtin/int/Int)): Index to access in each `IntTuple`.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The total storage size required for all elements at the specified index.

### `owned_copy`

`owned_copy(self) -> Self`

Create a deep copy of this `IntTuple` with its own memory ownership.

This method creates a completely independent copy of the `IntTuple` with
newly allocated memory. Unlike `__copyinit__`, this method can be called
on an existing instance to create a separate copy.

Example:

```mojo
from layout import IntTuple

var original = IntTuple(1, 2, 3)
var copy = original.owned_copy()
# Modifying copy will not affect original
```

**Returns:**

`Self`: A new `IntTuple` containing the same data as this one but with
independent memory ownership.

### `replace_entry`

`replace_entry(self, idx: Int, value: Self) -> Self`

Replace an entry in the tuple with another `IntTuple`.

Creates a new `IntTuple` with the element at the specified index replaced
by the provided `IntTuple`.

Note:
If the index is out of bounds, assertion fails with an error message.

**Args:**

* â€‹idx ([`Int`](/mojo/std/builtin/int/Int)): The index of the element to replace.
* â€‹value (`Self`): The `IntTuple` to insert at the specified index.

**Returns:**

`Self`: A new `IntTuple` with the replacement applied.

`replace_entry(mut self, idx: Int, *, int_value: Int)`

Replace an integer value at the specified index in-place.

Directly modifies the tuple by replacing the integer value at the given index.
This is more efficient than creating a new tuple when only a single value
needs to be changed.

Note:
If the index is out of bounds, assertion fails with an error message.

**Args:**

* â€‹idx ([`Int`](/mojo/std/builtin/int/Int)): The index of the element to replace.
* â€‹int\_value ([`Int`](/mojo/std/builtin/int/Int)): The integer value to insert at the specified index.

### `count_values`

`count_values(self) -> Int`

Count the total number of integer values in this tuple hierarchy.

Recursively traverses the nested tuple structure and counts all integer values.
This is useful for determining the size needed for flattened representations.

Note:
For a flat tuple, this will return the same value as `len(self)`.
For nested tuples, it counts all leaf integer values.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The total count of integer values in this tuple and all nested tuples.

### `flatten`

`flatten(self) -> Self`

Flatten a nested `IntTuple` into a single-level `IntTuple`.

This function converts a hierarchical `IntTuple` structure into a flat
sequence of integer values, preserving the order of elements.

**Returns:**

`Self`: A new `IntTuple` containing all integer values in a flat structure.

### `product_flatten`

`product_flatten(self) -> Self`

Coalesces a nested `IntTuple` into a single-level `IntTuple`, by multiplying all the values together.

**Returns:**

`Self`: A new `IntTuple` containing the products of each top level tuple, in a flat structure.

### `all_known`

`all_known(self) -> Bool`

Check if all values in this tuple hierarchy are known (not `UNKNOWN_VALUE`).

Recursively traverses the nested tuple structure and checks if any value
is equal to `UNKNOWN_VALUE`.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if all values in this tuple and nested tuples are known,
False if any value is `UNKNOWN_VALUE`.

`all_known[start: Int, end: Int](self) -> Bool`

Check if all values in this tuple hierarchy are known (not `UNKNOWN_VALUE`).

Recursively traverses the nested tuple structure and checks if any value
is equal to `UNKNOWN_VALUE`.

**Parameters:**

* â€‹start ([`Int`](/mojo/std/builtin/int/Int)): The starting index (inclusive) for the range to check.
* â€‹end ([`Int`](/mojo/std/builtin/int/Int)): The ending index (exclusive) for the range to check.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if all values in this tuple and nested tuples are known,
False if any value is `UNKNOWN_VALUE`.

### `append`

`append(mut self, *elements: Self)`

Append one or more `IntTuple` elements to this tuple.

This method modifies the tuple in-place by adding the provided elements
to the end of the tuple. It handles both value tuples and nested tuples.

Notes:

* This operation requires reallocating the underlying `IntArray` storage to accommodate
  the new elements, which may impact performance for large tuples.

**Args:**

* â€‹\*elements (`Self`): Variable number of `IntTuple` objects to append to this tuple.

### `extend`

`extend(mut self, tuple: Self)`

Extends this tuple by appending all elements from another tuple.

This method modifies the tuple in-place by adding all elements from the provided
tuple to the end of this tuple. It efficiently handles both value elements and
nested tuples.

Notes:

* This operation requires reallocating the underlying `IntArray` storage
  to accommodate the new elements, which may impact performance for large tuples.
* If the input tuple is empty, this method returns without making any changes.

**Args:**

* â€‹tuple (`Self`): The `IntTuple` whose elements will be appended to this tuple.

### `size`

`size(self) -> Int`

Returns the total size of the `IntTuple` in memory.

For owning tuples, returns the size of the underlying `IntArray`.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The total size in memory units.

### `tuple_size`

`static tuple_size(data: IntArray) -> Int`

Recursively calculates the size of a tuple represented by an `IntArray`.

This method traverses the tuple structure, accounting for both direct values
and nested sub-tuples to compute the total memory footprint.

**Args:**

* â€‹data ([`IntArray`](/mojo/kernels/layout/int_tuple/IntArray)): `IntArray` containing the tuple data.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The total size of the tuple in memory units.

### `validate_structure`

`validate_structure(self)`

Validates the internal structure of the `IntTuple`.

Ensures that the actual size of the underlying data matches the computed size
based on the tuple's structure. This helps detect memory corruption or
implementation errors.

Assertion fails with an error message if validation fails.

### `__len__`

`__len__(self) -> Int`

Returns the number of elements in the `IntTuple`.

This is the logical length of the tuple, not its memory size.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The number of elements in the tuple.

### `__iter__`

`__iter__(ref self) -> _IntTupleIter[origin_of((muttoimm self_is_origin))]`

Returns an iterator over the elements of the `IntTuple`.

This enables iteration through the tuple using for-loops.

**Returns:**

`_IntTupleIter`: An iterator object for this `IntTuple`.

### `is_value`

`is_value(self) -> Bool`

Determines if this `IntTuple` represents a single value rather than a tuple.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if this `IntTuple` contains exactly one element that is a value,
False otherwise.

`is_value(self, i: Int) -> Bool`

Determines if the element at the specified index is a value rather than a tuple.

Notes:
If index is out of bounds, assertion fails with an error message.

**Args:**

* â€‹i ([`Int`](/mojo/std/builtin/int/Int)): The index of the element to check.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the element at index i is a value, False if it's a tuple.

### `is_tuple`

`is_tuple(self) -> Bool`

Determines if this `IntTuple` represents a tuple rather than a single value.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if this `IntTuple` is a tuple (not a single value), False otherwise.

`is_tuple(self, i: Int) -> Bool`

Determines if the element at the specified index is a tuple rather than a value.

Notes:
This is the complement of is\_value(i).

**Args:**

* â€‹i ([`Int`](/mojo/std/builtin/int/Int)): The index of the element to check.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the element at index i is a tuple, False if it's a value.

### `value`

`value(self) -> Int`

Retrieves the value of this `IntTuple` if it represents a single value.

This method should only be called if `is_value()` returns True.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The integer value stored in this `IntTuple`.

`value(self, i: Int) -> Int`

Retrieves the value of the element at the specified index.

This method should only be called if `is_value(i)` returns True.

Notes:
If the element is not a value, the behavior is undefined.

**Args:**

* â€‹i ([`Int`](/mojo/std/builtin/int/Int)): The index of the element to retrieve.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The integer value stored at the specified index.

### `tuple`

`tuple(ref self) -> ref [self] Self`

Returns a reference to this `IntTuple` as a tuple.

Notes:
This method is used to access the current `IntTuple` as a tuple
without creating a copy of the data.

**Returns:**

`ref`: A reference to this `IntTuple` to avoid unnecessary copying.

### `write_to`

`write_to(self, mut writer: T)`

Writes a string representation of this `IntTuple` to the provided writer.

Notes:
For single values, writes just the value.
For tuples, writes a comma-separated list of elements enclosed in parentheses.

**Args:**

* â€‹writer (`T`): The writer to output the string representation to.

### `__str__`

`__str__(self) -> String`

Returns a string representation of this `IntTuple`.

**Returns:**

`String`: A string representation of the `IntTuple`, using the `write_to` method.

### `is_equal`

`static is_equal(a, b: Self) -> Bool`

Compares two `IntTuple`s for equality.

Notes:
Handles nested tuples and special cases where a single-element tuple
is equivalent to its contained value.

**Args:**

* â€‹a (`Self`): The first `IntTuple` to compare.
* â€‹b (`Self`): The second `IntTuple` to compare.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the `IntTuple`s are equal in structure and values, False otherwise.

### `__repr__`

`__repr__(self) -> String`

Returns a string representation of this `IntTuple` for debugging.

**Returns:**

`String`: A string representation of the `IntTuple`, same as `__str__`.

### `__int__`

`__int__(self) -> Int`

Converts this `IntTuple` to an integer.

This method should only be called if `is_value()` returns True.

Aborts:
If the `IntTuple` is not a single value.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The integer value stored in this `IntTuple`.

</section>

---

## abs

<section class='mojo-docs'>

`abs(t: IntTuple) -> IntTuple`

Compute the absolute value of each element in an `IntTuple`.

This function applies the absolute value operation to each integer
in a potentially nested `IntTuple` structure.

**Args:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` to transform.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` with the same structure but with absolute values.

</section>

---

## apply

<section class='mojo-docs'>

`apply[func: fn(Int) capturing -> Int](t: IntTuple) -> IntTuple`

Apply a function to each integer value in an `IntTuple`.

This function recursively applies the given function to each integer value
in a potentially nested `IntTuple` structure, preserving the structure.

**Parameters:**

* â€‹func (`fn(Int) capturing -> Int`): Function to apply to each integer value.

**Args:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` to transform.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` with the same structure but with each integer value
transformed by the function.

</section>

---

## apply_predicate

<section class='mojo-docs'>

`apply_predicate[predicate: fn(IntTuple, IntTuple) -> Bool](a: IntTuple, b: IntTuple) -> Bool`

Apply a predicate function recursively to two `IntTuple`s.

This function traverses two `IntTuple`s with the same structure and applies
a predicate function to corresponding elements. The predicate is applied
only to the leaf nodes (integer values).

Note:
If the structures of the two `IntTuple`s don't match (different nesting or length),
the function returns False without applying the predicate.

**Parameters:**

* â€‹predicate (`fn(IntTuple, IntTuple) -> Bool`): A function that takes two `IntTuple`s (containing integer values)
  and returns a boolean result.

**Args:**

* â€‹a ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): First `IntTuple` to compare.
* â€‹b ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Second `IntTuple` to compare.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the predicate returns True for all corresponding elements and
the structures match, False otherwise.

</section>

---

## apply_zip

<section class='mojo-docs'>

`apply_zip[func: fn(IntTuple, IntTuple) -> IntTuple](t1: IntTuple, t2: IntTuple) -> IntTuple`

Apply a function to pairs of elements from two `IntTuple`s.

This function zips two `IntTuple`s together and applies the given function
to each pair of elements, creating a new `IntTuple` with the results.

**Parameters:**

* â€‹func (`fn(IntTuple, IntTuple) -> IntTuple`): Function that takes two `IntTuple`s and returns an `IntTuple`.

**Args:**

* â€‹t1 ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): First `IntTuple`.
* â€‹t2 ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Second `IntTuple`.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` containing the results of applying func to each pair.

`apply_zip[func: fn(IntTuple, IntTuple) capturing -> IntTuple](t1: IntTuple, t2: IntTuple) -> IntTuple`

Apply a capturing function to pairs of elements from two `IntTuple`s.

This overload allows the function to capture variables from its environment.

**Parameters:**

* â€‹func (`fn(IntTuple, IntTuple) capturing -> IntTuple`): Capturing function that takes two `IntTuple`s and returns an `IntTuple`.

**Args:**

* â€‹t1 ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): First `IntTuple`.
* â€‹t2 ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Second `IntTuple`.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` containing the results of applying func to each pair.

`apply_zip[func: fn(IntTuple, IntTuple, IntTuple) -> IntTuple](t1: IntTuple, t2: IntTuple, t3: IntTuple) -> IntTuple`

Apply a function to triplets of elements from three `IntTuple`s.

This function zips three `IntTuple`s together and applies the given function
to each triplet of elements, creating a new `IntTuple` with the results.

**Parameters:**

* â€‹func (`fn(IntTuple, IntTuple, IntTuple) -> IntTuple`): Function that takes three `IntTuple`s and returns an `IntTuple`.

**Args:**

* â€‹t1 ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): First `IntTuple`.
* â€‹t2 ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Second `IntTuple`.
* â€‹t3 ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Third `IntTuple`.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` containing the results of applying func to each triplet.

`apply_zip[func: fn(IntTuple, IntTuple, IntTuple) capturing -> IntTuple](t1: IntTuple, t2: IntTuple, t3: IntTuple) -> IntTuple`

Apply a capturing function to triplets of elements from three `IntTuple`s.

This overload allows the function to capture variables from its environment.

**Parameters:**

* â€‹func (`fn(IntTuple, IntTuple, IntTuple) capturing -> IntTuple`): Capturing function that takes three `IntTuple`s and returns an `IntTuple`.

**Args:**

* â€‹t1 ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): First `IntTuple`.
* â€‹t2 ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Second `IntTuple`.
* â€‹t3 ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Third `IntTuple`.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` containing the results of applying func to each triplet.

</section>

---

## compact_order

<section class='mojo-docs'>

`compact_order(shape: IntTuple, order: IntTuple) -> IntTuple`

Create a compact stride based on shape and order.

This function generates a stride tuple where lower order numbers imply
faster varying strides. The resulting shape and stride form a bijective layout.

Performance:

* Always inlined for optimal performance in tight loops.
* Flattens inputs and re-nests results for consistent behavior.

Example:

```mojo
from layout import IntTuple
from layout.int_tuple import compact_order

# Create a compact layout with dimensions (2,3,4,5) and ordering (1,4,3,5)
var x = compact_order(IntTuple(2,3,4,5), IntTuple(1,4,3,5))  # returns (1,8,2,24)

# Create a compact layout with nested dimensions and corresponding ordering
var y = compact_order(IntTuple(2,IntTuple(3,4),5), IntTuple(1,IntTuple(2,3),4))  # returns (1,(2,6),24)
```

**Args:**

* â€‹shape ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The shape tuple defining dimensions.
* â€‹order ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The order tuple defining the relative ordering of dimensions.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A stride tuple that creates a compact memory layout according to the
specified order.

</section>

---

## compatible

<section class='mojo-docs'>

`compatible(a: IntTuple, b: IntTuple) -> Bool`

Test if two shapes are compatible for tensor operations.

This function checks if shape A is compatible with shape B, meaning:

1. The total size of A and B are the same
2. Any coordinate into A can also be used as a coordinate into B

Compatible can also be thought of as a partial order on A and B: A <= B.

**Args:**

* â€‹a ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The first `IntTuple` to compare.
* â€‹b ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The second `IntTuple` to compare.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if shape A is compatible with shape B, False otherwise.

</section>

---

## congruent

<section class='mojo-docs'>

`congruent(a: IntTuple, b: IntTuple) -> Bool`

Test if two `IntTuple`s have the same hierarchical structure.

This function checks if two `IntTuple`s have identical nesting patterns,
regardless of the actual integer values they contain.

**Args:**

* â€‹a ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): First `IntTuple` to compare.
* â€‹b ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Second `IntTuple` to compare.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if both `IntTuple`s have the same hierarchical structure,
False otherwise.

</section>

---

## crd2idx

<section class='mojo-docs'>

`crd2idx(crd: IntTuple, shape: IntTuple) -> Int`

Map a logical coordinate to a linear index.

This function converts a multi-dimensional coordinate to a linear index based on the shape.
It uses default strides computed from the shape.

**Args:**

* â€‹crd ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The coordinate tuple to convert.
* â€‹shape ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The shape of the tensor/array.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The linear index corresponding to the coordinate.

`crd2idx(crd: IntTuple, shape: IntTuple, _stride: IntTuple) -> Int`

Map a logical coordinate to a linear index with custom strides.

This function converts a multi-dimensional coordinate to a linear index based on the shape
and stride information. If no stride is provided, it computes default strides from the shape.

The function handles various input combinations:

* Tuple coordinates with tuple shapes and strides
* Single integer coordinate with tuple shapes and strides
* Single integer coordinate with single integer shape and stride

Aborts:

```
- If coordinate and shape dimensions don't match.
- If shape and stride dimensions don't match.
- If input type combinations are invalid.
```

**Args:**

* â€‹crd ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The coordinate(s) to convert, can be a single value or a tuple of coordinates.
* â€‹shape ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The shape of the tensor/array, can be a single value or a tuple of dimensions.
* â€‹\_stride ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Optional custom strides, defaults to row-major strides if not provided.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The linear index corresponding to the coordinate.

</section>

---

## depth

<section class='mojo-docs'>

`depth(src: IntTuple) -> Int`

Calculates the maximum nesting depth of an `IntTuple`.

This function recursively traverses the `IntTuple` structure to determine
its maximum nesting depth. A scalar value has depth 0, a flat tuple has
depth 1, and nested tuples increase the depth accordingly.

Example:

```mojo
from layout import IntTuple, depth

print(depth(IntTuple(1))) # prints 0
print(depth(IntTuple(1, 2))) # prints 1
print(depth((IntTuple(1, 2)))) # prints 2
```

**Args:**

* â€‹src ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` to measure the depth of.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): An integer representing the maximum nesting depth.

</section>

---

## fill_like

<section class='mojo-docs'>

`fill_like(src: IntTuple, val: Int) -> IntTuple`

Creates an `IntTuple` with the same structure as the source but filled with a specified value.

This function recursively traverses the source `IntTuple` and creates a new `IntTuple`
with identical structure, but with all leaf values replaced by the specified value.

**Args:**

* â€‹src ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The source `IntTuple` whose structure will be copied.
* â€‹val ([`Int`](/mojo/std/builtin/int/Int)): The integer value to fill the new `IntTuple` with.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` with the same structure as src but filled with val.

</section>

---

## flatten

<section class='mojo-docs'>

`flatten(t: IntTuple) -> IntTuple`

Flatten a nested `IntTuple` into a single-level `IntTuple`.

This function converts a hierarchical `IntTuple` structure into a flat
sequence of integer values, preserving the order of elements.

**Args:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The nested `IntTuple` to flatten.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` containing all integer values in a flat structure.

</section>

---

## idx2crd

<section class='mojo-docs'>

`idx2crd(idx: IntTuple, shape: IntTuple) -> IntTuple`

Converts a linear index to a coordinate tuple within a given shape.

This function splits an index into a coordinate within a Shape via a
colexicographical enumeration of coordinates in Shape.

**Args:**

* â€‹idx ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The linear index to convert.
* â€‹shape ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The shape of the tensor/array.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` containing the coordinates corresponding to the linear index.

`idx2crd(idx: IntTuple, shape: IntTuple, _stride: IntTuple) -> IntTuple`

Converts a linear index to a coordinate tuple within a given shape using custom strides.

**Args:**

* â€‹idx ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The linear index to convert.
* â€‹shape ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The shape of the tensor/array.
* â€‹\_stride ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Custom strides to use for the conversion.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` containing the coordinates corresponding to the linear index.

</section>

---

## idx2crd2

<section class='mojo-docs'>

`idx2crd2(idx: IntTuple, shape: IntTuple, _stride: IntTuple) -> IntTuple`

Convert a linear index to coordinates.

This function handles the actual conversion logic for different input combinations.

Notes:

* Handles four cases: tuple-tuple-tuple, tuple-int-int, int-tuple-tuple, and int-int-int.
* When input shapes don't match, `abort()` will be called.

**Args:**

* â€‹idx ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The linear index to convert.
* â€‹shape ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The shape of the tensor/array.
* â€‹\_stride ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Custom strides to use for the conversion. If empty, strides are computed
  from the shape using prefix\_product.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new IntTuple containing the coordinates corresponding to the linear index.

</section>

---

## int_tuple

<section class='mojo-docs'>

Hierarchical integer tuple data structures for high-performance tensor operations.

This module provides a flexible, memory-efficient implementation of nested integer tuples
optimized for tensor shape, stride, and index operations in high-performance computing.
The core data structures support both flat and hierarchical representations with
efficient memory sharing and zero-copy views.

Key components:

* `IntArray`: Low-level register-passable array with direct memory management
* `IntTuple`: Hierarchical nested tuple with efficient memory layout and operations
* Utility functions for tensor shape manipulation, coordinate transformations, and layout operations

Performance features:

* Register-passable data structures for optimal compiler optimizations
* Zero-copy views for efficient memory sharing
* Specialized memory layout for nested structures
* Optimized algorithms for common tensor operations

Common operations:

* Shape manipulation: `flatten`, `to_nest`, `apply`, `product`, `sum`
* Coordinate transformations: `idx2crd`, `crd2idx`
* Layout operations: `compact_order`, `prefix_product`
* Structural comparisons: `congruent`, `compatible`, `weakly_congruent`

Example usage:

```mojo
from layout import IntTuple
from layout.int_tuple import flatten, compact_order, size

# Create nested tuples
var shape = IntTuple(2, IntTuple(3, 4), 5)  # Represents shape (2, (3, 4), 5)

# Flatten a nested tuple
var flat = flatten(shape)  # Results in (2, 3, 4, 5)

# Create compact strides for a given shape and order
var order = IntTuple(1, IntTuple(2, 3), 4)
var strides = compact_order(shape, order)  # Results in (1, (2, 6), 24)

# Calculate total size (product of all elements)
var total_size = size(shape)  # Results in 120
```

## `comptime` values

### `IntList`

`comptime IntList = List[Int]`

A type alias for a List of integers with ownership.

This alias defines a List that contains Int values and has ownership of its data.
It's used throughout the module for storing and manipulating collections of integers,
particularly for operations like permutations and indices.

### `UNKNOWN_VALUE`

`comptime UNKNOWN_VALUE = -1`

Special value indicating an unknown or unspecified dimension.

This constant is used throughout the `IntTuple` system to represent dimensions
that are not known at compile time or have not been specified.

## Structs

* [â€‹`IntArray`](./IntArray): A memory-efficient, register-passable array of integers.
* [â€‹`IntTuple`](./IntTuple): A hierarchical, nested tuple of integers with efficient memory management.

## Functions

* [â€‹`abs`](./abs): Compute the absolute value of each element in an `IntTuple`.
* [â€‹`apply`](./apply): Apply a function to each integer value in an `IntTuple`.
* [â€‹`apply_predicate`](./apply_predicate): Apply a predicate function recursively to two `IntTuple`s.
* [â€‹`apply_zip`](./apply_zip): Apply a function to pairs of elements from two `IntTuple`s.
* [â€‹`compact_order`](./compact_order): Create a compact stride based on shape and order.
* [â€‹`compatible`](./compatible): Test if two shapes are compatible for tensor operations.
* [â€‹`congruent`](./congruent): Test if two `IntTuple`s have the same hierarchical structure.
* [â€‹`crd2idx`](./crd2idx): Map a logical coordinate to a linear index.
* [â€‹`depth`](./depth): Calculates the maximum nesting depth of an `IntTuple`.
* [â€‹`fill_like`](./fill_like): Creates an `IntTuple` with the same structure as the source but filled with a specified value.
* [â€‹`flatten`](./flatten): Flatten a nested `IntTuple` into a single-level `IntTuple`.
* [â€‹`idx2crd`](./idx2crd): Converts a linear index to a coordinate tuple within a given shape.
* [â€‹`idx2crd2`](./idx2crd2): Convert a linear index to coordinates.
* [â€‹`inner_product`](./inner_product): Compute the inner product of two `IntTuple`s.
* [â€‹`is_flat`](./is_flat): Check if an `IntTuple` is flat.
* [â€‹`is_int`](./is_int): Check if an `IntTuple` represents a single integer value.
* [â€‹`is_tuple`](./is_tuple): Check if an `IntTuple` represents a nested tuple.
* [â€‹`mul`](./mul): Multiply each element in an `IntTuple` by a scalar value.
* [â€‹`prefix_product`](./prefix_product): Compute the exclusive prefix product of an `IntTuple`.
* [â€‹`product`](./product): Calculate the product of all values in an `IntTuple`.
* [â€‹`product_each`](./product_each): Compute the product of elements in each sub-tuple of an `IntTuple`.
* [â€‹`propagate_unknown`](./propagate_unknown): Propagates unknown dimensions from the target `IntTuple` to the source `IntTuple`.
* [â€‹`reduce`](./reduce): Apply a reduction function to an `IntTuple` with an initial value.
* [â€‹`reverse`](./reverse): Reverses the order of elements in an `IntTuple`, recursively.
* [â€‹`shallow_apply`](./shallow_apply): Apply a function to each top-level element of an `IntTuple`.
* [â€‹`shape_div`](./shape_div): Performs division operation between shape tuples.
* [â€‹`signum`](./signum): Calculate the sign of an integer.
* [â€‹`size`](./size): Calculate the total size (product of all elements) of an `IntTuple`.
* [â€‹`sorted`](./sorted): Sort an IntTuple using the provided comparison function.
* [â€‹`sum`](./sum): Calculate the sum of all values in an `IntTuple`.
* [â€‹`to_index_list`](./to_index_list): Converts an IntTuple to a flattened IndexList with the same values.
* [â€‹`to_nest`](./to_nest): Nests a flat `IntTuple` according to the structure of a nested `IntTuple`.
* [â€‹`to_unknown`](./to_unknown): Create an `IntTuple` with the same structure but filled with `UNKNOWN_VALUE`.
* [â€‹`tuple_max`](./tuple_max): Calculate the maximum value in an `IntTuple`.
* [â€‹`tuple_min`](./tuple_min): Compute the element-wise minimum of two `IntTuple`s.
* [â€‹`weakly_compatible`](./weakly_compatible): Test if shape A is weakly compatible with shape B.
* [â€‹`weakly_congruent`](./weakly_congruent): Test if two IntTuples have similar hierarchical structures.

</section>

---

## inner_product

<section class='mojo-docs'>

`inner_product(a: IntTuple, b: IntTuple) -> Int`

Compute the inner product of two `IntTuple`s.

For flat tuples, this is the sum of element-wise products.
For nested tuples, the function recurses into corresponding nested elements.

Note:
If the input tuples have different lengths, assertion fails.

**Args:**

* â€‹a ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): First `IntTuple`.
* â€‹b ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Second `IntTuple`.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The inner product as an `Int`.

</section>

---

## is_flat

<section class='mojo-docs'>

`is_flat(t: IntTuple) -> Bool`

Check if an `IntTuple` is flat.

This function checks if the `IntTuple` is flat, meaning it has no nested
elements.

**Args:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` to check.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the `IntTuple` is flat, False otherwise.

</section>

---

## is_int

<section class='mojo-docs'>

`is_int(t: IntTuple) -> Bool`

Check if an `IntTuple` represents a single integer value.

This function determines whether the given `IntTuple` contains a single integer value
rather than a nested tuple structure.

Example:

```mojo
from layout.int_tuple import is_int, IntTuple

var single_value = IntTuple(5)
var nested_tuple = IntTuple(1, 2, 3)

var result1 = is_int(single_value)  # Returns True
var result2 = is_int(nested_tuple)  # Returns False
```

**Args:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` to check.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the `IntTuple` contains a single integer value,
False if it's a nested tuple.

</section>

---

## is_tuple

<section class='mojo-docs'>

`is_tuple(t: IntTuple) -> Bool`

Check if an `IntTuple` represents a nested tuple.

This function determines whether the given `IntTuple` contains nested elements
rather than a single integer value. It is the complement of the `is_int` function.

Example:

```mojo
from layout.int_tuple import is_tuple, IntTuple

var single_value = IntTuple(5)
var nested_tuple = IntTuple(1, 2, 3)

var result1 = is_tuple(single_value)  # Returns False
var result2 = is_tuple(nested_tuple)  # Returns True
```

**Args:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` to check.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the `IntTuple` contains nested elements,
False if it's a single integer value.

</section>

---

## mul

<section class='mojo-docs'>

`mul(lhs: IntTuple, rhs: Int) -> IntTuple`

Multiply each element in an `IntTuple` by a scalar value.

This function creates a new `IntTuple` where each element (at any nesting level)
is multiplied by the provided integer value.

**Args:**

* â€‹lhs ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` whose elements will be multiplied.
* â€‹rhs ([`Int`](/mojo/std/builtin/int/Int)): The scalar integer to multiply each element by.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` with the same structure as the input but with all
elements multiplied by the scalar value.

</section>

---

## prefix_product

<section class='mojo-docs'>

`prefix_product(a: IntTuple) -> IntTuple`

Compute the exclusive prefix product of an `IntTuple`.

This is a convenience wrapper that initializes the prefix product with 1.

**Args:**

* â€‹a ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The input `IntTuple` to compute the prefix product for.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` containing the exclusive prefix product of the input.

`prefix_product(a: IntTuple, init: Int) -> IntTuple`

Compute the exclusive prefix product of an `IntTuple` with an initial value.

This function delegates to the implementation in prefix\_product2.

**Args:**

* â€‹a ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The input `IntTuple` to compute the prefix product for.
* â€‹init ([`Int`](/mojo/std/builtin/int/Int)): The initial value(s) for the prefix product, defaults to 1.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` containing the exclusive prefix product of the input.

</section>

---

## product

<section class='mojo-docs'>

`product(t: IntTuple) -> Int`

Calculate the product of all values in an `IntTuple`.

This function recursively computes the product of all integer values
in a potentially nested `IntTuple` structure.

**Args:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` to multiply.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The product of all integer values, or `UNKNOWN_VALUE` if any value
in the tuple is `UNKNOWN_VALUE`.

</section>

---

## product_each

<section class='mojo-docs'>

`product_each(t: IntTuple) -> IntTuple`

Compute the product of elements in each sub-tuple of an `IntTuple`.

For each immediate child of the input tuple, this function computes
the product of all elements within that child.

**Args:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` containing sub-tuples.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` where each element is the product of the corresponding
sub-tuple in the input.

</section>

---

## propagate_unknown

<section class='mojo-docs'>

`propagate_unknown(src: IntTuple, target: IntTuple) -> IntTuple`

Propagates unknown dimensions from the target `IntTuple` to the source `IntTuple`.

This function creates a new `IntTuple` by combining the source and target `IntTuple`s,
preserving unknown dimensions (UNKNOWN\_VALUE) from the target while using values
from the source for known dimensions.

**Args:**

* â€‹src ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The source `IntTuple` containing known dimension values.
* â€‹target ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The target `IntTuple` that may contain unknown dimensions (UNKNOWN\_VALUE).

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` with unknown dimensions from target and known dimensions from src.

</section>

---

## reduce

<section class='mojo-docs'>

`reduce[reducer: fn(a: Int, b: IntTuple) capturing -> Int](t: IntTuple, initializer: Int) -> Int`

Apply a reduction function to an `IntTuple` with an initial value.

This function iterates through each element of the `IntTuple` and applies
the provided reduction function cumulatively, starting with the initializer.

**Parameters:**

* â€‹reducer (`fn(a: Int, b: IntTuple) capturing -> Int`): A function that combines the accumulated result with the next element.

**Args:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` to reduce.
* â€‹initializer ([`Int`](/mojo/std/builtin/int/Int)): The initial value for the reduction operation.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The final accumulated result after applying the reduction function
to all elements in the `IntTuple`.

</section>

---

## reverse

<section class='mojo-docs'>

`reverse(src: IntTuple) -> IntTuple`

Reverses the order of elements in an `IntTuple`, recursively.

This function reverses the top-level elements of the `IntTuple` and
recursively reverses any nested `IntTuple`s.

Example:

```mojo
from layout.int_tuple import IntTuple, reverse
var t = IntTuple(1, 2, IntTuple(3, 4))
var reversed = reverse(t) # returns ((4, 3), 2, 1)
```

**Args:**

* â€‹src ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The source `IntTuple` to reverse.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` with elements in reversed order.

</section>

---

## shallow_apply

<section class='mojo-docs'>

`shallow_apply[func: fn(IntTuple) -> Int](t: IntTuple) -> IntTuple`

Apply a function to each top-level element of an `IntTuple`.

Unlike `apply()`, this function only operates on the immediate children
of the input tuple without recursing into nested tuples.

**Parameters:**

* â€‹func (`fn(IntTuple) -> Int`): Function that takes an `IntTuple` and returns an `Int`.

**Args:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` whose elements will be transformed.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` with the function applied to each top-level element.

</section>

---

## shape_div

<section class='mojo-docs'>

`shape_div[check: Bool = False](a: IntTuple, b: IntTuple) -> IntTuple`

Performs division operation between shape tuples.

Handles four cases:

1. tuple-tuple: Performs shape\_div element-wise when dimensions match
2. tuple-int: Folds the division of b across each element of a
   Example: `shape_div((4,5,6),40)` -> `shape_div((1,5,6),10)` -> `shape_div((1,1,6),2)` -> `(1,1,3)`
3. int-tuple: Returns `shape_div(a, product(b))`
4. int-int: Enforces the divisibility condition `a % b == 0 || b % a == 0` when possible
   Returns `a / b` with rounding away from `0` (that is, `1` or `-1` when `a < b`)

Notes:

* When tuple sizes don't match in the tuple-tuple case, `abort()` will be
  called.
* When values are incompatible (neither divides the other) in the int-int
  case, `abort()` will be called.

**Parameters:**

* â€‹check ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to check for incompatible shapes.

**Args:**

* â€‹a ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The dividend `IntTuple`.
* â€‹b ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The divisor `IntTuple`.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` containing the result of the division operation

</section>

---

## signum

<section class='mojo-docs'>

`signum(a: Int) -> Int`

Calculate the sign of an integer.

This function determines the sign of the input integer and returns a corresponding
indicator value.

Example:

```mojo
from layout.int_tuple import signum

var result1 = signum(5)    # Returns 1
var result2 = signum(-10)  # Returns -1
var result3 = signum(0)    # Returns 0
```

**Args:**

* â€‹a ([`Int`](/mojo/std/builtin/int/Int)): The integer value to determine the sign of.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): 1 if `a` > 0, -1 if `a` < 0, 0 if `a` == 0.

</section>

---

## size

<section class='mojo-docs'>

`size(a: IntTuple) -> Int`

Calculate the total size (product of all elements) of an `IntTuple`.

This function computes the product of all integer values in the `IntTuple`,
regardless of nesting level.

**Args:**

* â€‹a ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` whose elements will be multiplied together.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The product of all elements in the `IntTuple`.

</section>

---

## sorted

<section class='mojo-docs'>

`sorted[cmp: fn(IntTuple, IntTuple) -> Bool = __lt__](tuple: IntTuple) -> IntTuple`

Sort an IntTuple using the provided comparison function.

This function implements a merge sort algorithm to efficiently sort
the elements of an IntTuple. The sorting is stable and has `O(n log n)`
time complexity.

**Parameters:**

* â€‹cmp (`fn(IntTuple, IntTuple) -> Bool`): A comparison function that takes two `IntTuple` elements and
  returns True if the first should come before the second.
  Defaults to the `lt` function which performs lexicographical ordering.

**Args:**

* â€‹tuple ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` to be sorted.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` containing the same elements as the input but sorted
according to the comparison function.

</section>

---

## sum

<section class='mojo-docs'>

`sum(t: IntTuple) -> Int`

Calculate the sum of all values in an `IntTuple`.

This function recursively computes the sum of all integer values
in a potentially nested `IntTuple` structure.

**Args:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` to sum.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The sum of all integer values, or `UNKNOWN_VALUE` if any value
in the tuple is `UNKNOWN_VALUE`.

</section>

---

## to_index_list

<section class='mojo-docs'>

`to_index_list[rank: Int, element_type: DType = DType.int64](t: IntTuple) -> IndexList[rank, element_type=element_type]`

Converts an IntTuple to a flattened IndexList with the same values.

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the resulting IndexList.
* â€‹element\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Element type, must be integer type.

**Args:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` defining the values.

**Returns:**

`IndexList`: An IndexList filled with the values of t.

</section>

---

## to_nest

<section class='mojo-docs'>

`to_nest(nested: IntTuple, flat: IntTuple) -> IntTuple`

Nests a flat `IntTuple` according to the structure of a nested `IntTuple`.

This function reshapes a flat sequence of values into a hierarchical structure
that matches the pattern of a template nested `IntTuple`.

Example:

```mojo
from layout import IntTuple
from layout.int_tuple import to_nest

var result = to_nest(IntTuple(2, IntTuple(3, 4), 5), IntTuple(1, 2, 3, 4))
# returns IntTuple(1, (2, 3), 4)
```

**Args:**

* â€‹nested ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The template `IntTuple` defining the desired structure.
* â€‹flat ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The flat `IntTuple` containing the values to be nested.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` with the values from flat arranged in the structure of nested.

</section>

---

## to_unknown

<section class='mojo-docs'>

`to_unknown(t: IntTuple) -> IntTuple`

Create an `IntTuple` with the same structure but filled with `UNKNOWN_VALUE`.

This function preserves the hierarchical structure of the input `IntTuple`
but replaces all integer values with `UNKNOWN_VALUE`.

**Args:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The template `IntTuple` defining the structure.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` with the same structure as t but with all values
replaced by `UNKNOWN_VALUE`.

</section>

---

## tuple_max

<section class='mojo-docs'>

`tuple_max(t: IntTuple) -> Int`

Calculate the maximum value in an `IntTuple`.

This function recursively finds the maximum integer value
in a potentially nested `IntTuple` structure.

**Args:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` to search.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The maximum integer value found in the tuple.

</section>

---

## tuple_min

<section class='mojo-docs'>

`tuple_min(a: IntTuple, b: IntTuple) -> IntTuple`

Compute the element-wise minimum of two `IntTuple`s.

This function compares corresponding elements of two `IntTuple`s and
returns a new `IntTuple` containing the minimum value at each position.

Aborts:
If the input tuples have different lengths.

Note:
If either input contains `UNKNOWN_VALUE`, the result will be `UNKNOWN_VALUE`.

**Args:**

* â€‹a ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): First `IntTuple`.
* â€‹b ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Second `IntTuple`.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` with each element being the minimum of the corresponding
elements in a and b.

</section>

---

## weakly_compatible

<section class='mojo-docs'>

`weakly_compatible(a: IntTuple, b: IntTuple) -> Bool`

Test if shape A is weakly compatible with shape B.

A shape A is weakly compatible with shape B if there exists a shape C
congruent to A such that compatible(elem\_scale(A,C), B). This establishes
a partial order relation between shapes where A <= B.

Specifically, this checks if the size of B is divisible by the size of A,
which is a necessary condition for weak compatibility.

**Args:**

* â€‹a ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The first `IntTuple` to compare.
* â€‹b ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The second `IntTuple` to compare.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if shape A is weakly compatible with shape B, False otherwise.

</section>

---

## weakly_congruent

<section class='mojo-docs'>

`weakly_congruent(a: IntTuple, b: IntTuple) -> Bool`

Test if two IntTuples have similar hierarchical structures.

This function establishes a partial order relation between IntTuples
based on their hierarchical structure. It's less strict than congruent.

**Args:**

* â€‹a ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): First IntTuple to compare.
* â€‹b ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Second IntTuple to compare.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if a's structure is compatible with b's structure,
False otherwise.

</section>

---

## Layout (Layout)

<section class='mojo-docs'>

`struct Layout`

Represents a memory layout for multi-dimensional data.

The Layout struct is the primary implementation of the LayoutTrait,
providing a concrete representation of memory layouts using shape and
stride information. It maps between logical coordinates and linear
memory indices, enabling efficient access to multi-dimensional data.

A Layout consists of:

* shape: Defines the dimensions of the logical coordinate space
* stride: Defines the step sizes in memory for each dimension

The Layout struct supports various operations including:

* Creation of row-major and column-major layouts
* Conversion between coordinates and indices
* Composition with other layouts
* Iteration over sub-layouts

Layouts can be hierarchical, with nested shapes and strides, allowing
for complex memory access patterns like blocked or tiled layouts.

## Fields

* â€‹shape (`IntTuple`): The dimensions of the layout.
  This field defines the size of each dimension in the logical coordinate space.
  For example, a shape of (3, 4) represents a 3x4 grid of elements.
* â€‹stride (`IntTuple`): The memory step sizes for each dimension.
  This field defines how many elements to skip in memory when moving one unit
  in each dimension. For example, in a row-major 3x4 layout, the strides might
  be (4, 1), meaning moving one unit in the first dimension requires skipping
  4 elements in memory, while moving one unit in the second dimension requires
  skipping 1 element.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`Equatable`](/mojo/std/builtin/comparable/Equatable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Iterable`](/mojo/std/iter/Iterable),
[`LayoutTrait`](/mojo/kernels/layout/layout/LayoutTrait),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Sized`](/mojo/std/builtin/len/Sized),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = False`

### `__del__is_trivial`

`comptime __del__is_trivial = False`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `has_shape`

`comptime has_shape = True`

Indicates whether the layout has a valid shape.

### `IteratorType`

`comptime IteratorType[iterable_mut: Bool, //, iterable_origin: Origin[mut=iterable_mut]] = _LayoutIter[origin_of((muttoimm iterable_origin._mlir_origin))]`

The iterator type for Layout iteration.

#### Parameters

* â€‹iterable\_mut ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether the iterable is mutable.
* â€‹iterable\_origin ([`Origin`](/mojo/std/builtin/type_aliases/Origin)): The origin of the iterable.

## Methods

### `__init__`

`__init__(out self)`

Initializes an empty layout with no dimensions.

Creates a layout with empty shape and stride tuples, which can be
populated later using append operations.

`__init__(out self, shape: IntTuple)`

Initializes a layout with the given shape and column-major strides.

Creates a layout with the specified shape and automatically calculates
column-major strides (where the first dimension varies fastest in memory).

**Args:**

* â€‹shape ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The dimensions of the layout.

`__init__(out self, shape: IntTuple, stride: IntTuple)`

Initializes a layout with the given shape and stride.

Creates a layout with explicitly specified shape and stride values.
If an empty stride is provided, column-major strides are calculated.

**Args:**

* â€‹shape ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The dimensions of the layout.
* â€‹stride ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The memory step size for each dimension, or empty for column-major.

### `__getitem__`

`__getitem__(self, index: Int) -> Self`

Returns a sub-layout for the specified dimension.

**Args:**

* â€‹index ([`Int`](/mojo/std/builtin/int/Int)): The dimension index to extract.

**Returns:**

`Self`: A Layout containing the shape and stride for the specified dimension.

### `__eq__`

`__eq__(self, other: Self) -> Bool`

Checks if this layout is equal to another layout.

Two layouts are considered equal if they have identical shape and stride tuples.

**Args:**

* â€‹other (`Self`): The layout to compare with.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the layouts are equal, False otherwise.

### `idx2crd`

`idx2crd(self, idx: IntTuple) -> IntTuple`

Converts a linear index to logical coordinates.

This is the inverse operation of the **call** method, mapping from
a memory index back to the corresponding logical coordinates.

**Args:**

* â€‹idx ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The linear index to convert.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): The logical coordinates corresponding to the given index.

### `col_major`

`static col_major(*dims: Int) -> Self`

Creates a column-major layout with the specified dimensions.

In a column-major layout, the first dimension varies fastest in memory,
which is the default layout in languages like Fortran and MATLAB.

Example:

```mojo
from layout import Layout

# Create a 3x4 column-major layout
var layout = Layout.col_major(3, 4)
# Result: Layout with shape (3,4) and stride (1,3)
```

**Args:**

* â€‹\*dims ([`Int`](/mojo/std/builtin/int/Int)): Variable number of dimension sizes.

**Returns:**

`Self`: A column-major Layout with the specified dimensions

`static col_major(shape: IntTuple) -> Self`

Creates a column-major layout with the specified shape.

In a column-major layout, the first dimension varies fastest in memory,
which is the default layout in languages like Fortran and MATLAB.

Example:

```mojo
from layout import Layout
from layout.int_tuple import IntTuple

# Create a 3x4 column-major layout
var layout = Layout.col_major(IntTuple(3, 4))
# Result: Layout with shape (3,4) and stride (1,3)
```

**Args:**

* â€‹shape ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): An IntTuple specifying the dimensions.

**Returns:**

`Self`: A column-major Layout with the specified shape

`static col_major[rank: Int](dims: DimList) -> Self`

Creates a col-major layout from a DimList with compile-time rank.

This method creates a col-major layout where the first dimension varies fastest in memory.
It handles both known and unknown dimensions at compile time, properly calculating
strides for each dimension. If any dimension is unknown, subsequent strides will
also be marked as unknown.

Example:

```mojo
from layout import Layout
from layout.layout import DimList

# Create a col-major layout with compile-time rank
var dims = DimList(3, 4)
var layout = Layout.col_major[2](dims)
# Result: Layout with shape (3,4) and stride (1,3)
```

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The compile-time rank (number of dimensions) of the layout.

**Args:**

* â€‹dims ([`DimList`](/mojo/kernels/buffer/dimlist/DimList)): A DimList containing the dimensions of the layout.

**Returns:**

`Self`: A col-major Layout with the specified dimensions and computed strides.

`static col_major[rank: Int](tuple: IndexList[rank]) -> Self`

Creates a col-major layout from a IndexList with compile-time rank.

This method creates a col-major layout where the first dimension varies fastest in memory.
It handles both known and unknown dimensions at compile time, properly calculating
strides for each dimension. If any dimension is unknown, subsequent strides will
also be marked as unknown.

Example:

```mojo
from layout import Layout
from utils import IndexList

# Create a row-major layout with compile-time rank
var idx_list = IndexList[2](3, 4)
var layout = Layout.col_major[2](idx_list)
# Result: Layout with shape (3,4) and stride (1,3)
```

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The compile-time rank (number of dimensions) of the layout.

**Args:**

* â€‹tuple ([`IndexList`](/mojo/std/utils/index_/IndexList)): An IndexList containing the dimensions of the layout.

**Returns:**

`Self`: A col-major Layout with the specified dimensions and computed strides.

### `row_major`

`static row_major(*dims: Int) -> Self`

Creates a row-major layout with the specified dimensions.

In a row-major layout, the last dimension varies fastest in memory,
which is the default layout in languages like C, C++, and Python.

Example:

```mojo
from layout import Layout

# Create a 3x4 row-major layout
var layout = Layout.row_major(3, 4)
# Result: Layout with shape (3,4) and stride (4,1)
```

**Args:**

* â€‹\*dims ([`Int`](/mojo/std/builtin/int/Int)): Variable number of dimension sizes.

**Returns:**

`Self`: A row-major Layout with the specified dimensions

`static row_major[rank: Int](dims: DimList) -> Self`

Creates a row-major layout from a DimList with compile-time rank.

This method creates a row-major layout where the last dimension varies fastest in memory.
It handles both known and unknown dimensions at compile time, properly calculating
strides for each dimension. If any dimension is unknown, subsequent strides will
also be marked as unknown.

Example:

```mojo
from layout import Layout
from layout.layout import DimList

# Create a row-major layout with compile-time rank
var dims = DimList(3, 4)
var layout = Layout.row_major[2](dims)
# Result: Layout with shape (3,4) and stride (4,1)
```

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The compile-time rank (number of dimensions) of the layout.

**Args:**

* â€‹dims ([`DimList`](/mojo/kernels/buffer/dimlist/DimList)): A DimList containing the dimensions of the layout.

**Returns:**

`Self`: A row-major Layout with the specified dimensions and computed strides.

`static row_major[rank: Int](tuple: IndexList[rank]) -> Self`

Creates a row-major layout from a IndexList with compile-time rank.

This method creates a row-major layout where the last dimension varies fastest in memory.
It handles both known and unknown dimensions at compile time, properly calculating
strides for each dimension. If any dimension is unknown, subsequent strides will
also be marked as unknown.

Example:

```mojo
from layout import Layout
from utils import IndexList

# Create a row-major layout with compile-time rank
var idx_list = IndexList[2](3, 4)
var layout = Layout.row_major[2](idx_list)
# Result: Layout with shape (3,4) and stride (4,1)
```

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The compile-time rank (number of dimensions) of the layout.

**Args:**

* â€‹tuple ([`IndexList`](/mojo/std/utils/index_/IndexList)): An IndexList containing the dimensions of the layout.

**Returns:**

`Self`: A row-major Layout with the specified dimensions and computed strides.

`static row_major[rank: Int]() -> Self`

Creates a row-major layout with unknown values for each axis from a compile-time rank.

Example:

```mojo
from layout import Layout

var layout = Layout.row_major[2]()
# Result: Layout with shape (UNKNOWN_VALUE, UNKNOWN_VALUE)
```

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The compile-time rank (number of dimensions) of the layout.

**Returns:**

`Self`: A row-major Layout with the given rank.

`static row_major(shape: IntTuple) -> Self`

Creates a row-major layout from an IntTuple of dimensions.

In a row-major layout, the last dimension varies fastest in memory.
This method computes the appropriate strides for a row-major layout
given the input shape.

Example:

```mojo
from layout import Layout
from layout.int_tuple import IntTuple

# Create a row-major layout from a shape tuple
var shape = IntTuple(3, 4)
var layout = Layout.row_major(shape)
# Result: Layout with shape (3,4) and stride (4,1)
```

**Args:**

* â€‹shape ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): An IntTuple containing the dimensions of the layout.

**Returns:**

`Self`: A row-major Layout with the specified shape and computed strides.

### `make_shape_unknown`

`make_shape_unknown[axis: Int = -1](self) -> Self`

Creates a new Layout with unknown shape dimensions.

This method creates a copy of the current Layout but marks either all dimensions
or a specific dimension as unknown, while preserving the original strides.
This is useful for tiling tensors with runtime sizes where the tile's shape
is unknown but the memory layout (strides) remains constant.

Example:

```mojo
from layout import Layout
from layout.int_tuple import IntTuple

# Mark all dimensions as unknown
var layout = Layout(IntTuple(2, 3))
var unknown = layout.make_shape_unknown()
# Result: Layout with shape (?, ?) and original strides

# Mark only first dimension as unknown
var partial = layout.make_shape_unknown[0]()
# Result: Layout with shape (?, 3) and original strides
```

**Parameters:**

* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The dimension to mark as unknown. If UNKNOWN\_VALUE (default),
  all dimensions are marked as unknown.

**Returns:**

`Self`: A new Layout with the specified dimension(s) marked as unknown and
original strides preserved.

### `__str__`

`__str__(self) -> String`

Converts the layout to a string representation.

**Returns:**

`String`: A string representation of the layout in the format "(shape:stride)".

### `write_to`

`write_to(self, mut writer: T)`

Writes the layout to the specified writer.

Formats the layout as "(shape:stride)" and writes it to the provided writer.

**Args:**

* â€‹writer (`T`): The writer to output the layout representation to.

### `__len__`

`__len__(self) -> Int`

Returns the number of dimensions in the layout.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The number of elements in the shape tuple.

### `__iter__`

`__iter__(ref self) -> _LayoutIter[origin_of((muttoimm self_is_origin))]`

Returns an iterator over the layout's dimensions.

Each iteration yields a Layout containing the shape and stride for one dimension.

**Returns:**

`_LayoutIter`: An iterator over the layout's dimensions.

### `size`

`size(self) -> Int`

Returns the total number of elements in the layout's domain.

Calculates the product of all dimensions in the shape.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The total number of elements in the layout.

### `cosize`

`cosize(self) -> Int`

Returns the size of the memory region spanned by the layout.

Calculates the maximum memory index plus one, representing the total
memory footprint required by the layout.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The size of the memory region required by the layout.

### `rank`

`rank(self) -> Int`

Returns the number of dimensions in the layout.

This is equivalent to **len** and returns the number of elements in the
shape tuple.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The number of dimensions in the layout.

### `__call__`

`__call__(self, idx: IntTuple) -> Int`

Maps logical coordinates to a linear memory index.

This is the core functionality of a layout, converting multi-dimensional
coordinates to a linear memory location.

**Args:**

* â€‹idx ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The logical coordinates to map.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The linear memory index corresponding to the given coordinates.

### `append`

`append(mut self, item: Self)`

Appends another layout to this layout.

This method adds the shape and stride from the provided layout to this layout,
effectively increasing its dimensionality.

**Args:**

* â€‹item (`Self`): The layout to append to this layout.

### `all_dims_known`

`all_dims_known(self) -> Bool`

Checks if all dimensions in the layout have known values.

A dimension is considered unknown if its shape or stride is set to the
special `UNKNOWN_VALUE` constant.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if all dimensions have known shape and stride values, False otherwise.

### `known_shape`

`known_shape(self) -> Bool`

Checks if all shape dimensions in the layout have known values.

A dimension is considered unknown if its shape is set to the special
`UNKNOWN_VALUE` constant. This method only checks shapes, not strides.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if all shape dimensions have known values, False otherwise.

### `transpose`

`transpose(self) -> Self`

Transposes the layout by reversing the order of dimensions.

For an n-dimensional layout, this reverses the order of both shapes and strides.
For nested layouts, only the top-level dimensions are transposed, not the
hierarchical structure within nested tuples.

Example:

```mojo
from layout import Layout
from layout.int_tuple import IntTuple

# Simple 2D transpose (row-major to column-major)
var layout = Layout.row_major(3, 4)  # shape (3,4), stride (4,1)
var transposed = layout.transpose()  # shape (4,3), stride (1,4)

# 3D transpose
var layout3d = Layout.row_major(2, 3, 4)  # shape (2,3,4), stride (12,4,1)
var trans3d = layout3d.transpose()        # shape (4,3,2), stride (1,4,12)

# Nested layout - only top level transposed
var nested = Layout(
    IntTuple(IntTuple(2, 3), 4),
    IntTuple(IntTuple(12, 4), 1)
)
var trans_nested = nested.transpose()
# Result: shape (4, (2,3)), stride (1, (12,4))
```

**Returns:**

`Self`: A new Layout with transposed dimensions.

</section>

---

## LayoutTrait

<section class='mojo-docs'>

Defines the interface for mapping between logical coordinates and memory indices.

The `LayoutTrait` provides a common interface for all layout types, including
basic layouts, swizzles, and composed layouts. It enables mapping from
multi-dimensional logical coordinates to linear memory indices, which is
essential for tensor operations.

Implementations of this trait must provide methods for:

1. Mapping coordinates to indices via the `__call__` method
2. Calculating the total size of the layout's domain
3. Calculating the size of the layout's codomain (memory footprint)
4. Indicating whether the layout has a valid shape

This trait serves as the foundation for the layout system, allowing
different layout implementations to be used interchangeably in algorithms.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__copyinit__` is trivial.

The implementation of `__copyinit__` is considered to be trivial if:

* The struct has a compiler-generated trivial `__copyinit__` and all its fields
  have a trivial `__copyinit__` method.

In practice, it means the value can be copied by copying the bits from
one location to another without side effects.

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__moveinit__` is trivial.

The implementation of `__moveinit__` is considered to be trivial if:

* The struct has a compiler-generated `__moveinit__` and all its fields
  have a trivial `__moveinit__` method.

In practice, it means the value can be moved by moving the bits from
one location to another without side effects.

### `has_shape`

`comptime has_shape`

Indicates whether the layout has a valid shape.

Layouts and ComposedLayouts with at least one Layout have valid shapes
and can be used in layout algebra. Swizzles don't have shapes and
should be excluded from layout algebra.

## Required methods

### `__copyinit__`

`__copyinit__(out self: _Self, existing: _Self, /)`

Create a new instance of the value by copying an existing one.

**Args:**

* â€‹existing (`_Self`): The value to copy.

**Returns:**

`_Self`

### `__moveinit__`

`__moveinit__(out self: _Self, deinit existing: _Self, /)`

Create a new instance of the value by moving the value of another.

**Args:**

* â€‹existing (`_Self`): The value to move.

**Returns:**

`_Self`

### `__call__`

`__call__(self: _Self, index: IntTuple) -> Int`

Maps a logical coordinate to a linear memory index.

**Args:**

* â€‹index ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): An IntTuple representing the logical coordinates to map.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The linear memory index corresponding to the given coordinates.

### `size`

`size(self: _Self) -> Int`

Returns the total number of elements in the layout's domain.

For a layout with shape (m, n), this returns m \* n, representing
the total number of valid coordinates in the layout.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The total number of elements in the layout.

### `cosize`

`cosize(self: _Self) -> Int`

Returns the size of the memory region spanned by the layout.

For a layout with shape `(m, n)` and stride `(r, s)`, this returns
`(m-1)*r + (n-1)*s + 1`, representing the memory footprint.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The size of the memory region required by the layout.

## Provided methods

### `copy`

`copy(self: _Self) -> _Self`

Explicitly construct a copy of self.

**Returns:**

`_Self`: A copy of this value.

</section>

---

## MakeLayoutList

<section class='mojo-docs'>

`MakeLayoutList(v0: Layout, v1: Layout) -> LayoutList`

Creates a list containing two layouts.

This is a convenience function for creating a LayoutList with two elements.

**Args:**

* â€‹v0 ([`Layout`](/mojo/kernels/layout/layout/Layout)): The first layout to include in the list.
* â€‹v1 ([`Layout`](/mojo/kernels/layout/layout/Layout)): The second layout to include in the list.

**Returns:**

`LayoutList`: A LayoutList containing the two provided layouts.

</section>

---

## MakeTileLayoutList

<section class='mojo-docs'>

`MakeTileLayoutList[*tile_sizes: Int]() -> LayoutList`

Creates a list of layouts for tiling operations.

This function creates a list of simple layouts, each with a shape from the
provided tile\_sizes and a stride of 1. These layouts can be used for tiling
operations.

**Parameters:**

* â€‹\*tile\_sizes ([`Int`](/mojo/std/builtin/int/Int)): Variable number of integer tile dimensions.

**Returns:**

`LayoutList`: A LayoutList containing layouts for each tile size.

</section>

---

## apply_tiler

<section class='mojo-docs'>

`apply_tiler[func: fn(Layout, Layout) -> Layout](layout_a: Layout, tiler: List[Layout]) -> Layout`

Applies a layout transformation function to each element of a layout with a tiler.

This utility function applies the specified transformation function to each
corresponding pair of elements from the layout and tiler list. It's a generic
mechanism for implementing various tiling operations.

Example:

```mojo
from layout import Layout, LayoutList, IntTuple
from layout.layout import apply_tiler, logical_divide

# Apply logical_divide to each element of a layout with a tiler
var base = Layout.row_major(6, 8)
var tilers = LayoutList()
tilers.append(Layout(IntTuple(2, 2), IntTuple(1, 2)))
var result = apply_tiler[logical_divide](base, tilers)
```

**Parameters:**

* â€‹func (`fn(Layout, Layout) -> Layout`): A function that takes two layouts and returns a transformed layout.

**Args:**

* â€‹layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The base layout to transform.
* â€‹tiler ([`List`](/mojo/std/collections/list/List)): A list of layouts to use in the transformation.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout resulting from applying the transformation function to each pair.

</section>

---

## blocked_product

<section class='mojo-docs'>

`blocked_product(layout_a: Layout, layout_b: Layout, coalesce_output: Bool = False) -> Layout`

Creates a blocked layout by combining two layouts.

This function creates a hierarchical blocked layout by combining a base layout
with a block layout. The result is a layout where each element of the base
layout is replaced by a block defined by the second layout.

This is particularly useful for creating tiled layouts for efficient
cache utilization in tensor operations like matrix multiplication.

Example:

```mojo
from layout import Layout
from layout.layout import blocked_product

# Create a 2x3 matrix layout
var matrix = Layout.row_major(2, 3)
# Define 2x2 blocks
var block = Layout.row_major(2, 2)
# Create a blocked layout with 2x2 blocks
var blocked = blocked_product(block, matrix)
```

Output:

```plaintext
(((2, 2), (2, 3)):((2, 12), (1, 4)))
      0    1    2    3    4    5
   +----+----+----+----+----+----+
0  |  0 |  1 |  4 |  5 |  8 |  9 |
   +----+----+----+----+----+----+
1  |  2 |  3 |  6 |  7 | 10 | 11 |
   +----+----+----+----+----+----+
2  | 12 | 13 | 16 | 17 | 20 | 21 |
   +----+----+----+----+----+----+
3  | 14 | 15 | 18 | 19 | 22 | 23 |
   +----+----+----+----+----+----+
```

**Args:**

* â€‹layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The base layout to be blocked.
* â€‹layout\_b ([`Layout`](/mojo/kernels/layout/layout/Layout)): The block layout defining the structure within each block.
* â€‹coalesce\_output ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to coalesce the output layout. Default is False.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout representing the blocked structure

</section>

---

## coalesce

<section class='mojo-docs'>

`coalesce(layout: Layout, keep_rank: Bool = False) -> Layout`

Simplifies a layout by combining dimensions with contiguous strides.

This function reduces the rank of a layout by merging dimensions that have
contiguous memory layouts, resulting in a simpler but equivalent layout.

Example:

```mojo
from layout import Layout, IntTuple
from layout.layout import coalesce

# A layout with shape (2, (1, 4)) and stride (1, (4, 2)) can be coalesced
var layout = Layout(IntTuple(2, IntTuple(1, 4)), IntTuple(1, IntTuple(4, 2)))
var coalesced = coalesce(layout)
# Result: Layout with shape (8) and stride (1)
```

**Args:**

* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to coalesce.
* â€‹keep\_rank ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, maintains the original rank of the layout. Default is False.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A simplified layout with reduced rank where possible.

</section>

---

## complement

<section class='mojo-docs'>

`complement(layout: Layout, size: Int = 1) -> Layout`

Computes the complement layout for a given layout.

This function creates a layout that represents the "gaps" or complementary
structure of the input layout. It's useful for creating hierarchical layouts
where you need to fill in the spaces between existing layout elements.

Example:

```mojo
from layout import Layout, IntTuple
from layout.layout import complement

# Compute the complement of a layout
var base = Layout(IntTuple(2, 3), IntTuple(3, 1))
var comp = complement(base, 10)
# Result: A layout that fills the gaps in the original layout
```

**Args:**

* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The input layout to compute the complement for.
* â€‹size ([`Int`](/mojo/std/builtin/int/Int)): The total size of the memory region to consider. Defaults to 1.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout representing the complement of the input layout.

</section>

---

## composition

<section class='mojo-docs'>

`composition(layout_a: Layout, layout_b: Layout) -> Layout`

Composes two layouts to create a new layout.

This function creates a new layout by composing two layouts, where the first
layout defines the outer structure and the second layout defines the inner
structure.

The new layout is compatible with `layout_b` (that is, it has the same `size`
and every set of coordinates in `layout_b` has an equivalent in the new
layout). You can think of `layout_b` as selecting a subset of elements
from `layout_a`.

Example:

```mojo
from layout.layout import Layout, IntTuple
from layout.layout import composition

# Compose a row-major layout with a tiling layout
var base = Layout.row_major(6, 8)
var tiling = Layout(IntTuple(3, 2), IntTuple(1, 3))
var composed = composition(base, tiling)
# Result: A layout that represents a 3x2 tile from
# layout_a
```

**Args:**

* â€‹layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The outer layout.
* â€‹layout\_b ([`Layout`](/mojo/kernels/layout/layout/Layout)): The inner layout.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout representing the composition of the two layouts.

`composition(layout_a: Layout, tiler: List[Layout]) -> Layout`

Composes a layout with a list of layouts to create a hierarchical layout.

This function creates a new layout by composing each element of the first layout
with the corresponding element in the tiler list. If the tiler list is shorter
than the layout, the remaining elements from the layout are appended unchanged.

Example:

```mojo
from layout import Layout, LayoutList, IntTuple
from layout.layout import composition

# Compose a layout with a list of tiling layouts
var base = Layout.row_major(6, 8)
var tilers = LayoutList()
tilers.append(Layout(IntTuple(2, 2), IntTuple(1, 2)))
tilers.append(Layout(IntTuple(3, 3), IntTuple(1, 3)))
var composed = composition(base, tilers)
# Result: A layout with hierarchical tiling based on the tiler list
```

**Args:**

* â€‹layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The base layout to compose with the tiler.
* â€‹tiler ([`List`](/mojo/std/collections/list/List)): A list of layouts to compose with the base layout.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout representing the composition of the base layout with the tiler.

</section>

---

## cosize

<section class='mojo-docs'>

`cosize(l: Layout) -> Int`

Returns the size of the memory region spanned by the layout.

This is a standalone function equivalent to the Layout.cosize() method.

**Args:**

* â€‹l ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to calculate the cosize for.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The size of the memory region required by the layout.

</section>

---

## downcast

<section class='mojo-docs'>

`downcast(layout: Layout, factor: Int) -> Layout`

Splits elements in a layout to create a finer layout without changing the total number of elements so that the alignment is preserved.

This function is useful for converting between different data type granularities,
such as from uint128 to bf16.

**Args:**

* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to downcast.
* â€‹factor ([`Int`](/mojo/std/builtin/int/Int)): The number of elements to split into.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout with adjusted shape and stride for the finer granularity.

</section>

---

## expand_modes_alike

<section class='mojo-docs'>

`expand_modes_alike(shape_a: IntTuple, stride_a: IntTuple, shape_b: IntTuple, stride_b: IntTuple) -> InlineArray[IntTuple, 3]`

Aligns two shape-stride pairs to have the same hierarchical structure.

This function is used to make two layouts compatible for operations by ensuring
they have the same hierarchical structure, expanding scalar values into tuples
as needed.

**Args:**

* â€‹shape\_a ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The first shape tuple.
* â€‹stride\_a ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The first stride tuple.
* â€‹shape\_b ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The second shape tuple.
* â€‹stride\_b ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The second stride tuple.

**Returns:**

[`InlineArray`](/mojo/std/collections/inline_array/InlineArray): An array containing three tuples: the common shape, the expanded stride\_a,
and the expanded stride\_b.

`expand_modes_alike(layout_a: Layout, layout_b: Layout) -> InlineArray[Layout, 2]`

Aligns two layouts to have the same hierarchical structure.

This function tiles both layouts so they mirror each other's structure,
making them compatible for operations that require matching hierarchies.

Example:

Given layouts with different structures:

* layout\_0: (((3, (5, 2)), 4):((1, (24, 12)), 3))
* layout\_1: ((30, (2, 2)):(2, (60, 1)))

The result would be two layouts with matching structures:

* (((3, (5, 2)), (2, 2)):((1, (24, 12)), (3, 6)))
* (((3, (5, 2)), (2, 2)):((2, (6, 30)), (60, 1)))

```mojo
from layout import Layout, IntTuple
from layout.layout import expand_modes_alike

comptime layout_0 = Layout(
    IntTuple(IntTuple(3, IntTuple(5, 2)), 4),
    IntTuple(IntTuple(1, IntTuple(24, 12)), 3),
)
comptime layout_1 = Layout(
    IntTuple(30, IntTuple(2, 2)), IntTuple(2, IntTuple(60, 1))
)
comptime uc = expand_modes_alike(layout_0, layout_1)
print(uc[0])
# (((3, (5, 2)), (2, 2)):((1, (24, 12)), (3, 6)))
print(uc[1])
# (((3, (5, 2)), (2, 2)):((2, (6, 30)), (60, 1)))
```

**Args:**

* â€‹layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The first layout to align.
* â€‹layout\_b ([`Layout`](/mojo/kernels/layout/layout/Layout)): The second layout to align.

**Returns:**

[`InlineArray`](/mojo/std/collections/inline_array/InlineArray): An array containing two layouts with matching hierarchical structures.

</section>

---

## expand_strides

<section class='mojo-docs'>

`expand_strides(shape: IntTuple, stride: Int) -> IntTuple`

Expands a scalar stride into a stride tuple matching a shape tuple.

This function creates a stride tuple that matches the structure of a shape tuple,
with each stride value calculated based on the cumulative product of shape
dimensions.

**Args:**

* â€‹shape ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The shape tuple to match.
* â€‹stride ([`Int`](/mojo/std/builtin/int/Int)): The base stride value to expand.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A stride tuple matching the structure of the shape tuple.

</section>

---

## format_layout

<section class='mojo-docs'>

`format_layout[W: Writer](layout: Layout, mut writer: W)`

Formats a 2D layout as a table and writes it to the specified writer.

This function creates a visual representation of a 2D layout as a table
showing the memory indices for each logical coordinate.

**Parameters:**

* â€‹W ([`Writer`](/mojo/std/io/write/Writer)): Type parameter representing a Writer implementation.

**Args:**

* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The 2D layout to format.
* â€‹writer (`W`): The writer to output the formatted layout to.

</section>

---

## hierarchical_unzip

<section class='mojo-docs'>

`hierarchical_unzip(layout_a: Layout, tiler: List[Layout]) -> Layout`

Hierarchically unzips a layout according to a list of layouts.

This function creates a hierarchical layout by unzipping the first layout
according to the layouts in the tiler list. It's useful for decomposing
a layout into hierarchical components for more efficient memory access
patterns or to enable specialized tensor operations.

Example:

```mojo
from layout import Layout, LayoutList, IntTuple
from layout.layout import hierarchical_unzip

# Create a layout to unzip
var base = Layout.row_major(6, 8)
var tilers = LayoutList()
tilers.append(Layout(IntTuple(2, 2)))
var result = hierarchical_unzip(base, tilers)
```

**Args:**

* â€‹layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to be unzipped.
* â€‹tiler ([`List`](/mojo/std/collections/list/List)): A list of layouts defining the unzipping patterns.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout representing the hierarchical unzipping with components
from both the original layout and the tiler layouts.

`hierarchical_unzip(layout_a: Layout, layout_b: Layout) -> Layout`

Hierarchically unzips a layout according to another layout.

This function creates a hierarchical layout by unzipping the first layout
according to the second layout. It's a fundamental operation for decomposing
a layout into hierarchical components, which enables more efficient memory
access patterns for various tensor operations.

Example:

```mojo
from layout import Layout, IntTuple
from layout.layout import hierarchical_unzip

# Create layouts
var base = Layout.row_major(6, 8)
var pattern = Layout(IntTuple(2, 2))
var result = hierarchical_unzip(base, pattern)
```

**Args:**

* â€‹layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to be unzipped.
* â€‹layout\_b ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout defining the unzipping pattern.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout representing the hierarchical unzipping of layout\_a
according to the pattern defined by layout\_b.

</section>

---

## layout (3)

<section class='mojo-docs'>

Provides a high-performance tensor layout system for memory mapping and indexing.

The layout module implements a comprehensive system for describing memory layouts
of multi-dimensional tensors, enabling efficient mapping between logical tensor
coordinates and physical memory locations. This is a critical component for
high-performance tensor operations in machine learning and scientific computing.
These low-level primitives require careful use to avoid errors.
Understanding the relationship between tensor shapes, strides, and
memory layout is essential for effective use.

Key components:

* `LayoutTrait`: Core trait defining the interface for all layout types
* `Layout`: Primary struct implementing memory layout with shape and stride information
* Layout algebra: Functions for composing, dividing, and transforming layouts
* Tiling operations: Functions for hierarchical decomposition of layouts

Performance features:

* Zero-cost abstractions for mapping between logical and physical indices
* Support for both compile-time and runtime-determined shapes
* Efficient memory access patterns through layout transformations
* Hierarchical tiling for cache-friendly memory access

Common use cases:

* Defining memory layouts for tensors with different storage formats (row-major, column-major)
* Implementing efficient tensor operations with optimal memory access patterns
* Supporting hardware-specific memory layouts for accelerators
* Enabling zero-copy tensor views and reshaping operations

Example:

```mojo
from layout import Layout, IntTuple
from layout.layout import blocked_product

# Create a 3x4 row-major layout
var layout = Layout.row_major(3, 4)

# Access the memory location for logical coordinates (1, 2)
var memory_idx = layout([1, 2])

# Create a tiled layout for blocked matrix multiplication
var tiled = blocked_product(layout, Layout([2, 2]))
```

## `comptime` values

### `LayoutList`

`comptime LayoutList = List[Layout]`

Type alias for a list of Layout objects.

## Structs

* [â€‹`Layout`](./Layout): Represents a memory layout for multi-dimensional data.

## Traits

* [â€‹`LayoutTrait`](./LayoutTrait): Defines the interface for mapping between logical coordinates and memory indices.

## Functions

* [â€‹`apply_tiler`](./apply_tiler): Applies a layout transformation function to each element of a layout with a tiler.
* [â€‹`blocked_product`](./blocked_product): Creates a blocked layout by combining two layouts.
* [â€‹`coalesce`](./coalesce): Simplifies a layout by combining dimensions with contiguous strides.
* [â€‹`complement`](./complement): Computes the complement layout for a given layout.
* [â€‹`composition`](./composition): Composes two layouts to create a new layout.
* [â€‹`cosize`](./cosize): Returns the size of the memory region spanned by the layout.
* [â€‹`downcast`](./downcast): Splits elements in a layout to create a finer layout without changing the total number of elements so that the alignment is preserved.
* [â€‹`expand_modes_alike`](./expand_modes_alike): Aligns two shape-stride pairs to have the same hierarchical structure.
* [â€‹`expand_strides`](./expand_strides): Expands a scalar stride into a stride tuple matching a shape tuple.
* [â€‹`format_layout`](./format_layout): Formats a 2D layout as a table and writes it to the specified writer.
* [â€‹`hierarchical_unzip`](./hierarchical_unzip): Hierarchically unzips a layout according to a list of layouts.
* [â€‹`is_contiguous_dim`](./is_contiguous_dim): Checks if a flat layout is contiguous in a specific dimension.
* [â€‹`is_row_major`](./is_row_major): Checks if a layout has row-major ordering for the specified rank.
* [â€‹`logical_divide`](./logical_divide): Divides a layout into blocks according to another layout.
* [â€‹`logical_product`](./logical_product): Creates a product of two layouts.
* [â€‹`make_layout`](./make_layout): Creates a composite layout by concatenating multiple layouts.
* [â€‹`make_ordered_layout`](./make_ordered_layout): Creates a layout with strides ordered according to a specified traversal order.
* [â€‹`MakeLayoutList`](./MakeLayoutList): Creates a list containing two layouts.
* [â€‹`MakeTileLayoutList`](./MakeTileLayoutList): Creates a list of layouts for tiling operations.
* [â€‹`print_layout`](./print_layout): Prints a 2D layout to the standard output.
* [â€‹`right_inverse`](./right_inverse): Creates a right inverse of a layout.
* [â€‹`size`](./size): Returns the total number of elements in the layout's domain.
* [â€‹`sublayout`](./sublayout): Creates a sublayout by selecting specific dimensions from a layout.
* [â€‹`tile_to_shape`](./tile_to_shape): Creates a layout by tiling a base layout to match a target shape.
* [â€‹`upcast`](./upcast): Fuses consecutive elements in a layout to create a coarser layout.
* [â€‹`zip_modes`](./zip_modes): Combines corresponding modes from two layouts.
* [â€‹`zipped_divide`](./zipped_divide): Divides a layout into blocks according to another layout.

</section>

---

## is_contiguous_dim

<section class='mojo-docs'>

`is_contiguous_dim(layout: Layout, dim: Int) -> Bool`

Checks if a flat layout is contiguous in a specific dimension.

This function checks if a flat layout is contiguous in a specified
dimension, considering both positive strides and zero strides with a single
element. The latter case is necessary for coalesced layouts.

**Args:**

* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to check.
* â€‹dim ([`Int`](/mojo/std/builtin/int/Int)): The dimension to check.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the layout is contiguous in the specified dimension,
False otherwise.

</section>

---

## is_row_major

<section class='mojo-docs'>

`is_row_major[rank: Int](layout: Layout) -> Bool`

Checks if a layout has row-major ordering for the specified rank.

A row-major layout has strides that decrease from left to right, with the
rightmost dimension having a stride of 1.

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The expected rank of the layout.

**Args:**

* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to check.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the layout has row-major ordering for the specified rank,
False otherwise.

</section>

---

## logical_divide

<section class='mojo-docs'>

`logical_divide(layout_a: Layout, _layout_b: Layout) -> Layout`

Divides a layout into blocks according to another layout.

This function creates a hierarchical layout by dividing the first layout
according to the second layout. It's useful for creating blocked or tiled
representations of tensors.

**Args:**

* â€‹layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to be divided.
* â€‹\_layout\_b ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout defining the division pattern.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout representing the hierarchical division.

`logical_divide(layout_a: Layout, tiler: List[Layout]) -> Layout`

Divides a layout into blocks according to a list of layouts.

This is a variant of logical\_divide that works with a list of layouts
for more complex tiling patterns.

**Args:**

* â€‹layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to be divided.
* â€‹tiler ([`List`](/mojo/std/collections/list/List)): A list of layouts defining the division patterns.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout representing the hierarchical division.

</section>

---

## logical_product

<section class='mojo-docs'>

`logical_product(_layout_a: Layout, layout_b: Layout) -> Layout`

Creates a product of two layouts.

This function creates a hierarchical layout by taking the logical product
of two layouts. It's a fundamental operation for creating blocked or tiled
layouts.

**Args:**

* â€‹\_layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The first layout.
* â€‹layout\_b ([`Layout`](/mojo/kernels/layout/layout/Layout)): The second layout.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout representing the logical product of the two layouts.

`logical_product(layout_a: Layout, tiler: List[Layout]) -> Layout`

Creates a product of a layout with a list of layouts.

This is a variant of logical\_product that works with a list of layouts
for more complex tiling patterns. It applies the logical\_product operation
to each element of the layout with the corresponding element in the tiler list.

Example:

```mojo
from layout import Layout, LayoutList, IntTuple
from layout.layout import logical_product

# Create a product of a layout with a list of layouts
var base = Layout.row_major(6, 8)
var tilers = LayoutList()
tilers.append(Layout(IntTuple(2, 2)))
var result = logical_product(base, tilers)
```

**Args:**

* â€‹layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The base layout to create products with.
* â€‹tiler ([`List`](/mojo/std/collections/list/List)): A list of layouts defining the product patterns.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout representing the logical product with the tiler layouts.

</section>

---

## make_layout

<section class='mojo-docs'>

`make_layout(*layouts: Layout) -> Layout`

Creates a composite layout by concatenating multiple layouts.

This function combines multiple layouts into a single layout by concatenating
their shapes and strides. The resulting layout represents a hierarchical
structure where each input layout becomes a component of the output layout.

Example:

```mojo
from layout import Layout, IntTuple
from layout.layout import make_layout

var layout1 = Layout(IntTuple(2, 3), IntTuple(3, 1))
var layout2 = Layout(IntTuple(4, 5), IntTuple(5, 1))
var combined = make_layout(layout1, layout2)
# Result: Layout with shape ((2, 3), (4, 5)) and stride ((3, 1), (5, 1))
```

**Args:**

* â€‹\*layouts ([`Layout`](/mojo/kernels/layout/layout/Layout)): Variable number of `Layout` objects to combine.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new Layout with concatenated shapes and strides from the input layouts.

`make_layout(layout_a: Layout, layout_b: Layout) -> Layout`

Creates a composite layout from two layouts.

This is a specialized version of make\_layout that takes exactly two layouts
and combines them into a single layout. This function exists as a workaround
for compiler limitations.

**Args:**

* â€‹layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The first layout to include in the composite.
* â€‹layout\_b ([`Layout`](/mojo/kernels/layout/layout/Layout)): The second layout to include in the composite.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new `Layout` with concatenated shapes and strides from the input layouts.

</section>

---

## make_ordered_layout

<section class='mojo-docs'>

`make_ordered_layout(shape: IntTuple, order: IntTuple) -> Layout`

Creates a layout with strides ordered according to a specified traversal order.

This function generates a compact (bijective) layout where the stride values
follow the traversal order specified by the order parameter. This allows
creating layouts with custom memory traversal patterns while maintaining
a compact memory representation.

Example:

```mojo
from layout import IntTuple, Layout
from layout.layout import make_ordered_layout

# Create a layout with shape (2,3,4,5) where dimensions are traversed
# in the order: dim0, dim3, dim2, dim1
var layout = make_ordered_layout(
    IntTuple(2, 3, 4, 5),
    IntTuple(1, 4, 3, 2)
)
# Result: Layout with shape (2,3,4,5) and stride (1,40,10,2)
```

**Args:**

* â€‹shape ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The shape of the layout.
* â€‹order ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The traversal order priority (lower values indicate higher priority).

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A `Layout` with the specified shape and strides ordered according to the
traversal order.

</section>

---

## print_layout

<section class='mojo-docs'>

`print_layout(layout: Layout)`

Prints a 2D layout to the standard output.

This function visualizes a 2D layout by printing a formatted table showing
the memory indices for each logical coordinate.

**Args:**

* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The 2D layout to print.

</section>

---

## right_inverse

<section class='mojo-docs'>

`right_inverse(layout: Layout) -> Layout`

Creates a right inverse of a layout.

The right inverse of a layout maps memory indices back to logical coordinates.
This is useful for converting between different memory layouts.

**Args:**

* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to invert.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout representing the right inverse of the input layout.

</section>

---

## size (Layout)

<section class='mojo-docs'>

`size(l: Layout) -> Int`

Returns the total number of elements in the layout's domain.

This is a standalone function equivalent to the Layout.size() method.

**Args:**

* â€‹l ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to calculate the size for.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The total number of elements in the layout.

</section>

---

## sublayout

<section class='mojo-docs'>

`sublayout(layout: Layout, *modes: Int) -> Layout`

Creates a sublayout by selecting specific dimensions from a layout.

This function extracts a subset of dimensions from a layout to create a new
layout with lower rank. For example, from a 3D layout, you could extract
a 2D layout containing only the first and third dimensions.

Example:

From a layout with shape (3,4,5), sublayout(layout, 0, 2) would
create a layout with shape (3,5).

**Args:**

* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The source layout to extract dimensions from.
* â€‹\*modes ([`Int`](/mojo/std/builtin/int/Int)): The indices of dimensions to include in the sublayout.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout containing only the specified dimensions.

</section>

---

## tile_to_shape

<section class='mojo-docs'>

`tile_to_shape(tile: Layout, target_shape: IntTuple, order: Optional[IntTuple] = None) -> Layout`

Creates a layout by tiling a base layout to match a target shape.

This function creates a hierarchical layout by repeating a tile layout to match
a target shape. It calculates how many times the tile needs to be repeated in
each dimension to reach the target shape, and creates a tiler layout with this
information.

Example:

```mojo
from layout import Layout, IntTuple
from layout.layout import tile_to_shape

# Create a 2x2 tile layout
var tile = Layout.row_major(2, 2)
# Tile it to create a 6x4 layout
var tiled = tile_to_shape(tile, IntTuple(6, 4))
# Result: A layout with 3x2 tiles of size 2x2 each
```

**Args:**

* â€‹tile ([`Layout`](/mojo/kernels/layout/layout/Layout)): The base layout to be tiled.
* â€‹target\_shape ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The desired final shape to tile to.
* â€‹order ([`Optional`](/mojo/std/collections/optional/Optional)): Optional memory ordering for the tiler layout. If None, defaults to
  column-major ordering.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout representing the tiled structure that matches the target shape.

</section>

---

## upcast

<section class='mojo-docs'>

`upcast[check: Bool = True](layout: Layout, factor: Int) -> Layout`

Fuses consecutive elements in a layout to create a coarser layout.

This function is useful for converting between different data type granularities,
such as from bytes to larger data types like bfloat16 or tf32.

**Parameters:**

* â€‹check ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to check for incompatible factors.

**Args:**

* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to upcast.
* â€‹factor ([`Int`](/mojo/std/builtin/int/Int)): The number of consecutive elements to fuse into one.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout with adjusted shape and stride for the coarser granularity.

</section>

---

## zip_modes

<section class='mojo-docs'>

`zip_modes(layout_a: Layout, layout_b: Layout) -> Layout`

Combines corresponding modes from two layouts.

This function creates a new layout by combining corresponding dimensions
from two layouts. If a dimension in layout\_b has a non-positive shape,
the corresponding dimension from layout\_a is used directly.

**Args:**

* â€‹layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The first layout.
* â€‹layout\_b ([`Layout`](/mojo/kernels/layout/layout/Layout)): The second layout.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout with combined dimensions from both input layouts.

</section>

---

## zipped_divide

<section class='mojo-docs'>

`zipped_divide(layout_a: Layout, layout_b: Layout) -> Layout`

Divides a layout into blocks according to another layout.

This function creates a hierarchical layout by dividing the first layout
according to the second layout. It's an alias for hierarchical\_unzip that provides a
more intuitive name for the division operation. This is useful for creating
blocked or tiled representations of tensors.

Example:

```mojo
from layout import Layout, IntTuple
from layout.layout import zipped_divide

# Create layouts
var base = Layout.row_major(6, 8)
var pattern = Layout(IntTuple(2, 2))
var result = zipped_divide(base, pattern)
```

**Args:**

* â€‹layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to be divided.
* â€‹layout\_b ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout defining the division pattern.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout representing the hierarchical division of layout\_a according
to layout\_b.

`zipped_divide(layout_a: Layout, tiler: List[Layout]) -> Layout`

Divides a layout into blocks according to a list of layouts.

This function creates a hierarchical layout by dividing the first layout
according to the layouts in the tiler list. It's an alias for hierarchical\_unzip that
provides a more intuitive name for the division operation when working with
multiple tiling patterns.

Example:

```mojo
from layout import Layout, LayoutList, IntTuple
from layout.layout import zipped_divide

# Create layouts
var base = Layout.row_major(6, 8)
var tilers = LayoutList()
tilers.append(Layout(IntTuple(2, 2)))
var result = zipped_divide(base, tilers)
```

**Args:**

* â€‹layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to be divided.
* â€‹tiler ([`List`](/mojo/std/collections/list/List)): A list of layouts defining the division patterns.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): A new layout representing the hierarchical division of layout\_a according
to the patterns in tiler.

</section>

---

## LayoutTensor

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct LayoutTensor[mut: Bool, //, dtype: DType, layout: Layout, origin: Origin[mut=mut], /, *, address_space: AddressSpace = AddressSpace.GENERIC, element_layout: Layout = Layout(IntTuple(1), IntTuple(1)), layout_int_type: DType = _get_layout_type(layout, address_space), linear_idx_type: DType = _get_index_type(layout, address_space), masked: Bool = False, alignment: Int = align_of[dtype]()]`

A high-performance tensor with explicit memory layout and hardware-optimized access patterns.

`LayoutTensor` provides a powerful abstraction for multi-dimensional data
with precise control over memory organization. It supports various memory
layouts (row-major, column-major, tiled), hardware-specific optimizations,
and efficient parallel access patterns.

Example:

```mojo
from layout import Layout, LayoutTensor

# Create tensor on CPU using InlineArray to allocate storage space.
var storage = InlineArray[Float32, 5 * 4](uninitialized=True)
var tensor_5x4 = LayoutTensor[DType.float32, Layout.row_major(5, 4)](storage)
```

## Parameters

* â€‹mut ([`Bool`](/mojo/std/builtin/bool/Bool)): The inferred mutability of the underlying pointer.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the underlying pointer.
* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The memory layout of the tensor.
* â€‹origin ([`Origin`](/mojo/std/builtin/type_aliases/Origin)): The origin of the underlying pointer.
* â€‹address\_space ([`AddressSpace`](/mojo/std/memory/pointer/AddressSpace)): The address space of the underlying pointer.
* â€‹element\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The memory layout of each element in the tensor.
* â€‹layout\_int\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The integer type of each dimension of runtime layout.
* â€‹linear\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The integer type of the index pointing to memory
  locations.
* â€‹masked ([`Bool`](/mojo/std/builtin/bool/Bool)): If true the tensor is masked and runtime layouts determine the
  shape.
* â€‹alignment ([`Int`](/mojo/std/builtin/int/Int)): Alignment of the data pointer.

## Fields

* â€‹ptr (`LegacyUnsafePointer[Scalar[dtype], address_space=address_space, mut=mut, origin=origin]`): Pointer to the underlying memory buffer containing the tensor data.
  This pointer respects the specified address space, alignment, mutability,
  and origin tracking for memory safety and performance optimization.
* â€‹runtime\_layout (`LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].RuntimeLayoutType`): Runtime representation of the tensor's memory layout.
  Handles both compile-time and runtime-determined dimensions, enabling
  efficient mapping between logical tensor coordinates and physical memory
  locations.
* â€‹runtime\_element\_layout (`LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].RuntimeElementLayoutType`): Runtime representation of each element's internal layout.
  Used when elements themselves have structure, such as in blocked or tiled
  layouts.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable),
[`_Expable`](/mojo/std/math/math/_Expable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `AddressSpaceCastType`

`comptime AddressSpaceCastType[address_space: AddressSpace = address_space] = LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Type alias for address-space-cast result tensors.

#### Parameters

* â€‹address\_space ([`AddressSpace`](/mojo/std/memory/pointer/AddressSpace)): The target address space for the result tensor.

### `BitcastType`

`comptime BitcastType[new_dtype: DType, /, address_space: AddressSpace = address_space, element_layout: Layout = element_layout] = LayoutTensor[new_dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked]`

Type alias for bitcast result tensors.

#### Parameters

* â€‹new\_dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The target data type to cast to.
* â€‹address\_space ([`AddressSpace`](/mojo/std/memory/pointer/AddressSpace)): The address space for the result tensor.
* â€‹element\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The element layout for the result tensor.

### `CoalesceType`

`comptime CoalesceType[element_layout: Layout] = LayoutTensor[dtype, coalesce(layout, False), origin, address_space=address_space, element_layout=element_layout]`

Type alias for coalesced result tensors.

#### Parameters

* â€‹element\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The element layout for the coalesced tensor.

### `CompositionType`

`comptime CompositionType[rhs_layout: Layout, dst_layout: Layout = composition(layout, rhs_layout)] = LayoutTensor[dtype, dst_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type]`

Type alias for composed layout tensor types.

#### Parameters

* â€‹rhs\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to compose with.
* â€‹dst\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The resulting composed layout.

### `CornerCoordsType`

`comptime CornerCoordsType = IndexList[len[IntTuple](flatten(layout.shape)), element_type=layout_int_type]`

Index list type for corner coordinates.

### `device_type`

`comptime device_type = LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

The device-side type representation.

### `DistributeType`

`comptime DistributeType[threads_layout: Layout, axis: OptionalReg[Int] = None] = LayoutTensor[dtype, _compute_distribute_layout[layout, threads_layout, axis]()[1], origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked if masked else _distribute_is_masked[layout, threads_layout, axis]() if is_nvidia_gpu() else False]`

Type alias for distributed tensor types.

#### Parameters

* â€‹threads\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout describing thread distribution.
* â€‹axis ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional axis to distribute along.

### `DynamicSplitType`

`comptime DynamicSplitType[axis: Int = 0] = LayoutTensor[dtype, layout.make_shape_unknown[axis](), origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type]`

Type alias for dynamic split result tensors.

#### Parameters

* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis along which to split.

### `element_size`

`comptime element_size = element_layout.size()`

The number of scalar values in each element of the tensor.

### `element_type`

`comptime element_type = SIMD[dtype, LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].element_size]`

The SIMD vector type used for vectorized operations on tensor elements.

### `FlattenedType`

`comptime FlattenedType = LayoutTensor[dtype, Layout(IntTuple(-1)), origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Type alias for flattened tensor types.

### `GenericAddressSpaceLayoutTensor`

`comptime GenericAddressSpaceLayoutTensor = LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

LayoutTensor variant using generic address space.

### `GenericLayoutTensorType`

`comptime GenericLayoutTensorType = LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

LayoutTensor type with generic address space.

### `idx_list_t`

`comptime idx_list_t[rank: Int = LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank] = IndexList[rank, element_type=linear_idx_type]`

Type alias for index lists of the tensor's rank.

#### Parameters

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the index list.

### `MutableAnyType`

`comptime MutableAnyType = LayoutTensor[dtype, layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Mutable LayoutTensor type with MutAnyOrigin.

### `num_strides`

`comptime num_strides = LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].RuntimeLayoutType.StrideType.scalar_length`

Number of stride values in the layout.

### `OriginCastType`

`comptime OriginCastType[mut: Bool, origin: Origin[mut=mut]] = LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Type alias for origin-cast result tensors.

#### Parameters

* â€‹mut ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether the result tensor is mutable.
* â€‹origin ([`Origin`](/mojo/std/builtin/type_aliases/Origin)): The origin for the result tensor.

### `rank`

`comptime rank = layout.rank()`

The number of dimensions in the tensor's layout.

### `ReshapeType`

`comptime ReshapeType[dst_layout: Layout] = LayoutTensor[dtype, dst_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Type alias for reshaped tensor types.

#### Parameters

* â€‹dst\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The target layout for the reshaped tensor.

### `RuntimeElementLayoutType`

`comptime RuntimeElementLayoutType = RuntimeLayout[element_layout, element_type=DType.int32, linear_idx_type=linear_idx_type]`

Type alias for the runtime element layout.

### `RuntimeLayoutType`

`comptime RuntimeLayoutType = RuntimeLayout[layout, element_type=layout_int_type, linear_idx_type=linear_idx_type]`

Type alias for the runtime layout.

### `ShapeVectorizedType`

`comptime ShapeVectorizedType[origin: ImmutOrigin, vector_shape: IntTuple, linear_vectorize: Bool] = LayoutTensor[dtype, coalesce(LayoutTensor._tuple_divide_tiles[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment](vector_shape, linear_vectorize)[1], True), origin, address_space=address_space, element_layout=LayoutTensor._tuple_divide_tiles[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment](vector_shape, linear_vectorize)[0], layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked]`

Type alias for shape-vectorized tensor types.

#### Parameters

* â€‹origin ([`ImmutOrigin`](/mojo/std/builtin/type_aliases/#immutorigin)): The origin of the result tensor.
* â€‹vector\_shape ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The shape of each vector unit.
* â€‹linear\_vectorize ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to vectorize in a linear manner.

### `SIMDTileType`

`comptime SIMDTileType[tile_size: Int] = LayoutTensor[dtype, LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, tile_size, simd_width_of[dtype]()]()[0], origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked if masked else _tile_is_masked[layout, tile_size, simd_width_of[dtype]()](), alignment=alignment]`

Type alias for SIMD-sized tile tensors.

#### Parameters

* â€‹tile\_size ([`Int`](/mojo/std/builtin/int/Int)): The size of the tile along the tiled axis.

### `SIMDVectorizedType`

`comptime SIMDVectorizedType = LayoutTensor[dtype, coalesce(LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, 1, simd_width_of[dtype]()]()[1], True), origin, address_space=address_space, element_layout=LayoutTensor._divide_tiles[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, 1, simd_width_of[dtype]()]()[0], layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked]`

Result type for SIMD-width vectorization.

### `SliceType`

`comptime SliceType[d0_slice: Slice, d1_slice: Slice] = LayoutTensor[dtype, LayoutTensor._compute_slice_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment](d0_slice, d1_slice), origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type]`

Type alias for 2D slice result tensors.

#### Parameters

* â€‹d0\_slice ([`Slice`](/mojo/std/builtin/builtin_slice/Slice)): Slice specification for the first dimension.
* â€‹d1\_slice ([`Slice`](/mojo/std/builtin/builtin_slice/Slice)): Slice specification for the second dimension.

### `SliceType1D`

`comptime SliceType1D[d0_slice: Slice, slice_indices: IndexList[1], __offset_dims: Int = (LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank - 1)] = LayoutTensor[dtype, LayoutTensor._compute_slice_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment](d0_slice, slice_indices.__getitem__[1, DType.int64, Int](0)), origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type]`

Type alias for 1D slice result tensors from higher-rank tensors.

#### Parameters

* â€‹d0\_slice ([`Slice`](/mojo/std/builtin/builtin_slice/Slice)): Slice specification for the selected dimension.
* â€‹slice\_indices ([`IndexList`](/mojo/std/utils/index_/IndexList)): Index of the dimension to slice.
* â€‹\_\_offset\_dims ([`Int`](/mojo/std/builtin/int/Int)): Number of fixed dimensions.

### `SliceType2D`

`comptime SliceType2D[d0_slice: Slice, d1_slice: Slice, slice_indices: IndexList[2], __offset_dims: Int = (LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank - 2)] = LayoutTensor[dtype, LayoutTensor._compute_slice_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment](d0_slice, d1_slice, slice_indices.__getitem__[2, DType.int64, Int](0), slice_indices.__getitem__[2, DType.int64, Int](1)), origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type]`

Type alias for 2D slice result tensors from higher-rank tensors.

#### Parameters

* â€‹d0\_slice ([`Slice`](/mojo/std/builtin/builtin_slice/Slice)): Slice specification for the first selected dimension.
* â€‹d1\_slice ([`Slice`](/mojo/std/builtin/builtin_slice/Slice)): Slice specification for the second selected dimension.
* â€‹slice\_indices ([`IndexList`](/mojo/std/utils/index_/IndexList)): Indices of the two dimensions to slice.
* â€‹\_\_offset\_dims ([`Int`](/mojo/std/builtin/int/Int)): Number of fixed dimensions.

### `SplitElementType`

`comptime SplitElementType[count: Int, axis: Int = 0] = LayoutTensor[dtype, LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, (layout.shape[axis].value() // count), axis]()[0], origin, address_space=address_space, element_layout=element_layout, alignment=alignment]`

Type alias for split element tensors.

#### Parameters

* â€‹count ([`Int`](/mojo/std/builtin/int/Int)): Number of portions to split into.
* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis along which to split.

### `StackTensorType`

`comptime StackTensorType = LayoutTensor[dtype, layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

LayoutTensor type for stack-allocated tensors.

### `StaticSplitType`

`comptime StaticSplitType[count: Int, axis: Int = 0] = StaticTuple[LayoutTensor[dtype, LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, (layout.shape[axis].value() // count), axis]()[0], origin, address_space=address_space, element_layout=element_layout, alignment=alignment], count]`

Type alias for static split result tuples.

#### Parameters

* â€‹count ([`Int`](/mojo/std/builtin/int/Int)): Number of portions to split into.
* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis along which to split.

### `storage_size`

`comptime storage_size = (size_of[dtype]() * layout.size())`

Total storage size in bytes for the tensor data.

### `TiledIteratorType`

`comptime TiledIteratorType[*tile_sizes: Int, *, axis: Int = 0] = LayoutTensorIter[dtype, LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, tile_sizes]()[0], origin, address_space=address_space, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked if masked else _tile_is_masked[layout, tile_sizes]()]`

Type alias for tiled iterator types.

#### Parameters

* â€‹\*tile\_sizes ([`Int`](/mojo/std/builtin/int/Int)): The dimensions of each tile along each axis.
* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis along which to iterate.

### `TileType`

`comptime TileType[*tile_sizes: Int] = LayoutTensor[dtype, LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, tile_sizes]()[0], origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked if masked else _tile_is_masked[layout, tile_sizes](), alignment=alignment]`

The tile type returned by the `tile()` method given the specified set of tile sizes.

#### Parameters

* â€‹\*tile\_sizes ([`Int`](/mojo/std/builtin/int/Int)): The dimensions of each tile along each axis of the
  tensor.

### `TransposeType`

`comptime TransposeType = LayoutTensor[dtype, layout.transpose(), origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type]`

Result type for transpose operations.

### `VectorizedType`

`comptime VectorizedType[*vector_shape: Int] = LayoutTensor[dtype, coalesce(LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, vector_shape]()[1], True), origin, address_space=address_space, element_layout=LayoutTensor._divide_tiles[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, vector_shape]()[0], layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked]`

Type alias for vectorized tensor types.

#### Parameters

* â€‹\*vector\_shape ([`Int`](/mojo/std/builtin/int/Int)): The shape of each vector unit along each axis.

## Methods

### `__init__`

`__init__(span: Span[Scalar[dtype], origin]) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].GenericAddressSpaceLayoutTensor`

Create a `LayoutTensor` with a `Span`.

**Constraints:**

Layout must be fully static.

**Args:**

* â€‹span ([`Span`](/mojo/std/memory/span/Span)): The `Span` pointing to the underlying data.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

`__init__(span: Span[Scalar[dtype], origin], runtime_layout: RuntimeLayout[layout, element_type=element_type, linear_idx_type=linear_idx_type]) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].GenericAddressSpaceLayoutTensor`

Create a `LayoutTensor` with a `Span` and a runtime layout for the tensor. The runtime layout element type will be casted to the layout tensor layout integer type.

**Constraints:**

* Element layout must be fully static.

**Args:**

* â€‹span ([`Span`](/mojo/std/memory/span/Span)): The `Span` pointing to the underlying data.
* â€‹runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout of the LayoutTensor.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

`__init__(span: Span[Scalar[dtype], origin], runtime_layout: RuntimeLayout[layout, element_type=element_type, linear_idx_type=linear_idx_type], element_runtime_layout: RuntimeLayout[element_layout, element_type=element_type, linear_idx_type=linear_idx_type]) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].GenericAddressSpaceLayoutTensor`

Create a `LayoutTensor` with a `Span`, a runtime layout of the tensor, and the runtime layout of each element. The runtime layout element type will be casted to the layout tensor layout integer type.

**Constraints:**

* Runtime layout and `LayoutTensor` must have the same bitwidth and
  index type.

**Args:**

* â€‹span ([`Span`](/mojo/std/memory/span/Span)): The `Span` pointing to the underlying data.
* â€‹runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout of the `LayoutTensor`.
* â€‹element\_runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout of each element.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

`__init__(unsafe_ptr: LegacyUnsafePointer[Scalar[dtype], address_space=address_space, mut=mut, origin=origin]) -> Self`

Create a `LayoutTensor` with an `UnsafePointer`.

**Constraints:**

Layout must be fully static.

**Args:**

* â€‹unsafe\_ptr (`LegacyUnsafePointer`): The `UnsafePointer` pointing to the underlying data.

`__init__(unsafe_ptr: LegacyUnsafePointer[Scalar[dtype], address_space=address_space, mut=mut, origin=origin], runtime_layout: RuntimeLayout[layout, element_type=element_type, linear_idx_type=linear_idx_type]) -> Self`

Create a `LayoutTensor` with an `UnsafePointer` and a runtime layout for the tensor. The runtime layout element type will be casted to the layout tensor layout integer type.

**Constraints:**

Element layout must be fully static.

**Args:**

* â€‹unsafe\_ptr (`LegacyUnsafePointer`): The UnsafePointer pointing to the underlying data.
* â€‹runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout of the LayoutTensor.

`__init__(unsafe_ptr: LegacyUnsafePointer[Scalar[dtype], address_space=address_space, mut=mut, origin=origin], runtime_layout: RuntimeLayout[layout, element_type=element_type, linear_idx_type=linear_idx_type], element_runtime_layout: RuntimeLayout[element_layout, element_type=element_type, linear_idx_type=linear_idx_type]) -> Self`

Create a `LayoutTensor` with an `UnsafePointer`, a runtime layout for the tensor, and the runtime layout of each element. The runtime layout element type will be casted to the layout tensor layout integer type.

**Args:**

* â€‹unsafe\_ptr (`LegacyUnsafePointer`): The `UnsafePointer` pointing to the underlying data.
* â€‹runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout of the `LayoutTensor`.
* â€‹element\_runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout of each element.

`__init__(ref [origin] device_buffer: DeviceBuffer[dtype]) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].GenericLayoutTensorType`

Create a `LayoutTensor` from a `DeviceBuffer`. The layout must have statically known dimensions.

Note that the device buffer memory is on the accelerator device (GPU
global memory). Code running on the CPU can use the
[`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext) to
allocate a `DeviceBuffer` and use that to construct a `LayoutTensor`
that can be accessed on the GPU. You cannot directly access data in the
`DeviceBuffer` or `LayoutTensor` from the CPU.

The following example shows a typical pattern for using `DeviceBuffer`
to construct a `LayoutTensor` that you can use on the GPU.

```mojo
from gpu.host import DeviceContext, DeviceBuffer
from layout import Layout, LayoutTensor

comptime dtype = DType.float32

var ctx = DeviceContext()
# Allocate buffers
var dev_buf = ctx.enqueue_create_buffer[dtype](16)
var host_buf = ctx.enqueue_create_host_buffer[dtype](16)
# Ensure buffers have been created
ctx.synchronize()

# Initialize host buffer and copy to device buffer
for i in range(16):
    host_buf[i] = i
ctx.enqueue_copy(dev_buf, host_buf)

# Create LayoutTensor to use on device
comptime layout = Layout.row_major(4, 4)
var tensor = LayoutTensor[dtype, layout](dev_buf)
...
```

**Constraints:**

* Layout must be fully static.

**Args:**

* â€‹device\_buffer ([`DeviceBuffer`](/mojo/std/gpu/host/device_context/DeviceBuffer)): Contains the underlying data to point to.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

`__init__(ref [origin] host_buffer: HostBuffer[dtype]) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].GenericLayoutTensorType`

Create a `LayoutTensor` from a `HostBuffer`. The layout must have statically known dimensions.

The resulting tensor's data can only be accessed on the CPU.

```mojo
from gpu.host import DeviceContext, HostBuffer
from layout import Layout, LayoutTensor

comptime dtype = DType.float32

var ctx = DeviceContext()
var dev_buf = ctx.enqueue_create_host_buffer[dtype](8)

comptime layout = Layout.row_major(4, 4)
var tensor = LayoutTensor[dtype, layout](dev_buf)
```

**Constraints:**

* Layout must be fully static.

**Args:**

* â€‹host\_buffer ([`HostBuffer`](/mojo/std/gpu/host/device_context/HostBuffer)): Contains the underlying data to point to.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

`__init__(ref [origin] device_buffer: DeviceBuffer[dtype], runtime_layout: RuntimeLayout[layout, element_type=element_type, linear_idx_type=linear_idx_type]) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].GenericLayoutTensorType`

Create a `LayoutTensor` from a `DeviceBuffer` and a runtime layout. The runtime layout element type will be casted to the layout tensor layout integer type.

The resulting tensor's data can only be accessed on the GPU.

**Constraints:**

* Element layout must be fully static.

**Args:**

* â€‹device\_buffer ([`DeviceBuffer`](/mojo/std/gpu/host/device_context/DeviceBuffer)): The `DeviceBuffer` containing to the underlying data.
* â€‹runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout of the LayoutTensor.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

`__init__(ref [origin] host_buffer: HostBuffer[dtype], runtime_layout: RuntimeLayout[layout, element_type=element_type, linear_idx_type=linear_idx_type]) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].GenericLayoutTensorType`

Create a `LayoutTensor` from a `HostBuffer` and a runtime layout. The runtime layout element type will be casted to the layout tensor layout integer type.

The resulting tensor's data can only be accessed on the CPU.

**Constraints:**

* Element layout must be fully static.

**Args:**

* â€‹host\_buffer ([`HostBuffer`](/mojo/std/gpu/host/device_context/HostBuffer)): The `HostBuffer` containing to the underlying data.
* â€‹runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout of the `LayoutTensor`.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

`__init__(ref [origin] device_buffer: DeviceBuffer[dtype], runtime_layout: RuntimeLayout[layout, element_type=element_type, linear_idx_type=linear_idx_type], element_runtime_layout: RuntimeLayout[element_layout, element_type=element_type, linear_idx_type=linear_idx_type]) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].GenericLayoutTensorType`

Create a `LayoutTensor` from a `DeviceBuffer`, a runtime layout for the tensor, and the runtime layout of each element. The runtime layout element type will be casted to the layout tensor layout integer type.

The resulting tensor's data can only be accessed on the GPU.

**Args:**

* â€‹device\_buffer ([`DeviceBuffer`](/mojo/std/gpu/host/device_context/DeviceBuffer)): The `DeviceBuffer` containing to the underlying data.
* â€‹runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout of the `LayoutTensor`.
* â€‹element\_runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout of each element.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

`__init__(ref [origin] host_buffer: HostBuffer[dtype], runtime_layout: RuntimeLayout[layout, element_type=element_type, linear_idx_type=linear_idx_type], element_runtime_layout: RuntimeLayout[element_layout, element_type=element_type, linear_idx_type=linear_idx_type]) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].GenericLayoutTensorType`

Create a `LayoutTensor` from a `HostBuffer`, a runtime layout for the tensor, and the runtime layout of each element. The runtime layout element type will be casted to the layout tensor layout integer type.

The resulting tensor's data can only be accessed on the CPU.

**Args:**

* â€‹host\_buffer ([`HostBuffer`](/mojo/std/gpu/host/device_context/HostBuffer)): The `HostBuffer` containing to the underlying data.
* â€‹runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout of the `LayoutTensor`.
* â€‹element\_runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The runtime layout of each element.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `__getitem__`

`__getitem__[*Tys: Indexer](self, *args: *Tys) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].element_type`

Retrieves a single element from the tensor at the specified indices.

This method provides array-like indexing for the tensor. The number of
indices provided must match the rank of the tensor, otherwise an error
will occur at runtime.

**Parameters:**

* â€‹\*Tys ([`Indexer`](/mojo/std/builtin/int/Indexer)): The type of the indices. Must implement the `Indexer` trait,
  and match the rank of the tensor.

**Args:**

* â€‹\*args (`*Tys`): The indices specifying the element's position in the tensor.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): The element at the specified position with the tensor's data type.

`__getitem__(self, crd: RuntimeTuple[S, element_type=element_type]) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].element_type`

Retrieves a single element from the tensor at the specified indices.

This method provides array-like indexing for the tensor. The number of
indices provided must match the rank of the tensor, otherwise an error
will occur at runtime.

**Args:**

* â€‹crd ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The coordinate specifying the element's position in each dimension. For example, in a 3D tensor, you would use (i, j, k).

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): The element at the specified position with the tensor's data type.

### `__setitem__`

`__setitem__[*Tys: Indexer](self, *args: *Tys, *, val: SIMD[dtype, LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].element_size])`

Sets a single element in a tensor at the specified indices.

This method provides array-like element assignment for tensors.

Notes:

* No bounds checking is performed. Accessing out-of-bounds indices
  will result in undefined behavior.

**Parameters:**

* â€‹\*Tys ([`Indexer`](/mojo/std/builtin/int/Indexer)): The type of the indices. Must implement the `Indexer` trait,
  and match the rank of the tensor.

**Args:**

* â€‹\*args (`*Tys`): The indices specifying the element's position in the tensor.
* â€‹val ([`SIMD`](/mojo/std/builtin/simd/SIMD)): The value to write to the tensor at the specified position.

### `__add__`

`__add__(self, other: Scalar[dtype]) -> LayoutTensor[dtype, layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Add a scalar value to each element of the tensor.

Performs an elementwise addition operation, adding the scalar value to
each element in the tensor. This operation creates a new tensor with the
results.

Performance:

* This operation creates a copy of the tensor before performing the
  addition.
* For in-place addition, use the `__iadd__` method instead (`+=`
  operator).

**Args:**

* â€‹other ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The scalar value to add to each element.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new tensor containing the results of the addition operation.

`__add__[other_layout: Layout](self, other: LayoutTensor[dtype, other_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> LayoutTensor[dtype, layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Add another tensor to this tensor elementwise.

Performs an elementwise addition between this tensor and another tensor.
This operation creates a new tensor with the results.

Limited broadcasting is supported:

* For tensors of the same rank, shapes must match exactly.
* For rank-1 to rank-2 broadcasting, the rank-1 tensor's dimension must
  match the corresponding dimension of the rank-2 tensor.

Performance:

* This operation creates a copy of the tensor before performing the
  addition.
* For in-place addition, use the `__iadd__` method instead (`+=`
  operator).

**Parameters:**

* â€‹other\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the other tensor.

**Args:**

* â€‹other ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The tensor to add to this tensor.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new tensor containing the results of the addition operation.

### `__sub__`

`__sub__(self, other: Scalar[dtype]) -> LayoutTensor[dtype, layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Subtract a scalar value from each element of the tensor.

Performs an elementwise subtraction operation, subtracting the scalar
value from each element in the tensor. This operation creates a new
tensor with the results.

Performance:

* This operation creates a copy of the tensor before performing the
  subtraction.
* For in-place subtraction, use the `__isub__` method instead (`-=`
  operator).

**Args:**

* â€‹other ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The scalar value to subtract from each element.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new tensor containing the results of the subtraction operation.

`__sub__[other_layout: Layout](self, other: LayoutTensor[dtype, other_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> LayoutTensor[dtype, layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Subtract another tensor from this tensor elementwise.

Performs an elementwise subtraction between this tensor and another
tensor. This operation creates a new tensor with the results.

Limited broadcasting is supported:

* For tensors of the same rank, shapes must match exactly.
* For rank-1 to rank-2 broadcasting, the rank-1 tensor's dimension must
  match the corresponding dimension of the rank-2 tensor.

Performance:

* This operation creates a copy of the tensor before performing the
  subtraction.
* For in-place subtraction, use the `__isub__` method instead (`-=`
  operator).

**Parameters:**

* â€‹other\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the other tensor.

**Args:**

* â€‹other ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The tensor to subtract from this tensor.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new tensor containing the results of the subtraction operation.

### `__mul__`

`__mul__(self, other: Scalar[dtype]) -> LayoutTensor[dtype, layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Multiply each element of the tensor by a scalar value.

Performs an elementwise multiplication operation, multiplying each
element in the tensor by the scalar value. This operation creates a new
tensor with the results.

Performance:

* This operation creates a copy of the tensor before performing the
  multiplication.
* For in-place multiplication, use the `__imul__` method instead
  (`*=` operator).

**Args:**

* â€‹other ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The scalar value to multiply with each element.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new tensor containing the results of the multiplication operation.

`__mul__[other_layout: Layout](self, other: LayoutTensor[dtype, other_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> LayoutTensor[dtype, layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Multiply this tensor with another tensor elementwise.

Performs an elementwise multiplication (Hadamard product) between this tensor
and another tensor. This operation creates a new tensor with the results.

Limited broadcasting is supported:

* For tensors of the same rank, shapes must match exactly.
* For rank-1 to rank-2 broadcasting, the rank-1 tensor's dimension must
  match the corresponding dimension of the rank-2 tensor.

Note: This is NOT a matrix multiplication operation. For matrix
multiplication, use the appropriate matmul function instead.

Performance:

* This operation creates a copy of the tensor before performing the
  multiplication.
* For in-place multiplication, use the `__imul__` method instead
  (`*=` operator).

**Parameters:**

* â€‹other\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the other tensor.

**Args:**

* â€‹other ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The tensor to multiply with this tensor.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new tensor containing the results of the elementwise
multiplication.

### `__truediv__`

`__truediv__(self, other: Scalar[dtype]) -> LayoutTensor[dtype, layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Divide each element of the tensor by a scalar value.

Performs an elementwise division operation, dividing each element in the
tensor by the scalar value. This operation creates a new tensor with the
results.

Performance:

* This operation creates a copy of the tensor before performing the
  division.
* For in-place division, use the `__itruediv__` method instead
  (`/=` operator).

Notes:

* Division by zero will result in undefined behavior or errors
  depending on the dtype.
* For integer dtypes, this performs integer division.

**Args:**

* â€‹other ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The scalar value to divide each element by.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new tensor containing the results of the division operation.

`__truediv__[other_layout: Layout](self, other: LayoutTensor[dtype, other_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> LayoutTensor[dtype, layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Divide this tensor by another tensor elementwise.

Performs an elementwise division between this tensor and another tensor.
This operation creates a new tensor with the results.

Limited broadcasting is supported:

* For tensors of the same rank, shapes must match exactly.
* For rank-1 to rank-2 broadcasting, the rank-1 tensor's dimension must
  match the corresponding dimension of the rank-2 tensor.

Performance:

* This operation creates a copy of the tensor before performing the
  division.
* For in-place division, use the `__itruediv__` method instead
  (`/=` operator).

Notes:

* Division by zero will result in undefined behavior or errors depending on the dtype.
* For integer dtypes, this performs integer division.

**Parameters:**

* â€‹other\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the other tensor.

**Args:**

* â€‹other ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The tensor to divide this tensor by.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new tensor containing the results of the division operation.

### `__iadd__`

`__iadd__(self, other: Scalar[dtype])`

Add a scalar value to each element of the tensor in-place.

Performs an elementwise addition operation, adding the scalar value to
each element in the tensor. This operation modifies the tensor in-place.

Performance:

* This operation modifies the tensor directly without creating a copy.

**Args:**

* â€‹other ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The scalar value to add to each element.

`__iadd__[other_layout: Layout](self, other: LayoutTensor[dtype, other_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Add another tensor to this tensor elementwise in-place.

Performs an elementwise addition between this tensor and another tensor.
This operation modifies the tensor in-place.

Limited broadcasting is supported:

* For tensors of the same rank, shapes must match exactly.
* For rank-1 to rank-2 broadcasting, the rank-1 tensor's dimension must
  match the corresponding dimension of the rank-2 tensor.

Performance:

* This operation modifies the tensor directly without creating a
  copy.

**Parameters:**

* â€‹other\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the other tensor.

**Args:**

* â€‹other ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The tensor to add to this tensor.

### `__isub__`

`__isub__(self, other: Scalar[dtype])`

Subtract a scalar value from each element of the tensor in-place.

Performs an elementwise subtraction operation, subtracting the scalar
value from each element in the tensor. This operation modifies the
tensor in-place.

Performance:

* This operation modifies the tensor directly without creating a copy.

**Args:**

* â€‹other ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The scalar value to subtract from each element.

`__isub__[other_layout: Layout](self, other: LayoutTensor[dtype, other_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Subtract another tensor from this tensor elementwise in-place.

Performs an elementwise subtraction between this tensor and another
tensor. This operation modifies the tensor in-place.

Limited broadcasting is supported:

* For tensors of the same rank, shapes must match exactly.
* For rank-1 to rank-2 broadcasting, the rank-1 tensor's dimension must
  match the corresponding dimension of the rank-2 tensor.

Performance:

* This operation modifies the tensor directly without creating a copy.

**Parameters:**

* â€‹other\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the other tensor.

**Args:**

* â€‹other ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The tensor to subtract from this tensor.

### `__imul__`

`__imul__(self, other: Scalar[dtype])`

Multiply each element of the tensor by a scalar value in-place.

Performs an elementwise multiplication operation, multiplying each
element in the tensor by the scalar value. This operation modifies the
tensor in-place.

Performance:

* This operation modifies the tensor directly without creating a copy.

**Args:**

* â€‹other ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The scalar value to multiply with each element.

`__imul__[other_layout: Layout](self, other: LayoutTensor[dtype, other_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Multiply this tensor with another tensor elementwise in-place.

Performs an elementwise multiplication (Hadamard product) between this
tensor and another tensor. This operation modifies the tensor in-place.

Limited broadcasting is supported:

* For tensors of the same rank, shapes must match exactly.
* For rank-1 to rank-2 broadcasting, the rank-1 tensor's dimension must
  match the corresponding dimension of the rank-2 tensor.

Note: This is NOT a matrix multiplication operation. For matrix
multiplication, use the appropriate matmul function instead.

Performance:

* This operation modifies the tensor directly without creating a copy.

**Parameters:**

* â€‹other\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the other tensor.

**Args:**

* â€‹other ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The tensor to multiply with this tensor.

### `__itruediv__`

`__itruediv__(self, other: Scalar[dtype])`

Divide each element of the tensor by a scalar value in-place.

Performs an elementwise division operation, dividing each element in the
tensor by the scalar value. This operation modifies the tensor in-place.

Performance:

* This operation modifies the tensor directly without creating a copy.

Notes:

* Division by zero will result in undefined behavior or errors depending on the dtype.
* For integer dtypes, this performs integer division.

**Args:**

* â€‹other ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The scalar value to divide each element by.

`__itruediv__[other_layout: Layout](self, other: LayoutTensor[dtype, other_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Divide this tensor by another tensor elementwise in-place.

Performs an elementwise division between this tensor and another tensor.
This operation modifies the tensor in-place.

Limited broadcasting is supported:

* For tensors of the same rank, shapes must match exactly.
* For rank-1 to rank-2 broadcasting, the rank-1 tensor's dimension must
  match the corresponding dimension of the rank-2 tensor.

Performance:

* This operation modifies the tensor directly without creating a copy.

Notes:

* Division by zero will result in undefined behavior or errors depending on the dtype.
* For integer dtypes, this performs integer division.

**Parameters:**

* â€‹other\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the other tensor.

**Args:**

* â€‹other ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The tensor to divide this tensor by.

### `get_type_name`

`static get_type_name() -> String`

Gets the name of the host type (the one implementing this trait).

**Returns:**

`String`: The host type's name.

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name.

**Returns:**

`String`: The device type's name.

### `__merge_with__`

`__merge_with__[other_type: AnyStruct[LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]]](self) -> LayoutTensor[dtype, layout, origin_of((mutcast origin._mlir_origin), (mutcast origin._mlir_origin)), address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Returns a tensor merged with the specified `other_type`.

**Parameters:**

* â€‹other\_type (`AnyStruct`): The type of the tensor to merge with.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A tensor merged with the specified `other_type`.

### `bitcast`

`bitcast[new_dtype: DType, /, target_address_space: AddressSpace = address_space, _element_layout: Layout = element_layout](self) -> LayoutTensor[new_dtype, layout, origin, address_space=target_address_space, element_layout=_element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked]`

Bitcast the underlying pointer to a new data type.

**Parameters:**

* â€‹new\_dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The new data type it is casting to.
* â€‹target\_address\_space ([`AddressSpace`](/mojo/std/memory/pointer/AddressSpace)): The address space of the returned `LayoutTensor`.
* â€‹\_element\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The element layout of the returned `LayoutTensor`.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new `LayoutTensor` with the same memory location but with the
specified data type, address space, and element layout.

### `as_any_origin`

`as_any_origin(self: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> LayoutTensor[dtype, layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Casts the origin of the mutable `LayoutTensor` to `MutAnyOrigin`.

This requires the tensor to already be mutable as casting mutability
is inherently very unsafe.

It is usually preferred to maintain concrete origin values instead of
using `MutAnyOrigin`. However, if it is needed, keep in mind that
`MutAnyOrigin` can alias any memory value, so Mojo's ASAP
destruction will not apply during the lifetime of the tensor.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A pointer with the origin set to `MutAnyOrigin`.

`as_any_origin(self: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> LayoutTensor[dtype, layout, ImmutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Casts the origin of the immutable `LayoutTensor` to `ImmutAnyOrigin`.

It is usually preferred to maintain concrete origin values instead of
using `ImmutAnyOrigin`. However, if it is needed, keep in mind that
`ImmutAnyOrigin` can alias any memory value, so Mojo's ASAP
destruction will not apply during the lifetime of the tensor.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A tensor with the origin set to `ImmutAnyOrigin`.

### `address_space_cast`

`address_space_cast[target_address_space: AddressSpace = address_space](self) -> LayoutTensor[dtype, layout, origin, address_space=target_address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Changes the address space of the `LayoutTensor`.

**Parameters:**

* â€‹target\_address\_space ([`AddressSpace`](/mojo/std/memory/pointer/AddressSpace)): The new address space.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new `LayoutTensor` object with the same type and origin
as the original `LayoutTensor`, and the new specified address\_space.

### `get_immutable`

`get_immutable(self) -> LayoutTensor[dtype, layout, origin_of((muttoimm origin._mlir_origin)), address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Return an immutable version of this tensor.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A `LayoutTensor` covering the same elements, but without mutability.

### `ptr_at_offset`

`ptr_at_offset(self, coords: IndexList[size, element_type=element_type]) -> LegacyUnsafePointer[Scalar[dtype], address_space=address_space]`

Get a pointer offset at the given flattened coordinates.

**Args:**

* â€‹coords ([`IndexList`](/mojo/std/utils/index_/IndexList)): A flattened list of the offset coordinates.

**Returns:**

`LegacyUnsafePointer`: A pointer offset at the given flattened coordinates.

### `__exp__`

`__exp__(self) -> Self`

Computes element-wise exponential function.

Returns a new tensor containing the
[element-wise exponential](/mojo/std/math/math/exp/) of the input tensor.

**Returns:**

`Self`: A new tensor containing the element-wise exponential.

### `load_scalar`

`load_scalar[*Tys: Indexer](self, *args: *Tys) -> Scalar[dtype]`

Retrieves a single scalar from the tensor at the specified indices.

This method provides scalar element access for the tensor, which is
useful in generic contexts where `__getitem__` returns a SIMD vector
of `element_size` elements. This method always returns a single scalar
value (the 0th lane of the element).

The number of indices provided must match the rank of the tensor,
otherwise an error will occur at runtime.

**Parameters:**

* â€‹\*Tys ([`Indexer`](/mojo/std/builtin/int/Indexer)): The type of the indices. Must implement the `Indexer` trait,
  and match the rank of the tensor.

**Args:**

* â€‹\*args (`*Tys`): The indices specifying the element's position in the tensor.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The scalar value at the specified position with the tensor's dtype.

`load_scalar(self, crd: RuntimeTuple[S, element_type=element_type]) -> Scalar[dtype]`

Retrieves a single scalar from the tensor at the specified coordinates.

This method provides scalar element access for the tensor, which is
useful in generic contexts where `__getitem__` returns a SIMD vector
of `element_size` elements. This method always returns a single scalar
value (the 0th lane of the element).

**Args:**

* â€‹crd ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The coordinate specifying the element's position in each
  dimension. For example, in a 3D tensor, you would use (i, j, k).

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The scalar value at the specified position with the tensor's dtype.

### `load`

`load[width: Int](self, m: Int, n: Int) -> SIMD[dtype, width]`

Load a SIMD vector from the tensor at the specified 2D coordinates.

Performs a vectorized load operation from the tensor's memory,
retrieving `width` consecutive elements starting at position (m, n).
This method enables efficient SIMD operations on tensor data.

Performance:

* Uses unaligned memory access which may be slower on some
  architectures.
* For aligned access, use `aligned_load` instead when data alignment is
  guaranteed.
* The load operation is optimized based on the tensor's memory layout.

Notes:

* No bounds checking is performed. Accessing out-of-bounds indices will
  result in undefined behavior.
* The elements are loaded according to the tensor's stride configuration.

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): The number of elements to load into the SIMD vector. Should match
  the target hardware's vector width for optimal performance.

**Args:**

* â€‹m ([`Int`](/mojo/std/builtin/int/Int)): The row index (first dimension).
* â€‹n ([`Int`](/mojo/std/builtin/int/Int)): The column index (second dimension).

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): A SIMD vector containing 'width' consecutive elements from the tensor.

`load[width: Int](self, coords: IndexList[size, element_type=element_type]) -> SIMD[dtype, width]`

Load a SIMD vector from the tensor at the specified coordinates.

Performs a vectorized load operation from the tensor's memory,
retrieving `width` consecutive elements starting at the position specified
by `coords`. This method enables efficient SIMD operations on tensor data
and works with tensors of any rank.

Performance:

* Uses unaligned memory access which may be slower on some
  architectures.
* For aligned access, use `aligned_load` instead when data alignment is
  guaranteed.
* The load operation is optimized based on the tensor's memory layout.

Notes:

* No bounds checking is performed. Accessing out-of-bounds indices will
  result in undefined behavior.
* The elements are loaded according to the tensor's stride configuration.

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): The number of elements to load into the SIMD vector. Should match
  the target hardware's vector width for optimal performance.

**Args:**

* â€‹coords ([`IndexList`](/mojo/std/utils/index_/IndexList)): The coordinates to index. Must have the same size as the tensor's rank.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): A SIMD vector containing 'width' consecutive elements from the tensor.

### `prefetch`

`prefetch(self, m: Int, n: Int)`

Prefetch tensor data at the specified 2D coordinates into cache.

Issues a software prefetch hint to the processor to load the data at
position (m, n) into the cache hierarchy. This can improve performance
by reducing memory latency for subsequent accesses to the same location.

Performance:

* Prefetching is a performance hint and does not guarantee data will be
  cached.
* Most effective when issued sufficiently ahead of the actual data
  access.
* Uses high locality prefetch to the data cache, optimized for data that
  will be accessed multiple times.
* Can reduce memory access latency by 50-90% when used correctly.

Notes:

* Excessive prefetching can pollute the cache and degrade performance.
* Most beneficial for predictable access patterns that would otherwise
  cause cache misses.
* No operation is performed on the prefetched data.

**Args:**

* â€‹m ([`Int`](/mojo/std/builtin/int/Int)): The row index (first dimension).
* â€‹n ([`Int`](/mojo/std/builtin/int/Int)): The column index (second dimension).

`prefetch(self, coords: IndexList[size, element_type=element_type])`

Prefetch tensor data at the specified coordinates into cache.

Issues a software prefetch hint to the processor to load the data at
coords into the cache hierarchy. This can improve performance
by reducing memory latency for subsequent accesses to the same location.

Performance:

* Prefetching is a performance hint and does not guarantee data will be
  cached.
* Most effective when issued sufficiently ahead of the actual data
  access.
* Uses high locality prefetch to the data cache, optimized for data that
  will be accessed multiple times.
* Can reduce memory access latency by 50-90% when used correctly.

Notes:

* Excessive prefetching can pollute the cache and degrade performance.
* Most beneficial for predictable access patterns that would otherwise
  cause cache misses.
* No operation is performed on the prefetched data.

**Args:**

* â€‹coords ([`IndexList`](/mojo/std/utils/index_/IndexList)): The indices.

### `aligned_load`

`aligned_load[width: Int](self, m: Int, n: Int) -> SIMD[dtype, width]`

Load a SIMD vector with alignment guarantees from the tensor.

Performs an aligned vectorized load operation from the tensor's memory,
retrieving `width` consecutive elements starting at position (m, n). The
alignment is automatically calculated based on the SIMD width and dtype.

Performance:

* Uses aligned memory access which is faster than unaligned access on
  most architectures.
* The alignment is automatically calculated based on the SIMD width and
  dtype.
* Can be up to 2x faster than unaligned loads on architectures that
  require alignment.

Notes:

* The caller must ensure that the memory at (m, n) is properly aligned.
  Misaligned access with this method may cause hardware exceptions on
  some architectures.
* No bounds checking is performed. Accessing out-of-bounds indices will
  result in undefined behavior.

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): The number of elements to load into the SIMD vector. Should
  match the target hardware's vector width for optimal performance.

**Args:**

* â€‹m ([`Int`](/mojo/std/builtin/int/Int)): The row index (first dimension).
* â€‹n ([`Int`](/mojo/std/builtin/int/Int)): The column index (second dimension).

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): A SIMD vector containing 'width' consecutive elements from the tensor.

`aligned_load[width: Int](self, coords: IndexList[size, element_type=element_type]) -> SIMD[dtype, width]`

Load a SIMD vector with alignment guarantees from the tensor.

Performs an aligned vectorized load operation from the tensor's memory,
retrieving `width` consecutive elements starting at the position specified
by `coords`. The alignment is automatically calculated based on the SIMD width
and dtype. This method enables efficient SIMD operations on tensor data and
works with tensors of any rank.

Performance (copied from 'aligned\_load[width](m,n)'):

* Uses aligned memory access which is faster than unaligned access on
  most architectures.
* The alignment is automatically calculated based on the SIMD width and
  dtype.
* Can be up to 2x faster than unaligned loads on architectures that
  require alignment.

Notes:

* The caller must ensure that the memory at the specified coordinates is
  properly aligned. Misaligned access with this method may cause hardware
  exceptions on some architectures.
* No bounds checking is performed. Accessing out-of-bounds indices will
  result in undefined behavior.
* The elements are loaded according to the tensor's stride configuration.
* The last dimension must have unit stride (stride == 1) for this operation
  to be valid.

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): The number of elements to load into the SIMD vector. Should
  match the target hardware's vector width for optimal performance.

**Args:**

* â€‹coords ([`IndexList`](/mojo/std/utils/index_/IndexList)): The coordinates to index. Must have the same size as the tensor's rank.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): A SIMD vector containing 'width' consecutive elements from the tensor.

### `store`

`store[width: Int](self: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], m: Int, n: Int, val: SIMD[dtype, width])`

Store a SIMD vector to the tensor at the specified 2D coordinates.

Performs a vectorized store operation to the tensor's memory, writing
'width' consecutive elements starting at position (m, n). This method
enables efficient SIMD operations on tensor data.

Performance:

* Uses unaligned memory access which may be slower on some
  architectures.
* For aligned access, use aligned\_store instead when data alignment is
  guaranteed.
* The store operation is optimized based on the tensor's memory layout.

Notes:

* No bounds checking is performed. Accessing out-of-bounds indices will
  result in undefined behavior.
* The elements are stored according to the tensor's stride configuration.
* This operation modifies the tensor's data in-place.

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): The number of elements in the SIMD vector to store. Should
  match the target hardware's vector width for optimal performance.

**Args:**

* â€‹m ([`Int`](/mojo/std/builtin/int/Int)): The row index (first dimension) where the store operation begins.
* â€‹n ([`Int`](/mojo/std/builtin/int/Int)): The column index (second dimension) where the store operation
  begins.
* â€‹val ([`SIMD`](/mojo/std/builtin/simd/SIMD)): The SIMD vector containing the values to store in the tensor.

`store[width: Int](self, coords: IndexList[size, element_type=element_type], val: SIMD[dtype, width])`

Store a SIMD vector to the tensor at the specified ND coordinates.

Performs a vectorized store operation to the tensor's memory, writing
'width' consecutive elements starting at position (m, n). This method
enables efficient SIMD operations on tensor data.

Performance:

* Uses unaligned memory access which may be slower on some
  architectures.
* For aligned access, use aligned\_store instead when data alignment is
  guaranteed.
* The store operation is optimized based on the tensor's memory layout.

Notes:

* No bounds checking is performed. Accessing out-of-bounds indices will
  result in undefined behavior.
* The elements are stored according to the tensor's stride configuration.
* This operation modifies the tensor's data in-place.

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): The number of elements in the SIMD vector to store. Should
  match the target hardware's vector width for optimal performance.

**Args:**

* â€‹coords ([`IndexList`](/mojo/std/utils/index_/IndexList)): The coordinates to index.
* â€‹val ([`SIMD`](/mojo/std/builtin/simd/SIMD)): The SIMD vector containing the values to store in the tensor.

### `aligned_store`

`aligned_store[width: Int](self: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], m: Int, n: Int, val: SIMD[dtype, width])`

Store a SIMD vector with alignment guarantees to the tensor.

Performs an aligned vectorized store operation to the tensor's memory,
writing `width` consecutive elements starting at position (m, n). The
alignment is automatically calculated based on the SIMD width and dtype.

Performance:

* Uses aligned memory access which is faster than unaligned access on
  most architectures.
* The alignment is automatically calculated based on the SIMD width and
  dtype.
* Can be up to 2x faster than unaligned stores on architectures that
  require alignment.
* Particularly important for streaming stores that bypass the cache.

Notes:

* The caller must ensure that the memory at (m, n) is properly aligned.
  Misaligned access with this method may cause hardware exceptions on
  some architectures.
* No bounds checking is performed. Accessing out-of-bounds indices will
  result in undefined behavior.
* This operation modifies the tensor's data in-place.

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): The number of elements in the SIMD vector to store. Should
  match the target hardware's vector width for optimal performance.

**Args:**

* â€‹m ([`Int`](/mojo/std/builtin/int/Int)): The row index (first dimension) where the store operation begins.
* â€‹n ([`Int`](/mojo/std/builtin/int/Int)): The column index (second dimension) where the store operation
  begins.
* â€‹val ([`SIMD`](/mojo/std/builtin/simd/SIMD)): The SIMD vector containing the values to store in the tensor.

### `size`

`size(self) -> Int`

Get the total number of elements that the tensor can contain.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The total number of elements that can be stores in the tensor.

### `stack_allocation`

`static stack_allocation[*, stack_alignment: Int = alignment]() -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].StackTensorType`

Allocates stack memory for a `LayoutTensor` with a fully static layout.

Creates a new `LayoutTensor` instance with memory allocated on the stack
rather than the heap. This provides deterministic memory management and
potentially better performance for tensors with known sizes at compile
time.

Performance:

* Stack allocation is typically faster than heap allocation.
* Proper alignment can significantly improve memory access performance,
  especially for vectorized operations.
* No dynamic memory management overhead (no malloc/free calls).

Notes:

* Only works with tensors that have fully static layouts known at
  compile time.
* Stack memory is limited, so this should only be used for reasonably
  sized tensors.
* The allocated memory is automatically freed when the function returns.

**Constraints:**

* The layout must be fully static (all dimensions known at compile
  time).
* The alignment must be a multiple of the tensor's minimum required
  alignment.

**Parameters:**

* â€‹stack\_alignment ([`Int`](/mojo/std/builtin/int/Int)): Memory alignment value for the allocation in bytes. Must
  be a multiple of the tensor's minimum required alignment.
  Default is the tensor's natural alignment based on its data type
  and layout.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new `LayoutTensor` instance with memory allocated on the stack.

### `null`

`static null() -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].StackTensorType`

Returns a null `LayoutTensor` object.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A null `LayoutTensor` object.

### `to_device_buffer`

`to_device_buffer(self, ctx: DeviceContext) -> DeviceBuffer[dtype]`

Convert the tensor to a `DeviceBuffer`.

**Args:**

* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): The device context to use.

**Returns:**

[`DeviceBuffer`](/mojo/std/gpu/host/device_context/DeviceBuffer): A `DeviceBuffer` containing the tensor's data.

### `is_static_shape`

`static is_static_shape[idx: Int]() -> Bool`

Returns the whether the specified dimension is statically known.

Performance:

* This is a compile-time operation with no runtime cost when used
  with static dimensions.

Notes:

* This is a static method that operates on the tensor's type information,
  not on a specific tensor instance.

**Parameters:**

* â€‹idx ([`Int`](/mojo/std/builtin/int/Int)): The dimension index to query (0-based).
  For example, in a 3D tensor with shape \[10, UNKNOWN\_VALUE, 30]:
  \- `shape[0]()` returns True (first dimension).
  \- `shape[1]()` returns False (second dimension).
  \- `shape[2]()` returns True (third dimension).

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): The True if the dimension is statically known, False otherwise.

### `shape`

`static shape[idx: Int]() -> Int`

Returns the size of the tensor along the specified dimension.

Provides static access to the tensor's shape information. This method
returns the size of a specific dimension without requiring an instance
of the tensor, as the shape is part of the tensor's static type
information.

Performance:

* This is a compile-time operation with no runtime cost when used
  with static dimensions.

Notes:

* This is a static method that operates on the tensor's type information,
  not on a specific tensor instance.

**Parameters:**

* â€‹idx ([`Int`](/mojo/std/builtin/int/Int)): The dimension index to query (0-based).
  For example, in a 3D tensor with shape \[10, 20, 30]:
  * `shape[0]()` returns 10 (first dimension).
  * `shape[1]()` returns 20 (second dimension).
  * `shape[2]()` returns 30 (third dimension).

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The size of the tensor along the specified dimension as an integer.

### `get_shape`

`get_shape(self) -> IndexList[LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

Get the flattened shape of a LayoutTensor.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The flattened shape of a LayoutTensor.

### `get_stride`

`get_stride(self) -> IndexList[LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

Get the flattened stride of a LayoutTensor.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The flattened shape of a LayoutTensor.

### `stride`

`static stride[idx: Int]() -> Int`

Returns the memory stride of the tensor along the specified dimension.

Provides static access to the tensor's stride information. The stride
represents the number of elements to skip in memory to move one position
along a particular dimension. This method returns the stride without
requiring an instance of the tensor, as the stride is part of the
tensor's static type information.

Performance:

* This is a compile-time operation with no runtime cost when used
  with static dimensions.
* Understanding stride patterns is crucial for optimizing memory access
  patterns in performance-critical code.

Notes:

* Strides depend on the memory layout (row-major, column-major, or
  custom).
* For non-contiguous tensors (e.g., tensor slices), strides may not
  follow a simple pattern.

**Parameters:**

* â€‹idx ([`Int`](/mojo/std/builtin/int/Int)): The dimension index to query (0-based).
  For example, in a 2D tensor with shape \[10, 20] and row-major
  layout:
  * `stride[0]()` might return 20 (moving one row requires
    skipping 20 elements).
  * `stride[1]()` might return 1 (moving one column requires
    skipping 1 element).

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The memory stride of the tensor along the specified dimension as an
integer.

`stride(self, idx: Int) -> Int`

Returns the runtime stride of the tensor along the specified axis.

Unlike the static `stride` method, this instance method takes a runtime
dimension index.

**Args:**

* â€‹idx ([`Int`](/mojo/std/builtin/int/Int)): The dimension index to query (0-based).
  For example, in a row-major 3D tensor with shape `[10, 20, 30]`:
  * `stride(0)` returns 600 (first dimension).
  * `stride(1)` returns 30 (second dimension).
  * `stride(2)` returns 1 (third dimension).

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The dimension of the tensor along the specified axis as an integer.

### `dim`

`dim(self, idx: Int) -> Int`

Returns the runtime dimension size of the tensor along the specified axis.

Unlike the static `dim` method, this instance method takes a runtime
dimension index.

**Args:**

* â€‹idx ([`Int`](/mojo/std/builtin/int/Int)): The dimension index to query (0-based).
  For example, in a 3D tensor with shape `[10, 20, 30]`:
  * `dim(0)` returns 10 (first dimension).
  * `dim(1)` returns 20 (second dimension).
  * `dim(2)` returns 30 (third dimension).

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The dimension of the tensor along the specified axis as an integer.

`dim[idx: Int](self) -> Int`

Returns the dimension size of the tensor along the specified axis.

Unlike the static `shape` method, this instance method provides access
to the tensor's actual dimension sizes. If the dimension is unknown,
the runtime layout is used to get the dimension size.

Performance:

* For static dimensions known at compile time, prefer the static
  `shape` method when possible for better performance.

Notes:

* This method works with both static and dynamic dimensions.
* For tensors with masked or partial views, this returns the actual
  size of the view, not the original tensor.

**Constraints:**

* Only works with tensors that have depth-1 layouts (no nested
  shapes).

**Parameters:**

* â€‹idx ([`Int`](/mojo/std/builtin/int/Int)): The dimension index to query (0-based).
  For example, in a 3D tensor with shape `[10, 20, 30]`:
  * `dim[0]()` returns 10 (first dimension).
  * `dim[1]()` returns 20 (second dimension).
  * `dim[2]()` returns 30 (third dimension).

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The size of the tensor along the specified dimension as an integer.

### `coalesce`

`coalesce(self) -> LayoutTensor[dtype, coalesce(layout, False), origin, address_space=address_space, element_layout=element_layout]`

Creates a tensor with a coalesced memory layout from this tensor.

Coalescing a tensor's layout means reorganizing its memory
representation to be as contiguous as possible, which can improve memory
access patterns and performance. This operation does not move or copy
data; it only changes how the same memory is interpreted.

Performance:

* Coalesced layouts typically provide better cache utilization and
  memory access patterns.
* This operation is zero-cost at runtime as it only changes the
  layout information, not the actual data.
* Particularly beneficial before operations that perform sequential
  memory access or vectorized operations.

Notes:

* The coalesced tensor shares the same memory as the original tensor,
  so modifications to one will affect the other.
* The shape of the tensor remains the same, only the stride information
  is optimized.
* For already optimally coalesced tensors, this operation has no effect.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A tensor with the same data but with a coalesced memory layout.
The returned tensor has type `LayoutTensor` with the same dtype but
with a coalesced layout.

### `tile`

`tile[*tile_sizes: Int](self, *tile_coords: Int) -> LayoutTensor[dtype, LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, tile_sizes]()[0], origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked if masked else _tile_is_masked[layout, tile_sizes](), alignment=alignment]`

Extract a tile (sub-tensor) from this tensor with specified dimensions and position.

Tiling is a fundamental operation for high-performance tensor
computations that divides a tensor into smaller blocks for better cache
locality and parallelism. This method extracts a specific tile at the
given coordinates without copying data.

Example:

For a 4x4 tensor with values:

```
[1 2 3 4]
[2 3 4 5]
[5 4 3 2]
[1 1 1 1]
```

`tile[2, 2](1, 0)` will extract the tile:

```
[5 4]
[1 1]
```

Performance:

* Creates a view without copying data, making it very efficient.
* Optimized for both static and dynamic layouts with different code paths.
* Properly handles edge cases where tiles may be partially outside the tensor.
* Maintains stride information for efficient memory access within the tile.

Notes:

* The resulting tile is a view into the original tensor, so modifications
  to the tile will affect the original tensor.
* For tiles at the edges of the tensor, the actual dimensions may be smaller
  than the requested tile\_sizes if masking is enabled.
* The implementation automatically selects between static and dynamic tiling
  based on the tensor's layout properties.

**Parameters:**

* â€‹\*tile\_sizes ([`Int`](/mojo/std/builtin/int/Int)): The dimensions of each tile along each axis of the
  tensor. For example, in a 2D tensor, `tile[32, 32]` creates
  32x32 tiles.

**Args:**

* â€‹\*tile\_coords ([`Int`](/mojo/std/builtin/int/Int)): The coordinates of the specific tile to extract. For
  example, `tile[32, 32](1, 2)` extracts the tile at position
  (1, 2) in the grid of 32x32 tiles.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A view into the original tensor representing the specified tile.

### `simd_tile`

`simd_tile[tile_size: Int](self, tile_idx: Int) -> LayoutTensor[dtype, LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, tile_size, simd_width_of[dtype]()]()[0], origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked if masked else _tile_is_masked[layout, tile_size, simd_width_of[dtype]()](), alignment=alignment]`

Return a SIMD\[dtype] sized tile of size `tile_size` at `tile_idx`.

**Parameters:**

* â€‹tile\_size ([`Int`](/mojo/std/builtin/int/Int)): The size of the tile along the tiled axis used for
  vectorization.

**Args:**

* â€‹tile\_idx ([`Int`](/mojo/std/builtin/int/Int)): The index of the tile to extract along the tiled axis.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A SIMD\[dtype] tile of size `tile_size` at `tile_idx`

### `tile_with_offset`

`tile_with_offset[*tile_sizes: Int](self, *tile_coords: Int) -> Tuple[LayoutTensor[dtype, LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, tile_sizes]()[0], origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked if masked else _tile_is_masked[layout, tile_sizes](), alignment=alignment], LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].CornerCoordsType, Scalar[linear_idx_type]]`

Similar to `tile`, but also returns the corner coordinates of the tile as well as the offset.

**Parameters:**

* â€‹\*tile\_sizes ([`Int`](/mojo/std/builtin/int/Int)): The dimensions of each tile along each axis of the
  tensor.

**Args:**

* â€‹\*tile\_coords ([`Int`](/mojo/std/builtin/int/Int)): The coordinates of the specific tile to extract.

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple): A tuple containing:

* The extracted tile as a `LayoutTensor`.
* The corner coordinates of the tile.
* The offset of the tile.

### `tiled_iterator`

`tiled_iterator[*tile_sizes: Int, *, axis: Int = 0](self, *tile_coords: Int) -> LayoutTensorIter[dtype, LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, tile_sizes]()[0], origin, address_space=address_space, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked if masked else _tile_is_masked[layout, tile_sizes]()]`

Create an iterator that traverses tiles along a specified axis.

This method creates an iterator that allows efficient traversal of tiles
within a tensor. The iterator starts at the specified tile coordinates
and can move along the specified axis, providing access to consecutive
tiles.

Performance:

* Provides efficient sequential access to tiles with good cache
  locality.
* Optimized for both static and dynamic layouts with different code
  paths.
* Maintains stride information for efficient memory access within each
  tile.
* Properly handles edge cases where tiles may be partially outside the
  tensor.

Notes:

* The iterator provides views into the original tensor, so modifications
  through the iterator will affect the original tensor.
* For tiles at the edges of the tensor, the actual dimensions may be smaller
  than the requested tile\_sizes if masking is enabled.
* The iterator is not circular by default, meaning it will not wrap around
  when reaching the end of the tensor along the iteration axis.
* The implementation automatically selects between static and dynamic tiling
  based on the tensor's layout properties.

Example:

```mojo
var iter = tensor.tiled_iterator[16, 16, axis=0](0, 0)
for i in range(num_tiles_along_axis):
    var tile = iter.get()
    # Process tile
    iter.next()
```

**Parameters:**

* â€‹\*tile\_sizes ([`Int`](/mojo/std/builtin/int/Int)): The dimensions of each tile along each axis of the
  tensor. For example, in a 2D tensor, `tiled_iterator[32, 32]`
  creates an iterator over 32x32 tiles.
* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis along which the iterator will traverse. Default is 0
  (first dimension). For example, with axis=0, the iterator will
  move vertically through tiles.

**Args:**

* â€‹\*tile\_coords ([`Int`](/mojo/std/builtin/int/Int)): The starting coordinates of the tile where iteration
  begins.

**Returns:**

[`LayoutTensorIter`](/mojo/kernels/layout/layout_tensor/LayoutTensorIter): A `LayoutTensorIter` that can be used to traverse tiles along the
specified axis.

### `split`

`split[count: Int, axis: Int = 0](self) -> StaticTuple[LayoutTensor[dtype, LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, (layout.shape[axis].value() // count), axis]()[0], origin, address_space=address_space, element_layout=element_layout, alignment=alignment], count]`

Split the `LayoutTensor` along a axis and return a `StaticTuple` of `LayoutTensor`.

**Parameters:**

* â€‹count ([`Int`](/mojo/std/builtin/int/Int)): Number of portion to split.
* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis where the split is applied to.

**Returns:**

`StaticTuple`: A `StaticTuple` containing `count` `LayoutTensors`, each
representing an equal-sized partition of the original tensor along
the specified axis. Each partition has the same data type and memory
characteristics as the original tensor, but with a reduced size
along the split axis.

`split[axis: Int = 0, split_alignment: Int = 1](self, count: Int, idx: Int) -> LayoutTensor[dtype, layout.make_shape_unknown[axis](), origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type]`

Retrieve a specific partition of the tensor after splitting along a specified axis.

This method divides the tensor into 'count' partitions along the
specified axis and returns the partition at index 'idx'. The
partitioning is done with alignment considerations to optimize memory
access patterns.

Unlike the overloaded split method that returns all partitions, this
method returns only a single partition, making it more memory-efficient
for cases where only one partition is needed at a time.

Notes:

* The shape along the split axis becomes unknown at compile time.
* Only works with dimensions that have statically known sizes.
* The last partition may be smaller than others if the dimension size
  is not evenly divisible by `count`.
* Partition sizes are aligned up to the specified alignment value,
  which can improve performance for vectorized operations.

Performance:

* Uses aligned partitioning to improve memory access patterns.
* Avoids creating all partitions in memory, reducing memory usage.
* Maintains the original tensor's stride information for efficient
  element access within the partition.

**Constraints:**

* The dimension being split must have a statically known size.
* Cannot split dimensions with unknown or dynamic sizes.

**Parameters:**

* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis along which to split the tensor. Defaults to 0 (first
  dimension).
* â€‹split\_alignment ([`Int`](/mojo/std/builtin/int/Int)): Memory alignment value for the partition size. Defaults
  to 1.

**Args:**

* â€‹count ([`Int`](/mojo/std/builtin/int/Int)): The number of partitions to divide the tensor into.
* â€‹idx ([`Int`](/mojo/std/builtin/int/Int)): The index of the partition to return (0-based).

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A `LayoutTensor` representing the requested partition.

### `distribute`

`distribute[threads_layout: Layout, axis: OptionalReg[Int] = None, swizzle: OptionalReg[Swizzle] = None, submode_axis: OptionalReg[Int] = None](self, thread_id: UInt) -> LayoutTensor[dtype, _compute_distribute_layout[layout, threads_layout, axis]()[1], origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked if masked else _distribute_is_masked[layout, threads_layout, axis]() if is_nvidia_gpu() else False]`

Distribute tensor workload across multiple threads in a structured pattern.

This method partitions a tensor across multiple threads for parallel
processing, assigning each thread a specific portion of the tensor. The
distribution pattern is determined by the threads\_layout parameter,
which defines the logical arrangement of threads.

Example:

For a 4x4 row-major tensor distributed across 4 threads in a 2x2 row-major grid:

* Thread 0 will receive a LayoutTensor with a view into
  (0,0), (0,2), (2,0), (2,2) of the original tensor.
* Thread 1 will receive a LayoutTensor with a view into
  (0,1), (0,3), (2,1), (2,3) of the original tensor.
* Thread 2 will receive a LayoutTensor with a view into
  (1,0), (1,2), (3,0), (3,2) of the original tensor.
* Thread 3 will receive a LayoutTensor with a view into
  (1,1), (1,3), (3,1), (3,3) of the original tensor.

If axis=0 is specified with the same setup:

* Thread (0, 0) and Thread (0, 1) would get the same data (top half)
* Thread (1, 0) and Thread (1, 1) would get the same data (bottom half)

Performance:

* Creates a view without copying data, making it very efficient for
  parallel processing.
* The swizzle parameter can significantly improve cache locality and
  memory access patterns.
* Optimized for both static and dynamic layouts with different code
  paths.

Notes:

* The resulting tensor is a view into the original tensor, so
  modifications will affect the original tensor.
* For optimal performance, the `threads_layout` should match the
  hardware's thread organization (e.g., warp/wavefront size and shape).
* When using swizzling, carefully consider the memory access patterns to
  avoid cache thrashing or bank conflicts.
* This function is particularly useful for GPU programming where threads
  are organized in structured grids.

**Constraints:**

* For dynamic layouts, the shape must be known at runtime and the
  threads\_layout must be fully static.

**Parameters:**

* â€‹threads\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Defines the logical arrangement of threads (e.g.,
  2x2 grid of 4 threads). This layout determines how the tensor is
  partitioned.
* â€‹axis ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional. If specified, restricts distribution to only this
  axis. For example, with axis=0 in a 2D thread layout, threads
  that differ only in their second coordinate will receive the
  same data.
* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional. A function that remaps the distribution pattern
  to improve memory access patterns or cache locality.
* â€‹submode\_axis ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional. Specifies an axis for specialized
  distribution modes.

**Args:**

* â€‹thread\_id ([`UInt`](/mojo/std/builtin/uint/UInt)): The ID of the current thread (0-based).

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A view into the original tensor representing the portion assigned to
this thread.

### `distribute_with_offset`

`distribute_with_offset[threads_layout: Layout, axis: OptionalReg[Int] = None, swizzle: OptionalReg[Swizzle] = None, submode_axis: OptionalReg[Int] = None](self, thread_id: UInt) -> Tuple[LayoutTensor[dtype, _compute_distribute_layout[layout, threads_layout, axis]()[1], origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked if masked else _distribute_is_masked[layout, threads_layout, axis]() if is_nvidia_gpu() else False], IndexList[threads_layout.rank(), element_type=layout_int_type], Scalar[linear_idx_type]]`

Similar to `distribute`, but also returns the corner coordinates of the tile as well as the offset.

**Parameters:**

* â€‹threads\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the threads.
* â€‹axis ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): The axis to distribute along.
* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): An optional swizzle function.
* â€‹submode\_axis ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): An optional submode axis.

**Args:**

* â€‹thread\_id ([`UInt`](/mojo/std/builtin/uint/UInt)): The ID of the current thread (0-based).

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple): A tuple containing:

* The distributed tensor.
* The corner coordinates of the tile.
* The offset of the tile.

### `vectorize`

`vectorize[*vector_shape: Int](self) -> LayoutTensor[dtype, coalesce(LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, vector_shape]()[1], True), origin, address_space=address_space, element_layout=LayoutTensor._divide_tiles[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment, vector_shape]()[0], layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked]`

Reshape a tensor into a vectorized form for efficient SIMD operations.

This method transforms the tensor's logical layout to enable efficient
vectorized processing, treating blocks of elements as vector units. The
transformation is particularly useful for SIMD (Single Instruction
Multiple Data) operations and hardware acceleration.

Example:

For a 16x16 tensor, `vectorize[4, 4]` will produce a 4x4 tensor
where each element represents a 4x4 block from the original tensor.

Performance:

* Creates a view without copying data, making it very efficient.
* Enables hardware-accelerated vector operations on blocks of data.
* Improves cache locality by grouping related elements together.
* Particularly beneficial for operations that can leverage SIMD
  instructions.

Notes:

* The tensor dimensions must be divisible by the corresponding vector
  dimensions.
* For dimensions with unknown size, the corresponding vector dimension
  must be 1.
* The resulting tensor has the same data but a different logical
  organization.
* Modifications to the vectorized tensor affect the original tensor.
* This transformation is particularly useful for GPU and vector
  processor optimizations.

**Constraints:**

* Each tensor dimension must be divisible by the corresponding
  vector dimension.
* Vector dimensions must be smaller than or equal to the
  corresponding tensor dimensions.
* For dimensions with unknown size, the vector dimension must be 1.

**Parameters:**

* â€‹\*vector\_shape ([`Int`](/mojo/std/builtin/int/Int)): The dimensions of each vector unit along each axis of
  the tensor. or example, in a 2D tensor, `vectorize[4, 4]` treats
  4x4 blocks as vector units.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A view of the tensor with a vectorized layout, where each element in
the resulting tensor represents a vector of elements from the
original tensor.

`vectorize(self) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].SIMDVectorizedType`

Return a SIMD\[dtype] vectorized view of this tensor.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A `Self.VectorizedType[1, simd_width_of[Self.dtype]()]` view whose
width equals the SIMD width for the tensor's dtype.

### `slice`

`slice[d0_slice: Slice, d1_slice: Slice](self) -> LayoutTensor[dtype, LayoutTensor._compute_slice_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment](d0_slice, d1_slice), origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type]`

Extract a slice from a rank-2 tensor using slice objects.

This method creates a view into a subset of the tensor defined by the
slice specifications for each dimension. The slice is a continuous
region of the tensor with no gaps (step size must be 1).

Example:

For a 4x4 tensor, `t` with values:

```
[1 2 3 4]
[5 6 7 8]
[9 10 11 12]
[13 14 15 16]
```

```mojo
t.slice[Slice(1, 3), Slice(0, 2)]
```

will extract:

```
[5 6]
[9 10]
```

Performance:

* Creates a view without copying data, making it very efficient.
* Maintains the original tensor's stride information for efficient
  memory access.
* Zero-cost abstraction at runtime when used with compile-time constant
  slices.

Notes:

* The slice is a view into the original tensor, so modifications to the
  slice will affect the original tensor.
* Only supports rank-2 tensors. For higher-rank tensors, use the
  overloaded version with slice indices.
* The step size must be 1 (no gaps allowed in the slice).
* Slice bounds are not checked at runtime; accessing out-of-bounds
  indices will result in undefined behavior.

**Constraints:**

* Only works with rank-2 tensors.

**Parameters:**

* â€‹d0\_slice ([`Slice`](/mojo/std/builtin/builtin_slice/Slice)): Slice specification for the first dimension (rows).
  Defines the start and end indices for the slice along this
  dimension.
* â€‹d1\_slice ([`Slice`](/mojo/std/builtin/builtin_slice/Slice)): Slice specification for the second dimension (columns).
  Defines the start and end indices for the slice along this
  dimension.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A view into the original tensor representing the specified slice.

`slice[d0_slice: Slice, d1_slice: Slice, slice_indices: IndexList[2], __offset_dims: Int = (LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank - 2)](self, offsets: IndexList[__offset_dims]) -> LayoutTensor[dtype, LayoutTensor._compute_slice_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment](d0_slice, d1_slice, slice_indices.__getitem__[2, DType.int64, Int](0), slice_indices.__getitem__[2, DType.int64, Int](1)), origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type]`

Extract a 2D slice from a higher-rank tensor at specific indices.

This method creates a view into a 2D subset of a higher-rank tensor:

Selecting two dimensions to slice using the slice\_indices parameter.
Applying slice specifications to those dimensions.
Using fixed offsets for all other dimensions.

Example:

Given a 3x4x5 tensor, `t`, the following example extracts a 2x2 slice
from dimensions 0 and 2, with dimension 1 fixed at index 1.

```mojo
t.slice = t.slice[Slice(1, 3), Slice(0, 2), IndexList[2](0, 2)](1)
```

Performance:

* Creates a view without copying data, making it very efficient.
* Maintains the original tensor's stride information for efficient
  memory access.
* Zero-cost abstraction at runtime when used with compile-time constant
  slices.

Notes:

* The slice is a view into the original tensor, so modifications to the
  slice will affect the original tensor.
* The slice indices must be ordered (e.g., \[0, 2] is valid, \[2, 0] is
  not).
* The step size must be 1 (no gaps allowed in the slice).
* Slice bounds are not checked at runtime; accessing out-of-bounds
  indices will result in undefined behavior.

**Constraints:**

* Slice step size must be 1 (no gaps).
* Slice indices must be ordered (ascending).
* Tensor rank must be at least 2.

**Parameters:**

* â€‹d0\_slice ([`Slice`](/mojo/std/builtin/builtin_slice/Slice)): Slice specification for the first selected dimension.
* â€‹d1\_slice ([`Slice`](/mojo/std/builtin/builtin_slice/Slice)): Slice specification for the second selected dimension.
* â€‹slice\_indices ([`IndexList`](/mojo/std/utils/index_/IndexList)): Indices of the two dimensions to slice (must be
  ordered).
* â€‹\_\_offset\_dims ([`Int`](/mojo/std/builtin/int/Int)): Internal parameter representing number of fixed
  dimensions.

**Args:**

* â€‹offsets ([`IndexList`](/mojo/std/utils/index_/IndexList)): Fixed index values for all dimensions not being sliced.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A 2D view into the original tensor representing the specified slice.

### `slice_1d`

`slice_1d[d0_slice: Slice, slice_indices: IndexList[1], __offset_dims: Int = (LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank - 1)](self, offsets: IndexList[__offset_dims]) -> LayoutTensor[dtype, LayoutTensor._compute_slice_layout[mut, dtype, layout, origin, address_space, element_layout, layout_int_type, linear_idx_type, masked, alignment](d0_slice, slice_indices.__getitem__[1, DType.int64, Int](0)), origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type]`

Extract a 1D slice from a higher-rank tensor at a specific index.

This method creates a view into a 1D subset of a higher-rank tensor by:

1. Selecting one dimension to slice using the slice\_indices parameter
2. Applying a slice specification to that dimension
3. Using fixed offsets for all other dimensions

Example:

For a 3x4x5 tensor, `t`, the following example extracts a 1D slice from
dimension 0, with dimensions 1 and 2 fixed at indices 1 and 2:

```mojo
t.slice_1d[Slice(1, 3), IndexList[1](0)](1, 2)
```

Performance:

* Creates a view without copying data, making it very efficient.
* Maintains the original tensor's stride information for efficient
  memory access.
* Zero-cost abstraction at runtime when used with compile-time constant
  slices.

Notes:

* The slice is a view into the original tensor, so modifications
  to the slice will affect the original tensor.
* The step size must be 1 (no gaps allowed in the slice).
* Slice bounds are not checked at runtime; accessing out-of-bounds
  indices will result in undefined behavior.
* This function exists as a workaround for compiler limitations with
  overloading.

**Constraints:**

* Slice step size must be 1 (no gaps).
* Tensor rank must be at least 1.

**Parameters:**

* â€‹d0\_slice ([`Slice`](/mojo/std/builtin/builtin_slice/Slice)): Slice specification for the selected dimension.
* â€‹slice\_indices ([`IndexList`](/mojo/std/utils/index_/IndexList)): Index of the dimension to slice.
* â€‹\_\_offset\_dims ([`Int`](/mojo/std/builtin/int/Int)): Internal parameter representing number of fixed
  dimensions.

**Args:**

* â€‹offsets ([`IndexList`](/mojo/std/utils/index_/IndexList)): Fixed index values for all dimensions not being sliced.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A 1D view into the original tensor representing the specified slice.

### `transpose`

`transpose(self) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].TransposeType`

Create a transposed view of a tensor.

This method creates a view of the tensor with its dimensions swapped, effectively
converting rows to columns and columns to rows. The transposition is performed
without copying data, by adjusting the tensor's layout information.

Example:

For a 2x3 tensor with values:

```
[1 2 3]
[4 5 6]
```

`transpose()` will produce a 3x2 tensor:

```
[1 4]
[2 5]
[3 6]
```

Performance:

* Creates a view without copying data, making it very efficient.
* The operation is zero-cost at runtime as it only changes the layout
  information.
* Memory access patterns may be less efficient in the transposed view
  due to non-contiguous memory access, especially for row-major
  storage.

Notes:

* The transposed tensor shares the same memory as the original tensor,
  so modifications to one will affect the other.
* For optimal performance when repeatedly accessing the transposed data,
  consider creating a physical copy with the transposed layout.
* Transpose only works with statically known shapes.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A view of the tensor with dimensions transposed (rows become columns and vice versa).

### `reshape`

`reshape[dst_layout: Layout](self) -> LayoutTensor[dtype, dst_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Create a view of the tensor with a different shape.

This method creates a view of the tensor with a new shape, without changing
the underlying data. The total number of elements must remain the same.

Example:

Given a 2x6 row-major tensor, `reshape[Layout.col_major(3, 4)]()`
produces a 3x4 tensor with the same elements in column-major order.

Performance:

* Creates a view without copying data, making it very efficient.
* The operation is zero-cost at runtime as it only changes the layout
  information.
* Memory access patterns may change, potentially affecting performance
  depending on the original and target layouts.

Notes:

* The reshaped tensor shares the same memory as the original tensor,
  so modifications to one will affect the other.
* The total number of elements must remain the same after reshaping.
* The reshape operation assumes a row-major (C-style) memory layout.
* For tensors with complex strides or non-contiguous memory, reshaping
  may not produce the expected results.
* Masked tensors cannot be reshaped.

**Constraints:**

* Cannot reshape masked tensors.
* The total number of elements must be the same in both layouts.

**Parameters:**

* â€‹dst\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The target layout for the reshaped tensor. Must have the same
  total number of elements as the original tensor.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A view of the tensor with the new shape specified by dst\_layout.

`reshape[dst_layout: Layout](self, runtime_layout: RuntimeLayout[dst_layout]) -> LayoutTensor[dtype, dst_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Create a view of the tensor with a different shape.

This method creates a view of the tensor with a new shape, without changing
the underlying data. The total number of elements must remain the same.

Example:

Given a 2x6 row-major tensor, `reshape[Layout.col_major(3, 4)]()`
produces a 3x4 tensor with the same elements in column-major order.

Performance:

* Creates a view without copying data, making it very efficient.
* The operation is zero-cost at runtime as it only changes the layout
  information.
* Memory access patterns may change, potentially affecting performance
  depending on the original and target layouts.

Notes:

* The reshaped tensor shares the same memory as the original tensor,
  so modifications to one will affect the other.
* The total number of elements must remain the same after reshaping.
* The reshape operation assumes a row-major (C-style) memory layout.
* For tensors with complex strides or non-contiguous memory, reshaping
  may not produce the expected results.
* Masked tensors cannot be reshaped.

**Constraints:**

* Cannot reshape masked tensors.
* The total number of elements must be the same in both layouts.

**Parameters:**

* â€‹dst\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The target layout for the reshaped tensor. Must have the same
  total number of elements as the original tensor.

**Args:**

* â€‹runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The target RuntimeLayout for the reshaped tensor.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A view of the tensor with the new shape specified by dst\_layout.

### `flatten`

`flatten(self) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].FlattenedType`

Convert a LayoutTensor to a flattened dynamic layout.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A LayoutTensor to a flattened dynamic layout.

### `composition`

`composition[rhs_layout: Layout, dst_layout: Layout = composition(layout, rhs_layout)](self) -> LayoutTensor[dtype, dst_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type]`

Create a view of the tensor with a composed layout.

This method creates a view of the tensor with a new layout that is the
composition of the original layout with another layout. Layout
composition allows for complex transformations of the tensor's logical
structure without copying data.

Example:

For a 4x4 tensor with a standard row-major layout, composing with a
layout that represents a 2x2 tiling would result in a tensor that
logically views the data as 2x2 blocks.

Performance:

* Creates a view without copying data, making it very efficient.
* The operation is zero-cost at runtime as it only changes the layout information.
* Can be used to optimize memory access patterns for specific algorithms.

Notes:

* The composed tensor shares the same memory as the original tensor,
  so modifications to one will affect the other.
* Layout composition is a powerful tool for expressing complex data
  transformations like tiling, transposition, and reshaping in a
  unified framework.
* Understanding the mathematical properties of layout composition is
  important for correctly using this function.

**Constraints:**

* The layouts must be compatible for composition.
* The total number of elements must remain the same after
  composition.

**Parameters:**

* â€‹rhs\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout to compose with the tensor's current layout.
* â€‹dst\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The resulting layout after composition. Defaults to the
  composition of the tensor's layout with rhs\_layout.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A view of the tensor with the composed layout.

### `distance`

`distance(self, addr: LegacyUnsafePointer[Scalar[dtype], address_space=address_space, mut=mut, origin=origin]) -> Scalar[linear_idx_type]`

Calculate the element-wise distance between this tensor's pointer and another pointer.

This method computes the number of elements (not bytes) between the
tensor's pointer and the provided address. This is useful for
determining offsets within a larger memory allocation or for pointer
arithmetic operations.

Example:

If `tensor.ptr` points to an element at index 100 in a buffer, and
`addr` points to element at index 50, then `distance(addr)` returns 50.

Performance:

* This is a lightweight operation that only involves pointer arithmetic.
* The operation is optimized based on the address space, using smaller
  integer types for shared memory to improve efficiency.

Notes:

* The distance is calculated in elements, not bytes.
* The result can be positive or negative depending on the relative positions
  of the pointers.
* This function is particularly useful for GPU programming where understanding
  memory offsets is critical for performance.
* Care should be taken when using this with pointers from different allocations,
  as the result would be meaningless.

**Args:**

* â€‹addr (`LegacyUnsafePointer`): The target pointer to calculate the distance to.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The number of elements between this tensor's pointer and the
provided address. The result is of type `_uint_dtype`.

`distance[_layout: Layout, _uint_dtype: DType = _get_unsigned_type(_layout, address_space)](self, src: LayoutTensor[dtype, _layout, origin, address_space=address_space]) -> Scalar[_uint_dtype]`

Calculate the element-wise distance between this tensor and another tensor.

This method computes the number of elements (not bytes) between this
tensor's pointer and another tensor's pointer. This is useful for
determining the relative positions of tensors within a larger memory
allocation.

Example:

If tensor1 points to element at index 100 in a buffer, and tensor2 points
to element at index 50, then `tensor1.distance(tensor2)` would return 50.

Performance:

* This is a lightweight operation that only involves pointer arithmetic.
* The operation is optimized based on the address space and layout,
  using appropriate integer types for efficiency.

Notes:

* The distance is calculated in elements, not bytes.
* The result can be positive or negative depending on the relative
  positions of the tensors.
* This function is particularly useful for GPU programming where
  understanding memory offsets is critical for performance.
* Both tensors must be in the same address space for the result to be
  meaningful.
* This overload is more type-safe than the pointer-based version as it
  ensures the tensors have compatible data types and address spaces.

**Parameters:**

* â€‹\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the source tensor.
* â€‹\_uint\_dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The unsigned integer type to use for the result.
  Automatically determined based on the layout and address space.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor to calculate the distance to.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The number of elements between this tensor's pointer and the source
tensor's pointer. The result is of type \_uint\_dtype.

### `copy_from`

`copy_from(self: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], other: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Copy data from another tensor to this tensor.

This method performs an element-by-element copy from the source tensor
to this tensor, respecting the layouts of both tensors. The copy
operation handles different memory layouts correctly, ensuring that
elements are copied to their proper positions regardless of how the data
is arranged in memory.

* Both tensors must have statically known shapes.
* The total number of elements must be the same in both tensors.
* The element sizes must match between the tensors.

Example:

```mojo
from layout import LayoutTensor, Layout

var src_storage = InlineArray[Float32, 2 * 3](uninitialized=True)
var dst_storage = InlineArray[Float32, 3 * 2](uninitialized=True)
var src = LayoutTensor[
    DType.float32,
    Layout([2, 3]),
](src_storage).fill(1.0)

var dst = LayoutTensor[
    DType.float32,
    Layout([3, 2]),
](dst_storage)

dst.copy_from(src)  # Copies all elements from src to dst
```

Performance:

* Performs element-by-element copying, which may be less efficient than
  vectorized or bulk memory operations.
* The copy respects the memory layout of both tensors, which may involve
  non-contiguous memory access patterns.
* For optimal performance with large tensors, consider using specialized
  copy functions that can leverage hardware acceleration.

Notes:

* Both tensors must have statically known shapes.
* The total number of elements must be the same in both tensors.
* The element sizes must match between the tensors.
* This function handles different memory layouts correctly, making it suitable
  for copying between tensors with different shapes or strides.
* The copy is performed element by element, not as a bulk memory copy.

**Args:**

* â€‹other ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor to copy data from. Must have the same total
  number of elements as this tensor.

### `copy_from_async`

`copy_from_async[is_masked: Bool = False, swizzle: OptionalReg[Swizzle] = None, fill: Fill = Fill.NONE, eviction_policy: CacheEviction = CacheEviction.EVICT_NORMAL](self, src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src_idx_bound: Scalar[linear_idx_type] = 0, base_offset: Scalar[linear_idx_type] = 0)`

Asynchronously copy data from another tensor to this tensor using GPU hardware.

This method performs an asynchronous copy from the source tensor to this
tensor using GPU hardware acceleration. It's specifically designed for
copying data from global memory to shared memory in GPU kernels,
leveraging hardware-specific asynchronous copy mechanisms for improved
performance.

For optimal performance, you need to arrange the copy correctly. Use the
[`distribute()`](/mojo/kernels/layout/layout_tensor/LayoutTensor/#distribute)
method to create thread-local fragments of the source and
destination tensors, assigning each thread one or more elements to copy.

Optionally, use the
[`vectorize()`](/mojo/kernels/layout/layout_tensor/LayoutTensor/#vectorize)
method to get vectorized views of both tensors before calling
`distribute()`. This allows each thread to copy multiple elements of the
tensor. For example:

```mojo
var fragment = tensor.vectorize[1, simd_width]().distribute[
    thread_layout
](thread_id)
```

The copy operation is asynchronous, so you must call
[`async_copy_wait_all()`](/mojo/std/gpu/memory/memory/async_copy_wait_all/)
or
[`async_copy_wait_group()`](/mojo/std/gpu/memory/memory/async_copy_wait_group/)
to ensure the copy has completed before using the data.

Example:

```mojo
from layout import LayoutTensor, Layout
from gpu import thread_idx, block_idx, block_dim
from gpu.memory import async_copy_wait_all

comptime dtype = DType.float32
comptime in_size = 128
comptime block_size = 16
num_blocks = in_size // block_size
comptime input_layout = Layout.row_major(in_size, in_size)

fn kernel(tensor: LayoutTensor[dtype, input_layout, MutAnyOrigin]):
    # extract a tile from the input tensor.
    var global_tile = tensor.tile[block_size, block_size](block_idx.x, block_idx.y)

    # allocate a shared memory tile
    comptime tile_layout = Layout.row_major(block_size, block_size)
    var shared_tile = LayoutTensor[
        dtype,
        tile_layout,
        MutAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    # Create per-thread tile fragments for copying
    var tid = thread_idx.y + thread_idx.x * block_dim.x
    comptime thread_layout = Layout.row_major(block_size, block_size)
    var global_fragment = global_tile.distribute[thread_layout](tid)
    var shared_fragment = shared_tile.distribute[thread_layout](tid)

    # async copy to shared memory
    shared_fragment.copy_from_async(global_fragment)
    async_copy_wait_all()
    # ... do something with the shared tile
```

Performance:

* Supports vectorized copies for 4, 8, or 16-byte elements for better
  throughput.
* Can bypass L1 cache with appropriate eviction policies for specific
  access patterns.
* Swizzling can improve memory access patterns and reduce bank
  conflicts.

Notes:

* For vectorized copies, both tensors must have contiguous element
  layouts.
* Asynchronous copies allow computation to overlap with memory
  transfers.
* A synchronization barrier is required before using the copied data.

**Constraints:**

* Destination must be in shared memory.
* Source and destination data types must match.
* Element size must be 4, 8, or 16 bytes.
* Destination tensor must have a static layout.

**Parameters:**

* â€‹is\_masked ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to perform a masked copy, where elements outside
  the `src_idx_bound` are not copied or filled with zeros.
* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional swizzling function to rearrange the destination
  indices, which can improve memory access patterns.
* â€‹fill ([`Fill`](/mojo/std/gpu/memory/memory/Fill)): Fill policy for elements that are not copied (only used with
  masked copies).
* â€‹eviction\_policy ([`CacheEviction`](/mojo/std/gpu/memory/memory/CacheEviction)): Cache eviction policy for the source data.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor to copy data from.
* â€‹src\_idx\_bound ([`Scalar`](/mojo/std/builtin/simd/#scalar)): For masked copies, the upper bound index for valid
  source elements.
* â€‹base\_offset ([`Scalar`](/mojo/std/builtin/simd/#scalar)): Base offset for swizzling calculations.

### `fill`

`fill[*, use_runtime_layout: Bool = layout.all_dims_known().__bool__().__invert__() if (xor layout.all_dims_known()._mlir_value, True) else (layout.size() > 2048)](self: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], val: Scalar[dtype]) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Fill the entire tensor with a single value.

This method sets all elements of the tensor to the specified value. It
works with both statically and dynamically shaped tensors.

For statically known layouts, the fill operation is unrolled at compile
time. For dynamic layouts, a runtime loop is used. No vectorization is
applied, so performance may be suboptimal for large tensors. Consider
using hardware-specific fill operations for better performance with
large tensors.

This method can be used with tensors of any rank and shape. The
fill operation respects the tensor's layout, filling all
elements regardless of how they are arranged in memory. For
tensors with `element_layout`, all elements within each logical element
are filled with the same value.

Example:

```mojo
from layout import Layout, LayoutTensor

def main():
    var storage = InlineArray[Float32, 3 * 4](uninitialized=True)
    var tensor = LayoutTensor[
        DType.float32,
        Layout([3, 4]),
    ](storage).fill(0.0)
    print(tensor)
```

If not using method chaining, you can either reassign the result to the
tensor variable, or assign the result to the discard pattern (`_`) to
avoid warnings about an unused value:

```mojo
tensor = tensor.fill(0.0)
# or
_ = tensor.fill(0.0)
```

**Parameters:**

* â€‹use\_runtime\_layout ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to use the runtime layout for filling.
  This parameter is defaulted to `True` if the layout is not
  statically known. If loop bounds are too large, it's better to
  use the runtime layout to avoid long compilation time.

**Args:**

* â€‹val ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The value to fill the tensor with. Must be of the same data
  type as the tensor.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): The tensor itself (self), allowing for method chaining.

### `__str__`

`__str__(self) -> String`

Convert the tensor to a string representation.

This method converts the tensor to a human-readable string
representation by writing its contents to a string. It delegates to the
`write_to` method which formats the tensor appropriately based on its
rank and shape.

**Returns:**

`String`: A string representation of the tensor.

### `write_to`

`write_to(self, mut writer: T)`

Format and write the tensor's contents to a writer.

This method formats the tensor's contents and writes them to the
provided writer. For 2D tensors, it formats the output in a 2D grid. For
tensors of other ranks, it prints all values in column-major coordinate
order.

Example:

```mojo
from layout import Layout, LayoutTensor

def main():
    var storage = InlineArray[Float32, 2 * 3](uninitialized=True)
    var tensor = LayoutTensor[
        DType.float32,
        Layout([2, 3]),
    ](storage).fill(1.0)
    print(tensor)  # Internally calls `write_to` with a StringWriter
```

Output for a 2x3 tensor:

```
[[1.0, 1.0, 1.0],
    [1.0, 1.0, 1.0]]
```

Notes:

* For 2D tensors, the output is formatted as a 2D grid with rows and
  columns.
* For tensors of other ranks, values are printed in column-major
  coordinate order.
* Empty tensors (size 0) produce no output.
* This method is used by the `__str__` method to convert the tensor to a
  string.
* The formatting is designed for human readability rather than parsing.
* For large tensors, the output may be truncated to avoid excessive
  output.

**Args:**

* â€‹writer (`T`): The writer instance to write the formatted output to.

</section>

---

## LayoutTensorIter

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct LayoutTensorIter[mut: Bool, //, dtype: DType, layout: Layout, origin: Origin[mut=mut], /, *, address_space: AddressSpace = AddressSpace.GENERIC, alignment: Int = align_of[dtype](), circular: Bool = False, axis: OptionalReg[Int] = None, layout_int_type: DType = _get_index_type(address_space), linear_idx_type: DType = _get_index_type(address_space), masked: Bool = False]`

Iterator for traversing a memory buffer with a specific layout.

`LayoutTensorIter` provides a way to iterate through memory according to a
specific layout pattern, constructing layout tensors at each position. This
enables efficient traversal of multi-dimensional data structures with custom
memory layouts.

Notes:

The returned layout tensor is NOT vectorized. Users should explicitly vectorize
if needed for performance-critical operations.

## Parameters

* â€‹mut ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether the iterator allows mutation of the underlying data.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the tensor elements.
* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The memory layout pattern to follow during iteration.
* â€‹origin ([`Origin`](/mojo/std/builtin/type_aliases/Origin)): Origin tracking for memory safety.
* â€‹address\_space ([`AddressSpace`](/mojo/std/memory/pointer/AddressSpace)): The memory address space (`GLOBAL`, `SHARED`, etc.).
* â€‹alignment ([`Int`](/mojo/std/builtin/int/Int)): Memory alignment requirement for the data.
* â€‹circular ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether iteration wraps around at boundaries.
* â€‹axis ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional axis for dimension-specific operations.
* â€‹layout\_int\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Integer type used for layout indices.
* â€‹linear\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Integer type used for indexing into memory.
* â€‹masked ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to apply bounds masking during iteration.

## Fields

* â€‹ptr (`LegacyUnsafePointer[Scalar[dtype], address_space=address_space, mut=mut, origin=origin]`): Pointer to the memory region being iterated, with appropriate type and memory attributes.
* â€‹offset (`LayoutTensorIter[dtype, layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked].linear_uint_type`): Current offset from the base pointer, representing the iterator's position in memory.
* â€‹stride (`LayoutTensorIter[dtype, layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked].linear_uint_type`): Step size between consecutive elements or blocks in memory during iteration.
* â€‹bound (`LayoutTensorIter[dtype, layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked].linear_uint_type`): Upper bound of the memory region, limiting the iteration range.
* â€‹runtime\_layout (`LayoutTensorIter[dtype, layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked].RuntimeLayoutType`): Runtime representation of the layout pattern used for mapping logical indices to memory locations.
* â€‹dimension\_bound (`LayoutTensorIter[dtype, layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked].layout_uint_type`): Boundary value for the current dimension when iterating along a specific axis.
* â€‹idx (`LayoutTensorIter[dtype, layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked].linear_uint_type`): Current logical index position within the iteration sequence.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `BitcasType`

`comptime BitcasType[new_type: DType, *, address_space: AddressSpace = address_space, alignment: Int = alignment] = LayoutTensorIter[new_type, layout, origin, address_space=address_space, alignment=alignment, circular=circular, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked]`

Type alias for bitcast iterator types.

#### Parameters

* â€‹new\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The target data type.
* â€‹address\_space ([`AddressSpace`](/mojo/std/memory/pointer/AddressSpace)): The target address space.
* â€‹alignment ([`Int`](/mojo/std/builtin/int/Int)): The target memory alignment.

### `layout_uint_type`

`comptime layout_uint_type = Scalar[_unsigned_integral_type_of[layout_int_type]()]`

The unsigned integer type used for layout, based on layout and address space.

### `LayoutTensorType`

`comptime LayoutTensorType = LayoutTensor[dtype, layout, origin, address_space=address_space, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

The LayoutTensor type returned by this iterator.

### `linear_uint_type`

`comptime linear_uint_type = Scalar[_unsigned_integral_type_of[linear_idx_type]()]`

The unsigned integer type used for indexing into memory.

### `ReshapeType`

`comptime ReshapeType[dst_layout: Layout] = LayoutTensorIter[dtype, dst_layout, origin, address_space=address_space, alignment=alignment, circular=circular, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked]`

Type alias for reshaped iterator types.

#### Parameters

* â€‹dst\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The target layout for the reshaped iterator.

### `RuntimeLayoutType`

`comptime RuntimeLayoutType = RuntimeLayout[layout, element_type=layout_int_type, linear_idx_type=linear_idx_type]`

Type alias for the runtime layout.

## Methods

### `__init__`

`__init__() -> Self`

Initialize an empty iterator.

Creates a default iterator with zero values, typically used as a
placeholder or default value.

`__init__(ptr: LegacyUnsafePointer[Scalar[dtype], address_space=address_space, mut=mut, origin=origin], bound: Scalar[_unsigned_integral_type_of[linear_idx_type]()], stride: Scalar[_unsigned_integral_type_of[linear_idx_type]()] = layout.size(), offset: Scalar[_unsigned_integral_type_of[linear_idx_type]()] = 0) -> Self`

Initialize an iterator with a pointer and basic parameters.

Creates an iterator for a memory region with the specified bounds and
stride.

**Constraints:**

The layout must have all dimensions known at compile time.

**Args:**

* â€‹ptr (`LegacyUnsafePointer`): Pointer to the beginning of the memory region.
* â€‹bound ([`Scalar`](/mojo/std/builtin/simd/#scalar)): Upper bound of the memory region.
* â€‹stride ([`Scalar`](/mojo/std/builtin/simd/#scalar)): Step size between consecutive elements (defaults to layout
  size).
* â€‹offset ([`Scalar`](/mojo/std/builtin/simd/#scalar)): Initial offset from the base pointer.

`__init__(ptr: LegacyUnsafePointer[Scalar[dtype], address_space=address_space, mut=mut, origin=origin], bound: Scalar[_unsigned_integral_type_of[linear_idx_type]()], runtime_layout: RuntimeLayout[layout, element_type=element_type, linear_idx_type=linear_idx_type], stride: Scalar[_unsigned_integral_type_of[linear_idx_type]()] = layout.size() if layout.all_dims_known() else -1, offset: Scalar[_unsigned_integral_type_of[linear_idx_type]()] = 0, dimension_bound: Scalar[_unsigned_integral_type_of[layout_int_type]()] = 0, idx: Scalar[_unsigned_integral_type_of[linear_idx_type]()] = 0) -> Self`

Initialize an iterator with a runtime layout.

Creates an iterator with a runtime-determined layout, allowing for more
flexible memory traversal patterns.

**Constraints:**

The runtime layout must have the same bitwidth as specified for the
iterator. Circular iteration is not supported when an axis is
defined.

**Args:**

* â€‹ptr (`LegacyUnsafePointer`): Pointer to the beginning of the memory region.
* â€‹bound ([`Scalar`](/mojo/std/builtin/simd/#scalar)): Upper bound of the memory region.
* â€‹runtime\_layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): Layout determined at runtime.
* â€‹stride ([`Scalar`](/mojo/std/builtin/simd/#scalar)): Step size between consecutive elements.
* â€‹offset ([`Scalar`](/mojo/std/builtin/simd/#scalar)): Initial offset from the base pointer.
* â€‹dimension\_bound ([`Scalar`](/mojo/std/builtin/simd/#scalar)): Bound for the specified dimension when using masked
  iteration.
* â€‹idx ([`Scalar`](/mojo/std/builtin/simd/#scalar)): Initial index position.

### `__getitem__`

`__getitem__(self) -> LayoutTensorIter[dtype, layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked].LayoutTensorType`

Get the layout tensor at the current iterator position.

Operator overload that returns a layout tensor representing the data
at the current position of the iterator.

**Returns:**

[`LayoutTensorIter`](/mojo/kernels/layout/layout_tensor/LayoutTensorIter): A layout tensor at the current iterator position.

### `__iadd__`

`__iadd__[T: Intable](mut self, rhs: T)`

Increment the iterator by an integer value.

Advances the iterator by the specified number of positions.

Notes:

This function is unsafe. It omits bound checking for performance
reasons. Caller must ensure the index doesn't go out-of-bounds.

**Parameters:**

* â€‹T ([`Intable`](/mojo/std/builtin/int/Intable)): A type that can be converted to an integer.

**Args:**

* â€‹rhs (`T`): The number of positions to advance.

`__iadd__(mut self, rhs: Scalar[_unsigned_integral_type_of[linear_idx_type]()])`

Increment the iterator by a uint value.

Advances the iterator by the specified number of positions.

Notes:

This function is unsafe. It omits bound checking for performance
reasons. Caller must ensure the index doesn't go out-of-bounds.

**Args:**

* â€‹rhs ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The number of positions to advance.

### `get`

`get(self) -> LayoutTensorIter[dtype, layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked].LayoutTensorType`

Get the layout tensor at the current iterator position.

Returns a layout tensor representing the data at the current position
of the iterator.

**Returns:**

[`LayoutTensorIter`](/mojo/kernels/layout/layout_tensor/LayoutTensorIter): A tensor view at the current iterator position with the
same type, layout, and memory characteristics as specified by the
output parameter.

### `next`

`next[T: Intable](self, rhs: T) -> Self`

Return an iterator pointing to a position ahead by rhs steps.

Creates a new iterator that points rhs positions ahead of the current
one.

**Parameters:**

* â€‹T ([`Intable`](/mojo/std/builtin/int/Intable)): An integer-convertible type for the step size.

**Args:**

* â€‹rhs (`T`): The number of positions to advance.

**Returns:**

`Self`: A new iterator pointing to the advanced position.

`next(self, rhs: Scalar[_unsigned_integral_type_of[linear_idx_type]()] = 1) -> Self`

Return an iterator pointing to a position ahead by rhs steps.

Creates a new iterator that points rhs positions ahead of the current
one.

**Args:**

* â€‹rhs ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The number of positions to advance (defaults to 1).

**Returns:**

`Self`: A new iterator pointing to the advanced position.

### `next_unsafe`

`next_unsafe(self, rhs: Scalar[_unsigned_integral_type_of[linear_idx_type]()] = 1) -> Self`

Return an iterator pointing to a position ahead by rhs steps (unsafe version).

Creates a new iterator that points rhs positions ahead of the current
one. This is an unsafe version that omits certain checks for
performance.

**Constraints:**

Cannot be used with masked iterators.
User must ensure rhs < bound / stride.

**Args:**

* â€‹rhs ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The number of positions to advance (defaults to 1).

**Returns:**

`Self`: A new iterator pointing to the advanced position.

### `reshape`

`reshape[dst_layout: Layout](self) -> LayoutTensorIter[dtype, dst_layout, origin, address_space=address_space, alignment=alignment, circular=circular, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked]`

Reshape the iterator to a new layout.

This method creates a new iterator with a different layout while
preserving the underlying data. The new layout must have the same total
size as the original.

**Constraints:**

* The destination layout must have the same total size as the original.
* Both layouts must be contiguous.
* Both layouts must have compile-time known dimensions.

**Parameters:**

* â€‹dst\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The target layout to reshape to.

**Returns:**

[`LayoutTensorIter`](/mojo/kernels/layout/layout_tensor/LayoutTensorIter): A new iterator with the specified layout.

### `bitcast`

`bitcast[new_type: DType, *, target_address_space: AddressSpace = address_space, target_alignment: Int = alignment](self) -> LayoutTensorIter[new_type, layout, origin, address_space=address_space, alignment=alignment, circular=circular, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked]`

Reinterpret the iterator's underlying pointer as a different data type.

This method performs a bitcast operation, allowing you to view the same
memory location as a different data type without copying or converting
the data.

**Parameters:**

* â€‹new\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The target data type to cast to.
* â€‹target\_address\_space ([`AddressSpace`](/mojo/std/memory/pointer/AddressSpace)): The memory address space for the new
  iterator (defaults to current).
* â€‹target\_alignment ([`Int`](/mojo/std/builtin/int/Int)): Memory alignment requirement for the new
  iterator (defaults to current).

**Returns:**

[`LayoutTensorIter`](/mojo/kernels/layout/layout_tensor/LayoutTensorIter): A new LayoutTensorIter with the same layout but different data type.

</section>

---

## ThreadScope

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ThreadScope`

Represents the scope of thread operations in GPU programming.

This struct defines the scope at which thread operations are performed,
particularly for operations like tensor distribution and synchronization.
It provides two main scopes: `BLOCK` and `WARP`, which correspond to
different levels of thread grouping in GPU programming models.

Example:

```mojo
from layout.layout_tensor import copy_dram_to_sram, ThreadScope

# Distribute tensor at block level (all threads in block participate)
copy_dram_to_sram[layout, thread_scope=ThreadScope.BLOCK](dst, src)

# Distribute tensor at warp level (only threads in same warp participate)
copy_dram_to_sram[layout, thread_scope=ThreadScope.WARP](dst, src)
```

Performance:

* WARP scope operations typically have lower synchronization overhead
  than BLOCK scope operations.
* BLOCK scope operations allow coordination across all threads in a block,
  which is necessary for certain algorithms.
* The choice of scope can significantly impact performance and correctness
  of parallel algorithms.

Notes:

* The appropriate scope depends on the specific algorithm and hardware.
* WARP scope operations may be more efficient for operations that only
  require coordination within a warp.
* BLOCK scope operations are necessary when threads from different warps
  need to coordinate.
* The actual size of a warp or block is hardware-dependent.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `BLOCK`

`comptime BLOCK = ThreadScope(0)`

Represents operations at the thread block level, where all threads in a block participate.

### `WARP`

`comptime WARP = ThreadScope(1)`

Represents operations at the warp level, where only threads within the same warp participate.

## Methods

### `__init__`

`__init__(value: Int) -> Self`

Initialize a `ThreadScope` with the given integer value.

**Args:**

* â€‹value ([`Int`](/mojo/std/builtin/int/Int)): An integer representing the thread scope (0 for `BLOCK`,
  1 for `WARP`).

### `__eq__`

`__eq__(self, other: Self) -> Bool`

Compare two `ThreadScope` objects for equality.

**Args:**

* â€‹other (`Self`): Another `ThreadScope` object to compare with.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the thread scopes are equal, False otherwise.

### `__ne__`

`__ne__(self, other: Self) -> Bool`

Compare two `ThreadScope` objects for inequality.

**Args:**

* â€‹other (`Self`): Another `ThreadScope` object to compare with.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the thread scopes are not equal, False otherwise.

### `__str__`

`__str__(self) -> String`

Convert the `ThreadScope` to a human-readable string representation.

Aborts:
If the thread scope has an invalid value.

**Returns:**

`String`: A string representation of the thread scope ("BLOCK" or "WARP").

### `__int__`

`__int__(self) -> Int`

Convert the `ThreadScope` to an integer value.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The integer value of the thread scope (0 for BLOCK, 1 for WARP).

</section>

---

## copy_dram_to_local

<section class='mojo-docs'>

`copy_dram_to_local[src_thread_layout: Layout, num_threads: Int = src_thread_layout.size(), thread_scope: ThreadScope = ThreadScope.BLOCK, block_dim_count: Int = 1, cache_policy: CacheOperation = CacheOperation.ALWAYS](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src_base: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], offset: OptionalReg[UInt] = None)`

Efficiently copy data from global memory (DRAM) to registers for AMD GPUs.

This function implements an optimized memory transfer operation specifically
for AMD GPU architectures. It utilizes the hardware's buffer\_load intrinsic
to efficiently transfer data from global memory to registers while handling
bounds checking. The function distributes the copy operation across multiple
threads for maximum throughput.

Notes:

* The offset calculation method significantly impacts performance.
  Current implementation optimizes for throughput over flexibility.
* This function is particularly useful for prefetching data into registers
  before performing computations, reducing memory access latency.

**Constraints:**

* Only supported on AMD GPUs.
* The destination element layout size must match the SIMD width.
* Source fragments must be rank 2 with known dimensions.

**Parameters:**

* â€‹src\_thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout used to distribute the source tensor
  across threads. This determines how the workload is divided among
  participating threads.
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): Total number of threads participating in the copy
  operation. Defaults to the size of thread\_layout.
* â€‹thread\_scope ([`ThreadScope`](/mojo/kernels/layout/layout_tensor/ThreadScope)): Defines whether operations are performed at `BLOCK` or
  `WARP` level. `BLOCK` scope involves all threads in a thread block,
  while `WARP` scope restricts operations to threads within the same
  warp. Defaults to `ThreadScope.BLOCK`.
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the thread block.
* â€‹cache\_policy ([`CacheOperation`](/mojo/std/gpu/memory/memory/CacheOperation)): The cache policy to use for the copy operation.
  Defaults to `CacheOperation.ALWAYS`.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor in register memory (LOCAL address space).
* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor in global memory (DRAM) to be copied.
* â€‹src\_base ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The original global memory tensor from which src is derived.
  This is used to construct the buffer struct required by AMD's
  `buffer_load` intrinsic.
* â€‹offset ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): The offset in the global memory.

`copy_dram_to_local[src_thread_layout: Layout, num_threads: Int = src_thread_layout.size(), thread_scope: ThreadScope = ThreadScope.BLOCK, block_dim_count: Int = 1, cache_policy: CacheOperation = CacheOperation.ALWAYS](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src_iter: LayoutTensorIter[dtype, layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], bounds: UInt32)`

Efficiently copy data from global memory (DRAM) to registers for AMD GPUs.

This function implements an optimized memory transfer operation specifically
for AMD GPU architectures. It utilizes the hardware's buffer\_load intrinsic
to efficiently transfer data from global memory to registers while handling
bounds checking. The function distributes the copy operation across multiple
threads for maximum throughput.

Notes:

* The offset calculation method significantly impacts performance.
  Current implementation optimizes for throughput over flexibility.
* This function is particularly useful for prefetching data into registers
  before performing computations, reducing memory access latency.

**Constraints:**

* Only supported on AMD GPUs.
* The destination element layout size must match the SIMD width.
* Source fragments must be rank 2 with known dimensions.

**Parameters:**

* â€‹src\_thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout used to distribute the source tensor
  across threads. This determines how the workload is divided among
  participating threads.
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): Total number of threads participating in the copy
  operation. Defaults to the size of thread\_layout.
* â€‹thread\_scope ([`ThreadScope`](/mojo/kernels/layout/layout_tensor/ThreadScope)): Defines whether operations are performed at `BLOCK` or
  `WARP` level. `BLOCK` scope involves all threads in a thread block,
  while `WARP` scope restricts operations to threads within the same
  warp. Defaults to `ThreadScope.BLOCK`.
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the thread block.
* â€‹cache\_policy ([`CacheOperation`](/mojo/std/gpu/memory/memory/CacheOperation)): The cache policy to use for the copy operation.
  Defaults to `CacheOperation.ALWAYS`.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor in register memory (LOCAL address space).
* â€‹src\_iter ([`LayoutTensorIter`](/mojo/kernels/layout/layout_tensor/LayoutTensorIter)): The source tensor iterator.
* â€‹bounds ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Bounds of the buffer, based on the ptr of the src\_iter.

`copy_dram_to_local[src_thread_layout: Layout, num_threads: Int = src_thread_layout.size(), thread_scope: ThreadScope = ThreadScope.BLOCK, block_dim_count: Int = 1](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Efficiently copy data from global memory (DRAM) to registers.

This function implements an optimized memory transfer operation from
global memory to register memory. It distributes the copy operation across
multiple threads for maximum throughput while handling bounds checking for
safety.

**Constraints:**

* The source tensor must be in GLOBAL address space (DRAM).
* The destination tensor must be in LOCAL address space (registers).
* Both tensors must have compatible data types.

**Parameters:**

* â€‹src\_thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout used to distribute the source tensor
  across threads. This determines how the workload is divided among
  participating threads.
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): Total number of threads participating in the copy
  operation. Defaults to the size of thread\_layout.
* â€‹thread\_scope ([`ThreadScope`](/mojo/kernels/layout/layout_tensor/ThreadScope)): Defines whether operations are performed at `BLOCK` or
  `WARP` level. `BLOCK` scope involves all threads in a thread block,
  while `WARP` scope restricts operations to threads within the same
  warp. Defaults to `ThreadScope.BLOCK`.
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the thread block.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor in register memory (LOCAL address space).
* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor in global memory (DRAM).

</section>

---

## copy_dram_to_sram

<section class='mojo-docs'>

`copy_dram_to_sram[src_thread_layout: Layout, dst_thread_layout: Layout = src_thread_layout, swizzle: OptionalReg[Swizzle] = None, num_threads: Int = src_thread_layout.size(), thread_scope: ThreadScope = ThreadScope.BLOCK, block_dim_count: Int = 1](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Synchronously copy data from DRAM (global memory) to SRAM (shared memory) in a GPU context.

This function performs a synchronous copy operation from global memory
(DRAM) to shared memory (SRAM) in a GPU context, distributing the workload
across multiple threads for parallel execution. It uses thread affinity
mapping to ensure efficient work distribution and supports vectorized memory
operations for optimal performance.

Performance:

* Distributes the copy workload across multiple threads for parallel
  execution.
* Supports vectorized loads and stores for better memory throughput.
* Can use swizzling to optimize memory access patterns and reduce bank
  conflicts.
* Thread affinity mapping ensures efficient work distribution.
* For masked tensors, performs bounds checking to handle edge cases
  correctly.

Notes:

* The source tensor must be in GENERIC or GLOBAL address space (DRAM).
* The destination tensor must be in SHARED address space (SRAM).
* Both tensors must have the same data type.
* This function is synchronous, meaning all threads must complete their
  copy operations before proceeding.
* For optimal performance, the thread layouts should match the memory
  access patterns of the tensors.
* This function is particularly useful in GPU kernels for loading data
  from global memory to shared memory for faster access.

**Constraints:**

* Source and destination tensors must have the same data type.
* Source tensor must be in GENERIC or GLOBAL address space.
* Destination tensor must be in SHARED address space.
* For non-masked tensors, the fragment sizes must match.

**Parameters:**

* â€‹src\_thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout defining how threads are organized for the
  source tensor. This determines how the workload is distributed among
  threads.
* â€‹dst\_thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout defining how threads are organized for the
  destination tensor. Defaults to the same as `src_thread_layout` if
  not specified.
* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional swizzling function to rearrange the destination
  indices, which can improve memory access patterns and reduce bank
  conflicts.
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): Total number of threads participating in the copy
  operation. Defaults to the size of `src_thread_layout`.
* â€‹thread\_scope ([`ThreadScope`](/mojo/kernels/layout/layout_tensor/ThreadScope)): Scope at which thread operations are performed (`BLOCK` or
  `WARP`). Defaults to `ThreadScope.BLOCK`, where all threads in a
  block participate.
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the thread block.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor, which must be in shared memory (SRAM).
* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor, which must be in global or generic memory
  (DRAM).

`copy_dram_to_sram[src_thread_layout: Layout, dst_thread_layout: Layout = src_thread_layout, swizzle: OptionalReg[Swizzle] = None, num_threads: Int = src_thread_layout.size(), thread_scope: ThreadScope = ThreadScope.BLOCK, block_dim_count: Int = 1](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src_iter: LayoutTensorIter[dtype, layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], bound: Int)`

Efficiently copy data from global memory (DRAM) to shared memory (SRAM) on AMD GPUs.

This function implements an optimized memory transfer operation specifically
for AMD GPU architectures. It utilizes the hardware's `buffer_load`
intrinsic to efficiently transfer data while handling bounds checking. The
function distributes the copy operation across multiple threads for maximum
throughput.

**Parameters:**

* â€‹src\_thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout used to distribute the source tensor
  across threads. This determines how the workload is divided among
  participating threads.
* â€‹dst\_thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout used to distribute the destination tensor
  across threads. Defaults to the same layout as `src_thread_layout`.
* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional swizzling pattern to apply when distributing the
  destination tensor. This can improve memory access patterns and
  reduce bank conflicts. Defaults to None (no swizzling).
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): The total number of threads participating in the copy
  operation. Defaults to the size of `src_thread_layout`.
* â€‹thread\_scope ([`ThreadScope`](/mojo/kernels/layout/layout_tensor/ThreadScope)): Defines whether operations are performed at `BLOCK` or
  `WARP` level. `BLOCK` scope involves all threads in a thread block,
  while `WARP` scope restricts operations to threads within the same
  warp. Defaults to `ThreadScope.BLOCK`.
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the thread block.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor in shared memory (SRAM).
* â€‹src\_iter ([`LayoutTensorIter`](/mojo/kernels/layout/layout_tensor/LayoutTensorIter)): The source tensor iterator in global memory (DRAM) to be
  copied.
* â€‹bound ([`Int`](/mojo/std/builtin/int/Int)): The bound of the source tensor iterator.

`copy_dram_to_sram[thread_layout: Layout, swizzle: OptionalReg[Swizzle] = None, num_threads: Int = thread_layout.size(), thread_scope: ThreadScope = ThreadScope.BLOCK, block_dim_count: Int = 1](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src_iter: LayoutTensorIter[dtype, layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], bound: Int)`

Synchronously copy data from DRAM to SRAM using a unified thread layout for AMD GPUs.

This is a convenience wrapper around the more general `copy_dram_to_sram()`
function that uses the same layout for both source and destination tensors.
It's specifically designed for AMD GPUs where the buffer\_load intrinsic
requires the original base tensor.

Performance:

* Simplifies API usage when the same thread layout is appropriate for both
  source and destination tensors.
* Optimized for AMD GPUs using buffer\_load intrinsics for efficient memory
  transfers.
* Distributes the copy workload across multiple threads for parallel
  execution.

Notes:

* This function is only supported on AMD GPUs.
* The source tensor must be in GENERIC or GLOBAL address space (DRAM).
* The destination tensor must be in SHARED address space (SRAM).
* Both tensors must have the same data type.

**Parameters:**

* â€‹thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout defining how threads are organized for both source
  and destination. This determines how the workload is distributed
  among threads.
* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional swizzling function to rearrange the destination
  indices, which can improve memory access patterns and reduce bank
  conflicts.
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): Total number of threads participating in the copy
  operation. Defaults to the size of thread\_layout.
* â€‹thread\_scope ([`ThreadScope`](/mojo/kernels/layout/layout_tensor/ThreadScope)): Scope at which thread operations are performed (`BLOCK` or
  `WARP`). Defaults to `BLOCK`, where all threads in a block
  participate.
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the thread block.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor, which must be in shared memory (SRAM).
* â€‹src\_iter ([`LayoutTensorIter`](/mojo/kernels/layout/layout_tensor/LayoutTensorIter)): The source tensor iterator, which must be in global or generic
  memory (DRAM).
* â€‹bound ([`Int`](/mojo/std/builtin/int/Int)): The bound of the source tensor iterator.

`copy_dram_to_sram[thread_layout: Layout, swizzle: OptionalReg[Swizzle] = None, num_threads: Int = thread_layout.size(), thread_scope: ThreadScope = ThreadScope.BLOCK, block_dim_count: Int = 1](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Synchronously copy data from DRAM to SRAM using a unified thread layout.

This is a convenience wrapper around the more general `copy_dram_to_sram()`
function that uses the same layout for both source and destination tensors.
It simplifies the API for the common case where the same thread distribution
pattern works well for both tensors.

Performance:

* Simplifies API usage when the same thread layout is appropriate for both
  source and destination tensors.
* Distributes the copy workload across multiple threads for parallel
  execution.
* Supports vectorized loads and stores for better memory throughput.
* Can use swizzling to optimize memory access patterns and reduce bank
  conflicts.

Notes:

* The source tensor must be in `GENERIC` or `GLOBAL` address space (DRAM).
* The destination tensor must be in `SHARED` address space (SRAM).
* Both tensors must have the same data type.
* This function is synchronous, meaning all threads must complete their
  copy operations before proceeding.

**Parameters:**

* â€‹thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout defining how threads are organized for both source
  and destination. This determines how the workload is distributed
  among threads.
* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional swizzling function to rearrange the destination
  indices, which can improve memory access patterns and reduce bank
  conflicts.
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): Total number of threads participating in the copy
  operation. Defaults to the size of `thread_layout`.
* â€‹thread\_scope ([`ThreadScope`](/mojo/kernels/layout/layout_tensor/ThreadScope)): Scope at which thread operations are performed
  (`BLOCK` or `WARP`). Defaults to `ThreadScope.BLOCK`, where all
  threads in a block participate.
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the thread block.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor, which must be in shared memory (SRAM).
* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor, which must be in global or generic memory
  (DRAM).

</section>

---

## copy_dram_to_sram_async

<section class='mojo-docs'>

`copy_dram_to_sram_async[src_thread_layout: Layout, dst_thread_layout: Layout, swizzle: Bool = False, fill: Fill = Fill.NONE, eviction_policy: CacheEviction = CacheEviction.EVICT_NORMAL, num_threads: Int = src_thread_layout.size(), block_dim_count: Int = 1](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Asynchronously copy data from DRAM (global memory) to SRAM (shared memory) in a GPU context.

This function performs an asynchronous copy operation from global memory
(DRAM) to shared memory (SRAM) in a GPU context, using NVIDIA's cp.async
hardware mechanism. It distributes the workload across multiple threads and
allows computation to overlap with memory transfers for improved
performance.

Performance:

* Performs asynchronous transfers, allowing computation to overlap with
  memory operations.
* Distributes the copy workload across multiple threads for parallel
  execution.
* Can use swizzling to optimize memory access patterns and reduce bank
  conflicts.
* Supports different cache eviction policies to optimize memory hierarchy
  usage.
* For masked tensors, performs bounds checking to handle edge cases
  correctly.

Notes:

* This function requires NVIDIA GPUs with `cp.async` support (compute
  capability 8.0+).
* The source tensor must be in GENERIC or GLOBAL address space (DRAM).
* The destination tensor must be in SHARED address space (SRAM).
* Both tensors must have the same data type.
* This function is asynchronous, so you must call
  [`async_copy_wait_all()`](/mojo/std/gpu/memory/memory/async_copy_wait_all/)
  or
  [`async_copy_wait_group()`](/mojo/std/gpu/memory/memory/async_copy_wait_group/)
  to ensure the copy has completed before using the data.
* The maximum size of each element that can be copied is 16 bytes.

**Constraints:**

* Requires NVIDIA GPUs with cp.async support (compute capability 8.0+).
* Source tensor must be in `GENERIC` or `GLOBAL` address space.
* Destination tensor must be in `SHARED` address space.
* Both tensors must have the same data type.
* Element size must be 4, 8, or 16 bytes.

**Parameters:**

* â€‹src\_thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout defining how threads are organized for the
  source tensor. This determines how the workload is distributed among
  threads.
* â€‹dst\_thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout defining how threads are organized for the
  destination tensor.
* â€‹swizzle ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to apply swizzling to the destination indices to
  reduce bank conflicts. Defaults to False.
* â€‹fill ([`Fill`](/mojo/std/gpu/memory/memory/Fill)): Fill policy for handling out-of-bounds accesses. Options
  include:
  * `Fill.NONE`: No special handling (default).
  * `Fill.ZERO`: Fill out-of-bounds elements with zeros.
* â€‹eviction\_policy ([`CacheEviction`](/mojo/std/gpu/memory/memory/CacheEviction)): Cache eviction policy for the source data. Options
  include:
  * `CacheEviction.EVICT_NORMAL`: Normal eviction (default).
  * `CacheEviction.EVICT_FIRST`: Evict data after first use.
  * `CacheEviction.EVICT_LAST`: Keep data in cache until last use.
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): Total number of threads participating in the copy operation.
  Defaults to the size of src\_thread\_layout.
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the thread block.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor, which must be in shared memory (SRAM).
* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor, which must be in global or generic memory (DRAM).

`copy_dram_to_sram_async[thread_layout: Layout, swizzle: Bool = False, masked: Bool = False, fill: Fill = Fill.NONE, eviction_policy: CacheEviction = CacheEviction.EVICT_NORMAL, num_threads: Int = thread_layout.size(), block_dim_count: Int = 1](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Asynchronous copy from DRAM to SRAM with thread affinity mapping.

This function performs an asynchronous memory transfer from DRAM (global
memory) to SRAM (shared memory) using the specified thread layout for
distribution.

Notes:

This is a convenience wrapper around the more general
`copy_dram_to_sram_async()` function, using the same thread layout for
both source and destination.

**Parameters:**

* â€‹thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout used to distribute work across threads.
* â€‹swizzle ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to apply memory access swizzling for better performance.
* â€‹masked ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether the copy operation should use masking.
* â€‹fill ([`Fill`](/mojo/std/gpu/memory/memory/Fill)): Fill policy for uninitialized memory regions.
* â€‹eviction\_policy ([`CacheEviction`](/mojo/std/gpu/memory/memory/CacheEviction)): Cache eviction policy to use during the transfer.
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): Number of threads to use for the operation, defaults to
  the size of `thread_layout`.
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the thread block.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination tensor in SRAM.
* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Source tensor in DRAM.

</section>

---

## copy_local_to_dram

<section class='mojo-docs'>

`copy_local_to_dram[dst_thread_layout: Layout, num_threads: Int = dst_thread_layout.size(), thread_scope: ThreadScope = ThreadScope.BLOCK, block_dim_count: Int = 1](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Efficiently copy data from registers (LOCAL) to global memory (DRAM).

This function implements a high-performance memory transfer operation from
register memory to global memory. It distributes the copy operation across
multiple threads for maximum throughput while handling bounds checking for
safety.

**Constraints:**

* The source tensor must be in LOCAL address space (registers).
* The destination tensor must be in GENERIC or GLOBAL address space (DRAM).
* Both tensors must have compatible data types.

**Parameters:**

* â€‹dst\_thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout used to distribute the destination tensor
  across threads. This determines how the workload is divided among
  participating threads.
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): Total number of threads participating in the copy
  operation. Defaults to the size of thread\_layout.
* â€‹thread\_scope ([`ThreadScope`](/mojo/kernels/layout/layout_tensor/ThreadScope)): Defines whether operations are performed at `BLOCK` or
  `WARP` level. `BLOCK` scope involves all threads in a thread block,
  while `WARP` scope restricts operations to threads within the same
  warp. Defaults to `ThreadScope.BLOCK`.
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the thread block.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor in global memory (DRAM).
* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor in register memory (LOCAL) to be copied.

`copy_local_to_dram[dst_thread_layout: Layout, num_threads: Int = dst_thread_layout.size(), thread_scope: ThreadScope = ThreadScope.BLOCK, block_dim_count: Int = 1](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dst_base: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Efficiently copy data from registers (LOCAL) to global memory (DRAM) on AMD GPUs.

This function implements an optimized memory transfer operation specifically
for AMD GPU architectures. It utilizes the hardware's buffer\_store intrinsic
to efficiently transfer data from registers to global memory while handling
bounds checking. The function distributes the copy operation across multiple
threads for maximum throughput.

Notes:

* This function is particularly useful for writing computed results from
  registers back to global memory with minimal latency.
* The offset calculation is optimized for performance rather than
  flexibility.

**Constraints:**

* Only supported on AMD GPUs.
* Destination tensor must be in GLOBAL address space.
* Source tensor must be in LOCAL address space.
* Data types must match between source and destination tensors.

**Parameters:**

* â€‹dst\_thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout used to distribute the destination tensor
  across threads. This determines how the workload is divided among
  participating threads.
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): Total number of threads participating in the copy
  operation. Defaults to the size of thread\_layout.
* â€‹thread\_scope ([`ThreadScope`](/mojo/kernels/layout/layout_tensor/ThreadScope)): Defines whether operations are performed at `BLOCK` or
  `WARP` level. `BLOCK` scope involves all threads in a thread block,
  while `WARP` scope restricts operations to threads within the same
  warp. Defaults to `ThreadScope.BLOCK`.
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the thread block.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor in global memory (DRAM).
* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor in register memory (LOCAL address space) to be
  copied.
* â€‹dst\_base ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The original global memory tensor from which dst is derived.
  This is used to construct the buffer descriptor required by AMD's
  `buffer_store` intrinsic.

</section>

---

## copy_local_to_local

<section class='mojo-docs'>

`copy_local_to_local(dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Synchronously copy data between local memory (register) tensors with type conversion.

This function performs a synchronous copy operation between register tensors
in a GPU context, with support for converting from float32 to half-precision
formats (bfloat16/float16). It's particularly optimized for specific tensor
layouts commonly used in matrix multiplication operations.

Example:

```mojo
from layout import LayoutTensor, Layout
from layout.layout_tensor import copy_local_to_local

fn kernel():
    ...
    var src_reg = LayoutTensor[DType.float32,
        Layout.row_major(16, 8),
        MutAnyOrigin,
        address_space = AddressSpace.LOCAL,
    ].stack_allocation().fill(1)

    var dst_reg = LayoutTensor[DType.bfloat16,
        Layout.row_major(16, 8),
        MutAnyOrigin,
        address_space = AddressSpace.LOCAL,
    ].stack_allocation()

    # Process data in float32 registers
    # ...

    # Convert and copy to bfloat16 registers
    copy_local_to_local(dst_reg, src_reg)
```

Performance:

* Optimized for specific 2D tensor layouts with contiguous inner dimensions.
* Special fast path for 2D tensors with specific layouts used in matrix
  multiplication.
* For MMA (Matrix Multiply-Accumulate) operations, efficiently handles the
  conversion between output fragments and input fragments with different
  layouts.
* Falls back to element-wise copy for general cases.

Notes:

* Both source and destination tensors must be in `LOCAL` address space
  (registers).
* This function currently only supports copying from float32 to half-precision formats.
* For 2D tensors with stride\[1] == 1, a specialized fast path is used that's optimized
  for matrix multiplication patterns.
* This function is particularly useful in GPU kernels for converting between different
  precision formats while keeping data in registers.

**Constraints:**

* Destination tensor must be in `LOCAL` address space.
* Source tensor must be in `LOCAL` address space.
* Destination tensor must have a half-precision floating-point data type.
* Source tensor must have float32 data type.
* Both tensors must have the same total size.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor, which must be in local memory (registers)
  and have a half-precision floating-point data type (bfloat16 or
  float16).
* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor, which must be in local memory (registers) and
  have float32 data type.

</section>

---

## copy_local_to_shared

<section class='mojo-docs'>

`copy_local_to_shared[thread_layout: Layout, swizzle: OptionalReg[Swizzle] = None, num_threads: Int = thread_layout.size(), thread_scope: ThreadScope = ThreadScope.BLOCK, block_dim_count: Int = 1, *, row_major: Bool = False](dst: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Synchronously copy data from local memory (registers) to SRAM (shared memory).

This function performs a synchronous copy operation from register memory to
shared memory in a GPU context, distributing the workload across multiple
threads for parallel execution. It's particularly useful for transferring
processed data from registers to shared memory for inter-thread
communication.

Performance:

* Distributes the copy workload across multiple threads for parallel execution.
* Can use swizzling to optimize memory access patterns and reduce bank conflicts.
* Optimized for transferring data from registers to shared memory.
* On AMD GPUs, the `row_major` parameter can be used to match the memory
  access pattern used during prefetching from DRAM to registers.

Notes:

* The destination tensor must be in `SHARED` address space (SRAM).
* The source tensor must be in `LOCAL` address space (registers).
* This function is particularly useful in GPU kernels for sharing processed
  data between threads in the same block.
* The `row_major` parameter is specifically designed for AMD GPUs when using
  a prefetching pattern from DRAM to SRAM via registers.

**Constraints:**

* Destination tensor must be in SHARED address space.
* Source tensor must be in LOCAL address space.
* For optimal performance, the thread layout should match the memory
  access patterns of the tensors.

**Parameters:**

* â€‹thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout defining how threads are organized for the
  operation. This determines how the workload is distributed among
  threads.
* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional swizzling function to rearrange the destination
  indices, which can improve memory access patterns and reduce bank
  conflicts.
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): Total number of threads participating in the copy
  operation. Defaults to the size of thread\_layout.
* â€‹thread\_scope ([`ThreadScope`](/mojo/kernels/layout/layout_tensor/ThreadScope)): Defines whether operations are performed at `BLOCK` or
  `WARP` level. `BLOCK` scope involves all threads in a thread block,
  while `WARP` scope restricts operations to threads within the same
  warp. Defaults to `ThreadScope.BLOCK`.
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the thread block.
* â€‹row\_major ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to use row-major ordering for the copy operation.
  This is particularly relevant when prefetching from DRAM to SRAM
  via registers on AMD GPUs. Defaults to False.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor, which must be in shared memory (SRAM).
* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor, which must be in local memory (registers).

</section>

---

## copy_sram_to_dram

<section class='mojo-docs'>

`copy_sram_to_dram[thread_layout: Layout, swizzle: OptionalReg[Swizzle] = None, num_threads: Int = thread_layout.size(), block_dim_count: Int = 1, binary_op: OptionalReg[fn[dtype: DType, width: Int](lhs: SIMD[dtype, width], rhs: SIMD[dtype, width]) -> SIMD[dtype, width]] = None](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Synchronously copy data from SRAM (shared memory) to DRAM (global memory).

This function performs a synchronous memory transfer from SRAM (shared
memory) to DRAM (global memory) using the specified thread layout for
workload distribution. It supports optional swizzling for optimized memory
access patterns and binary operations for combining data during the
transfer.

Performance:

* Distributes the copy workload across multiple threads for parallel
  execution.
* Supports vectorized loads and stores for better memory throughput.
* Can use swizzling to optimize memory access patterns.
* Supports binary operations to combine data during transfer (e.g., for
  reduction operations).

Notes:

* The source tensor must be in `SHARED` address space (SRAM).
* The destination tensor must be in `GENERIC` or `GLOBAL` address space
  (DRAM).
* Supports FP32 to half-precision downcast during copy if needed.
* Handles masked tensors with proper bounds checking.
* This function is synchronous, meaning all threads must complete their
  copy operations before proceeding.

**Constraints:**

* Source tensor must be in SHARED address space with a static layout.
* Destination tensor must be in GENERIC or GLOBAL address space.
* For type conversion, only FP32 to half-precision is supported.
* For vectorized copy with type conversion, both tensors must have
  element layouts matching the SIMD width of the destination type.

**Parameters:**

* â€‹thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout defining how threads are organized for both source
  and destination. This determines how the workload is distributed
  among threads.
* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional swizzling function to rearrange the source indices,
  which can improve memory access patterns and reduce bank conflicts.
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): Total number of threads participating in the copy
  operation. Defaults to the size of thread\_layout.
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the thread block.
* â€‹binary\_op ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional binary operation to apply during the copy, combining
  source data with existing destination data.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor, which must be in global or generic memory
  (DRAM).
* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor, which must be in shared memory (SRAM).

</section>

---

## copy_sram_to_local

<section class='mojo-docs'>

`copy_sram_to_local[src_warp_layout: Layout, axis: OptionalReg[Int] = None](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Synchronously copy data from SRAM (shared memory) to local memory.

This function performs a synchronous memory transfer from SRAM (shared
memory) to local memory (registers) using the specified thread layout for
workload distribution.

Performance:

* Distributes the copy workload across multiple threads for parallel
  execution.
* Optimized for transferring data from shared memory to registers.
* Supports optional axis-specific distribution for specialized access
  patterns.

**Constraints:**

* The source tensor must be in SHARED address space (SRAM).
* The destination tensor must be in LOCAL address space (registers).
* Both tensors must have the same data type.

**Parameters:**

* â€‹src\_warp\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout defining how threads are organized for the
  source tensor. This determines how the workload is distributed among
  threads.
* â€‹axis ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional parameter specifying which axis to distribute along.
  When provided, distribution happens along the specified axis.
  When None (default), distribution uses the standard layout pattern.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor, which must be in local memory (registers).
* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor, which must be in shared memory (SRAM).

</section>

---

## cp_async_k_major

<section class='mojo-docs'>

`cp_async_k_major[dtype: DType, eviction_policy: CacheEviction = CacheEviction.EVICT_NORMAL](dst: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Asynchronously copy data from DRAM to SRAM using TMA (Tensor Memory Accelerator) with K-major layout.

This function performs an asynchronous copy operation from global memory
(DRAM) to shared memory (SRAM) using NVIDIA's Tensor Memory Accelerator
(TMA) hardware. It optimizes for K-major memory access patterns, which is
particularly beneficial for certain tensor operations like matrix
multiplications where the inner dimension (K) is accessed contiguously.

The function automatically determines the optimal tile size and thread
distribution based on the tensor shapes and hardware capabilities,
leveraging TMA's efficient memory transfer mechanisms.

Performance:

* Uses TMA hardware acceleration for optimal memory transfer performance.
* Optimizes for K-major access patterns, which can significantly improve
  performance for certain tensor operations like matrix multiplications.
* Performs asynchronous transfers, allowing computation to overlap with
  memory operations.
* Automatically determines optimal tile sizes based on tensor dimensions.
* Uses hardware-accelerated swizzling to reduce shared memory bank
  conflicts.

Notes:

* This function requires NVIDIA GPUs with TMA support (compute capability
  9.0+).
* The source tensor must be in GENERIC or GLOBAL address space (DRAM).
* The destination tensor must be in SHARED address space (SRAM).
* Both tensors must have the same data type.
* This function is asynchronous, so you must call
  [`async_copy_wait_all()`](/mojo/std/gpu/memory/memory/async_copy_wait_all/)
  or
  [`async_copy_wait_group()`](/mojo/std/gpu/memory/memory/async_copy_wait_group/)
  to ensure the copy has completed before using the data.
* K-major layout is particularly beneficial for matrix multiplication
  operations where the inner dimension (K) is accessed contiguously.

**Constraints:**

* Requires NVIDIA GPUs with TMA support (compute capability 9.0+).
* Source tensor must be in GENERIC or GLOBAL address space.
* Destination tensor must be in SHARED address space.
* Both tensors must have the same data type.
* Source and destination tensors must be 2D.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the tensor elements.
* â€‹eviction\_policy ([`CacheEviction`](/mojo/std/gpu/memory/memory/CacheEviction)): The cache eviction policy to use. Default is `CacheEviction.EVICT_NORMAL`.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor, which must be in shared memory (SRAM).
* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor, which must be in global or generic memory
  (DRAM).

</section>

---

## layout_tensor

<section class='mojo-docs'>

Provides the `LayoutTensor` type for representing multidimensional data.

## `comptime` values

### `binary_op_type`

`comptime binary_op_type = fn[dtype: DType, width: Int](lhs: SIMD[dtype, width], rhs: SIMD[dtype, width]) -> SIMD[dtype, width]`

Type alias for binary operations on SIMD vectors.

This type represents a function that takes two SIMD vectors of the same type and
width and returns a SIMD vector of the same type and width.

Args:
dtype: The data type of the SIMD vector elements.
width: The width of the SIMD vector.
lhs: Left-hand side SIMD vector operand.
rhs: Right-hand side SIMD vector operand.

Returns:
A SIMD vector containing the result of the binary operation.

## Structs

* [â€‹`LayoutTensor`](./LayoutTensor): A high-performance tensor with explicit memory layout and hardware-optimized access patterns.
* [â€‹`LayoutTensorIter`](./LayoutTensorIter): Iterator for traversing a memory buffer with a specific layout.
* [â€‹`ThreadScope`](./ThreadScope): Represents the scope of thread operations in GPU programming.

## Functions

* [â€‹`copy_dram_to_local`](./copy_dram_to_local): Efficiently copy data from global memory (DRAM) to registers for AMD GPUs.
* [â€‹`copy_dram_to_sram`](./copy_dram_to_sram): Synchronously copy data from DRAM (global memory) to SRAM (shared memory) in a GPU context.
* [â€‹`copy_dram_to_sram_async`](./copy_dram_to_sram_async): Asynchronously copy data from DRAM (global memory) to SRAM (shared memory) in a GPU context.
* [â€‹`copy_local_to_dram`](./copy_local_to_dram): Efficiently copy data from registers (LOCAL) to global memory (DRAM).
* [â€‹`copy_local_to_local`](./copy_local_to_local): Synchronously copy data between local memory (register) tensors with type conversion.
* [â€‹`copy_local_to_shared`](./copy_local_to_shared): Synchronously copy data from local memory (registers) to SRAM (shared memory).
* [â€‹`copy_sram_to_dram`](./copy_sram_to_dram): Synchronously copy data from SRAM (shared memory) to DRAM (global memory).
* [â€‹`copy_sram_to_local`](./copy_sram_to_local): Synchronously copy data from SRAM (shared memory) to local memory.
* [â€‹`cp_async_k_major`](./cp_async_k_major): Asynchronously copy data from DRAM to SRAM using TMA (Tensor Memory Accelerator) with K-major layout.
* [â€‹`stack_allocation_like`](./stack_allocation_like): Create a stack-allocated tensor with the same layout as an existing tensor.

</section>

---

## stack_allocation_like

<section class='mojo-docs'>

`stack_allocation_like[layout: Layout, dtype: DType, *, address_space: AddressSpace, target_address_space: AddressSpace = AddressSpace.GENERIC](in_tensor: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> LayoutTensor[dtype, layout, MutAnyOrigin, address_space=target_address_space, masked=masked]`

Create a stack-allocated tensor with the same layout as an existing tensor.

This function creates a new tensor on the stack with the same layout, data
type, and masking properties as the input tensor, but potentially with a
different address space. This is useful for creating temporary tensors that
match the structure of existing tensors.

Example:

```mojo
from layout import LayoutTensor, Layout
from layout.layout_tensor import stack_allocation_like

var global_tensor = LayoutTensor[
    DType.float32,
    Layout([10, 10]),
    MutAnyOrigin,
    address_space=AddressSpace.GLOBAL
].stack_allocation()

var shared_tensor = stack_allocation_like[
    target_address_space=AddressSpace.SHARED
](global_tensor)
```

Performance:

* Creates a tensor on the stack, which is typically faster to allocate and
  access than heap-allocated memory.
* Stack allocations have automatic lifetime management, reducing memory
  management overhead.
* Stack size is limited, so be cautious with large tensor allocations.

Notes:

* The new tensor will have the same layout, data type, and masking properties
  as the input tensor.
* The address space can be changed, which is useful for moving data between
  different memory regions (e.g., from global to shared memory).
* Stack allocations are automatically freed when they go out of scope.
* The function uses the stack\_allocation method of the result tensor type.

**Parameters:**

* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the tensor to allocate.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the tensor elements.
* â€‹address\_space ([`AddressSpace`](/mojo/std/memory/pointer/AddressSpace)): The address space of the input tensor.
* â€‹target\_address\_space ([`AddressSpace`](/mojo/std/memory/pointer/AddressSpace)): The address space for the new tensor. Defaults to
  GENERIC.

**Args:**

* â€‹in\_tensor ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor to match the layout of.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new tensor allocated on the stack with the same layout as the input
tensor.

</section>

---

## math

<section class='mojo-docs'>

Implements math methods that work on layout tensors.

## Functions

* [â€‹`max`](./max): Computes maximum reduction along specified axis.
* [â€‹`mean`](./mean): Computes the mean value of the elements in a buffer.
* [â€‹`outer_product_acc`](./outer_product_acc): Updates result tensor with the outer product of two vectors.
* [â€‹`sum`](./sum): Computes sum reduction along specified axis.
* [â€‹`variance`](./variance): Computes the variance value of the elements in a buffer.

</section>

---

## max

<section class='mojo-docs'>

`max[axis: Int](inp: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], outp: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Computes maximum reduction along specified axis.

Reduces the input tensor by taking maximum elements along the specified
axis and stores the result in the output tensor.

**Constraints:**

All tensors must have statically known shapes.
`outp.rank` must equal `inp.rank - 1`.
Non-reduction dimensions must match between `inp` and `outp`.
Currently only supports rank-2 inputs.

**Parameters:**

* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis to take maximum along.

**Args:**

* â€‹inp ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor to reduce.
* â€‹outp ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output tensor to store maximum results.

`max[axis: Int](inp: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> LayoutTensor[dtype, _reduce_res_row_major_shape(axis, layout), MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type]`

Computes maximum reduction along specified axis, returning a new tensor.

Reduces the input tensor by taking maximum elements along the specified
axis and returns a new tensor with the results.

**Constraints:**

All tensors must have statically known shapes.
Result will have rank equal to `inp.rank` - 1.
Non-reduction dimensions in the result match the input.
Currently only supports rank-2 inputs.

**Parameters:**

* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis to take maximum along.

**Args:**

* â€‹inp ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor to reduce.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new tensor containing the maximum values along the specified axis.

`max[dtype: DType, layout: Layout](x: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], y: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].MutableAnyType`

Computes element-wise maximum of two tensors.

Returns a new tensor containing the element-wise maximum between the
input tensors.

**Constraints:**

Input tensors must have statically known shapes and matching layouts.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the input tensors.
* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the input tensors.

**Args:**

* â€‹x ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): First input tensor.
* â€‹y ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Second input tensor.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new tensor containing the element-wise maximum.

</section>

---

## mean

<section class='mojo-docs'>

`mean(src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> Scalar[dtype]`

Computes the mean value of the elements in a buffer.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The buffer of elements for which the mean is computed.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The mean value of the elements in the given buffer.

**Raises:**

May raise on GPU targets when a device error occurs.

`mean[reduce_axis: Int](src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Computes the mean across reduce\_axis of an NDBuffer.

**Parameters:**

* â€‹reduce\_axis ([`Int`](/mojo/std/builtin/int/Int)): The axis to reduce across.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input buffer.
* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output buffer.

**Raises:**

May raise on GPU targets when a device error occurs.

</section>

---

## outer_product_acc

<section class='mojo-docs'>

`outer_product_acc(res: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], lhs: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], rhs: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Updates result tensor with the outer product of two vectors.

Computes `res += outer(lhs, rhs)` where `lhs` and `rhs` are vectors and
`res` is a matrix.

**Constraints:**

All tensors must have statically known shapes.
`res` must be rank 2.
`lhs` and `rhs` must be rank 1.
`res.shape[0]` `==` `lhs.shape[0]` and `res.shape[1]` `==` `rhs.shape[0]`.

**Args:**

* â€‹res ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The result matrix to accumulate into, shape (M, N).
* â€‹lhs ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The left-hand side vector, shape (M,).
* â€‹rhs ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The right-hand side vector, shape (N,).

</section>

---

## sum (Math)

<section class='mojo-docs'>

`sum[axis: Int](inp: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], outp: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Computes sum reduction along specified axis.

Reduces the input tensor by summing elements along the specified axis
and stores the result in the output tensor.

Example:

```mojo
from layout import LayoutTensor, Layout
from layout.math import sum

data = InlineArray[Int32, 6](0, 1, 2, 3, 4, 5)
tensor = LayoutTensor[DType.int32, Layout.row_major(2, 3)](data)
print(tensor)
print("-----")
print(sum[0](tensor))
```

Output:

```plaintext
0 1 2
3 4 5
-----
3 5 7
```

**Constraints:**

All tensors must have statically known shapes.
`outp.rank` must equal `inp.rank - 1`.
Non-reduction dimensions must match between inp and outp.
Currently only supports rank-2 inputs.

**Parameters:**

* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis to sum along.

**Args:**

* â€‹inp ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor to sum.
* â€‹outp ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output tensor to store sum results.

`sum[axis: Int](inp: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> LayoutTensor[dtype, _reduce_res_row_major_shape(axis, layout), MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type]`

Computes sum reduction along specified axis, returning a new tensor.

Reduces the input tensor by summing elements along the specified axis
and returns a new tensor with the results.

**Constraints:**

All tensors must have statically known shapes.
Result will have rank equal to `inp.rank` - 1.
Non-reduction dimensions in the result match the input.
Currently only supports rank-2 inputs.

**Parameters:**

* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis to sum along.

**Args:**

* â€‹inp ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor to sum.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): A new tensor containing the sum values along the specified axis.

</section>

---

## variance

<section class='mojo-docs'>

`variance(src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], correction: Int = 1) -> Scalar[dtype]`

Computes the variance value of the elements in a buffer.

```
variance(x) = sum((x - E(x))^2) / (size - correction)
```

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The buffer.
* â€‹correction ([`Int`](/mojo/std/builtin/int/Int)): Normalize variance by size - correction (Default=1).

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The variance value of the elements in a buffer.

**Raises:**

May raise on GPU targets when a device error occurs.

</section>

---

## RuntimeLayout

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct RuntimeLayout[layout: Layout, /, *, element_type: DType = DType.int64, linear_idx_type: DType = DType.int64]`

A runtime-configurable layout that uses `RuntimeTuple` for storage.

This struct provides a layout implementation that can be modified at runtime,
unlike the static [`Layout`](/mojo/kernels/layout/layout/Layout) type. It
uses [`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple) for
shape and stride storage.

The layout must have statically known dimensions at compile time, but the
actual shape and stride values can be modified during execution.

## Parameters

* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The static `Layout` type to base this runtime layout on.
* â€‹element\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The integer type of the each dimension element. Must be signed.
* â€‹linear\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The integer type of the linear index into memory returned by `crd2idx`. Must be signed.

## Fields

* â€‹shape (`RuntimeLayout[layout, element_type=element_type, linear_idx_type=linear_idx_type].ShapeType`): The shape of the layout as a runtime tuple.
  Stores the size of each dimension. Uses the specified bitwidth and is
  unsigned. Must match the static layout's shape dimensions.
* â€‹stride (`RuntimeLayout[layout, element_type=element_type, linear_idx_type=linear_idx_type].StrideType`): The stride of the layout as a runtime tuple.
  Stores the stride (step size) for each dimension. Uses 64-bit unsigned
  integers since strides can be large values. Must match the static layout's
  stride dimensions.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ShapeType`

`comptime ShapeType = RuntimeTuple[layout.shape, element_type=element_type]`

Type alias for the runtime shape tuple.

### `StrideType`

`comptime StrideType = RuntimeTuple[layout.stride, element_type=linear_idx_type]`

Type alias for the runtime stride tuple.

## Methods

### `__init__`

`__init__() -> Self`

Initialize a `RuntimeLayout` with default values.

Creates a new `RuntimeLayout` instance with default shape and stride
values. Requires that the static layout has known dimensions at compile
time.

**Constraints:**

The static layout that this runtime layout is based on must have all
dimensions known.

`__init__(shape: RuntimeTuple[layout.shape, element_type=element_type], stride: RuntimeTuple[layout.stride, element_type=linear_idx_type]) -> Self`

Initialize a `RuntimeLayout` with specified shape and stride.

**Args:**

* â€‹shape ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): A `RuntimeTuple` containing the dimensions of each axis.
* â€‹stride ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): A `RuntimeTuple` containing the stride values for each axis.

### `__call__`

`__call__(self, idx: Int) -> Scalar[linear_idx_type]`

Convert a single index to a flat linear index.

**Args:**

* â€‹idx ([`Int`](/mojo/std/builtin/int/Int)): The one-dimensional index to convert.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The corresponding flat linear index in the layout.

`__call__[t: IntTuple](self, idx: RuntimeTuple[t, element_type=element_type]) -> Scalar[linear_idx_type]`

Convert a multi-dimensional index to a flat linear index.

**Parameters:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` type for the index.

**Args:**

* â€‹idx ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): A `RuntimeTuple` containing the multi-dimensional coordinates.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The corresponding flat linear index in the layout.

### `idx2crd`

`idx2crd[t: IntTuple](self, idx: RuntimeTuple[t, element_type=element_type]) -> RuntimeTuple[idx2crd(t, layout.shape, layout.stride), element_type=element_type]`

Converts a linear index to logical coordinates.

This is the inverse operation of the **call** method, mapping from
a memory index back to the corresponding logical coordinates.

**Parameters:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` type for the index.

**Args:**

* â€‹idx ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The linear index to convert.

**Returns:**

[`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple): The logical coordinates corresponding to the given index.

### `size`

`size(self) -> Int`

Calculate the total number of elements in the layout.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The product of all dimensions in the shape, representing the total
number of elements that can be addressed by this layout.

### `bound_check_required`

`bound_check_required(self) -> Bool`

Determine if bounds checking is required for this layout.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if any dimension in the shape differs from the static layout's
shape, False otherwise.

### `cast`

`cast[_element_type: DType, /, *, target_linear_idx_type: DType = linear_idx_type](self) -> RuntimeLayout[layout, element_type=_element_type, linear_idx_type=target_linear_idx_type]`

Cast the layout to use a different element bitwidth.

**Parameters:**

* â€‹\_element\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The target data type.
* â€‹target\_linear\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The target linear idx type.

**Returns:**

[`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout): A new `RuntimeLayout` with the shape cast to the specified type.

### `__str__`

`__str__(self) -> String`

Convert the layout to a string representation.

**Returns:**

`String`: A string representation of the layout.

### `row_major`

`static row_major[rank: Int, //](shape: IndexList[rank, element_type=element_type]) -> Self`

Create a row-major layout from the given shape.

In row-major layout, elements with adjacent rightmost indices are
adjacent in memory.

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the layout.

**Args:**

* â€‹shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): An `IndexList` containing the dimensions of each axis.

**Returns:**

`Self`: A `RuntimeLayout` with row-major stride ordering.

### `col_major`

`static col_major[rank: Int, //](shape: IndexList[rank, element_type=element_type]) -> Self`

Create a column-major layout from the given shape.

In column-major layout, elements with adjacent leftmost indices are
adjacent in memory.

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions in the layout.

**Args:**

* â€‹shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): An `IndexList` containing the dimensions of each axis.

**Returns:**

`Self`: A `RuntimeLayout` with column-major stride ordering.

### `write_to`

`write_to(self, mut writer: T)`

Write a string representation of the layout to a writer.

**Args:**

* â€‹writer (`T`): The `Writer` object to write the layout representation to.

### `sublayout`

`sublayout[i: Int](self) -> RuntimeLayout[layout[i], element_type=element_type, linear_idx_type=linear_idx_type]`

Extract a nested sublayout at the specified index.

**Parameters:**

* â€‹i ([`Int`](/mojo/std/builtin/int/Int)): The index of the nested layout to extract.

**Returns:**

[`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout): A `RuntimeLayout` representing the nested layout at index i.

### `dim`

`dim(self, i: Int) -> Int`

Get the size of the dimension at the specified index.

**Args:**

* â€‹i ([`Int`](/mojo/std/builtin/int/Int)): The index of the dimension to retrieve.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The size of the dimension at index `i`.

### `__len__`

`static __len__() -> Int`

Get the number of dimensions in the layout.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The number of dimensions (rank) of the layout.

</section>

---

## coalesce (Runtime_layout)

<section class='mojo-docs'>

`coalesce[l: Layout, keep_rank: Bool = False](layout: RuntimeLayout[l, element_type=element_type, linear_idx_type=linear_idx_type]) -> RuntimeLayout[coalesce(l, keep_rank), element_type=element_type, linear_idx_type=linear_idx_type]`

Coalesce adjacent dimensions in a runtime layout when possible.

This optimizes the layout by merging adjacent dimensions when their
relationship allows it, potentially reducing the number of dimensions.

**Parameters:**

* â€‹l ([`Layout`](/mojo/kernels/layout/layout/Layout)): The static layout type to coalesce.
* â€‹keep\_rank ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to maintain the original rank (currently unsupported).

**Args:**

* â€‹layout ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The input `RuntimeLayout` to coalesce.

**Returns:**

[`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout): A new `RuntimeLayout` with coalesced dimensions.

</section>

---

## runtime_layout

<section class='mojo-docs'>

Provides the `RuntimeLayout` type and functions for working with it. You can use `RuntimeLayout` to define a layout where the dimensions are not known at compile time.

You can import these APIs from `layout.runtime_layout`.

```mojo
from layout.runtime_layout import RuntimeLayout, make_layout
```

## Structs

* [â€‹`RuntimeLayout`](./RuntimeLayout): A runtime-configurable layout that uses `RuntimeTuple` for storage.

## Functions

* [â€‹`coalesce`](./coalesce): Coalesce adjacent dimensions in a runtime layout when possible.
* [â€‹`make_layout`](./make_layout): Combine two runtime layouts into a single composite layout.

</section>

---

## make_layout (Runtime_layout)

<section class='mojo-docs'>

`make_layout[l1: Layout, l2: Layout, /, *, linear_idx_type: DType = DType.uint64](a: RuntimeLayout[l1, element_type=element_type, linear_idx_type=linear_idx_type], b: RuntimeLayout[l2, element_type=element_type, linear_idx_type=linear_idx_type]) -> RuntimeLayout[make_layout(l1, l2), element_type=element_type, linear_idx_type=linear_idx_type]`

Combine two runtime layouts into a single composite layout.

This creates a new layout by concatenating the dimensions and strides of the
input layouts.

**Parameters:**

* â€‹l1 ([`Layout`](/mojo/kernels/layout/layout/Layout)): The static layout type of `a`.
* â€‹l2 ([`Layout`](/mojo/kernels/layout/layout/Layout)): The static layout type of `b`.
* â€‹linear\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The integer type of the all index calculated by the returned
  runtime layout.

**Args:**

* â€‹a ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The first `RuntimeLayout` to combine.
* â€‹b ([`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout)): The second `RuntimeLayout` to combine.

**Returns:**

[`RuntimeLayout`](/mojo/kernels/layout/runtime_layout/RuntimeLayout): A new `RuntimeLayout` with dimensions from both input layouts.

</section>

---

## RuntimeTuple

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct RuntimeTuple[S: IntTuple = -1, /, *, element_type: DType = DType.int64]`

A struct representing tuple-like data with compile-time and runtime elements. RuntimeTuple combines static (compile-time) and dynamic (runtime) handling of tuple-like data structures, typically used for tensor shapes, indices, and coordinates in high-performance computing contexts. This struct is optimized for parallel execution and hardware acceleration, allowing efficient manipulation of multi-dimensional data. It supports both known compile-time values and runtime-determined values.

## Parameters

* â€‹S ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): `IntTuple` with compile-time known values (or `UNKNOWN_VALUE` for runtime values).
* â€‹element\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Integer type of the underlying elements.

## Fields

* â€‹value (`IndexList[RuntimeTuple[S, element_type=element_type].scalar_length, element_type=element_type]`): Storage for the actual tuple values, implemented as an IndexList with the appropriate size and element type.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Intable`](/mojo/std/builtin/int/Intable),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Sized`](/mojo/std/builtin/len/Sized),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `scalar_length`

`comptime scalar_length = len[IntTuple](flatten(S))`

The total number of scalar elements in this RuntimeTuple after flattening nested tuples.

## Methods

### `__init__`

`__init__() -> Self`

Initialize a `RuntimeTuple` with default values.

For dimensions with known compile-time values in S, uses those values.
For unknown dimensions, initializes them to UNKNOWN\_VALUE.

`__init__(*values: Int) -> Self`

Initialize a `RuntimeTuple` with the provided values.

**Args:**

* â€‹\*values ([`Int`](/mojo/std/builtin/int/Int)): Variadic number of integer values to initialize the tuple with.

`@implicit`
`__init__[l: Int](values: IndexList[l, element_type=element_type]) -> Self`

Initialize a `RuntimeTuple` from an `IndexList`.

**Parameters:**

* â€‹l ([`Int`](/mojo/std/builtin/int/Int)): Compile-time length of the input `IndexList`.

**Args:**

* â€‹values ([`IndexList`](/mojo/std/utils/index_/IndexList)): `IndexList` to initialize from. Must have same length as the `RuntimeTuple`.
  The values will be cast to the appropriate element type if needed.

### `__getitem__`

`__getitem__[i: Int](self) -> RuntimeTuple[S[i], element_type=element_type]`

Retrieves the element at the specified index in the tuple.

This method provides array-like indexing for RuntimeTuple, allowing access
to individual elements or sub-tuples. It handles the internal offset calculation
to access the correct elements in the flattened storage array.

**Parameters:**

* â€‹i ([`Int`](/mojo/std/builtin/int/Int)): The index of the element to retrieve.

**Returns:**

[`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple): A new `RuntimeTuple` containing the element or sub-tuple at the specified index.

### `__setitem__`

`__setitem__[i: Int](mut self, val: Scalar[element_type])`

Sets the value of the element at the specified index in the tuple.

This method enables array-like assignment for RuntimeTuple elements,
handling the internal offset calculation to modify the correct element
in the flattened storage array.

**Parameters:**

* â€‹i ([`Int`](/mojo/std/builtin/int/Int)): The index of the element to modify.

**Args:**

* â€‹val ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The new value to assign to the element.

### `offset_until`

`static offset_until[i: Int]() -> Int`

Calculates the offset in the flattened value array for a given tuple index.

This method computes the sum of lengths of all flattened subtuple elements
that come before the specified index, which is used for indexing into the
internal storage.

**Parameters:**

* â€‹i ([`Int`](/mojo/std/builtin/int/Int)): The tuple index to calculate the offset for.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The offset in the flattened array where the i-th element begins.

### `get_int`

`get_int(self) -> Scalar[element_type]`

Returns the integer value of this RuntimeTuple.

For tuples with a known compile-time value, returns that value.
For tuples with a runtime value, returns the first element of the
internal storage array.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The integer value of this RuntimeTuple.

### `__str__`

`__str__(self) -> String`

Converts the RuntimeTuple to its string representation.

This method provides a human-readable string representation of the tuple,
which is useful for debugging and logging.

**Returns:**

`String`: A string representation of the `RuntimeTuple`.

### `concat`

`concat[R: IntTuple](self, rhs: RuntimeTuple[R, element_type=element_type]) -> RuntimeTuple[concat(S, R), element_type=element_type]`

Concatenates two `RuntimeTuple`s together.

This method combines the current `RuntimeTuple` with another one, preserving
both compile-time and runtime values. It handles the complexity of merging
the underlying storage arrays while maintaining the proper semantic structure.

**Parameters:**

* â€‹R ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The `IntTuple` type parameter of the right-hand side RuntimeTuple.

**Args:**

* â€‹rhs ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The `RuntimeTuple` to concatenate to the end of this one.

**Returns:**

[`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple): A new `RuntimeTuple` containing all elements from both tuples in sequence.

### `flatten`

`flatten(self) -> RuntimeTuple[flatten(S), element_type=element_type]`

Flattens a potentially nested `RuntimeTuple` into a single-level tuple. This method converts a hierarchical structure of tuples into a flat representation, preserving all values but removing the nested structure. This is useful for operations that need to treat all elements uniformly.

**Returns:**

[`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple): A new `RuntimeTuple` containing all elements in a flat (non-nested) structure.

### `write_to`

`write_to(self, mut writer: T)`

Writes the RuntimeTuple to a Writer object.

This method is used by the string conversion system to generate a string
representation of the RuntimeTuple. It handles both scalar values and
nested tuple structures, producing a properly formatted output.

**Args:**

* â€‹writer (`T`): The Writer object to write the string representation to.

### `__len__`

`__len__(self) -> Int`

Returns the length (number of top-level elements) of the `RuntimeTuple`.

This method provides the standard Python-like len() functionality,
giving the number of elements at the top level of the tuple structure.
For nested tuples, this returns the number of first-level entries,
not the total number of scalar values.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The number of top-level elements in the tuple.

### `cast`

`cast[dtype: DType](self) -> RuntimeTuple[S, element_type=dtype]`

Casts the RuntimeTuple to use a different numeric type. This method creates a new RuntimeTuple with the same structure and values but using a different underlying numeric type for storage. This is useful for changing precision or signedness of the data.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The target DType to cast the elements to.

**Returns:**

[`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple): A new `RuntimeTuple` with elements cast to the specified type.

### `__int__`

`__int__(self) -> Int`

Converts the RuntimeTuple to an integer value.

This method enables implicit conversion of a RuntimeTuple to an integer,
but is constrained to only work on scalar tuples (those that contain a single value).

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The integer value of the tuple.

</section>

---

## coalesce_nested_tuple

<section class='mojo-docs'>

`coalesce_nested_tuple[t: IntTuple, out_t: IntTuple = _int_tuple_product_flatten[t]()](tuple: RuntimeTuple[t, element_type=element_type]) -> RuntimeTuple[out_t]`

Coalesces a nested `RuntimeTuple` into a single-level `RuntimeTuple`, by multiplying all the values together.

**Parameters:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The underlying Compile-time IntTuple backing the RuntimeTuple.
* â€‹out\_t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The flattened Compile-time IntTuple.

**Args:**

* â€‹tuple ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The RuntimeTuple to convert.

**Returns:**

[`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple): A new `IntTuple` containing the products of each top level tuple, in a flat structure.

</section>

---

## concat

<section class='mojo-docs'>

`concat(var lhs: IntTuple, rhs: IntTuple) -> IntTuple`

Concatenates two `IntTuple` instances into a single `IntTuple`.

This function appends all elements from the right-hand side tuple to the
left-hand side tuple, creating a new combined tuple. The operation preserves
the hierarchical structure of both tuples.

**Args:**

* â€‹lhs ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The left-hand side `IntTuple` that will be modified (var parameter).
* â€‹rhs ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The right-hand side `IntTuple` whose elements will be appended.

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple): A new `IntTuple` containing all elements from both tuples in sequence.

</section>

---

## crd2idx (Runtime_tuple)

<section class='mojo-docs'>

`crd2idx[crd_t: IntTuple, shape_t: IntTuple, stride_t: IntTuple, out_type: DType = DType.uint64](crd: RuntimeTuple[crd_t, element_type=element_type], shape: RuntimeTuple[shape_t, element_type=element_type], stride: RuntimeTuple[stride_t, element_type=element_type]) -> Scalar[out_type]`

Converts multi-dimensional coordinates to a linear index.

This function is the inverse of idx2crd, transforming a set of coordinates
into a flat index based on the provided shape and stride information.
This is essential for mapping multi-dimensional tensor elements to linear memory.

**Parameters:**

* â€‹crd\_t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Type of the coordinates.
* â€‹shape\_t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Type of the shape.
* â€‹stride\_t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Type of the stride.
* â€‹out\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The output data type for the index (default: uint64).

**Args:**

* â€‹crd ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The coordinates to convert.
* â€‹shape ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The shape of the multi-dimensional array.
* â€‹stride ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The stride values for each dimension.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): A scalar value representing the linear index corresponding to the given coordinates.

</section>

---

## idx2crd (Runtime_tuple)

<section class='mojo-docs'>

`idx2crd[idx_t: IntTuple, shape_t: IntTuple, stride_t: IntTuple](idx: RuntimeTuple[idx_t, element_type=element_type], shape: RuntimeTuple[shape_t, element_type=element_type], stride: RuntimeTuple[stride_t, element_type=element_type]) -> RuntimeTuple[idx2crd(idx_t, shape_t, stride_t), element_type=element_type]`

Converts a linear index to multi-dimensional coordinates. This function transforms a flat index into coordinate values based on the provided shape and stride information. This is essential for mapping linear memory accesses to multi-dimensional tensor elements.

**Constraints:**

The index must be a scalar value (not a tuple).

**Parameters:**

* â€‹idx\_t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): IntTuple type of the index.
* â€‹shape\_t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): IntTuple type of the shape.
* â€‹stride\_t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): IntTuple type of the stride.

**Args:**

* â€‹idx ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The linear index to convert.
* â€‹shape ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The shape of the multi-dimensional array.
* â€‹stride ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The stride values for each dimension.

**Returns:**

[`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple): A `RuntimeTuple` containing the multi-dimensional coordinates.

`idx2crd[idx_t: IntTuple, shape_t: IntTuple](idx: RuntimeTuple[idx_t, element_type=element_type], shape: RuntimeTuple[shape_t, element_type=element_type]) -> RuntimeTuple[idx2crd(idx_t, shape_t, prefix_product(shape_t)), element_type=element_type]`

Converts a linear index to multi-dimensional coordinates using shape-derived strides. This is a convenience overload of `idx2crd` that automatically calculates the stride values from the shape using `prefix_product`. This is the common case for row-major storage order tensors.

**Parameters:**

* â€‹idx\_t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): IntTuple type of the index.
* â€‹shape\_t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): IntTuple type of the shape.

**Args:**

* â€‹idx ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The linear index to convert.
* â€‹shape ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The shape of the multi-dimensional array.

**Returns:**

[`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple): A `RuntimeTuple` containing the multi-dimensional coordinates calculated using
automatically derived strides from the shape.

</section>

---

## runtime_tuple

<section class='mojo-docs'>

Provides the `RuntimeTuple` data structure and related utility functions for handling tuple-like data with both compile-time and runtime elements. `RuntimeTuple` is designed for high-performance tensor operations, supporting efficient manipulation of multi-dimensional data structures like shapes, indices, and coordinates.

Key features:

* Hybrid compile-time/runtime value handling
* Optimized for parallel execution and hardware acceleration
* Support for nested tuple structures
* Efficient conversion between linear indices and multi-dimensional coordinates
* Specialized operations for tensor shape calculations

The module includes functions for tuple manipulation (concatenation, flattening),
coordinate transformations (`idx2crd`, `crd2idx`), and specialized tensor operations
like shape division and prefix products.

## Structs

* [â€‹`RuntimeTuple`](./RuntimeTuple): A struct representing tuple-like data with compile-time and runtime elements. RuntimeTuple combines static (compile-time) and dynamic (runtime) handling of tuple-like data structures, typically used for tensor shapes, indices, and coordinates in high-performance computing contexts. This struct is optimized for parallel execution and hardware acceleration, allowing efficient manipulation of multi-dimensional data. It supports both known compile-time values and runtime-determined values.

## Functions

* [â€‹`coalesce_nested_tuple`](./coalesce_nested_tuple): Coalesces a nested `RuntimeTuple` into a single-level `RuntimeTuple`, by multiplying all the values together.
* [â€‹`concat`](./concat): Concatenates two `IntTuple` instances into a single `IntTuple`.
* [â€‹`crd2idx`](./crd2idx): Converts multi-dimensional coordinates to a linear index.
* [â€‹`idx2crd`](./idx2crd): Converts a linear index to multi-dimensional coordinates. This function transforms a flat index into coordinate values based on the provided shape and stride information. This is essential for mapping linear memory accesses to multi-dimensional tensor elements.
* [â€‹`is_int`](./is_int): Determines if a `RuntimeTuple` represents a scalar integer value.
* [â€‹`is_tuple`](./is_tuple): Determines if a `RuntimeTuple` represents a tuple rather than a scalar value.
* [â€‹`prefix_product`](./prefix_product): Computes the prefix products of elements in the `RuntimeTuple`.
* [â€‹`product`](./product): Computes the product of all elements in the `RuntimeTuple`.
* [â€‹`shape_div`](./shape_div): Performs specialized shape division between `RuntimeTuple`s.
* [â€‹`signum`](./signum): Returns the sign of an integer value.
* [â€‹`to_index_list`](./to_index_list): Converts a RuntimeTuple to an IndexList with the same values.

</section>

---

## is_int (Runtime_tuple)

<section class='mojo-docs'>

`is_int[t: IntTuple](tuple: RuntimeTuple[t, element_type=element_type]) -> Bool`

Determines if a `RuntimeTuple` represents a scalar integer value.

This function checks if the `RuntimeTuple` holds a single scalar value
rather than a tuple structure with multiple elements.

**Parameters:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The IntTuple type parameter of the RuntimeTuple.

**Args:**

* â€‹tuple ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The `RuntimeTuple` to check.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the `RuntimeTuple` represents a scalar integer, False otherwise.

</section>

---

## is_tuple (Runtime_tuple)

<section class='mojo-docs'>

`is_tuple[t: IntTuple](tuple: RuntimeTuple[t, element_type=element_type]) -> Bool`

Determines if a `RuntimeTuple` represents a tuple rather than a scalar value.

This function checks the structure of the underlying IntTuple to determine
if it represents a tuple with multiple elements or a single scalar value.

**Parameters:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The IntTuple type parameter of the RuntimeTuple.

**Args:**

* â€‹tuple ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The `RuntimeTuple` to check.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool): True if the `RuntimeTuple` represents a tuple, False if it represents a scalar.

</section>

---

## prefix_product (Runtime_tuple)

<section class='mojo-docs'>

`prefix_product[t: IntTuple](tuple: RuntimeTuple[t, element_type=element_type]) -> RuntimeTuple[prefix_product(t)]`

Computes the prefix products of elements in the `RuntimeTuple`.

This function calculates the running product of elements, where each
output element is the product of all previous elements in the input.
This is commonly used in tensor computations to calculate stride values.

**Parameters:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The IntTuple type parameter of the input RuntimeTuple.

**Args:**

* â€‹tuple ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The input `RuntimeTuple`.

**Returns:**

[`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple): A new `RuntimeTuple` containing the prefix products of the input elements.

</section>

---

## product (Runtime_tuple)

<section class='mojo-docs'>

`product[t: IntTuple](tuple: RuntimeTuple[t, element_type=element_type]) -> Int`

Computes the product of all elements in the `RuntimeTuple`.

This function multiplies all scalar values in the tuple, including
those in nested tuples after flattening. This is commonly used to
calculate the total size of a tensor from its shape.

**Parameters:**

* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The IntTuple type parameter of the input RuntimeTuple.

**Args:**

* â€‹tuple ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The input `RuntimeTuple`.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The product of all scalar elements in the tuple.

</section>

---

## shape_div (Runtime_tuple)

<section class='mojo-docs'>

`shape_div[a_t: IntTuple, b_t: IntTuple](a: RuntimeTuple[a_t, element_type=element_type], b: RuntimeTuple[b_t, element_type=element_type]) -> RuntimeTuple[shape_div(a_t, b_t)]`

Performs specialized shape division between `RuntimeTuple`s.

This function implements a special division operation specifically designed for
tensor shape calculations. Unlike standard division, it handles special cases:

1. If shapes are directly divisible (a % b == 0), returns a standard division (a // b)
2. If shapes are inversely divisible (b % a == 0), returns the signed reciprocal
3. If shapes are incompatible, aborts with an error

This operation is essential for transformations between tensor layouts and computing
broadcasting semantics.

**Parameters:**

* â€‹a\_t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Type of the first operand.
* â€‹b\_t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): Type of the second operand.

**Args:**

* â€‹a ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The dividend `RuntimeTuple`.
* â€‹b ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The divisor `RuntimeTuple`.

**Returns:**

[`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple): A new `RuntimeTuple` containing the result of the shape division.

</section>

---

## signum (Runtime_tuple)

<section class='mojo-docs'>

`signum(a: Int) -> Int`

Returns the sign of an integer value.

This helper function determines whether a number is positive, negative, or zero,
returning 1 for positive, -1 for negative, and 0 for zero.

**Args:**

* â€‹a ([`Int`](/mojo/std/builtin/int/Int)): The integer value to determine the sign of.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): 1 if a > 0, -1 if a < 0, 0 if a == 0.

</section>

---

## to_index_list (Runtime_tuple)

<section class='mojo-docs'>

`to_index_list[rank: Int, t: IntTuple](tuple: RuntimeTuple[t, element_type=element_type]) -> IndexList[rank]`

Converts a RuntimeTuple to an IndexList with the same values.

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the resulting IndexList.
* â€‹t ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The IntTuple template parameter of the RuntimeTuple.

**Args:**

* â€‹tuple ([`RuntimeTuple`](/mojo/kernels/layout/runtime_tuple/RuntimeTuple)): The RuntimeTuple to convert.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): An IndexList filled with the values of the RuntimeTuple.

</section>

---

## ComposedLayout

<section class='mojo-docs'>

`struct ComposedLayout[LayoutA: LayoutTrait, LayoutB: LayoutTrait, offset: OptionalReg[Int] = 0]`

Layout composed of two layouts applied sequentially.

Combines two layouts. Output of the first (`LayoutA`) is input to
the second (`LayoutB`), with optional offset in between.

## Parameters

* â€‹LayoutA ([`LayoutTrait`](/mojo/kernels/layout/layout/LayoutTrait)): The first layout to apply.
* â€‹LayoutB ([`LayoutTrait`](/mojo/kernels/layout/layout/LayoutTrait)): The second layout to apply.
* â€‹offset ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional offset between layouts (default: 0).

## Fields

* â€‹layout\_a (`LayoutA`): The first layout to apply.
* â€‹layout\_b (`LayoutB`): The second layout to apply.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`LayoutTrait`](/mojo/kernels/layout/layout/LayoutTrait),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = False`

### `__del__is_trivial`

`comptime __del__is_trivial = LayoutB.__del__is_trivial if LayoutA.__del__is_trivial else LayoutA.__del__is_trivial`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = LayoutB.__moveinit__is_trivial if LayoutA.__moveinit__is_trivial else LayoutA.__moveinit__is_trivial`

### `has_shape`

`comptime has_shape = LayoutA.has_shape if LayoutA.has_shape else LayoutB.has_shape`

True if either layout has a shape.

## Methods

### `__init__`

`__init__(out self, layout_a: LayoutA, layout_b: LayoutB)`

Initialize ComposedLayout with two layouts.

**Args:**

* â€‹layout\_a (`LayoutA`): The first layout.
* â€‹layout\_b (`LayoutB`): The second layout.

### `__copyinit__`

`__copyinit__(out self, other: Self)`

Copy constructor for ComposedLayout.

**Args:**

* â€‹other (`Self`): The ComposedLayout to copy from.

### `__call__`

`__call__(self, idx: IntTuple) -> Int`

Apply composed layout to an index.

Applies `LayoutA`, then adds offset, then applies `LayoutB`.

**Args:**

* â€‹idx ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The index to transform.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The transformed index.

`__call__(self, idx: IntTuple, offset_val: Int) -> Int`

Apply composed layout with runtime offset.

Applies `LayoutA`, then adds runtime `offset_val`, then `LayoutB`.
Static offset must not be set when using runtime offset.

**Args:**

* â€‹idx ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The index to transform.
* â€‹offset\_val ([`Int`](/mojo/std/builtin/int/Int)): Runtime offset to apply.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The transformed index.

### `size`

`size(self) -> Int`

Get the size of the composed layout.

Returns the size of the first layout (`LayoutA`).

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The size of the first layout.

### `cosize`

`cosize(self) -> Int`

Get the cosize of the composed layout.

Returns the cosize of the second layout (`LayoutB`).

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The cosize of the second layout.

</section>

---

## Swizzle

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct Swizzle`

Swizzle functor for memory access pattern optimization.

Implements a swizzling pattern to reduce bank conflicts in shared
memory accesses.  It XORs specific bits of memory indices based
on configurable parameters.

Swizzle operation:
Given index `i`, and Swizzle\[bits, base, shift]:

1. Extract `bits` number of bits from `i` starting from position
   `base + max(0, shift)`. Let's call this `YYY`.
2. Extract `bits` number of bits from `i` starting from position
   `base - min(0, shift)`. Let's call this `ZZZ`.
3. Result is `i ^ (YYY shifted by 'shift' positions)`.

Example (Swizzle\[2, 0, 3]):
Input index bits:  `xxxxxxxxxxxxxxxxYYxxxxxxxxxZZxxxx`
Output index bits: `xxxxxxxxxxxxxxxxYYxxxxxxxxxAAxxxx`
where `AA = ZZ ^ YY`.

Attributes:
bits (Int): Number of bits in the mask (YYY).
base (Int): Number of least significant bits to keep constant.
shift (Int): Shift distance for the mask (positive: right,
negative: left).
yyy\_mask (Int): Mask for the bits to be shifted (YYY).
zzz\_mask (Int): Mask for the target bits (ZZZ).

## Fields

* â€‹bits (`Int`): Number of bits in the mask.
* â€‹base (`Int`): Number of least significant bits to keep constant.
* â€‹shift (`Int`): Distance to shift the mask (pos right, neg left).
* â€‹yyy\_mask (`Int`): Mask for the bits to be shifted.
* â€‹zzz\_mask (`Int`): Mask for the target bits.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`LayoutTrait`](/mojo/kernels/layout/layout/LayoutTrait),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `has_shape`

`comptime has_shape = False`

Indicates if layout has shape. Swizzle always False.

## Methods

### `__init__`

`__init__(bits: Int, base: Int, shift: Int) -> Self`

Initialize a Swizzle object.

Configures the swizzle operation based on bits, base, and
shift parameters.

**Args:**

* â€‹bits ([`Int`](/mojo/std/builtin/int/Int)): Number of bits in the mask.
* â€‹base ([`Int`](/mojo/std/builtin/int/Int)): Least significant bits to keep constant.
* â€‹shift ([`Int`](/mojo/std/builtin/int/Int)): Distance to shift the mask.

### `__call__`

`__call__(self, index: IntTuple) -> Int`

Apply swizzle to an IntTuple index.

Unwraps the IntTuple and applies the swizzle to the integer
value.

**Args:**

* â€‹index ([`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)): The IntTuple index to swizzle.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The swizzled index value.

`__call__(self, offset: Int) -> Int`

Apply swizzle to an integer offset.

Performs the swizzle operation on an integer offset to
rearrange memory access patterns.

**Args:**

* â€‹offset ([`Int`](/mojo/std/builtin/int/Int)): The integer offset to swizzle.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The swizzled offset value.

`__call__(self, offset: Scalar[dtype]) -> Scalar[dtype]`

Apply swizzle to a scalar offset.

Scalar version of the swizzle operation.  Applies swizzle to
a scalar offset.

**Args:**

* â€‹offset ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The scalar offset to swizzle.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The swizzled scalar value.

### `size`

`size(self) -> Int`

Get the size of the swizzle pattern.

Calculates the size of the memory region affected by the
swizzle pattern.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The size of the swizzle pattern.

### `cosize`

`cosize(self) -> Int`

Get the cosize of the swizzle pattern.

Cosize is the same as size for swizzle layouts, representing
the output size.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The cosize of the swizzle pattern (same as size).

### `write_to`

`write_to(self, mut writer: T)`

Write the swizzle parameters to a writer.

Outputs the swizzle parameters (bits, base, shift) in a
tuple format.

**Args:**

* â€‹writer (`T`): The writer to write to.

### `__str__`

`__str__(self) -> String`

Convert the swizzle to a string representation.

**Returns:**

`String`: String representation of the swizzle parameters.

</section>

---

## eval_composed

<section class='mojo-docs'>

`eval_composed[composed_layout: ComposedLayout[Layout, Swizzle]](idx: UInt, offset: UInt = 0) -> UInt`

Evaluate a composed layout with swizzle.

Evaluates a `ComposedLayout[Layout, Swizzle]`. Applies the base
layout, adds an optional offset, and then applies the swizzle.

**Parameters:**

* â€‹composed\_layout ([`ComposedLayout`](/mojo/kernels/layout/swizzle/ComposedLayout)): The composed layout to evaluate, consisting of a base Layout
  and a Swizzle transformation.

**Args:**

* â€‹idx ([`UInt`](/mojo/std/builtin/uint/UInt)): The input index to transform.
* â€‹offset ([`UInt`](/mojo/std/builtin/uint/UInt)): Optional offset to apply between layouts (default: 0).

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt): The transformed index after applying both layouts.

</section>

---

## swizzle (Swizzle)

<section class='mojo-docs'>

Defines swizzle layouts for optimizing memory access patterns.

This module is designed for use in shared memory, especially in GPU
kernels, to reduce bank conflicts.  It provides tools to create and
apply swizzle transformations to memory indices.  Swizzling
rearranges memory access order to distribute accesses across
different memory banks.  This mitigates bank contention and improves
memory access efficiency.

Module components:

* `Swizzle` struct: Represents a swizzle transformation with
  configurable bits, base, and shift parameters.
* Helper functions: `make_ldmatrix_swizzle`, `make_swizzle` create
  predefined swizzle patterns. These are optimized for scenarios
  like `ldmatrix` instructions and general 2D memory access.
* `ComposedLayout` struct: Combines a base layout with a swizzle
  layout for complex memory access optimizations.

Primary use case: GPU kernel development where shared memory bank
conflicts can degrade performance.  Applying swizzle layouts
optimizes memory access patterns for higher throughput.

## Structs

* [â€‹`ComposedLayout`](./ComposedLayout): Layout composed of two layouts applied sequentially.
* [â€‹`Swizzle`](./Swizzle): Swizzle functor for memory access pattern optimization.

## Functions

* [â€‹`eval_composed`](./eval_composed): Evaluate a composed layout with swizzle.
* [â€‹`make_ldmatrix_swizzle`](./make_ldmatrix_swizzle): Make swizzle to avoid bank conflict for ldmatrix ops.
* [â€‹`make_swizzle`](./make_swizzle): Create a 2D swizzle to avoid bank conflicts.
* [â€‹`shiftl`](./shiftl): Shift left or right based on sign of shift amount.
* [â€‹`shiftr`](./shiftr): Shift right or left based on sign of shift amount.

</section>

---

## make_ldmatrix_swizzle

<section class='mojo-docs'>

`make_ldmatrix_swizzle[dtype: DType, row_size: Int, log2_vector_width: Int = 0]() -> Swizzle`

Make swizzle to avoid bank conflict for ldmatrix ops.

Creates a swizzle pattern optimized for `ldmatrix` operations.
Minimizes bank conflicts in shared memory for these operations.
Calculates swizzle parameters based on data type and row size.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the elements.
* â€‹row\_size ([`Int`](/mojo/std/builtin/int/Int)): Size of each row in elements.
* â€‹log2\_vector\_width ([`Int`](/mojo/std/builtin/int/Int)): Log2 of the vector width (default: 0).

**Returns:**

`Swizzle`: A `Swizzle` object configured for `ldmatrix`.

</section>

---

## make_swizzle

<section class='mojo-docs'>

`make_swizzle[num_rows: Int, row_size: Int, access_size: Int]() -> Swizzle`

Create a 2D swizzle to avoid bank conflicts.

Generates a swizzle pattern for 2D memory layout to minimize
bank conflicts in shared memory access.

**Parameters:**

* â€‹num\_rows ([`Int`](/mojo/std/builtin/int/Int)): Number of rows in the minimum access pattern.
* â€‹row\_size ([`Int`](/mojo/std/builtin/int/Int)): Size of each row in elements.
* â€‹access\_size ([`Int`](/mojo/std/builtin/int/Int)): Number of elements accessed at once.

**Returns:**

`Swizzle`: A `Swizzle` object for 2D memory access.

`make_swizzle[dtype: DType, mode: TensorMapSwizzle]() -> Swizzle`

Create swizzle based on predefined swizzle modes.

Returns a swizzle pattern based on standard modes (32B, 64B,
128B, none), adjusted for data type.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the elements.
* â€‹mode ([`TensorMapSwizzle`](/mojo/std/gpu/host/nvidia/tma/TensorMapSwizzle)): The swizzle mode to use (TensorMapSwizzle enum).

**Returns:**

`Swizzle`: A `Swizzle` object configured by the specified mode.

</section>

---

## shiftl

<section class='mojo-docs'>

`shiftl(a: Int, s: Int) -> Int`

Shift left or right based on sign of shift amount.

Performs a left shift if `s` is positive, or a right shift if
`s` is negative.

**Args:**

* â€‹a ([`Int`](/mojo/std/builtin/int/Int)): The integer value to shift.
* â€‹s ([`Int`](/mojo/std/builtin/int/Int)): The shift amount. Positive for left, negative for right.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The shifted integer value.

`shiftl(a: Scalar[dtype], s: Scalar[dtype]) -> Scalar[dtype]`

Shift left/right based on sign of shift for scalars.

Scalar version of `shiftl`.  Left shift if `s` is positive,
right shift if `s` is negative.

**Args:**

* â€‹a ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The scalar value to shift.
* â€‹s ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The scalar shift amount. Positive for left, negative right.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The shifted scalar value.

</section>

---

## shiftr

<section class='mojo-docs'>

`shiftr(a: Int, s: Int) -> Int`

Shift right or left based on sign of shift amount.

Performs a right shift if `s` is positive, or a left shift if
`s` is negative.

**Args:**

* â€‹a ([`Int`](/mojo/std/builtin/int/Int)): The integer value to shift.
* â€‹s ([`Int`](/mojo/std/builtin/int/Int)): The shift amount. Positive for right, negative for left.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The shifted integer value.

`shiftr(a: Scalar[dtype], s: Scalar[dtype]) -> Scalar[dtype]`

Shift right/left based on sign of shift for scalars.

Scalar version of `shiftr`.  Right shift if `s` is positive,
left shift if `s` is negative.

**Args:**

* â€‹a ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The scalar value to shift.
* â€‹s ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The scalar shift amount. Positive for right, negative left.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The shifted scalar value.

</section>

---

## TensorCore

<section class='mojo-docs'>

`struct TensorCore[out_type: DType, in_type: DType, shape: IndexList[3], transpose_b: Bool = False]`

TensorCore provides an abstraction for GPU tensor core hardware to perform optimized matrix operations.

This struct encapsulates the functionality required to efficiently map matrix operations to Tensor Cores
on NVIDIA and AMD GPUs. It handles loading matrix fragments, performing matrix multiply-accumulate
operations, and storing results with hardware-specific optimizations.

Note:
Different shapes and data types are supported depending on the GPU hardware.
For NVIDIA GPUs:

* float32: 16x8x8 or 16x8x4
* half-precision: 16x8x16
* float8: 16x8x32
  For AMD GPUs:
* float32: 16x16x4
* half-precision: 16x16x16 or 32x32x8

## Parameters

* â€‹out\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The data type for output/accumulation operations.
* â€‹in\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The data type for input matrix elements.
* â€‹shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The shape parameters for the matrix operation in the form \[M, N, K]
  where MxN is the output shape and K is the inner dimension.
* â€‹transpose\_b ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to transpose the B matrix before multiplication. Defaults to False.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `a_reg_type`

`comptime a_reg_type = SIMD[in_type, num_matrix_reg[shape.__getitem__[3, DType.int64, Int](0), shape.__getitem__[3, DType.int64, Int](2)]()]`

SIMD type for the A operand registers.

### `b_reg_type`

`comptime b_reg_type = SIMD[in_type, num_matrix_reg[shape.__getitem__[3, DType.int64, Int](2), shape.__getitem__[3, DType.int64, Int](1)]()]`

SIMD type for the B operand registers.

### `c_reg_tile_type`

`comptime c_reg_tile_type = LayoutTensor[out_type, Layout.col_major(1, num_matrix_reg[shape.__getitem__[3, DType.int64, Int](0), shape.__getitem__[3, DType.int64, Int](1)]()), MutAnyOrigin, address_space=AddressSpace.LOCAL]`

LayoutTensor type for the C register tile.

### `c_reg_type`

`comptime c_reg_type = SIMD[out_type, num_matrix_reg[shape.__getitem__[3, DType.int64, Int](0), shape.__getitem__[3, DType.int64, Int](1)]()]`

SIMD type for the C/accumulator operand registers.

### `supported_fp32`

`comptime supported_fp32 = (shape == IndexList[3, DType.int64](16, 8, 8, Tuple[]())) if is_nvidia_gpu() else (shape == IndexList[3, DType.int64](16, 16, 4, Tuple[]())) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 81) else (in_type is DType.float32)`

Whether float32 is supported for this tensor core configuration.

### `supported_fp64`

`comptime supported_fp64 = Tuple[IndexList[3], IndexList[3], IndexList[3], IndexList[3]](VariadicPack[True, origin_of(), True, Movable, IndexList[3], IndexList[3], IndexList[3], IndexList[3]](shape_8x8x4, shape_16x8x4, shape_16x8x8, shape_16x8x16)).__contains__[IndexList[3], IndexList[3], IndexList[3], IndexList[3], IndexList[3]](shape) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> out_type, "_mlir_value">>, 82) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 82) else (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 82) else (out_type is DType.float64) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 82) else (in_type is DType.float64) if is_nvidia_gpu() else False`

Whether float64 is supported for this tensor core configuration.

### `supported_fp8`

`comptime supported_fp8 = (shape == shape_16x8x32) if Tuple[DType, DType](VariadicPack[True, origin_of(), True, Movable, DType, DType](DType.float8_e4m3fn, DType.float8_e5m2)).__contains__[DType, DType, DType](in_type) else Tuple[DType, DType](VariadicPack[True, origin_of(), True, Movable, DType, DType](DType.float8_e4m3fn, DType.float8_e5m2)).__contains__[DType, DType, DType](in_type) if is_nvidia_gpu() else (shape == shape_16x16x32) if Tuple[DType, DType](VariadicPack[True, origin_of(), True, Movable, DType, DType](get_amd_fp8_dtype(), get_amd_bf8_dtype())).__contains__[DType, DType, DType](in_type) else Tuple[DType, DType](VariadicPack[True, origin_of(), True, Movable, DType, DType](get_amd_fp8_dtype(), get_amd_bf8_dtype())).__contains__[DType, DType, DType](in_type)`

Whether float8 is supported for this tensor core configuration.

### `supported_half`

`comptime supported_half = (shape == shape_16x8x16) if is_nvidia_gpu() else Tuple[IndexList[3], IndexList[3], IndexList[3], IndexList[3]](VariadicPack[True, origin_of(), True, Movable, IndexList[3], IndexList[3], IndexList[3], IndexList[3]](shape_16x16x16, shape_16x16x32, shape_32x32x8, shape_32x32x16)).__contains__[IndexList[3], IndexList[3], IndexList[3], IndexList[3], IndexList[3]](shape) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 80) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 80) else (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 79) else in_type.is_half_float()`

Whether half-precision float is supported for this configuration.

## Methods

### `__init__`

`__init__(out self)`

Initialize a new TensorCore instance.

### `get_shapes`

`static get_shapes[_out_type: DType, _in_type: DType]() -> List[IndexList[3]]`

Get supported shapes for given data types.

Returns a list of valid shapes for the specified output and input data types.

Note:
The returned shapes are hardware-dependent. Different shapes are supported
for different combinations of input and output types.

**Parameters:**

* â€‹\_out\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The output/accumulation data type.
* â€‹\_in\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The input matrix data type.

**Returns:**

[`List`](/mojo/std/collections/list/List): List\[IndexList\[3]]: Valid shapes for the matrix operations given the specified types.

### `load_a`

`load_a[swizzle: OptionalReg[Swizzle] = None](self, a: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> LayoutTensor[in_type, _get_a_reg_tile_layout[layout, shape](), MutAnyOrigin, address_space=AddressSpace.LOCAL]`

Load the A matrix fragments.

Loads matrix A from memory into a LayoutTensor suitable for tensor core operations.

**Parameters:**

* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional swizzle pattern for optimal memory access (AMD only).

**Args:**

* â€‹a ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source matrix A data.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): The loaded matrix fragments as a `LayoutTensor`.

`load_a[swizzle: OptionalReg[Swizzle] = None](self, warp_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], fragments: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mma_tile_coord_k: UInt = 0)`

Load A matrix fragments from shared memory.

Optimized version for loading A matrix fragments from shared memory.

**Parameters:**

* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional memory access pattern for to optimize memory bandwidth.

**Args:**

* â€‹warp\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source data in shared memory.
* â€‹fragments ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor for fragments.
* â€‹mma\_tile\_coord\_k ([`UInt`](/mojo/std/builtin/uint/UInt)): The K coordinate of the MMA tile. Defaults to 0.

### `load_b`

`load_b[swizzle: OptionalReg[Swizzle] = None](self, b: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> LayoutTensor[in_type, _get_b_reg_tile_layout[layout, shape, transpose_b](), MutAnyOrigin, address_space=AddressSpace.LOCAL]`

Load the B matrix fragments.

Loads matrix B from memory into a `LayoutTensor` suitable for tensor core operations.
The function handles different hardware architectures and memory access patterns.

Note:
If transpose\_b is `True`, the B matrix will be transposed during loading.
This is more efficient than transposing the matrix in memory.

**Parameters:**

* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional swizzle pattern for optimal memory access (AMD only).
  Will cause an error if used with NVIDIA GPUs.

**Args:**

* â€‹b ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source matrix B data.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor): The loaded matrix fragments as a `LayoutTensor`.

`load_b[swizzle: OptionalReg[Swizzle] = None](self, warp_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], fragments: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mma_tile_coord_k: UInt = 0, warp_tile_coord_n: UInt = 0)`

Load B matrix fragments from shared memory into registers for tensor core operations.

This function loads matrix B fragments from a warp tile in shared memory into register fragments
for use in tensor core matrix multiply operations. It handles hardware-specific optimizations
for both NVIDIA and AMD GPUs.

Note:
The `warp_tile` must be in shared memory. For NVIDIA GPUs, `swizzle` must be `None`.
For AMD GPUs, providing an appropriate `swizzle` pattern can improve performance.

**Parameters:**

* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional memory access pattern for AMD GPUs to optimize memory bandwidth.
  Must be None when running on NVIDIA GPUs. For NVIDIA GPUs, swizzle is always on.

**Args:**

* â€‹warp\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Source `LayoutTensor` in shared memory containing the B matrix data.
* â€‹fragments ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination `LayoutTensor` to store the loaded matrix fragments.
* â€‹mma\_tile\_coord\_k ([`UInt`](/mojo/std/builtin/uint/UInt)): K-dimension coordinate within the warp tile. Defaults to 0.
* â€‹warp\_tile\_coord\_n ([`UInt`](/mojo/std/builtin/uint/UInt)): N-dimension coordinate within the warp tile. Defaults to 0.

`load_b(self, warp_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], fragments: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scales: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mma_tile_coord_k: UInt = 0)`

Load quantized B matrix fragments from shared memory with dequantization.

This function loads int4 quantized matrix B fragments from shared memory, dequantizes them
using the provided scales, and stores the result in register fragments for tensor core operations.

Notes:

* The `warp_tile` must be in shared memory.
* The `fragments` and `scales` must be in local memory.
* This function only supports half-precision data types (bfloat16, float16).
* The quantized data is stored as int4 values packed into int32 elements.
* Each thread processes multiple fragments by unpacking and dequantizing the int4 values.

**Args:**

* â€‹warp\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Source `LayoutTensor` in shared memory containing the quantized B matrix data.
* â€‹fragments ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination `LayoutTensor` to store the dequantized matrix fragments.
* â€‹scales ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): `LayoutTensor` containing the scaling factors for dequantization.
* â€‹mma\_tile\_coord\_k ([`UInt`](/mojo/std/builtin/uint/UInt)): K-dimension coordinate within the warp tile. Defaults to 0.

### `load_c`

`load_c(self, c: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> TensorCore[out_type, in_type, shape, transpose_b].c_reg_tile_type`

Load the C matrix fragments.

Loads matrix C from memory into a `LayoutTensor` suitable for tensor core operations.
The function handles different hardware architectures and memory access patterns.

**Args:**

* â€‹c ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source matrix C data.

**Returns:**

`TensorCore`: The loaded matrix fragments as a `LayoutTensor`.

### `store_d`

`store_d(self, d_dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], d_src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Store matrix D to destination memory.

Stores the result matrix D from tensor core computation to the destination memory.

**Args:**

* â€‹d\_dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor to store the result.
* â€‹d\_src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor containing the computed result.

### `mma_op`

`mma_op(self, a: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> TensorCore[out_type, in_type, shape, transpose_b].c_reg_tile_type`

Perform matrix multiply-accumulate operation (MMA).

Executes `D = A * B + C` using tensor cores.

**Args:**

* â€‹a ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The A matrix input.
* â€‹b ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The B matrix input.
* â€‹c ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The C matrix input for accumulation.

**Returns:**

`TensorCore`: `Self.c_reg_tile_type`: The result of the MMA operation.

### `mma`

`mma(self, a_frag: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_frag: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_frag: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Perform matrix multiply-accumulate operation using tensor cores.

Executes C = A \* B + C using tensor cores, where A, B, and C are matrix fragments
stored in register memory. This function handles the mapping of fragments to
hardware tensor core operations.

Notes:

* All fragments must be properly loaded using the corresponding load functions.
* The function assumes fragments are vectorized layout tensors with dimensions num\_vectors x 1.
* The c\_frag shape\[0] must equal num\_m\_mmas \* num\_n\_mmas.
* The result is accumulated in-place in c\_frag.

**Args:**

* â€‹a\_frag ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Matrix A fragments as a `LayoutTensor`.
* â€‹b\_frag ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Matrix B fragments as a `LayoutTensor`.
* â€‹c\_frag ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Matrix C fragments as a `LayoutTensor` for both input and output.

</section>

---

## TiledTensorCore

<section class='mojo-docs'>

`struct TiledTensorCore[out_type: DType, in_type: DType, shape: IndexList[3], group_size: Int, transpose_b: Bool = False]`

TiledTensorCore provides a wrapper around TensorCore to support multiple MMAs along the K dimension.

Enables larger K dimension operations by decomposing them into multiple smaller MMA operations.
Currently only being used for AMD GPUs to enable 16x16x32 operations using two 16x16x16 MMAs.

## Parameters

* â€‹out\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The data type for output/accumulation operations.
* â€‹in\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The data type for input matrix elements.
* â€‹shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The shape parameters for individual MMA operations \[M, N, K].
* â€‹group\_size ([`Int`](/mojo/std/builtin/int/Int)): Number of MMA operations along the K dimension.
* â€‹transpose\_b ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to transpose the b matrix. Defaults to False.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `mma_op`

`comptime mma_op = TensorCore[out_type, in_type, shape, transpose_b]()`

The underlying TensorCore instance for MMA operations.

## Methods

### `mma`

`static mma[swap_a_b: Bool = False](a_reg_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_reg_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_reg_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Perform multiple matrix multiply-accumulate operations along the K dimension.

Executes group\_size MMA operations, processing slices of the K dimension
and accumulating results in c\_reg\_tile.

**Parameters:**

* â€‹swap\_a\_b ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to swap a and b operands. Defaults to False.

**Args:**

* â€‹a\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input matrix a fragments \[num\_m\_mmas, group\_size \* a\_frag\_size].
* â€‹b\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input matrix b fragments \[num\_n\_mmas, group\_size \* b\_frag\_size].
* â€‹c\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Accumulation matrix c fragments, modified in-place.

</section>

---

## get_fragment_size

<section class='mojo-docs'>

`get_fragment_size[mma_shape: IndexList[3]]() -> IndexList[3]`

Calculates the fragment size per thread for a given MMA shape.

For tensor core operations, each thread in a warp handles a portion of the
computation. This function determines how many elements each thread needs to
process for the A, B, and C/D matrices based on the MMA shape.

**Parameters:**

* â€‹mma\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): An `IndexList[3]` containing the MMA dimensions \[M, N, K].

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): An `IndexList[3]` containing the fragment sizes per thread for matrices
A, B, and C/D respectively, calculated as:
`[M*K/WARP_SIZE, N*K/WARP_SIZE, M*N/WARP_SIZE]`.

</section>

---

## get_mma_shape

<section class='mojo-docs'>

`get_mma_shape[input_type: DType, accum_type: DType, shape_id: Int = 0]() -> IndexList[3]`

Returns the appropriate matrix multiply-accumulate (MMA) shape for tensor core operations.

Selects the optimal MMA shape based on the GPU architecture, input data type,
accumulation data type, and optional shape identifier. This function handles
different configurations for both NVIDIA and AMD GPUs.

**Parameters:**

* â€‹input\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the input matrices (A and B).
* â€‹accum\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The data type used for accumulation (C and D).
* â€‹shape\_id ([`Int`](/mojo/std/builtin/int/Int)): Optional identifier to select between multiple valid shapes (default: 0).

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): An `IndexList[3]` containing the MMA dimensions in the format `[M, N, K]`,
where `MxN` is the output matrix size and `K` is the reduction dimension.

</section>

---

## tensor_core

<section class='mojo-docs'>

Tensor Core Module for High-Performance Matrix Operations

Provides abstractions for using GPU Tensor Cores to perform optimized matrix operations.
It supports both NVIDIA and AMD GPU architectures with hardware-specific optimizations.

## Key Components:

* `TensorCore`: Core struct that encapsulates tensor core operations with support for various
  data types and matrix shapes. It handles loading matrix fragments, performing matrix
  multiply-accumulate operations, and storing results.

* Matrix Fragment Management: Functions for loading and storing matrix fragments to/from
  shared memory with hardware-specific optimizations.

* Matrix Multiply-Accumulate (MMA): Optimized implementations of matrix multiplication
  operations using tensor cores.

## Supported Operations:

* Matrix loading with various layouts and swizzling patterns
* Matrix multiply-accumulate (D = A \* B + C)
* Matrix storing with hardware-specific optimizations

## Supported Data Types:

* NVIDIA: float32, bfloat16, float16, float8\_e4m3fn, float8\_e5m2
* AMD: float32, bfloat16, float16

## Supported Matrix Shapes:

* NVIDIA: 16x8x8, 16x8x4, 16x8x16, 8x8x4, 16x8x32
* AMD: 16x16x4, 16x16x16, 32x32x8

## `comptime` values

### `shape_16x16x16`

`comptime shape_16x16x16 = IndexList[3, DType.int64](16, 16, 16, Tuple[]())`

AMDGPU tensor core shape 16x16x16.

### `shape_16x16x32`

`comptime shape_16x16x32 = IndexList[3, DType.int64](16, 16, 32, Tuple[]())`

AMDGPU tensor core shape 16x16x32.

### `shape_16x16x4`

`comptime shape_16x16x4 = IndexList[3, DType.int64](16, 16, 4, Tuple[]())`

AMDGPU tensor core shape 16x16x4.

### `shape_16x8x16`

`comptime shape_16x8x16 = IndexList[3, DType.int64](16, 8, 16, Tuple[]())`

Tensor core shape 16x8x16.

### `shape_16x8x32`

`comptime shape_16x8x32 = IndexList[3, DType.int64](16, 8, 32, Tuple[]())`

Tensor core shape 16x8x32.

### `shape_16x8x4`

`comptime shape_16x8x4 = IndexList[3, DType.int64](16, 8, 4, Tuple[]())`

Tensor core shape 16x8x4.

### `shape_16x8x8`

`comptime shape_16x8x8 = IndexList[3, DType.int64](16, 8, 8, Tuple[]())`

Tensor core shape 16x8x8.

### `shape_32x32x16`

`comptime shape_32x32x16 = IndexList[3, DType.int64](32, 32, 16, Tuple[]())`

AMDGPU tensor core shape 32x32x16.

### `shape_32x32x8`

`comptime shape_32x32x8 = IndexList[3, DType.int64](32, 32, 8, Tuple[]())`

AMDGPU tensor core shape 32x32x8.

### `shape_8x8x4`

`comptime shape_8x8x4 = IndexList[3, DType.int64](8, 8, 4, Tuple[]())`

Tensor core shape 8x8x4.

### `shape_null`

`comptime shape_null = IndexList[3, DType.int64](0, 0, 0, Tuple[]())`

Null tensor core shape (0x0x0).

## Structs

* [â€‹`TensorCore`](./TensorCore): TensorCore provides an abstraction for GPU tensor core hardware to perform optimized matrix operations.
* [â€‹`TiledTensorCore`](./TiledTensorCore): TiledTensorCore provides a wrapper around TensorCore to support multiple MMAs along the K dimension.

## Functions

* [â€‹`get_fragment_size`](./get_fragment_size): Calculates the fragment size per thread for a given MMA shape.
* [â€‹`get_mma_shape`](./get_mma_shape): Returns the appropriate matrix multiply-accumulate (MMA) shape for tensor core operations.
* [â€‹`load_b_nt`](./load_b_nt): Loads the b operand tile for AMD tensor core MFMA from (N, K) storage.
* [â€‹`load_b_tr`](./load_b_tr): Loads the b operand tile for AMD tensor core MFMA instructions using transposed memory access.
* [â€‹`num_matrix_reg`](./num_matrix_reg): Calculates the number of matrix registers required per thread.

</section>

---

## load_b_nt

<section class='mojo-docs'>

`load_b_nt[mma_shape: IndexList[3], swizzle: OptionalReg[Swizzle] = OptionalReg[Swizzle]()](tile: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> SIMD[dtype, 8]`

Loads the b operand tile for AMD tensor core MFMA from (N, K) storage.

This function supports double-rate MFMA shapes (32x32x16, 16x16x32) with bfloat16 input.
Unlike load\_b\_tr which expects (K, N) storage, this function works with (N, K) storage
which is common when transpose\_b=True and B is stored row-major.

The input tile (shape = (mma\_shape\[1], mma\_shape\[2])) is split along the K dimension into
two halves of shape (MMA\_N, MMA\_K//2). Each half is loaded using `_load_tr16_b64_warp`,
which performs a transposed (column-major) load from shared memory. The hardware transpose
effectively converts the (N, K) storage to (K, N) format needed by MMA.

Example:
For 16x16x32 MMA with B stored as (N, K) = (16, 32) in LDS:

```mojo
# B tile in LDS: shape (16, 32) = (MMA_N, MMA_K)
var b_tile = smem_b.tile[16, 32](n_idx, k_idx)
var b_reg = load_b_nt[IndexList[3](16, 16, 32)](b_tile)
# b_reg now contains 8 bf16 values ready for MFMA
```

**Parameters:**

* â€‹mma\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The MMA instruction tile shape (only 32x32x16 or 16x16x32 supported).
* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional swizzle pattern for bank-conflict-free LDS access.

**Args:**

* â€‹tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): A `LayoutTensor`, residing in shared memory, with shape (mma\_shape\[1], mma\_shape\[2])
  and dtype `DType.bfloat16`. This is (N, K) storage order.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): SIMD\[tile.dtype, 8]: Concatenated transposed SIMD loads from both halves of the tile.

</section>

---

## load_b_tr

<section class='mojo-docs'>

`load_b_tr[mma_shape: IndexList[3], swizzle: OptionalReg[Swizzle] = OptionalReg[Swizzle]()](tile: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> SIMD[dtype, 8]`

Loads the b operand tile for AMD tensor core MFMA instructions using transposed memory access.

This function supports double-rate MFMA shapes (32x32x16, 16x16x32) with bfloat16 input.
The input tile (shape = (mma\_shape\[2], mma\_shape\[1])) is split along the K dimension into
two halves of shape (MMA\_K//2, MMA\_N). Each half is loaded using `_load_tr16_b64_warp`, which
performs a transposed (column-major) load from shared memory. The resulting two 4-element SIMD
vectors are concatenated into a single `SIMD[tile.dtype, 8]` vector.

**Parameters:**

* â€‹mma\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The MMA instruction tile shape (only 32x32x16 or 16x16x32 supported).
* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional swizzle pattern for bank-conflict-free LDS access.

**Args:**

* â€‹tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): A `LayoutTensor`, residing in shared memory, with shape (mma\_shape\[2], mma\_shape\[1])
  and dtype `DType.bfloat16`.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): SIMD\[tile.dtype, 8]: Concatenated transposed SIMD loads from both halves of the tile.

</section>

---

## num_matrix_reg

<section class='mojo-docs'>

`num_matrix_reg[dim_1: Int, dim_2: Int]() -> Int`

Calculates the number of matrix registers required per thread.

Determines how many registers each thread in a warp needs to store a matrix
of the given dimensions. This is calculated by dividing the total number of
elements (dim\_1 \* dim\_2) by the warp size, as the matrix is distributed
across all threads in the warp.

**Parameters:**

* â€‹dim\_1 ([`Int`](/mojo/std/builtin/int/Int)): First dimension of the matrix.
* â€‹dim\_2 ([`Int`](/mojo/std/builtin/int/Int)): Second dimension of the matrix.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The number of matrix registers needed per thread.

</section>

---

## TensorCoreAsync

<section class='mojo-docs'>

`struct TensorCoreAsync[c_type: DType, a_type: DType, b_type: DType, mma_shape: IndexList[3], /, a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_NONE, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_NONE, transpose_b: Bool = False]`

High-performance asynchronous tensor core operations for matrix multiplication.

This struct provides methods for utilizing NVIDIA's Tensor Cores for asynchronous
matrix multiplication operations, with support for various data types and swizzling
configurations.

## Parameters

* â€‹c\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the output matrix C.
* â€‹a\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the input matrix A.
* â€‹b\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the input matrix B.
* â€‹mma\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): Dimensions for the matrix multiply-accumulate (MMA) operation as \[M, N, K].
* â€‹a\_swizzle ([`TensorMapSwizzle`](/mojo/std/gpu/host/nvidia/tma/TensorMapSwizzle)): Swizzling mode for matrix A (default: SWIZZLE\_NONE).
* â€‹b\_swizzle ([`TensorMapSwizzle`](/mojo/std/gpu/host/nvidia/tma/TensorMapSwizzle)): Swizzling mode for matrix B (default: SWIZZLE\_NONE).
* â€‹transpose\_b ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to transpose matrix B (default: False).

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

## Methods

### `__init__`

`__init__(out self)`

Initialize the `TensorCoreAsync` instance.

Ensures that the provided MMA shape is supported.

Note:
Fails to compile if `mma_shape` is not supported.

### `wgmma`

`static wgmma[num_warp_groups: Int = 1, scale_c: Int = 1, scale_a: Int = 1, scale_b: Int = 1, num_k_iters: OptionalReg[Int] = None](a_smem_tile: LayoutTensor[a_type, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_smem_tile: LayoutTensor[b_type, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_reg_tile: LayoutTensor[c_type, layout, origin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], wg_idx: Int = 0)`

Perform asynchronous matrix multiplication using warp group matrix multiply-accumulate (WGMMA).

This method handles the case where both A and B matrices are in shared memory.

**Parameters:**

* â€‹num\_warp\_groups ([`Int`](/mojo/std/builtin/int/Int)): Number of warp groups to distribute work across (default: 1).
* â€‹scale\_c ([`Int`](/mojo/std/builtin/int/Int)): Scale factor for matrix C. Valid values are 1 or 0 (default: 1).
* â€‹scale\_a ([`Int`](/mojo/std/builtin/int/Int)): Scale factor for matrix A. Valid values are 1 or -1 (default: 1).
* â€‹scale\_b ([`Int`](/mojo/std/builtin/int/Int)): Scale factor for matrix B. Valid values are 1 or -1 (default: 1).
* â€‹num\_k\_iters ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Number of iterations for the K dimension. This is useful to save computation when we pad shared memory. (default: None which is just `a_smem_layout[1].size() // mma_shape[2]`).

**Args:**

* â€‹a\_smem\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Matrix A in shared memory.
* â€‹b\_smem\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Matrix B in shared memory.
* â€‹c\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output matrix C in register memory.
* â€‹wg\_idx ([`Int`](/mojo/std/builtin/int/Int)): Warp group index for multi-warp group scenarios (default: 0).

`static wgmma(a_frag_tile: LayoutTensor[a_type, layout, origin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_smem_tile: LayoutTensor[b_type, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_reg_tile: LayoutTensor[c_type, layout, origin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Perform asynchronous matrix multiplication using warp group matrix multiply-accumulate (WGMMA).

This overloaded method handles the case where matrix A is in register memory and matrix B
is in shared memory.

**Args:**

* â€‹a\_frag\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Matrix A in register memory.
* â€‹b\_smem\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Matrix B in shared memory.
* â€‹c\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output matrix C in register memory.

### `arrive`

`static arrive()`

Ensures memory consistency by creating a fence for WGMMA operations.

This method should be called before committing a group to ensure all
shared memory accesses are properly aligned and visible.

### `commit_group`

`static commit_group()`

Commits the current warp group for execution.

This synchronizes the warp group and commits all pending WGMMA operations
that have been previously issued.

### `wait_group`

`static wait_group[group: Int = 0]()`

Waits for the completion of a specific warp group's operations.

This method blocks until all WGMMA operations from the specified group are complete.

**Parameters:**

* â€‹group ([`Int`](/mojo/std/builtin/int/Int)): The group ID to wait for (default: 0).

</section>

---

## tensor_core_async

<section class='mojo-docs'>

Tensor Core Async Module

This module provides high-performance abstractions for utilizing NVIDIA's Tensor Cores
to perform asynchronous matrix multiplication operations. It implements optimized memory
layouts and access patterns for efficient tensor core computations.

Key components:

* Layout creation functions for K-major and MN-major memory arrangements
* Swizzling support for improved memory access patterns
* WGMMA (Warp Group Matrix Multiply-Accumulate) descriptor generation
* TensorCoreAsync struct with methods for asynchronous matrix multiplication

The module supports various data types, matrix dimensions, and memory configurations,
enabling efficient implementation of deep learning primitives and other tensor operations
that can leverage hardware acceleration.

Performance features:

* Asynchronous execution model to overlap computation and memory access
* Support for different swizzling modes to optimize memory bandwidth
* Efficient register and shared memory utilization
* Support for multi-warp group execution

This implementation is specifically optimized for NVIDIA GPUs with Tensor Core support.

## `comptime` values

### `WGMMA_K_BYTES`

`comptime WGMMA_K_BYTES = 32`

Size of WGMMA K dimension in bytes.

## Structs

* [â€‹`TensorCoreAsync`](./TensorCoreAsync): High-performance asynchronous tensor core operations for matrix multiplication.

## Functions

* [â€‹`select_k_atom`](./select_k_atom): Creates a core matrix layout for tensor core operations.
* [â€‹`st_matrix_n_atom`](./st_matrix_n_atom): Creates a layout for N-major `st_matrix` atom in the context of WGMMA C matrix.
* [â€‹`st_matrix_n_layout`](./st_matrix_n_layout): Creates a layout for N-major `st_matrix` in the context of WGMMA C matrix.
* [â€‹`tile_layout_k_major`](./tile_layout_k_major): Creates a K-major layout for tensor core operations.
* [â€‹`tile_layout_mn_major`](./tile_layout_mn_major): Creates an MN-major layout for tensor core operations.
* [â€‹`tile_sf_layout_k_major`](./tile_sf_layout_k_major): Creates a K-major layout for tensor core scale factors.
* [â€‹`tile_to_descriptor`](./tile_to_descriptor): Transforms a layout into a WGMMA descriptor-compatible layout.
* [â€‹`warpgroup_fence`](./warpgroup_fence): Code motion fence to ensure the registers of the WGMMA instruction do not get touched by anything.
* [â€‹`wgmma_c_layout`](./wgmma_c_layout): Generates three layouts for mapping WGMMA C matrix coordinates.
* [â€‹`wgmma_c_thread_layout`](./wgmma_c_thread_layout): Returns the thread layout component for WGMMA C matrix.
* [â€‹`wgmma_output_layout`](./wgmma_output_layout): Returns the output layout component for WGMMA C matrix.

</section>

---

## select_k_atom

<section class='mojo-docs'>

`select_k_atom[dtype: DType, swizzle_mode: TensorMapSwizzle]() -> Layout`

Creates a core matrix layout for tensor core operations.

Constructs the fundamental atomic layout for tensor core operations based on the
specified data type and swizzle mode. This layout represents the minimal dense
matrix structure that can be efficiently processed by tensor cores.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Element data type of the tensor.
* â€‹swizzle\_mode ([`TensorMapSwizzle`](/mojo/std/gpu/host/nvidia/tma/TensorMapSwizzle)): Memory access pattern swizzling mode.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): `Layout` - A core matrix layout optimized for tensor core operations.

</section>

---

## st_matrix_n_atom

<section class='mojo-docs'>

`st_matrix_n_atom[num_stmatrix: Int]() -> Layout`

Creates a layout for N-major `st_matrix` atom in the context of WGMMA C matrix.

The domain of this layout is the warp group local thread index. Thus, the
layout takes \[0, 128) as input and returns an offset for a logical array
with an element size of 128-bit.

**Parameters:**

* â€‹num\_stmatrix ([`Int`](/mojo/std/builtin/int/Int)): Number of N-dimension tiles in the C matrix.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): `Layout` - A layout that maps warp group local thread index to an offset
for a logical array with an element size of 128-bit.

</section>

---

## st_matrix_n_layout

<section class='mojo-docs'>

`st_matrix_n_layout[c_type: DType, WG_BN: Int, num_m_mmas: Int, num_consumer: Int]() -> Layout`

Creates a layout for N-major `st_matrix` in the context of WGMMA C matrix.

The layout modes are: the warp group local thread index, the N-dimension
tiling size `WG_BN // 16`, the number of MMA tiles `num_m_mmas` in the
M-dimension, and the number of consumers `num_consumer`. The output is an
offset for a logical array with the element type `c_type`.

**Parameters:**

* â€‹c\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the C matrix.
* â€‹WG\_BN ([`Int`](/mojo/std/builtin/int/Int)): Size of the K dimension in the C matrix in shared memory.
* â€‹num\_m\_mmas ([`Int`](/mojo/std/builtin/int/Int)): Number of MMA tiles in the M dimension.
* â€‹num\_consumer ([`Int`](/mojo/std/builtin/int/Int)): Number of consumers.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): `Layout` - A layout that maps warp group local thread index to an offset
for a logical array with the element type `c_type`.

</section>

---

## tile_layout_k_major

<section class='mojo-docs'>

`tile_layout_k_major[dtype: DType, BM: Int, BK: Int, swizzle_mode: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_NONE]() -> Layout`

Creates a K-major layout for tensor core operations.

Constructs a layout optimized for K-major access patterns in tensor core operations,
with optional swizzling for improved memory access patterns.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Element data type of the tensor.
* â€‹BM ([`Int`](/mojo/std/builtin/int/Int)): Size of the M dimension in the tile.
* â€‹BK ([`Int`](/mojo/std/builtin/int/Int)): Size of the K dimension in the tile.
* â€‹swizzle\_mode ([`TensorMapSwizzle`](/mojo/std/gpu/host/nvidia/tma/TensorMapSwizzle)): Memory access pattern swizzling mode (default: SWIZZLE\_NONE).

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): `Layout` - A K-major layout configured for the specified dimensions and swizzle mode.

</section>

---

## tile_layout_mn_major

<section class='mojo-docs'>

`tile_layout_mn_major[dtype: DType, mn_dim: Int, k_dim: Int, swizzle_mode: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_NONE]() -> Layout`

Creates an MN-major layout for tensor core operations.

Constructs a unit layout optimized for MN-major access patterns in shared memory,
with optional swizzling for improved memory access patterns.

Note:
This returns the "unit" layout; the actual shared memory layout can be a multiple of this unit.
Currently only supports SWIZZLE\_NONE and SWIZZLE\_128B modes.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Element data type of the tensor.
* â€‹mn\_dim ([`Int`](/mojo/std/builtin/int/Int)): Size of the MN dimension.
* â€‹k\_dim ([`Int`](/mojo/std/builtin/int/Int)): Size of the K dimension.
* â€‹swizzle\_mode ([`TensorMapSwizzle`](/mojo/std/gpu/host/nvidia/tma/TensorMapSwizzle)): Memory access pattern swizzling mode (default: SWIZZLE\_NONE).

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): `Layout` - An MN-major layout configured for the specified dimensions and swizzle mode.

</section>

---

## tile_sf_layout_k_major

<section class='mojo-docs'>

`tile_sf_layout_k_major[BM: Int, BK: Int, SF_SCALE_SIZE: Int]() -> Layout`

Creates a K-major layout for tensor core scale factors.

Constructs a layout for K-major access patterns for scale factors.

**Parameters:**

* â€‹BM ([`Int`](/mojo/std/builtin/int/Int)): Size of the M dimension in the tile.
* â€‹BK ([`Int`](/mojo/std/builtin/int/Int)): Size of the K dimension in the tile.
* â€‹SF\_SCALE\_SIZE ([`Int`](/mojo/std/builtin/int/Int)): Number of elements in a scale factor vector.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): `Layout` - A K-major layout configured for the specified dimensions and scale factor size.

</section>

---

## tile_to_descriptor

<section class='mojo-docs'>

`tile_to_descriptor[dtype: DType, layout: Layout, is_k_major: Bool = True]() -> Layout`

Transforms a layout into a WGMMA descriptor-compatible layout.

Converts a standard layout into a form that can be used with WGMMA descriptors,
handling both K-major and MN-major layouts differently.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Element data type of the tensor.
* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Input layout to transform.
* â€‹is\_k\_major ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether the layout is K-major (True) or MN-major (False).

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): \`Layout - A transformed layout compatible with WGMMA descriptors.

</section>

---

## warpgroup_fence

<section class='mojo-docs'>

`warpgroup_fence[accum_type: DType, accum_layout: Layout, //](accum: LayoutTensor[accum_type, accum_layout, origin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Code motion fence to ensure the registers of the WGMMA instruction do not get touched by anything.

This has no impact on kernel correctness. It serves purely as an NVVM code motion barrier,
preventing other operations from modifying the WGMMA instruction's
registers during execution of the WGMMA instruction batch.

**Parameters:**

* â€‹accum\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Element data type of the tensor.
* â€‹accum\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Register layout of the accumulator.

**Args:**

* â€‹accum ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): A LayoutTensor with the accum\_type and accum\_layout.

</section>

---

## wgmma_c_layout

<section class='mojo-docs'>

`wgmma_c_layout[mma_m: Int, mma_n: Int, C: Layout]() -> List[Layout]`

Generates three layouts for mapping WGMMA C matrix coordinates.

This function creates three layout mappings that are essential for working with WGMMA
(Warp Group Matrix Multiply-Accumulate) operations:

1. A projection layout that maps linearized indices to row coordinates (i)
2. A projection layout that maps linearized indices to column coordinates (j)
3. A composite layout that maps thread and vector coordinates to linearized indices
   across multiple MMA tiles

These layouts are particularly useful for operations like attention masking and
matrix multiplication epilogues, where register values need to be mapped to the
coordinate system of the C matrix.

Note:
This function enforces constraints on the WGMMA dimensions and ensures the C matrix
dimensions are compatible with the WGMMA instruction size.

**Parameters:**

* â€‹mma\_m ([`Int`](/mojo/std/builtin/int/Int)): The M dimension (rows) of a single WGMMA instruction, must be 64.
* â€‹mma\_n ([`Int`](/mojo/std/builtin/int/Int)): The N dimension (columns) of a single WGMMA instruction, must be multiple of 8.
* â€‹C ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the C matrix within a thread block.

**Returns:**

[`List`](/mojo/std/collections/list/List): `List[Layout]` - A list containing three layouts:

1. proj\_i: Maps linearized indices to row coordinates
2. proj\_j: Maps linearized indices to column coordinates
3. TV\_tile\_to\_idx: Maps thread/vector/tile coordinates to linearized indices

</section>

---

## wgmma_c_thread_layout

<section class='mojo-docs'>

`wgmma_c_thread_layout[C: Layout]() -> Layout`

Returns the thread layout component for WGMMA C matrix.

Generates the first mode of the WGMMA C layout, which maps thread coordinates
to linearized indices in the output matrix.

**Parameters:**

* â€‹C ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the C matrix.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): `Layout` - A layout mapping thread coordinates to linearized indices.

</section>

---

## wgmma_output_layout

<section class='mojo-docs'>

`wgmma_output_layout[mma_n: Int, C: Layout]() -> Layout`

Returns the output layout component for WGMMA C matrix.

Generates the second mode of the WGMMA C layout, which maps output vector
coordinates to linearized indices in the output matrix.

**Parameters:**

* â€‹mma\_n ([`Int`](/mojo/std/builtin/int/Int)): The N dimension of the WGMMA instruction.
* â€‹C ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the C matrix.

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout): `Layout` - A layout mapping output vector coordinates to linearized indices.

</section>

---

## PipelineState

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct PipelineState[num_stages: Int]`

Manages state for a multi-stage pipeline with circular buffer semantics.

PipelineState provides a mechanism for tracking the current stage in a
multi-stage pipeline, particularly useful for double or triple buffering
in GPU tensor operations. It maintains an index that cycles through the
available stages, a phase bit that toggles when the index wraps around,
and a monotonically increasing count.

This struct is commonly used with TMA operations to coordinate the use of
multiple buffers in a pipeline fashion, allowing for overlapping computation
and data transfer.

## Parameters

* â€‹num\_stages ([`Int`](/mojo/std/builtin/int/Int)): The number of stages in the pipeline (e.g., 2 for double buffering,
  3 for triple buffering).

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__() -> Self`

Initialize a PipelineState with default values.

Creates a new PipelineState with index 0, phase 0, and count 0.

`__init__(index: Int, phase: Int, count: Int) -> Self`

Initialize a PipelineState with specific values.

Creates a new PipelineState with the specified index, phase, and count.

**Args:**

* â€‹index ([`Int`](/mojo/std/builtin/int/Int)): The initial stage index.
* â€‹phase ([`Int`](/mojo/std/builtin/int/Int)): The initial phase value (0 or 1).
* â€‹count ([`Int`](/mojo/std/builtin/int/Int)): The initial count value.

### `index`

`index(self) -> UInt32`

Get the current stage index.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): The current index value, which ranges from 0 to num\_stages-1.

### `phase`

`phase(self) -> UInt32`

Get the current phase bit.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): The current phase value (0 or 1), which toggles when the index wraps around.

### `step`

`step(mut self)`

Advance the pipeline state to the next stage.

Increments the index and count. When the index reaches num\_stages,
it wraps around to 0 and toggles the phase bit.

This function is used to move to the next buffer in a multi-buffer
pipeline, implementing circular buffer semantics.

### `next`

`next(mut self) -> Self`

Advance the pipeline state to the next stage and return the new state.

This function is used to move to the next buffer in a multi-buffer
pipeline, implementing circular buffer semantics.

**Returns:**

`Self`: The new pipeline state after advancing to the next stage.

### `__enter__`

`__enter__(var self) -> Self`

Enter the context manager.

**Returns:**

`Self`: The pipeline state instance for use in a `with` statement.

</section>

---

## RaggedTensorMap

<section class='mojo-docs'>

`struct RaggedTensorMap[descriptor_rank: Int, //, dtype: DType, descriptor_shape: IndexList[descriptor_rank], remaining_global_dim_rank: Int, swizzle_mode: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_NONE]`

Creates a TMA descriptor that can handle stores with varying lengths. This struct is mainly used for MHA, where sequence lengths may vary between sample.

This struct only supports one dimension being ragged. The continous dimension (where stride is 1) cannot be ragged.

## Parameters

* â€‹descriptor\_rank ([`Int`](/mojo/std/builtin/int/Int)):
  The rank of the descriptor shape (inferred).
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)):
  The data type of the tensor.
* â€‹descriptor\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)):
  The shape of the shared memory descriptor.
* â€‹remaining\_global\_dim\_rank ([`Int`](/mojo/std/builtin/int/Int)):
  The rank of the remaining global tensor dimensions.
* â€‹swizzle\_mode ([`TensorMapSwizzle`](/mojo/std/gpu/host/nvidia/tma/TensorMapSwizzle)):
  The swizzling mode to use for memory access optimization. Swizzling can improve
  memory access patterns for specific hardware configurations. Defaults to SWIZZLE\_NONE.

## Fields

* â€‹descriptor (`TMADescriptor`): The TMA descriptor that will be used to store the ragged tensor.
* â€‹max\_length (`Int`): The maximum length present in the sequences of the ragged tensor.
* â€‹global\_shape (`IndexList[RaggedTensorMap[dtype, descriptor_shape, remaining_global_dim_rank, swizzle_mode].global_rank]`): The shape of the global tensor.
* â€‹global\_stride (`IndexList[RaggedTensorMap[dtype, descriptor_shape, remaining_global_dim_rank, swizzle_mode].global_rank]`): The stride of the global tensor.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = False`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `device_type`

`comptime device_type = RaggedTensorMap[dtype, descriptor_shape, remaining_global_dim_rank, swizzle_mode]`

The TensorMapDescriptorArray type.

### `global_rank`

`comptime global_rank = (remaining_global_dim_rank + 3)`

The rank of the global tensor.

### `ragged_descriptor_shape`

`comptime ragged_descriptor_shape = RaggedTensorMap._descriptor_shape[descriptor_rank, dtype, descriptor_shape, remaining_global_dim_rank, swizzle_mode]()`

The shape of the descriptor that will tile and load from shared -> global memory.

## Methods

### `__init__`

`__init__(out self, ctx: DeviceContext, global_ptr: LegacyUnsafePointer[Scalar[dtype]], max_length: Int, ragged_stride: Int, batch_size: Int, global_last_dim: Int, remaining_global_dims: IndexList[remaining_global_dim_rank], remaining_global_stride: IndexList[remaining_global_dim_rank])`

Initializes a TensorMapDescriptorArray with descriptors for all power-of-2 lengths.

This constructor creates a complete set of TMA descriptors, one for each power of 2
from 1 up to max\_descriptor\_length. Each descriptor is configured to handle a different
first dimension size (1, 2, 4, 8, ..., max\_descriptor\_length) while maintaining the
same remaining tile shape specified by desc\_remaining\_tile\_shape.

**Args:**

* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)):
  The device context used to create the TMA descriptors.
* â€‹global\_ptr (`LegacyUnsafePointer`):
  The source tensor in global memory that will be accessed using the descriptors.
* â€‹max\_length ([`Int`](/mojo/std/builtin/int/Int)):
  The maximum length present in the sequences of the ragged tensor.
* â€‹ragged\_stride ([`Int`](/mojo/std/builtin/int/Int)):
  The stride of the ragged dimension in the global tensor.
* â€‹batch\_size ([`Int`](/mojo/std/builtin/int/Int)):
  The total number of sequences in the ragged tensor.
* â€‹global\_last\_dim ([`Int`](/mojo/std/builtin/int/Int)):
  The last dimension of the global tensor.
* â€‹remaining\_global\_dims ([`IndexList`](/mojo/std/utils/index_/IndexList)):
  The dimensions of the remaining global tensor.
* â€‹remaining\_global\_stride ([`IndexList`](/mojo/std/utils/index_/IndexList)):
  The stride of the remaining global tensor.

**Raises:**

If the operation fails.

### `get_type_name`

`static get_type_name() -> String`

Returns a string representation of the TensorMapDescriptorArray type.

**Returns:**

`String`: A string containing the type name with all template parameters.

### `get_device_type_name`

`static get_device_type_name() -> String`

Returns the device type name for this descriptor array.

**Returns:**

`String`: A string containing the type name with all template parameters.

### `store_ragged_tile`

`store_ragged_tile[rank: Int, //, using_max_descriptor_size: Bool = False](self, coordinates: IndexList[rank], preceding_cumulative_length: Int, store_length: Int, mut tile_iterator: LayoutTensorIter[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked])`

Stores a ragged tile from shared memory to global memory.

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)):
  The rank of the coordinates.
* â€‹using\_max\_descriptor\_size ([`Bool`](/mojo/std/builtin/bool/Bool)):
  If True, optimizes the store around the max descriptor size.

**Args:**

* â€‹coordinates ([`IndexList`](/mojo/std/utils/index_/IndexList)):
  The starting coordinates of all dimensions except the ragged dimension.
* â€‹preceding\_cumulative\_length ([`Int`](/mojo/std/builtin/int/Int)):
  The cumulative length of the preceding sequences.
* â€‹store\_length ([`Int`](/mojo/std/builtin/int/Int)):
  The length of the current sequence to be stored.
* â€‹tile\_iterator ([`LayoutTensorIter`](/mojo/kernels/layout/layout_tensor/LayoutTensorIter)):
  The iterator over the tile in shared memory.

### `prefetch_descriptor`

`prefetch_descriptor(self)`

Prefetches the TMA descriptor into cache.

</section>

---

## SharedMemBarrier

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SharedMemBarrier`

A hardware-accelerated synchronization primitive for GPU shared memory operations.

This struct provides a barrier mechanism optimized for coordinating thread execution
and memory transfers in GPU kernels, particularly for Tensor Memory Accelerator (TMA)
operations. It enables efficient synchronization between threads and memory operations
by leveraging hardware-specific barrier instructions.

Key features:

* Thread synchronization across thread blocks
* Memory transfer completion tracking
* Hardware-accelerated barrier operations
* Support for phased synchronization

This barrier is particularly useful for ensuring that shared memory operations
complete before dependent computations begin, which is critical for maintaining
data consistency in high-performance GPU kernels.

## Fields

* â€‹mbar (`Int64`): Shared memory location used for the barrier state.
  This field stores an 8-byte aligned shared memory location that
  maintains the state of the barrier. The memory must be in shared address
  space to be accessible by all threads in a block.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `init`

`init(ref [3] self, num_threads: Int32 = 1)`

Initialize the barrier state with the expected number of threads.

Sets up the barrier to expect arrivals from the specified number of threads
before it can be satisfied. This is essential for coordinating thread
synchronization in GPU kernels.

**Args:**

* â€‹num\_threads ([`Int32`](/mojo/std/builtin/simd/#int32)): Number of threads that must arrive at the barrier
  before it is satisfied. Defaults to 1.

### `expect_bytes`

`expect_bytes(ref [3] self, bytes: Int32)`

Configure the barrier to expect a specific number of bytes to be transferred.

Used with TMA operations to indicate the expected size of data transfer.
The barrier will be satisfied when the specified number of bytes has been
transferred, enabling efficient coordination of memory operations.

**Args:**

* â€‹bytes ([`Int32`](/mojo/std/builtin/simd/#int32)): Number of bytes expected to be transferred.

### `expect_bytes_relaxed`

`expect_bytes_relaxed(ref [3] self, bytes: Int32) -> UInt64`

Configure the barrier to expect a specific number of bytes to be transferred.

Used with TMA operations to indicate the expected size of data transfer.
The barrier will be satisfied when the specified number of bytes has been
transferred, enabling efficient coordination of memory operations.

**Args:**

* â€‹bytes ([`Int32`](/mojo/std/builtin/simd/#int32)): Number of bytes expected to be transferred.

**Returns:**

`UInt64`: The state.

### `arrive_and_expect_bytes`

`arrive_and_expect_bytes(ref [3] self, bytes: Int32, cta_id: UInt32, pred: UInt32)`

Configure the barrier to expect a specific number to bytes to be transferred at a remote CTA.

Used with TMA operations to indicate the expected size of data transfer.
The barrier will be satisfied when the specified number of bytes has been
transferred at the specified CTA in the cluster.

**Args:**

* â€‹bytes ([`Int32`](/mojo/std/builtin/simd/#int32)): Number of bytes expected to be transferred.
* â€‹cta\_id ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The CTA ID in a cluster to configure an arrival.
* â€‹pred ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Predication on the arrival configuration instruction. Use UInt32 to match `selp.u32` in ptx.

### `wait`

`wait[ticks: Optional[UInt32] = None](ref [3] self, phase: UInt32 = 0)`

Wait until the barrier is satisfied.

Blocks the calling thread until the barrier is satisfied, either by
the expected number of threads arriving or the expected data transfer
completing. This method implements an efficient spin-wait mechanism
optimized for GPU execution.

Note:
Minimizes thread divergence during synchronization by using
hardware-accelerated barrier instructions.

**Parameters:**

* â€‹ticks ([`Optional`](/mojo/std/collections/optional/Optional)): The number of ticks to wait before timing out in nanoseconds.
  Defaults to None.

**Args:**

* â€‹phase ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The phase value to check against. Defaults to 0.

### `wait_acquire`

`wait_acquire[scope: Scope](ref [3] self, phase: UInt32 = 0)`

Acquire and wait until the barrier is satisfied.

Blocks the calling thread until the barrier is satisfied, either by
the expected number of threads arriving or the expected data transfer
completing. This method implements an efficient spin-wait mechanism
optimized for GPU execution.

Note:
Minimizes thread divergence during synchronization by using
hardware-accelerated barrier instructions.

**Parameters:**

* â€‹scope ([`Scope`](/mojo/std/gpu/intrinsics/Scope)): The scope of the barrier.

**Args:**

* â€‹phase ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The phase value to check against. Defaults to 0.

### `wait_relaxed`

`wait_relaxed[scope: Scope](ref [3] self, phase: UInt32 = 0)`

Wait until the barrier is satisfied with relaxed ordering.

Blocks the calling thread until the barrier is satisfied, either by
the expected number of threads arriving or the expected data transfer
completing. This method implements an efficient spin-wait mechanism
optimized for GPU execution.

Note:
Minimizes thread divergence during synchronization by using
hardware-accelerated barrier instructions.

**Parameters:**

* â€‹scope ([`Scope`](/mojo/std/gpu/intrinsics/Scope)): The scope of the barrier.

**Args:**

* â€‹phase ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The phase value to check against. Defaults to 0.

### `unsafe_ptr`

`unsafe_ptr[mut: Bool, //, origin: Origin[mut=mut]](ref [origin, 3] self) -> LegacyUnsafePointer[Int64, address_space=AddressSpace.SHARED, mut=mut, origin=origin]`

Get an unsafe pointer to the barrier's memory location.

Provides low-level access to the shared memory location storing the barrier state.
This method is primarily used internally by other barrier operations that need
direct access to the underlying memory.

**Parameters:**

* â€‹mut ([`Bool`](/mojo/std/builtin/bool/Bool)): Mutability of self.
* â€‹origin ([`Origin`](/mojo/std/builtin/type_aliases/Origin)): Origin of self.

**Returns:**

`LegacyUnsafePointer`: An unsafe pointer to the barrier's memory location in shared memory,
properly typed and aligned for barrier operations.

### `arrive_cluster`

`arrive_cluster(ref [3] self, cta_id: UInt32, count: UInt32 = 1)`

Signal arrival at the barrier from a specific CTA (Cooperative Thread Array) in a cluster.

This method is used in multi-CTA scenarios to coordinate barrier arrivals
across different CTAs within a cluster. It enables efficient synchronization
across thread blocks in clustered execution models.

**Args:**

* â€‹cta\_id ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The ID of the CTA (Cooperative Thread Array) that is arriving.
* â€‹count ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The number of arrivals to signal. Defaults to 1.

### `arrive`

`arrive(ref [3] self) -> Int`

Signal arrival at the barrier and return the arrival count.

This method increments the arrival count at the barrier and returns
the updated count. It's used to track how many threads have reached
the synchronization point.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The updated arrival count after this thread's arrival.

</section>

---

## TMATensorTile

<section class='mojo-docs'>

`struct TMATensorTile[dtype: DType, layout: Layout, desc_layout: Layout = layout, is_k_major: Bool = True]`

A hardware-accelerated tensor memory access (TMA) tile for efficient asynchronous data movement.

The TMATensorTile struct provides a high-performance interface for asynchronous data transfers
between global memory and shared memory in GPU tensor operations. It encapsulates a TMA descriptor
that defines the memory access pattern and provides methods for various asynchronous operations.

Performance:

* Hardware-accelerated memory transfers using TMA instructions
* Supports prefetching of descriptors for latency hiding
* Enforces 128-byte alignment requirements for optimal memory access

## Parameters

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType
  The data type of the tensor elements.
* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout
  The layout of the tile in shared memory, typically specified as row\_major.
* â€‹desc\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout = layout
  The layout of the descriptor, which can be different from the shared memory layout
  to accommodate hardware requirements like WGMMA.
* â€‹is\_k\_major ([`Bool`](/mojo/std/builtin/bool/Bool)): Bool = True
  Whether the shared memory is k-major.

## Fields

* â€‹descriptor (`TMADescriptor`): The TMA descriptor that defines the memory access pattern.
  This field stores the hardware descriptor that encodes information about:

  * The source tensor's memory layout and dimensions
  * The tile shape and access pattern
  * Swizzling configuration for optimal memory access

  The descriptor is used by the GPU's Tensor Memory Accelerator hardware to
  efficiently transfer data between global and shared memory.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = False`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `device_type`

`comptime device_type = TMATensorTile[dtype, layout, desc_layout, is_k_major]`

The device-side type representation.

## Methods

### `__init__`

`@implicit`
`__init__(out self, descriptor: TMADescriptor)`

Initializes a new TMATensorTile with the provided TMA descriptor.

**Args:**

* â€‹descriptor ([`TMADescriptor`](/mojo/std/gpu/host/nvidia/tma/TMADescriptor)): The TMA descriptor that defines the memory access pattern.

### `__copyinit__`

`__copyinit__(out self, other: Self)`

Copy initializes this `TMATensorTile` from another instance.

**Args:**

* â€‹other (`Self`): The other `TMATensorTile` instance to copy from.

### `get_type_name`

`static get_type_name() -> String`

Gets this type's name, for use in error messages when handing arguments to kernels.

**Returns:**

`String`: This type's name.

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name, for use in error messages when handing arguments to kernels.

**Returns:**

`String`: This type's name.

### `prefetch_descriptor`

`prefetch_descriptor(self)`

Prefetches the TMA descriptor into cache to reduce latency.

This method helps hide memory access latency by prefetching the descriptor
before it's needed for actual data transfers.

### `async_copy`

`async_copy[cta_group: Int = 1](self, dst: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ref [3] mem_barrier: SharedMemBarrier, coords: Tuple[UInt, UInt])`

Schedules an asynchronous copy from global memory to shared memory at specified coordinates.

This method initiates a hardware-accelerated asynchronous transfer of data from global memory
to the specified destination in shared memory. The transfer is tracked by the provided memory
barrier.

**Constraints:**

* The destination tensor must be 128-byte aligned in shared memory.
* The descriptor layout may be smaller than the shared memory tile shape
  to accommodate hardware requirements.

**Parameters:**

* â€‹cta\_group ([`Int`](/mojo/std/builtin/int/Int)): Int
  If the TMA is issued with cta\_group == 2, only the leader CTA needs
  to be notified upon completion.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor in shared memory where data will be copied.
  Must be 128-byte aligned.
* â€‹mem\_barrier ([`SharedMemBarrier`](/mojo/kernels/layout/tma_async/SharedMemBarrier)): The memory barrier used to track and synchronize the asynchronous transfer.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): The 2D coordinates in the source tensor from which to copy data.

`async_copy[rank: Int, //, cta_group: Int = 1](self, dst: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ref [3] mem_barrier: SharedMemBarrier, coords: StaticTuple[UInt32, rank])`

Schedules an asynchronous copy from global memory to shared memory for N-dimensional tensors.

This is a generic dispatcher that selects the appropriate rank-specific async copy method
based on the tensor rank. It provides a unified interface for initiating TMA transfers
across 2D, 3D, 4D, and 5D tensors using `StaticTuple` coordinates.

**Constraints:**

* The rank must be 2, 3, 4, or 5.
* The destination tensor must be 128-byte aligned in shared memory.

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The dimensionality of the tensor (must be 2, 3, 4, or 5).
* â€‹cta\_group ([`Int`](/mojo/std/builtin/int/Int)): If set to 2, only the leader CTA needs to be notified upon completion.
  Defaults to 1.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor in shared memory where data will be copied.
  Must be 128-byte aligned.
* â€‹mem\_barrier ([`SharedMemBarrier`](/mojo/kernels/layout/tma_async/SharedMemBarrier)): The memory barrier used to track and synchronize the asynchronous transfer.
* â€‹coords ([`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)): The N-dimensional coordinates in the source tensor from which to copy data,
  provided as a `StaticTuple` of `UInt32` values.

### `async_copy_3d`

`async_copy_3d(self, dst: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ref [3] mem_barrier: SharedMemBarrier, coords: Tuple[UInt, UInt, UInt])`

Schedules an asynchronous copy from global memory to shared memory at specified 3D coordinates.

This method initiates a hardware-accelerated asynchronous transfer of data from global memory
to the specified destination in shared memory for 3D tensors. The transfer is tracked by the
provided memory barrier.

**Constraints:**

* The destination tensor must be 128-byte aligned in shared memory.
* The descriptor layout may be smaller than the shared memory tile shape
  to accommodate hardware requirements.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor in shared memory where data will be copied.
  Must be 128-byte aligned.
* â€‹mem\_barrier ([`SharedMemBarrier`](/mojo/kernels/layout/tma_async/SharedMemBarrier)): The memory barrier used to track and synchronize the asynchronous transfer.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): The 3D coordinates in the source tensor from which to copy data.

### `async_copy_4d`

`async_copy_4d[cta_group: Int = 1](self, dst: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ref [3] mem_barrier: SharedMemBarrier, coords: Tuple[UInt, UInt, UInt, UInt])`

Schedules an asynchronous copy from global memory to shared memory at specified 4D coordinates.

This method initiates a hardware-accelerated asynchronous transfer of data from global memory
to the specified destination in shared memory for 4D tensors. The transfer is tracked by the
provided memory barrier.

**Constraints:**

* The destination tensor must be 128-byte aligned in shared memory.
* The descriptor layout may be smaller than the shared memory tile shape
  to accommodate hardware requirements.

**Parameters:**

* â€‹cta\_group ([`Int`](/mojo/std/builtin/int/Int)): Int
  If the TMA is issued with cta\_group == 2, only the leader CTA needs
  to be notified upon completion.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor in shared memory where data will be copied.
  Must be 128-byte aligned.
* â€‹mem\_barrier ([`SharedMemBarrier`](/mojo/kernels/layout/tma_async/SharedMemBarrier)): The memory barrier used to track and synchronize the asynchronous transfer.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): The 4D coordinates in the source tensor from which to copy data.

### `async_copy_5d`

`async_copy_5d[cta_group: Int = 1](self, dst: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ref [3] mem_barrier: SharedMemBarrier, coords: Tuple[UInt, UInt, UInt, UInt, UInt])`

Schedules an asynchronous copy from global memory to shared memory at specified 5D coordinates.

This method initiates a hardware-accelerated asynchronous transfer of data from global memory
to the specified destination in shared memory for 5D tensors. The transfer is tracked by the
provided memory barrier.

**Constraints:**

* The destination tensor must be 128-byte aligned in shared memory.
* The descriptor layout may be smaller than the shared memory tile shape
  to accommodate hardware requirements.

**Parameters:**

* â€‹cta\_group ([`Int`](/mojo/std/builtin/int/Int)): Int
  If the TMA is issued with cta\_group == 2, only the leader CTA needs
  to be notified upon completion.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor in shared memory where data will be copied.
  Must be 128-byte aligned.
* â€‹mem\_barrier ([`SharedMemBarrier`](/mojo/kernels/layout/tma_async/SharedMemBarrier)): The memory barrier used to track and synchronize the asynchronous transfer.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): The 5D coordinates in the source tensor from which to copy data.

### `async_store`

`async_store[rank: Int, //, cta_group: Int = 1](self, dst: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], coords: StaticTuple[UInt32, rank])`

Schedules an asynchronous store from shared memory to global memory for N-dimensional tensors.

This is a generic dispatcher that selects the appropriate rank-specific async store method
based on the tensor rank. It provides a unified interface for initiating TMA store operations
across 2D, 3D, 4D, and 5D tensors using `StaticTuple` coordinates.

**Constraints:**

* The rank must be 2, 3, 4, or 5.
* The source tensor must be 128-byte aligned in shared memory.

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The dimensionality of the tensor (must be 2, 3, 4, or 5).
* â€‹cta\_group ([`Int`](/mojo/std/builtin/int/Int)): CTA group configuration for the store operation. Defaults to 1.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor in shared memory from which data will be copied to global memory.
  Must be 128-byte aligned.
* â€‹coords ([`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)): The N-dimensional coordinates in the destination global tensor where data
  will be stored, provided as a `StaticTuple` of `UInt32` values.

`async_store(self, src: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], coords: Tuple[UInt, UInt])`

Schedules an asynchronous store from shared memory to global memory.

This method initiates a hardware-accelerated asynchronous transfer of data from shared memory
to global memory at the specified coordinates.

**Constraints:**

The source tensor must be 128-byte aligned in shared memory.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): LayoutTensor
  The source tensor in shared memory from which data will be copied.
  Must be 128-byte aligned.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Tuple\[UInt, UInt]
  The 2D coordinates in the destination tensor where data will be stored.

### `async_multicast_load`

`async_multicast_load[cta_group: Int = 1](self, dst: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ref [3] mem_barrier: SharedMemBarrier, coords: Tuple[UInt, UInt], multicast_mask: UInt16)`

Schedules an asynchronous multicast load from global memory to multiple shared memory locations.

This method initiates a hardware-accelerated asynchronous transfer of data from global memory
to multiple destination locations in shared memory across different CTAs (Cooperative Thread Arrays)
as specified by the multicast mask.

**Constraints:**

The destination tensor must be 128-byte aligned in shared memory.

**Parameters:**

* â€‹cta\_group ([`Int`](/mojo/std/builtin/int/Int)): Int
  If the TMA is issued with cta\_group == 2, only the leader CTA needs
  to be notified upon completion.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): LayoutTensor
  The destination tensor in shared memory where data will be copied.
  Must be 128-byte aligned.
* â€‹mem\_barrier ([`SharedMemBarrier`](/mojo/kernels/layout/tma_async/SharedMemBarrier)): SharedMemBarrierArray
  The memory barrier used to track and synchronize the asynchronous transfer.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Tuple\[UInt, UInt]
  The 2D coordinates in the source tensor from which to copy data.
* â€‹multicast\_mask ([`UInt16`](/mojo/std/builtin/simd/#uint16)): UInt16
  A bit mask specifying which CTAs should receive the data.

### `async_multicast_load_3d`

`async_multicast_load_3d[cta_group: Int = 1](self, dst: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ref [3] mem_barrier: SharedMemBarrier, coords: Tuple[UInt, UInt, UInt], multicast_mask: UInt16)`

Schedules an asynchronous 3D multicast load from global memory to multiple shared memory locations.

This method initiates a hardware-accelerated asynchronous transfer of data from global memory
to multiple destination locations in shared memory across different CTAs (Cooperative Thread Arrays)
as specified by the multicast mask.

**Constraints:**

The destination tensor must be 128-byte aligned in shared memory.

**Parameters:**

* â€‹cta\_group ([`Int`](/mojo/std/builtin/int/Int)): Int
  If the TMA is issued with cta\_group == 2, only the leader CTA needs
  to be notified upon completion.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): LayoutTensor
  The destination tensor in shared memory where data will be copied.
  Must be 128-byte aligned.
* â€‹mem\_barrier ([`SharedMemBarrier`](/mojo/kernels/layout/tma_async/SharedMemBarrier)): SharedMemBarrierArray
  The memory barrier used to track and synchronize the asynchronous transfer.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Tuple\[UInt, UInt, UInt]
  The 2D coordinates in the source tensor from which to copy data.
* â€‹multicast\_mask ([`UInt16`](/mojo/std/builtin/simd/#uint16)): UInt16
  A bit mask specifying which CTAs should receive the data.

### `async_multicast_load_partitioned`

`async_multicast_load_partitioned[tma_rows: Int, tma_load_size: Int](self, dst: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=128], ref [3] mem_barrier: SharedMemBarrier, rank: UInt, coords: Tuple[UInt, UInt], multicast_mask: UInt16)`

Performs a partitioned multicast load where each rank loads a distinct slice of data.

This method is designed for clustered execution where different ranks (CTAs) load
different, contiguous slices of the source tensor. Each rank's slice is offset
by `rank * tma_rows` in the second dimension and stored at offset `rank * tma_load_size`
in shared memory.

Note:
This is typically used in matrix multiplication kernels where the input matrices
are partitioned across multiple CTAs for parallel processing.

**Parameters:**

* â€‹tma\_rows ([`Int`](/mojo/std/builtin/int/Int)): The number of rows each rank is responsible for loading.
* â€‹tma\_load\_size ([`Int`](/mojo/std/builtin/int/Int)): The size in elements of each rank's slice in shared memory.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The destination tensor in shared memory where data will be copied.
  Must be 128-byte aligned.
* â€‹mem\_barrier ([`SharedMemBarrier`](/mojo/kernels/layout/tma_async/SharedMemBarrier)): The memory barrier used to track and synchronize the asynchronous transfer.
* â€‹rank ([`UInt`](/mojo/std/builtin/uint/UInt)): The rank ID (0-based) that determines which slice to load.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): The base 2D coordinates in the source tensor from which to copy data.
  The second coordinate will be offset by `rank * tma_rows`.
* â€‹multicast\_mask ([`UInt16`](/mojo/std/builtin/simd/#uint16)): A bit mask specifying which CTAs should receive the data.

### `async_store_3d`

`async_store_3d(self, src: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], coords: Tuple[UInt, UInt, UInt])`

Schedules an asynchronous store from shared memory to global memory at specified 3D coordinates.

This method initiates a hardware-accelerated asynchronous transfer of data from shared memory
to the specified destination in global memory for 3D tensors.

**Constraints:**

* The source tensor must be 128-byte aligned in shared memory.
* The descriptor layout may be smaller than the shared memory tile shape
  to accommodate hardware requirements.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor in shared memory from which data will be copied.
  Must be 128-byte aligned.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): The 3D coordinates in the destination tensor where data will be stored.

### `async_store_4d`

`async_store_4d(self, src: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], coords: Tuple[UInt, UInt, UInt, UInt])`

Schedules an asynchronous store from shared memory to global memory at specified 4D coordinates.

This method initiates a hardware-accelerated asynchronous transfer of data from shared memory
to the specified destination in global memory for 4D tensors.

**Constraints:**

* The source tensor must be 128-byte aligned in shared memory.
* The descriptor layout may be smaller than the shared memory tile shape
  to accommodate hardware requirements.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor in shared memory from which data will be copied.
  Must be 128-byte aligned.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): The 4D coordinates in the destination tensor where data will be stored.

### `async_store_5d`

`async_store_5d(self, src: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], coords: Tuple[UInt, UInt, UInt, UInt, UInt])`

Schedules an asynchronous store from shared memory to global memory at specified 5D coordinates.

This method initiates a hardware-accelerated asynchronous transfer of data from shared memory
to the specified destination in global memory for 5D tensors.

**Constraints:**

* The source tensor must be 128-byte aligned in shared memory.
* The descriptor layout may be smaller than the shared memory tile shape
  to accommodate hardware requirements.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor in shared memory from which data will be copied.
  Must be 128-byte aligned.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): The 5D coordinates in the destination tensor where data will be stored.

### `async_reduce`

`async_reduce[reduction_kind: ReduceOp](self, src: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], coords: Tuple[UInt, UInt])`

Schedules an asynchronous reduction operation from shared memory to global memory.

This method initiates a hardware-accelerated asynchronous reduction operation that combines
data from shared memory with data in global memory using the specified reduction operation.
The reduction is performed element-wise at the specified coordinates in the global tensor.

**Constraints:**

The source tensor must be 128-byte aligned in shared memory.

**Parameters:**

* â€‹reduction\_kind ([`ReduceOp`](/mojo/std/gpu/memory/memory/ReduceOp)): The type of reduction operation to perform (e.g., ADD, MIN, MAX).
  This determines how values are combined during the reduction.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The source tensor in shared memory containing the data to be reduced.
  Must be 128-byte aligned.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): The 2D coordinates in the destination tensor where the reduction will be applied.

### `commit_group`

`commit_group(self)`

Commits all prior initiated but uncommitted TMA instructions into a group.

This function behaves the same as `cp_async_bulk_commit_group`, which creates
a synchronization point for bulk TMA transfer.

### `wait_group`

`wait_group[n: Int = 0](self)`

Wait for the completion of asynchronous copy until a specified number of groups are waiting.

This function behaves the same as `cp_async_bulk_wait_group`, which causes the executing
thread to wait until a specified number of the most recent TMA copy are pending.

**Parameters:**

* â€‹n ([`Int`](/mojo/std/builtin/int/Int)): The number of pending groups left.

### `smem_tensormap_init`

`smem_tensormap_init(self, smem_tma_descriptor_ptr: LegacyUnsafePointer[TMADescriptor, address_space=AddressSpace.SHARED])`

Initializes a TMA descriptor in shared memory from this tensor tile's descriptor.

This method copies the TMA descriptor from global memory to shared memory, allowing
for faster access during kernel execution. The descriptor is copied in 16-byte chunks
using asynchronous copy operations for efficiency.

Note:

* Only one thread should call this method to avoid race conditions
* The descriptor is copied in 8 chunks of 16 bytes each (total 128 bytes)

**Args:**

* â€‹smem\_tma\_descriptor\_ptr (`LegacyUnsafePointer`): Pointer to the location in shared memory where the
  descriptor will be stored. Must be properly aligned.

### `replace_tensormap_global_address_in_gmem`

`replace_tensormap_global_address_in_gmem[_dtype: DType](self, src_ptr: LegacyUnsafePointer[Scalar[_dtype]])`

Replaces the global memory address in the TMA descriptor stored in global memory.

This method allows dynamically changing the source tensor for TMA operations without
recreating the entire descriptor, which is useful for reusing descriptors with different
data sources. The operation modifies the descriptor in global memory directly.

Note:
A memory fence may be required after this operation to ensure visibility
of the changes to other threads.

**Parameters:**

* â€‹\_dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the new source tensor.

**Args:**

* â€‹src\_ptr (`LegacyUnsafePointer`): The new source tensor whose address will replace the current one in the descriptor.
  Must have compatible layout with the original tensor.

### `tensormap_fence_acquire`

`tensormap_fence_acquire(self)`

Establishes a memory fence for TMA operations with acquire semantics.

This method ensures proper ordering of memory operations by creating a barrier
that prevents subsequent TMA operations from executing before prior operations
have completed. It is particularly important when reading from a descriptor
that might have been modified by other threads or processes.

The acquire semantics ensure that all memory operations after this fence
will observe any modifications made to the descriptor before the fence.

Notes:

* The entire warp must call this function as the instruction is warp-aligned.
* Typically used in pairs with `tensormap_fence_release` for proper synchronization.

### `tensormap_fence_release`

`tensormap_fence_release(self)`

Establishes a memory fence for TMA operations with release semantics.

This method ensures proper ordering of memory operations by creating a barrier
that ensures all prior memory operations are visible before subsequent operations
can proceed. It is particularly important when modifying a TMA descriptor in
global memory that might be read by other threads or processes.

The release semantics ensure that all memory operations before this fence
will be visible to any thread that observes operations after the fence.

Notes:

* Typically used after modifying a tensormap descriptor in global memory.
* Often paired with `tensormap_fence_acquire` for proper synchronization.

### `replace_tensormap_global_address_in_shared_mem`

`replace_tensormap_global_address_in_shared_mem[_dtype: DType](self, smem_tma_descriptor_ptr: LegacyUnsafePointer[TMADescriptor, address_space=AddressSpace.SHARED, mut=mut, origin=origin], src_ptr: LegacyUnsafePointer[Scalar[_dtype]])`

Replaces the global memory address in the TMA descriptor stored in shared memory.

This method allows dynamically changing the source tensor for TMA operations without
recreating the entire descriptor, which is useful for reusing descriptors with different
data sources. The operation modifies a descriptor that has been previously copied to
shared memory.

Notes:

* Only one thread should call this method to avoid race conditions.
* A memory fence may be required after this operation to ensure visibility
  of the changes to other threads.
* Typically used with descriptors previously initialized with `smem_tensormap_init`.

**Parameters:**

* â€‹\_dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the new source tensor.

**Args:**

* â€‹smem\_tma\_descriptor\_ptr (`LegacyUnsafePointer`): Pointer to the TMA descriptor in shared memory that will be modified.
* â€‹src\_ptr (`LegacyUnsafePointer`): The new source tensor whose address will replace the current one in the descriptor.

### `tensormap_cp_fence_release`

`tensormap_cp_fence_release(self, smem_tma_descriptor_ptr: LegacyUnsafePointer[TMADescriptor, address_space=AddressSpace.SHARED])`

Establishes a memory fence for TMA operations with release semantics for shared memory descriptors.

This method ensures proper ordering of memory operations by creating a barrier
that ensures all prior memory operations are visible before subsequent operations
can proceed. It is specifically designed for synchronizing between global memory and
shared memory TMA descriptors.

The release semantics ensure that all memory operations before this fence
will be visible to any thread that observes operations after the fence.

Notes:

* The entire warp must call this function as the instruction is warp-aligned
* Typically used after modifying a tensormap descriptor in shared memory
* More specialized than the general `tensormap_fence_release` for cross-memory space synchronization

**Args:**

* â€‹smem\_tma\_descriptor\_ptr (`LegacyUnsafePointer`): Pointer to the TMA descriptor in shared memory that
  is being synchronized with the global memory descriptor.

### `replace_tensormap_global_dim_strides_in_shared_mem`

`replace_tensormap_global_dim_strides_in_shared_mem[_dtype: DType, only_update_dim_0: Bool, /, *, rank: Int](self, smem_tma_descriptor_ptr: LegacyUnsafePointer[TMADescriptor, address_space=AddressSpace.SHARED, mut=mut, origin=origin], gmem_dims: IndexList[rank], gmem_strides: IndexList[rank])`

Replaces dimensions and strides in a TMA descriptor stored in shared memory. Note: This function is only supported for CUDA versions >= 12.5.

This function allows dynamically modifying the dimensions and strides of a TMA
descriptor that has been previously initialized in shared memory. If only the first dimension (dim 0) is updated, then updating strides can be skipped.

Notes:

* Only one thread should call this method to avoid race conditions.
* A memory fence may be required after this operation to ensure visibility
  of the changes to other threads.

**Parameters:**

* â€‹\_dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the new source tensor.
* â€‹only\_update\_dim\_0 ([`Bool`](/mojo/std/builtin/bool/Bool)): If true, only the first dimension (dim 0) is updated with updating strides.
* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the tensor.

**Args:**

* â€‹smem\_tma\_descriptor\_ptr (`LegacyUnsafePointer`): Pointer to the TMA descriptor in shared memory that will be modified.
* â€‹gmem\_dims ([`IndexList`](/mojo/std/utils/index_/IndexList)): The global dimensions of the tensor to be updated.
* â€‹gmem\_strides ([`IndexList`](/mojo/std/utils/index_/IndexList)): The global strides of the tensor to be updated.

`replace_tensormap_global_dim_strides_in_shared_mem[_dtype: DType, tensor_rank: Int, dim_idx: Int](self, smem_tma_descriptor_ptr: LegacyUnsafePointer[TMADescriptor, address_space=AddressSpace.SHARED, mut=mut, origin=origin], dim_value: UInt32, dim_stride: Optional[UInt64] = None)`

Replaces dimensions and strides in a TMA descriptor stored in shared memory. Note: This function is only supported for CUDA versions >= 12.5. This function allows dynamically modifying the dimensions and strides of a TMA descriptor that has been previously initialized in shared memory. If only the first dimension is updated, then updating strides can be skipped.

Notes:

* Only one thread should call this method to avoid race conditions.
* A memory fence may be required after this operation to ensure visibility
  of the changes to other threads.

**Parameters:**

* â€‹\_dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the source tensor in GMEM.
* â€‹tensor\_rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the source tensor in GMEM.
* â€‹dim\_idx ([`Int`](/mojo/std/builtin/int/Int)): The index of the dimension to be updated in the TMA descriptor with the provided dimension and stride values at runtime.

**Args:**

* â€‹smem\_tma\_descriptor\_ptr (`LegacyUnsafePointer`): Pointer to the TMA descriptor in shared memory that will be modified.
* â€‹dim\_value ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The new dimension value to be set.
* â€‹dim\_stride ([`Optional`](/mojo/std/collections/optional/Optional)): The new stride value to be set.

</section>

---

## TMATensorTileArray

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TMATensorTileArray[num_of_tensormaps: Int, dtype: DType, cta_tile_layout: Layout, desc_layout: Layout]`

An array of TMA descripotr.

## Parameters

* â€‹num\_of\_tensormaps ([`Int`](/mojo/std/builtin/int/Int)): Int
  The number of TMA descriptors aka tensor map.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType
  The data type of the tensor elements.
* â€‹cta\_tile\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout
  The layout of the tile in shared memory, typically specified as row\_major.
* â€‹desc\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout
  The layout of the descriptor, which can be different from the shared memory layout
  to accommodate hardware requirements like WGMMA.

## Fields

* â€‹tensormaps\_ptr (`LegacyUnsafePointer[UInt8]`): A static tuple of pointers to TMA descriptors.
  This field stores an array of pointers to `TMATensorTile` instances, where each pointer
  references a TMA descriptor in device memory. The array has a fixed size determined by
  the num\_of\_tensormaps parameter.

  The TMA descriptors are used by the GPU hardware to efficiently transfer data between
  global and shared memory with specific memory access patterns defined by the layouts.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `descriptor_bytes`

`comptime descriptor_bytes = 128`

Size of the TMA descriptor in bytes.

This is a constant value that represents the size of the TMA descriptor in bytes.
It is used to calculate the offset of the TMA descriptor in the device memory.

### `device_type`

`comptime device_type = TMATensorTileArray[num_of_tensormaps, dtype, cta_tile_layout, desc_layout]`

The device-side type representation.

## Methods

### `__init__`

`__init__(tensormaps_device: DeviceBuffer[DType.uint8]) -> Self`

Initializes a new TMATensorTileArray.

**Args:**

* â€‹tensormaps\_device ([`DeviceBuffer`](/mojo/std/gpu/host/device_context/DeviceBuffer)): Device buffer to store TMA descriptors.

### `__getitem__`

`__getitem__(self, index: Int) -> LegacyUnsafePointer[TMATensorTile[dtype, cta_tile_layout, desc_layout]]`

Retrieve a TMA descriptor.

**Args:**

* â€‹index ([`Int`](/mojo/std/builtin/int/Int)): Index of the TMA descriptor.

**Returns:**

`LegacyUnsafePointer`: `UnsafePointer` to the `TMATensorTile` at the specified index.

### `get_type_name`

`static get_type_name() -> String`

Gets this type's name, for use in error messages when handing arguments to kernels.

**Returns:**

`String`: This type's name.

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name, for use in error messages when handing arguments to kernels.

**Returns:**

`String`: This type's name.

</section>

---

## create_split_tma

<section class='mojo-docs'>

`create_split_tma[rank: Int, dtype: DType, //, smem_shape: IndexList[rank], gmem_shape: IndexList[rank], swizzle_mode: TensorMapSwizzle](ctx: DeviceContext, ptr: LegacyUnsafePointer[Scalar[dtype]], runtime_dim0: Int, out res: TMATensorTile[dtype, _split_last_layout[dtype](smem_shape, swizzle_mode, True), _ragged_desc_layout[dtype](smem_shape, swizzle_mode)])`

Creates a TMA tensor tile assuming that the first dimension in global memory has `UNKNOWN_VALUE`.

This function creates a `TMATensorTile` that optionally splits the last dimension
of the tensor into multiples of swizzle granularity. This functionality is currently
disabled because it was not found to improve performance.

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions of the tensor.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the tensor elements.
* â€‹smem\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The shape of the tile in shared memory.
* â€‹gmem\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The shape of the global memory tensor.
* â€‹swizzle\_mode ([`TensorMapSwizzle`](/mojo/std/gpu/host/nvidia/tma/TensorMapSwizzle)): The swizzling mode for memory access optimization.

**Args:**

* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): The CUDA device context used to create the TMA descriptor.
* â€‹ptr (`LegacyUnsafePointer`): Pointer to the global memory tensor data.
* â€‹runtime\_dim0 ([`Int`](/mojo/std/builtin/int/Int)): The runtime size of the first dimension of the global tensor.

**Returns:**

[`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile): The resulting TMA tensor tile with split layout.

**Raises:**

If TMA descriptor creation fails.

`create_split_tma[rank: Int, dtype: DType, //, smem_shape: IndexList[rank], gmem_shape: IndexList[rank], swizzle_mode: TensorMapSwizzle](ctx: DeviceContext, ptr: LegacyUnsafePointer[Scalar[dtype]], runtime_dim0: Int, runtime_dim1: Int, out res: TMATensorTile[dtype, _split_last_layout[dtype](smem_shape, swizzle_mode, True), _ragged_desc_layout[dtype](smem_shape, swizzle_mode)])`

Creates a TMA tensor tile assuming that the first two dimensions in global memory has `UNKNOWN_VALUE`.

This function creates a `TMATensorTile` that optionally splits the last dimension
of the tensor into multiples of swizzle granularity. This functionality is currently
disabled because it was not found to improve performance.

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The number of dimensions of the tensor.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the tensor elements.
* â€‹smem\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The shape of the tile in shared memory.
* â€‹gmem\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The shape of the global memory tensor.
* â€‹swizzle\_mode ([`TensorMapSwizzle`](/mojo/std/gpu/host/nvidia/tma/TensorMapSwizzle)): The swizzling mode for memory access optimization.

**Args:**

* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): The CUDA device context used to create the TMA descriptor.
* â€‹ptr (`LegacyUnsafePointer`): Pointer to the global memory tensor data.
* â€‹runtime\_dim0 ([`Int`](/mojo/std/builtin/int/Int)): The runtime size of the first dimension of the global tensor.
* â€‹runtime\_dim1 ([`Int`](/mojo/std/builtin/int/Int)): The runtime size of the second dimension of the global tensor.

**Returns:**

[`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile): The resulting TMA tensor tile with split layout.

**Raises:**

If TMA descriptor creation fails.

</section>

---

## create_tma_tile

<section class='mojo-docs'>

`create_tma_tile[*tile_sizes: Int, *, swizzle_mode: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_NONE](ctx: DeviceContext, tensor: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> TMATensorTile[dtype, Layout.row_major(_to_int_tuple[tile_sizes]())]`

Creates a `TMATensorTile` with specified tile dimensions and swizzle mode.

This function creates a hardware-accelerated Tensor Memory Access (TMA) descriptor
for efficient asynchronous data transfers between global memory and shared memory.
It configures the tile dimensions and memory access patterns based on the provided
parameters.

**Constraints:**

* The last dimension's size in bytes must not exceed the swizzle mode's byte limit
  (32B for SWIZZLE\_32B, 64B for SWIZZLE\_64B, 128B for SWIZZLE\_128B).
* Only supports 2D tensors in this overload.

**Parameters:**

* â€‹\*tile\_sizes ([`Int`](/mojo/std/builtin/int/Int)): The dimensions of the tile to be transferred. For 2D tensors, this should be
  \[height, width]. The dimensions determine the shape of data transferred in each
  TMA operation.
* â€‹swizzle\_mode ([`TensorMapSwizzle`](/mojo/std/gpu/host/nvidia/tma/TensorMapSwizzle)):
  The swizzling mode to use for memory access optimization. Swizzling can improve
  memory access patterns for specific hardware configurations.

**Args:**

* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)):
  The CUDA device context used to create the TMA descriptor.
* â€‹tensor ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)):
  The source tensor from which data will be transferred. This defines the
  global memory layout and data type.

**Returns:**

[`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile): A `TMATensorTile` configured with the specified tile dimensions and swizzle mode,
ready for use in asynchronous data transfer operations.

`create_tma_tile[dtype: DType, rank: Int, //, tile_shape: IndexList[rank], /, k_major_tma: Bool = True, swizzle_mode: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_NONE, *, __tile_layout: Layout = Layout.row_major(tile_shape.__getitem__[rank, DType.int64, Int](0), tile_shape.__getitem__[rank, DType.int64, Int](1)), __desc_layout: Layout = _tma_desc_tile_layout[dtype, rank, tile_shape, swizzle_mode]()](ctx: DeviceContext, tensor: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> TMATensorTile[dtype, __tile_layout, __desc_layout, k_major_tma]`

Creates a `TMATensorTile` with advanced configuration options for 2D, 3D, 4D, or 5D tensors.

This overload provides more control over the TMA descriptor creation, allowing
specification of data type, rank, and layout orientation. It supports 2D, 3D, 4D, and 5D
tensors and provides fine-grained control over the memory access patterns.

**Constraints:**

* Only supports 2D, 3D, 4D, and 5D tensors (rank must be 2, 3, 4, or 5).
* For non-SWIZZLE\_NONE modes, the K dimension size in bytes must be a multiple
  of the swizzle mode's byte size.
* For MN-major layout, only SWIZZLE\_128B is supported.
* For 3D, 4D, and 5D tensors, only K-major layout is supported.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType
  The data type of the tensor elements.
* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): Int
  The dimensionality of the tensor (must be 2, 3, 4, or 5).
* â€‹tile\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): IndexList\[rank]
  The shape of the tile to be transferred.
* â€‹k\_major\_tma ([`Bool`](/mojo/std/builtin/bool/Bool)): Bool = True
  Whether the tma should copy desc into shared memory following a
  column-major (if `True`) or row-major (if `False`) pattern.
* â€‹swizzle\_mode ([`TensorMapSwizzle`](/mojo/std/gpu/host/nvidia/tma/TensorMapSwizzle)): TensorMapSwizzle = TensorMapSwizzle.SWIZZLE\_NONE
  The swizzling mode to use for memory access optimization.
* â€‹\_\_tile\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout = Layout.row\_major(tile\_shape\[0], tile\_shape\[1])
  Internal parameter for the tile layout in shared memory.
* â€‹\_\_desc\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout = \_tma\_desc\_tile\_layout\[...]
  Internal parameter for the descriptor layout, which may differ from the
  tile layout to accommodate hardware requirements.

**Args:**

* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): DeviceContext
  The CUDA device context used to create the TMA descriptor.
* â€‹tensor ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): LayoutTensor\[dtype, \**, \*\**]
  The source tensor from which data will be transferred. This defines the
  global memory layout and must match the specified data type.

**Returns:**

[`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile): A `TMATensorTile` configured with the specified parameters, ready for use in
asynchronous data transfer operations.

</section>

---

## create_tma_tile_template

<section class='mojo-docs'>

`create_tma_tile_template[dtype: DType, rank: Int, tile_shape: IndexList[rank], /, swizzle_mode: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_NONE, *, __tile_layout: Layout = Layout.row_major(tile_shape.__getitem__[rank, DType.int64, Int](0), tile_shape.__getitem__[rank, DType.int64, Int](1)), __desc_layout: Layout = _tma_desc_tile_layout[dtype, rank, tile_shape, swizzle_mode]()]() -> TMATensorTile[dtype, __tile_layout, __desc_layout]`

Same as create\_tma\_tile expect the descriptor is only a placeholder or a template for later replacement.

specification of data type, rank, and layout orientation. It supports both 2D and 3D
tensors and provides fine-grained control over the memory access patterns.

**Constraints:**

* Only supports 2D and 3D tensors (rank must be 2 or 3).
* For non-SWIZZLE\_NONE modes, the K dimension size in bytes must be a multiple
  of the swizzle mode's byte size.
* For MN-major layout, only SWIZZLE\_128B is supported.
* For 3D tensors, only K-major layout is supported.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType
  The data type of the tensor elements.
* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): Int
  The dimensionality of the tensor (must be 2 or 3).
* â€‹tile\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): IndexList\[rank]
  The shape of the tile to be transferred.
* â€‹swizzle\_mode ([`TensorMapSwizzle`](/mojo/std/gpu/host/nvidia/tma/TensorMapSwizzle)): TensorMapSwizzle = TensorMapSwizzle.SWIZZLE\_NONE
  The swizzling mode to use for memory access optimization.
* â€‹\_\_tile\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout = Layout.row\_major(tile\_shape\[0], tile\_shape\[1])
  Internal parameter for the tile layout in shared memory.
* â€‹\_\_desc\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout = \_tma\_desc\_tile\_layout\[...]
  Internal parameter for the descriptor layout, which may differ from the
  tile layout to accommodate hardware requirements.

**Returns:**

[`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile): A `TMATensorTile` configured with the specified parameters, ready for use in
asynchronous data transfer operations.

</section>

---

## tma_async

<section class='mojo-docs'>

Tensor Memory Accelerator (TMA) Asynchronous Operations Module

Provides high-performance abstractions for NVIDIA's Tensor Memory Accelerator (TMA),
enabling efficient asynchronous data movement between global and shared memory in GPU kernels.
It is designed for use with NVIDIA Hopper architecture and newer GPUs that support TMA instructions.

## Key Components:

* `TMATensorTile`: Core struct that encapsulates a TMA descriptor for efficient data transfers
  between global and shared memory with various access patterns and optimizations.

* `SharedMemBarrier`: Synchronization primitive for coordinating asynchronous TMA operations,
  ensuring data transfers complete before dependent operations begin.

* `PipelineState`: Helper struct for managing multi-stage pipeline execution with circular
  buffer semantics, enabling efficient double or triple buffering techniques.

* `create_tma_tile`: Factory functions for creating optimized `TMATensorTile` instances with
  various configurations for different tensor shapes and memory access patterns.

## `comptime` values

### `SplitLastDimTMATensorTile`

`comptime SplitLastDimTMATensorTile[rank: Int, //, dtype: DType, smem_shape: IndexList[rank], swizzle_mode: TensorMapSwizzle] = TMATensorTile[dtype, _split_last_layout[dtype](smem_shape, swizzle_mode, True), _ragged_desc_layout[dtype](smem_shape, swizzle_mode)]`

A specialized TMA tensor tile type alias that handles layouts where the last dimension is split based on swizzle granularity for optimal memory access patterns. The current behavior is to not actually split the last dimension.

#### Parameters

* â€‹rank ([`Int`](/std/builtin/int/Int)): The number of dimensions of the tensor.
* â€‹dtype ([`DType`](/std/builtin/dtype/DType)): The data type of the tensor elements.
* â€‹smem\_shape ([`IndexList`](/std/utils/index_/IndexList)): The shape of the tile in shared memory. The last dimension will be
  padded if necessary to align with the swizzle granularity.
* â€‹swizzle\_mode ([`TensorMapSwizzle`](/std/gpu/host/nvidia/tma/TensorMapSwizzle)): The swizzling mode for memory access optimization. Determines
  the granularity at which the last dimension is split or padded.

## Structs

* [â€‹`PipelineState`](./PipelineState): Manages state for a multi-stage pipeline with circular buffer semantics.
* [â€‹`RaggedTensorMap`](./RaggedTensorMap): Creates a TMA descriptor that can handle stores with varying lengths. This struct is mainly used for MHA, where sequence lengths may vary between sample.
* [â€‹`SharedMemBarrier`](./SharedMemBarrier): A hardware-accelerated synchronization primitive for GPU shared memory operations.
* [â€‹`TMATensorTile`](./TMATensorTile): A hardware-accelerated tensor memory access (TMA) tile for efficient asynchronous data movement.
* [â€‹`TMATensorTileArray`](./TMATensorTileArray): An array of TMA descripotr.

## Functions

* [â€‹`create_split_tma`](./create_split_tma): Creates a TMA tensor tile assuming that the first dimension in global memory has `UNKNOWN_VALUE`.
* [â€‹`create_tma_tile`](./create_tma_tile): Creates a `TMATensorTile` with specified tile dimensions and swizzle mode.
* [â€‹`create_tma_tile_template`](./create_tma_tile_template): Same as create\_tma\_tile expect the descriptor is only a placeholder or a template for later replacement.

</section>

---

## accumulate

<section class='mojo-docs'>

</section>

---

## dot_at_b

<section class='mojo-docs'>

`dot_at_b(c: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## dot_at_b_impl

<section class='mojo-docs'>

`dot_at_b_impl(c: LayoutTensor[DType.float32, Layout.row_major(16, 16), MutAnyOrigin], a: LayoutTensor[DType.float32, Layout.row_major(16, 16), ImmutAnyOrigin], b: LayoutTensor[DType.float32, Layout.row_major(16, 16), ImmutAnyOrigin])`

`dot_at_b_impl(c: LayoutTensor[DType.float16, Layout.row_major(32, 32), MutAnyOrigin], a: LayoutTensor[DType.float16, Layout.row_major(32, 32), ImmutAnyOrigin], b: LayoutTensor[DType.float16, Layout.row_major(32, 32), ImmutAnyOrigin])`

</section>

---

## extrx

<section class='mojo-docs'>

`extrx(gpr: Int)`

Extracts a row or moves it to x, result in amx0.

</section>

---

## extry

<section class='mojo-docs'>

`extry(gpr: Int)`

Extracts a row or moves it to y, result in amx0.

</section>

---

## fma

<section class='mojo-docs'>

`fma[mode: StringSlice[StaticConstantOrigin], dtype: DType](z_row_index: Int, x_row_index: Int, y_row_index: Int, clear_z: Bool)`

</section>

---

## fma16

<section class='mojo-docs'>

`fma16(gpr: Int)`

Float16 matrix multiply and subtract.

</section>

---

## fma32

<section class='mojo-docs'>

`fma32(gpr: Int)`

Float32 matrix multiply and add.

</section>

---

## fma64

<section class='mojo-docs'>

`fma64(gpr: Int)`

Float64 matrix multiply and add.

</section>

---

## fms16

<section class='mojo-docs'>

`fms16(gpr: Int)`

Float16 matrix multiply and add.

</section>

---

## fsm32

<section class='mojo-docs'>

`fsm32(gpr: Int)`

Float32 matrix multiply and subtract.

</section>

---

## fsm64

<section class='mojo-docs'>

`fsm64(gpr: Int)`

Float64 matrix multiply and subtract.

</section>

---

## genlut

<section class='mojo-docs'>

`genlut(gpr: Int)`

</section>

---

## apple_amx_intrinsics

<section class='mojo-docs'>

## Functions

* [â€‹`dot_at_b`](./dot_at_b):
* [â€‹`dot_at_b_impl`](./dot_at_b_impl):
* [â€‹`extrx`](./extrx): Extracts a row or moves it to x, result in amx0.
* [â€‹`extry`](./extry): Extracts a row or moves it to y, result in amx0.
* [â€‹`fma`](./fma):
* [â€‹`fma16`](./fma16): Float16 matrix multiply and subtract.
* [â€‹`fma32`](./fma32): Float32 matrix multiply and add.
* [â€‹`fma64`](./fma64): Float64 matrix multiply and add.
* [â€‹`fms16`](./fms16): Float16 matrix multiply and add.
* [â€‹`fsm32`](./fsm32): Float32 matrix multiply and subtract.
* [â€‹`fsm64`](./fsm64): Float64 matrix multiply and subtract.
* [â€‹`genlut`](./genlut):
* [â€‹`ldx`](./ldx):
* [â€‹`ldy`](./ldy):
* [â€‹`ldz`](./ldz):
* [â€‹`ldzi`](./ldzi):
* [â€‹`load_z`](./load_z):
* [â€‹`mac16`](./mac16): SI16 matrix multiply and add.
* [â€‹`matfp`](./matfp): Float16 matrix multiply.
* [â€‹`max_int__`](./max_int__): UI16 matrix multiply.
* [â€‹`read_x`](./read_x):
* [â€‹`read_y`](./read_y):
* [â€‹`store_x`](./store_x):
* [â€‹`store_y`](./store_y):
* [â€‹`store_z`](./store_z):
* [â€‹`stx`](./stx):
* [â€‹`sty`](./sty):
* [â€‹`stz`](./stz):
* [â€‹`stzi`](./stzi):
* [â€‹`transpose_z_to_x_or_y`](./transpose_z_to_x_or_y):
* [â€‹`vec_int__`](./vec_int__): Horizontal ui16 multiply `z0[i] += x0[i] + y0[i]`.
* [â€‹`vecfp`](./vecfp): Horizontal float16 multiply `z0[i] += x0[i] + y0[i]`.

</section>

---

## ldx

<section class='mojo-docs'>

`ldx(gpr: Int)`

</section>

---

## ldy

<section class='mojo-docs'>

`ldy(gpr: Int)`

</section>

---

## ldz

<section class='mojo-docs'>

`ldz(gpr: Int)`

</section>

---

## ldzi

<section class='mojo-docs'>

`ldzi(gpr: Int)`

</section>

---

## load_z

<section class='mojo-docs'>

`load_z[row_count: Int, dtype: DType](src: LegacyUnsafePointer[Scalar[dtype]], start_index: Int)`

</section>

---

## mac16

<section class='mojo-docs'>

`mac16(gpr: Int)`

SI16 matrix multiply and add.

</section>

---

## matfp

<section class='mojo-docs'>

`matfp(gpr: Int)`

Float16 matrix multiply.

</section>

---

## max_int__

<section class='mojo-docs'>

`max_int__(gpr: Int)`

UI16 matrix multiply.

</section>

---

## read_x

<section class='mojo-docs'>

`read_x[row_count: Int, dtype: DType](src: LegacyUnsafePointer[Scalar[dtype]], start_index: Int)`

</section>

---

## read_y

<section class='mojo-docs'>

`read_y[row_count: Int, dtype: DType](src: LegacyUnsafePointer[Scalar[dtype]], start_index: Int)`

</section>

---

## store_x

<section class='mojo-docs'>

`store_x[row_count: Int, dtype: DType](src: LegacyUnsafePointer[Scalar[dtype]], start_index: Int)`

</section>

---

## store_y

<section class='mojo-docs'>

`store_y[row_count: Int, dtype: DType](src: LegacyUnsafePointer[Scalar[dtype]], start_index: Int)`

</section>

---

## store_z

<section class='mojo-docs'>

`store_z[row_count: Int, dtype: DType](src: LegacyUnsafePointer[Scalar[dtype]], start_index: Int)`

</section>

---

## stx

<section class='mojo-docs'>

`stx(gpr: Int)`

</section>

---

## sty

<section class='mojo-docs'>

`sty(gpr: Int)`

</section>

---

## stz

<section class='mojo-docs'>

`stz(gpr: Int)`

</section>

---

## stzi

<section class='mojo-docs'>

`stzi(gpr: Int)`

</section>

---

## transpose_z_to_x_or_y

<section class='mojo-docs'>

`transpose_z_to_x_or_y[destination: StringSlice[StaticConstantOrigin], dtype: DType](z_col_index: Int, xy_row_index: Int, z_row_suboffset: Int)`

</section>

---

## vec_int__

<section class='mojo-docs'>

`vec_int__(gpr: Int)`

Horizontal ui16 multiply `z0[i] += x0[i] + y0[i]`.

</section>

---

## vecfp

<section class='mojo-docs'>

`vecfp(gpr: Int)`

Horizontal float16 multiply `z0[i] += x0[i] + y0[i]`.

</section>

---

## cpu

<section class='mojo-docs'>

Provides cpu architecture specific utility functions.

## Modules

* [â€‹`apple_amx_intrinsics`](./apple_amx_intrinsics/):
* [â€‹`neon_intrinsics`](./neon_intrinsics/):
* [â€‹`vnni_intrinsics`](./vnni_intrinsics/):

</section>

---

## neon_intrinsics

<section class='mojo-docs'>

</section>

---

## dot_i16_to_i32_AVX2

<section class='mojo-docs'>

`dot_i16_to_i32_AVX2[width: Int, a_type: DType, b_type: DType, c_type: DType](src: SIMD[c_type, width], a: SIMD[a_type, (width * 2)], b: SIMD[b_type, (width * 2)]) -> SIMD[c_type, width]`

The dot product of the two words in each int32 element of a and b plus a int32 from src.

**Constraints:**

Requires AVX2.
The size of the output vector must be 4, 8 or 16.

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): Size of the output SIMD vector.
* â€‹a\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for a.
* â€‹b\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for b.
* â€‹c\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for c.

**Args:**

* â€‹src ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A int32 SIMD vector.
* â€‹a ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A int16 SIMD vector.
* â€‹b ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A int16 SIMD vector.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): A SIMD vector of width elements.

</section>

---

## dot_i16_to_i32_x86

<section class='mojo-docs'>

`dot_i16_to_i32_x86[width: Int, a_type: DType, b_type: DType, c_type: DType](src: SIMD[c_type, width], a: SIMD[a_type, (width * 2)], b: SIMD[b_type, (width * 2)]) -> SIMD[c_type, width]`

The dot product of the two words in each int32 element of a and b plus a int32 from src using VNNI or AVX2.

**Constraints:**

Requires AVX512\_VNNI or AVX2.
The size of the output vector must be 4, 8 or 16.

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): Size of the output SIMD vector.
* â€‹a\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for a.
* â€‹b\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for b.
* â€‹c\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for c.

**Args:**

* â€‹src ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A int32 SIMD vector.
* â€‹a ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A int16 SIMD vector.
* â€‹b ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A int16 SIMD vector.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): A SIMD vector of width elements.

</section>

---

## dot_i8_to_i32_AVX2

<section class='mojo-docs'>

`dot_i8_to_i32_AVX2[width: Int, a_type: DType, b_type: DType, c_type: DType](src: SIMD[c_type, width], a: SIMD[a_type, width], b: SIMD[b_type, width]) -> SIMD[c_type, width]`

The dot product of the four bytes in each int32 element of a and b plus a int32 from src.

**Constraints:**

Requires AVX2.
The size of the output vector must be 4, 8 or 16.
The a argument has range \[0,255].
The b argument has range \[-128,127].

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): Size of the output SIMD vector.
* â€‹a\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for a.
* â€‹b\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for b.
* â€‹c\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for c.

**Args:**

* â€‹src ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A int32 SIMD vector.
* â€‹a ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A uint8 SIMD vector.
* â€‹b ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A int8 SIMD vector.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): A SIMD vector of width elements.

</section>

---

## dot_i8_to_i32_saturated_AVX2

<section class='mojo-docs'>

`dot_i8_to_i32_saturated_AVX2[width: Int, a_type: DType, b_type: DType, c_type: DType](src: SIMD[c_type, width], a: SIMD[a_type, width], b: SIMD[b_type, width]) -> SIMD[c_type, width]`

The dot product of the four bytes in each int32 element of a and b plus a int32 from src.

**Constraints:**

Requires AVX2.
The size of the output vector must be 4, 8 or 16.
The a argument has range \[0,127] not \[0, 255].
The b argument has range \[-128,127].

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): Size of the output SIMD vector.
* â€‹a\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for a.
* â€‹b\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for b.
* â€‹c\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for c.

**Args:**

* â€‹src ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A int32 SIMD vector.
* â€‹a ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A uint8 SIMD vector.
* â€‹b ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A int8 SIMD vector.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): A SIMD vector of width elements.

</section>

---

## dot_i8_to_i32_saturated_x86

<section class='mojo-docs'>

`dot_i8_to_i32_saturated_x86[width: Int, a_type: DType, b_type: DType, c_type: DType](src: SIMD[c_type, width], a: SIMD[a_type, width], b: SIMD[b_type, width]) -> SIMD[c_type, width]`

The dot product of the four bytes in each int32 element of a and b plus a int32 from src using VNNI or AVX2.

**Constraints:**

Requires AVX512\_VNNI or AVX2.
The size of the output vector must be 4, 8 or 16.
The a argument has range \[0,127] not \[0, 255].
The b argument has range \[-128,127].

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): Size of the output SIMD vector.
* â€‹a\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for a.
* â€‹b\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for b.
* â€‹c\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for c.

**Args:**

* â€‹src ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A int32 SIMD vector.
* â€‹a ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A uint8 SIMD vector.
* â€‹b ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A int8 SIMD vector.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): A SIMD vector of width elements.

</section>

---

## dot_i8_to_i32_x86

<section class='mojo-docs'>

`dot_i8_to_i32_x86[width: Int, a_type: DType, b_type: DType, c_type: DType](src: SIMD[c_type, width], a: SIMD[a_type, width], b: SIMD[b_type, width]) -> SIMD[c_type, width]`

The dot product of the four bytes in each int32 element of a and b plus a int32 from src using VNNI or AVX2.

**Constraints:**

Requires AVX512\_VNNI or AVX2.
The size of the output vector must be 4, 8 or 16.
The a argument has range \[0,255].
The b argument has range \[-128,127].

**Parameters:**

* â€‹width ([`Int`](/mojo/std/builtin/int/Int)): Size of the output SIMD vector.
* â€‹a\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for a.
* â€‹b\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for b.
* â€‹c\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The DType for c.

**Args:**

* â€‹src ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A int32 SIMD vector.
* â€‹a ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A uint8 SIMD vector.
* â€‹b ([`SIMD`](/mojo/std/builtin/simd/SIMD)): A int8 SIMD vector.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): A SIMD vector of width elements.

</section>

---

## vnni_intrinsics

<section class='mojo-docs'>

## Functions

* [â€‹`dot_i16_to_i32_AVX2`](./dot_i16_to_i32_AVX2): The dot product of the two words in each int32 element of a and b plus a int32 from src.
* [â€‹`dot_i16_to_i32_x86`](./dot_i16_to_i32_x86): The dot product of the two words in each int32 element of a and b plus a int32 from src using VNNI or AVX2.
* [â€‹`dot_i8_to_i32_AVX2`](./dot_i8_to_i32_AVX2): The dot product of the four bytes in each int32 element of a and b plus a int32 from src.
* [â€‹`dot_i8_to_i32_saturated_AVX2`](./dot_i8_to_i32_saturated_AVX2): The dot product of the four bytes in each int32 element of a and b plus a int32 from src.
* [â€‹`dot_i8_to_i32_saturated_x86`](./dot_i8_to_i32_saturated_x86): The dot product of the four bytes in each int32 element of a and b plus a int32 from src using VNNI or AVX2.
* [â€‹`dot_i8_to_i32_x86`](./dot_i8_to_i32_x86): The dot product of the four bytes in each int32 element of a and b plus a int32 from src using VNNI or AVX2.
* [â€‹`pmaddubs`](./pmaddubs):
* [â€‹`pmaddw`](./pmaddw):
* [â€‹`vpdpbusd`](./vpdpbusd):
* [â€‹`vpdpbusds`](./vpdpbusds):
* [â€‹`vpdpwssd`](./vpdpwssd):
* [â€‹`vpdpwssds`](./vpdpwssds):

</section>

---

## pmaddubs

<section class='mojo-docs'>

`pmaddubs[width: Int](a: SIMD[DType.int32, width], b: SIMD[DType.int32, width]) -> SIMD[DType.int32, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## pmaddw

<section class='mojo-docs'>

`pmaddw[width: Int](a: SIMD[DType.int32, width], b: SIMD[DType.int32, width]) -> SIMD[DType.int32, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## vpdpbusd

<section class='mojo-docs'>

`vpdpbusd[width: Int, a_type: DType, b_type: DType, c_type: DType](src: SIMD[c_type, width], a: SIMD[a_type, width], b: SIMD[b_type, width]) -> SIMD[c_type, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## vpdpbusds

<section class='mojo-docs'>

`vpdpbusds[width: Int, a_type: DType, b_type: DType, c_type: DType](src: SIMD[c_type, width], a: SIMD[a_type, width], b: SIMD[b_type, width]) -> SIMD[c_type, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## vpdpwssd

<section class='mojo-docs'>

`vpdpwssd[width: Int, a_type: DType, b_type: DType, c_type: DType](src: SIMD[c_type, width], a: SIMD[a_type, (width * 2)], b: SIMD[b_type, (width * 2)]) -> SIMD[c_type, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## vpdpwssds

<section class='mojo-docs'>

`vpdpwssds[width: Int, a_type: DType, b_type: DType, c_type: DType](src: SIMD[c_type, width], a: SIMD[a_type, (width * 2)], b: SIMD[b_type, (width * 2)]) -> SIMD[c_type, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## arch

<section class='mojo-docs'>

Provides architecture specific utility functions.

## Packages

* [â€‹`cpu`](./cpu/): Provides cpu architecture specific utility functions.
* [â€‹`sm100`](./sm100/): Provides Nvidia Blackwell architecture specific utility functions.

</section>

---

## sm100

<section class='mojo-docs'>

Provides Nvidia Blackwell architecture specific utility functions.

## Modules

* [â€‹`mma`](./mma/):

</section>

---

## Major

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct Major`

## Fields

* â€‹val (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `K`

`comptime K = Major(0)`

### `MN`

`comptime MN = Major(1)`

## Methods

### `__eq__`

`__eq__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## MmaOpSM100_BlockScaled_SS

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MmaOpSM100_BlockScaled_SS[c_type: DType, a_type: DType, b_type: DType, sfa_dtype: DType, sfb_dtype: DType, block_tile_shape: IndexList[3], mma_shape: IndexList[3], /, *, accum_type: DType = DType.float32, cta_group: Int = 1, cluster_shape: IndexList[3] = Index(1, 1, 1), a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, transpose_b: Bool = False]`

## Fields

* â€‹idesc (`UMMAInsDescriptor[MmaOpSM100_BlockScaled_SS._get_umma_kind[c_type, a_type, b_type, sfa_dtype, sfb_dtype, block_tile_shape, mma_shape, accum_type, cta_group, cluster_shape, a_swizzle, b_swizzle, transpose_b, a_type]()]`):
* â€‹mask (`UInt16`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__() -> Self`

### `mma`

`mma(self, a: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_tmem: UInt32, sfa_tmem: UInt32, sfb_tmem: UInt32, init_c: Bool)`

MMA input tiles.

The layout assumes that coalesce(A) has shape (bm, sw\_k, num\_sw\_k), we currently
assumes bm = mma\_m. In future, we can tile it to (mma\_m, sw\_k, num\_sw\_k, num\_mma\_m)
The same logic applies to matrix B.

### `commit`

`commit(self, ptr_mbar: LegacyUnsafePointer[type, address_space=AddressSpace.SHARED, mut=mut, origin=origin])`

### `wait`

`wait(self)`

</section>

---

## MmaOpSM100_SS

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MmaOpSM100_SS[c_type: DType, a_type: DType, b_type: DType, block_tile_shape: IndexList[3], mma_shape: IndexList[3], /, *, accum_type: DType = DType.float32, cta_group: Int = 1, cluster_shape: IndexList[3] = Index(1, 1, 1), a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, transpose_b: Bool = False]`

## Fields

* â€‹idesc (`UMMAInsDescriptor[MmaOpSM100_SS._get_umma_kind[c_type, a_type, b_type, block_tile_shape, mma_shape, accum_type, cta_group, cluster_shape, a_swizzle, b_swizzle, transpose_b, a_type]()]`):
* â€‹mask (`UInt16`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__() -> Self`

### `mma`

`mma(self, a: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_tmem: UInt32, init_c: Bool)`

MMA input tiles.

The layout assumes that coalesce(A) has shape (bm, sw\_k, num\_sw\_k), we currently
assumes bm = mma\_m. In future, we can tile it to (mma\_m, sw\_k, num\_sw\_k, num\_mma\_m)
The same logic applies to matrix B.

### `commit`

`commit(self, ptr_mbar: LegacyUnsafePointer[type, address_space=AddressSpace.SHARED, mut=mut, origin=origin])`

### `wait`

`wait(self)`

</section>

---

## extract_first_2_modes

<section class='mojo-docs'>

`extract_first_2_modes[l: Layout]() -> Layout`

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout)

</section>

---

## mma

<section class='mojo-docs'>

## Structs

* [â€‹`Major`](./Major):
* [â€‹`MmaOpSM100_BlockScaled_SS`](./MmaOpSM100_BlockScaled_SS):
* [â€‹`MmaOpSM100_SS`](./MmaOpSM100_SS):

## Functions

* [â€‹`extract_first_2_modes`](./extract_first_2_modes):
* [â€‹`max_contiguous_tile_shape`](./max_contiguous_tile_shape): Returns the maximum shape of a tile that's contiguous in memory for mma op. This is used to create TMA descriptor.
* [â€‹`smem_descriptor`](./smem_descriptor):

</section>

---

## max_contiguous_tile_shape

<section class='mojo-docs'>

`max_contiguous_tile_shape[rank: Int, //, dtype: DType, tile_shape: IndexList[rank], /, *, major: Major = Major.K, swizzle_mode: SwizzleMode = SwizzleMode.NONE]() -> IntTuple`

Returns the maximum shape of a tile that's contiguous in memory for mma op. This is used to create TMA descriptor.

**Returns:**

`IntTuple`

</section>

---

## smem_descriptor

<section class='mojo-docs'>

`smem_descriptor[dtype: DType, //, *, BMN: Int, BK: Int, swizzle_mode: TensorMapSwizzle, is_k_major: Bool](ptr: LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED, mut=mut, origin=origin]) -> MMASmemDescriptorPair`

**Returns:**

`MMASmemDescriptorPair`

</section>

---

## batched_matmul

<section class='mojo-docs'>

`batched_matmul[rank: Int, a_type: DType, b_type: DType, c_type: DType, //, *, transpose_a: Bool, transpose_b: Bool, elementwise_epilogue_fn: OptionalReg[fn[c_type: DType, width: Int, rank: Int, *, alignment: Int = 1](IndexList[rank], SIMD[c_type, width]) capturing -> None] = None, saturated_vnni: Bool = False, single_thread_blocking_override: Bool = False, target: StringSlice[StaticConstantOrigin] = "cpu"](c_buf: NDBuffer[c_type, rank, origin, shape, strides], a_buf: NDBuffer[a_type, rank, origin, shape, strides], b_buf: NDBuffer[b_type, rank, origin, shape, strides], *, context: DeviceContextPtr = DeviceContextPtr())`

`batched_matmul[rank: Int, a_type: DType, b_type: DType, c_type: DType, //, *, transpose_b: Bool, elementwise_epilogue_fn: OptionalReg[fn[c_type: DType, width: Int, rank: Int, *, alignment: Int = 1](IndexList[rank], SIMD[c_type, width]) capturing -> None] = None, saturated_vnni: Bool = False, target: StringSlice[StaticConstantOrigin] = "cpu"](c_buf: NDBuffer[c_type, rank, origin, shape, strides], a_buf: NDBuffer[a_type, rank, origin, shape, strides], b_buf: NDBuffer[b_type, rank, origin, shape, strides], *, context: DeviceContextPtr = DeviceContextPtr())`

</section>

---

## batched_matmul_dynamic_scaled_fp8

<section class='mojo-docs'>

`batched_matmul_dynamic_scaled_fp8[c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, //, input_scale_granularity: StringSlice[StaticConstantOrigin], weight_scale_granularity: StringSlice[StaticConstantOrigin], m_scale_granularity: Int, n_scale_granularity: Int, k_scale_granularity: Int, transpose_b: Bool = False, target: StringSlice[StaticConstantOrigin] = "cpu"](c: NDBuffer[c_type, 3, origin, shape], a: NDBuffer[a_type, 3, origin, shape], b: NDBuffer[b_type, 3, origin, shape], a_scales: NDBuffer[a_scales_type, 3, origin, shape], b_scales: NDBuffer[b_scales_type, 3, origin, shape], ctx: DeviceContext)`

</section>

---

## batched_matmul_dynamic_scaled_fp8_naive

<section class='mojo-docs'>

`batched_matmul_dynamic_scaled_fp8_naive[c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, //, *, scales_granularity_mnk: IndexList[3], transpose_b: Bool = False](c_device: NDBuffer[c_type, 3, origin, shape], a_device: NDBuffer[a_type, 3, origin, shape], b_device: NDBuffer[b_type, 3, origin, shape], a_scales_device: NDBuffer[a_scales_type, 3, origin, shape], b_scales_device: NDBuffer[b_scales_type, 3, origin, shape], ctx: DeviceContext)`

</section>

---

## batched_matmul_kernel_gpu

<section class='mojo-docs'>

`batched_matmul_kernel_gpu[c_type: DType, a_type: DType, b_type: DType, layout_c: Layout, layout_a: Layout, layout_b: Layout, transpose_b: Bool, config: MatmulConfig[a_type, b_type, c_type, transpose_b], elementwise_lambda_fn: OptionalReg[fn[c_type: DType, width: Int, rank: Int, *, alignment: Int = 1](IndexList[rank], SIMD[c_type, width]) capturing -> None] = None](c_tensor: LayoutTensor[c_type, layout_c, MutAnyOrigin], a_tensor: LayoutTensor[a_type, layout_a, MutAnyOrigin], b_tensor: LayoutTensor[b_type, layout_b, MutAnyOrigin], m: Int, n: Int, k: Int)`

</section>

---

## batched_matmul_shape

<section class='mojo-docs'>

`batched_matmul_shape[rank: Int, a_type: DType, b_type: DType, single_thread_blocking_override: Bool](a_buff: NDBuffer[a_type, rank, origin], b_buff: NDBuffer[b_type, rank, origin]) -> IndexList[rank]`

Compute the output shape of a `batch_matmul` operation, and assert the inputs are compatible.

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): Rank of the input and output tensors.
* â€‹a\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the lhs input tensor.
* â€‹b\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the rhs input tensor.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.

**Args:**

* â€‹a\_buff ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): The lhs input tensor.
* â€‹b\_buff ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): The rhs input tensor.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## bmm_sm100_blockwise_scaled_fp8

<section class='mojo-docs'>

`bmm_sm100_blockwise_scaled_fp8[a_layout: Layout, b_layout: Layout, c_layout: Layout, a_scales_layout: Layout, b_scales_layout: Layout, c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, *, transpose_b: Bool, umma_shape: IndexList[3], block_tile_shape: IndexList[3], a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, elementwise_lambda_fn: OptionalReg[fn[c_type: DType, width: Int, rank: Int, *, alignment: Int = 1](IndexList[rank], SIMD[c_type, width]) capturing -> None] = None](c: LayoutTensor[c_type, c_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[a_type, a_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[b_type, b_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_scales: LayoutTensor[a_scales_type, a_scales_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_scales: LayoutTensor[b_scales_type, b_scales_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

</section>

---

## get_shape_index_list

<section class='mojo-docs'>

`get_shape_index_list[rank: Int, dtype: DType, layout: Layout](tensor: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> IndexList[rank]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## bmm

<section class='mojo-docs'>

## `comptime` values

### `elementwise_epilogue_type`

`comptime elementwise_epilogue_type = fn[c_type: DType, width: Int, rank: Int, *, alignment: Int = 1](IndexList[rank], SIMD[c_type, width]) capturing -> None`

### `logger`

`comptime logger = Logger[DEFAULT_LEVEL](stdout, "", False)`

## Functions

* [â€‹`batched_matmul`](./batched_matmul):
* [â€‹`batched_matmul_dynamic_scaled_fp8`](./batched_matmul_dynamic_scaled_fp8):
* [â€‹`batched_matmul_dynamic_scaled_fp8_naive`](./batched_matmul_dynamic_scaled_fp8_naive):
* [â€‹`batched_matmul_kernel_gpu`](./batched_matmul_kernel_gpu):
* [â€‹`batched_matmul_shape`](./batched_matmul_shape): Compute the output shape of a `batch_matmul` operation, and assert the inputs are compatible.
* [â€‹`bmm_sm100_blockwise_scaled_fp8`](./bmm_sm100_blockwise_scaled_fp8):
* [â€‹`get_shape_index_list`](./get_shape_index_list):
* [â€‹`naive_batched_matmul_kernel`](./naive_batched_matmul_kernel):

</section>

---

## naive_batched_matmul_kernel

<section class='mojo-docs'>

`naive_batched_matmul_kernel[rank: Int, c_type: DType, a_type: DType, b_type: DType, c_layout: Layout, a_layout: Layout, b_layout: Layout, elementwise_lambda_fn: OptionalReg[fn[c_type: DType, width: Int, rank: Int, *, alignment: Int = 1](IndexList[rank], SIMD[c_type, width]) capturing -> None] = None, accum_type: DType = get_accum_type[c_type]()](c_tensor: LayoutTensor[c_type, c_layout, MutAnyOrigin], a_tensor: LayoutTensor[a_type, a_layout, MutAnyOrigin], b_tensor: LayoutTensor[b_type, b_layout, MutAnyOrigin], c_buff_nd_shape: IndexList[rank])`

</section>

---

## distributed_matmul

<section class='mojo-docs'>

## `comptime` values

### `elementwise_epilogue_type`

`comptime elementwise_epilogue_type = fn[input_index: Int, dtype: DType, rank: Int, width: Int, *, alignment: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None`

## Functions

* [â€‹`matmul_allreduce`](./matmul_allreduce): Performs C = matmul(A, B^T) followed with Out = allreduce(C) operation across multiple GPUs. Split the A or B and C matrices into `num_partitions` submatrices at dimension `partition_dim`. This way we can perform `num_partitions` independent matmul + allreduce kernels, and overlap some of the computation.

</section>

---

## matmul_allreduce

<section class='mojo-docs'>

`matmul_allreduce[ngpus: Int, partition_dim: Int, outputs_lambda: elementwise_epilogue_type, a_dtype: DType, b_dtype: DType, out_dtype: DType, a_static_shape: DimList, b_static_shape: DimList, c_static_shape: DimList, out_static_shape: DimList, overlap_with_dpl: Bool = True](a_buffers: InlineArray[NDBuffer[a_dtype, 2, MutAnyOrigin, a_static_shape], ngpus], b_buffers: InlineArray[NDBuffer[b_dtype, 2, MutAnyOrigin, b_static_shape], ngpus], c_temp_buffers: InlineArray[NDBuffer[out_dtype, 2, MutAnyOrigin, c_static_shape], ngpus], output_buffers: InlineArray[NDBuffer[out_dtype, 2, MutAnyOrigin, out_static_shape], ngpus], rank_sigs: InlineArray[LegacyUnsafePointer[Signal], 8], ctxs: List[DeviceContext], num_partitions: ValOrDim[dim])`

Performs C = matmul(A, B^T) followed with Out = allreduce(C) operation across multiple GPUs. Split the A or B and C matrices into `num_partitions` submatrices at dimension `partition_dim`. This way we can perform `num_partitions` independent matmul + allreduce kernels, and overlap some of the computation.

`matmul_allreduce[ngpus: Int, outputs_lambda: elementwise_epilogue_type, a_dtype: DType, b_dtype: DType, out_dtype: DType, a_static_shape: DimList, b_static_shape: DimList, c_static_shape: DimList, out_static_shape: DimList](a_buffers: InlineArray[NDBuffer[a_dtype, 2, MutAnyOrigin, a_static_shape], ngpus], b_buffers: InlineArray[NDBuffer[b_dtype, 2, MutAnyOrigin, b_static_shape], ngpus], c_temp_buffers: InlineArray[NDBuffer[out_dtype, 2, MutAnyOrigin, c_static_shape], ngpus], output_buffers: InlineArray[NDBuffer[out_dtype, 2, MutAnyOrigin, out_static_shape], ngpus], rank_sigs: InlineArray[LegacyUnsafePointer[Signal], 8], ctxs: List[DeviceContext])`

Performs C = matmul(A, B^T) followed with Out = allreduce(C) operation across multiple GPUs. The implementation might potentially split A / B / C matrices and overlap computation to speedup performance.

</section>

---

## config_in_smem

<section class='mojo-docs'>

`config_in_smem[a_type: DType, b_type: DType, c_type: DType, transpose_b: Bool, //, max_smem: Int](config: MatmulConfig[a_type, b_type, c_type, transpose_b]) -> MatmulConfig[a_type, b_type, c_type, transpose_b]`

**Returns:**

[`MatmulConfig`](/mojo/kernels/linalg/utils_gpu/MatmulConfig)

</section>

---

## dual_gemm

<section class='mojo-docs'>

`dual_gemm[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, //, *, transpose_b: Bool, binary_lambda_fn: binary_fn_type = swilu, config: OptionalReg[MatmulConfig[a_type, b_type, c_type, transpose_b]] = None, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: NDBuffer[c_type, 2, MutAnyOrigin, c_shape], a: NDBuffer[a_type, 2, MutAnyOrigin, a_shape], b0: NDBuffer[b_type, 2, MutAnyOrigin, b_shape], b1: NDBuffer[b_type, 2, MutAnyOrigin, b_shape], ctx: DeviceContext)`

</section>

---

## dual_gemv

<section class='mojo-docs'>

`dual_gemv[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, //, *, binary_lambda_fn: binary_fn_type = swilu, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: NDBuffer[c_type, 2, MutAnyOrigin, c_shape], a: NDBuffer[a_type, 2, MutAnyOrigin, a_shape], b0: NDBuffer[b_type, 2, MutAnyOrigin, b_shape], b1: NDBuffer[b_type, 2, MutAnyOrigin, b_shape], ctx: DeviceContext)`

</section>

---

## dual_gemv_kernel

<section class='mojo-docs'>

`dual_gemv_kernel[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, simd_width: UInt, tile_m: UInt, tile_n: UInt, num_threads: UInt, binary_lambda_fn: binary_fn_type, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, s_type: DType = get_accum_type[c_type]()](c: NDBuffer[c_type, 2, MutAnyOrigin, c_shape], a: NDBuffer[a_type, 2, MutAnyOrigin, a_shape], b0: NDBuffer[b_type, 2, MutAnyOrigin, b_shape], b1: NDBuffer[b_type, 2, MutAnyOrigin, b_shape])`

</section>

---

## dual_gemm (Dual_gemm)

<section class='mojo-docs'>

## `comptime` values

### `binary_fn_type`

`comptime binary_fn_type = fn[type: DType, width: Int](SIMD[type, width], SIMD[type, width]) -> SIMD[type, width]`

## Functions

* [â€‹`config_in_smem`](./config_in_smem):
* [â€‹`dual_gemm`](./dual_gemm):
* [â€‹`dual_gemv`](./dual_gemv):
* [â€‹`dual_gemv_kernel`](./dual_gemv_kernel):
* [â€‹`multistage_dual_gemm`](./multistage_dual_gemm):
* [â€‹`multistage_dual_gemm_kernel`](./multistage_dual_gemm_kernel):
* [â€‹`multistage_dual_mma`](./multistage_dual_mma):
* [â€‹`swilu`](./swilu):
* [â€‹`swishGLU`](./swishGLU): Reference:     GLU Variants Improve Transformer     by Noam Shazeer     <https://arxiv.org/pdf/2002.05202v1> The implementation follows cutlass, using one kernel invocation and writing to the destination once.

</section>

---

## multistage_dual_gemm

<section class='mojo-docs'>

`multistage_dual_gemm[c_type: DType, c_layout: Layout, a_type: DType, a_layout: Layout, b_type: DType, b_layout: Layout, //, transpose_b: Bool, config: MatmulConfig[a_type, b_type, c_type, transpose_b], binary_lambda_fn: binary_fn_type = swilu, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, c_layout, origin], a: LayoutTensor[a_type, a_layout, origin], b0: LayoutTensor[b_type, b_layout, origin], b1: LayoutTensor[b_type, b_layout, origin], ctx: DeviceContext)`

`multistage_dual_gemm[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, //, *, transpose_b: Bool, config: MatmulConfig[a_type, b_type, c_type, transpose_b], binary_lambda_fn: binary_fn_type = swilu, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, num_k_partitions: Int = 1](c: NDBuffer[c_type, 2, origin, c_shape], a: NDBuffer[a_type, 2, origin, a_shape], b0: NDBuffer[b_type, 2, origin, b_shape], b1: NDBuffer[b_type, 2, origin, b_shape], ctx: DeviceContext)`

</section>

---

## multistage_dual_gemm_kernel

<section class='mojo-docs'>

`multistage_dual_gemm_kernel[c_type: DType, c_layout: Layout, a_type: DType, a_layout: Layout, b_type: DType, b_layout: Layout, transpose_b: Bool, config: MatmulConfig[a_type, b_type, c_type, transpose_b], binary_lambda_fn: binary_fn_type, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, c_layout, MutAnyOrigin], a: LayoutTensor[a_type, a_layout, MutAnyOrigin], b0: LayoutTensor[b_type, b_layout, MutAnyOrigin], b1: LayoutTensor[b_type, b_layout, MutAnyOrigin])`

</section>

---

## multistage_dual_mma

<section class='mojo-docs'>

`multistage_dual_mma[c_type: DType, c_layout: Layout, a_type: DType, a_layout: Layout, a_smem_layout: Layout, b_type: DType, b_layout: Layout, b_smem_layout: Layout, //, BM: Int, BN: Int, BK: Int, WM: Int, WN: Int, num_threads: Int, num_pipeline_stages: Int, transpose_b: Bool, /, *, swizzle_a: Bool = True, static_num_iters: Dim = Dim(), k_group_size: UInt = 1](c0: LayoutTensor[c_type, c_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c1: LayoutTensor[c_type, c_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_iter_arg: LayoutTensorIter[dtype, a_layout, MutAnyOrigin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], b0_iter_arg: LayoutTensorIter[b_type, b_layout, MutAnyOrigin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], b1_iter_arg: LayoutTensorIter[b_type, b_layout, MutAnyOrigin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], a_smem_iter_arg: LayoutTensorIter[a_type, a_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], mut b0_smem_iter: LayoutTensorIter[b_type, b_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], mut b1_smem_iter: LayoutTensorIter[b_type, b_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], num_iters: Int, /, *, num_b_rows: OptionalReg[Int] = None)`

</section>

---

## swilu

<section class='mojo-docs'>

`swilu[dtype: DType, width: Int](x: SIMD[dtype, width], y: SIMD[dtype, width]) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## swishGLU

<section class='mojo-docs'>

`swishGLU[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, //, target: StringSlice[StaticConstantOrigin] = "cpu"](a: NDBuffer[a_type, 2, MutAnyOrigin, a_shape], b0: NDBuffer[b_type, 2, MutAnyOrigin, b_shape], b1: NDBuffer[b_type, 2, MutAnyOrigin, b_shape], c: NDBuffer[c_type, 2, MutAnyOrigin, c_shape], ctx: DeviceContextPtr)`

Reference:     GLU Variants Improve Transformer     by Noam Shazeer     <https://arxiv.org/pdf/2002.05202v1> The implementation follows cutlass, using one kernel invocation and writing to the destination once.

</section>

---

## block_scaled_matmul

<section class='mojo-docs'>

`block_scaled_matmul[c_type: DType, a_type: DType, b_type: DType, scales_dtype: DType, //, *, SF_VECTOR_SIZE: Int, transpose_b: Bool = True, target: StringSlice[StaticConstantOrigin] = "cpu", elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None](c_device: NDBuffer[c_type, 2, MutAnyOrigin, shape], a_device: NDBuffer[a_type, 2, MutAnyOrigin, shape], b_device: NDBuffer[b_type, 2, MutAnyOrigin, shape], a_scales_device: NDBuffer[scales_dtype, 5, MutAnyOrigin, shape], b_scales_device: NDBuffer[scales_dtype, 5, MutAnyOrigin, shape], tensor_sf: Float32, ctx: DeviceContext)`

</section>

---

## block_scales_interleave

<section class='mojo-docs'>

`block_scales_interleave[scales_dtype: DType, //, *, SF_VECTOR_SIZE: Int, target: StringSlice[StaticConstantOrigin] = "cpu"](output_scales_device: NDBuffer[scales_dtype, 5, MutAnyOrigin, shape], input_scales_device: NDBuffer[scales_dtype, 2, MutAnyOrigin, shape], ctx: DeviceContext)`

</section>

---

## block_scales_interleave_fp4

<section class='mojo-docs'>

`block_scales_interleave_fp4[scales_dtype: DType, input_scales_layout: Layout, output_scales_layout: Layout, //, *, SF_VECTOR_SIZE: Int = 16, num_max_threads: Int = 1024](ctx: DeviceContext, input_scales: LayoutTensor[scales_dtype, input_scales_layout, MutAnyOrigin], output_scales: LayoutTensor[scales_dtype, output_scales_layout, MutAnyOrigin])`

</section>

---

## block_scales_interleave_fp4_kernel

<section class='mojo-docs'>

`block_scales_interleave_fp4_kernel[scales_dtype: DType, input_scales_layout: Layout, output_scales_layout: Layout, *, SF_VECTOR_SIZE: Int = 16, num_max_threads: Int = 1024](input_scales: LayoutTensor[scales_dtype, input_scales_layout, MutAnyOrigin], output_scales: LayoutTensor[scales_dtype, output_scales_layout, MutAnyOrigin])`

</section>

---

## fp4_quantization

<section class='mojo-docs'>

## `comptime` values

### `logger`

`comptime logger = Logger[DEFAULT_LEVEL](stdout, "", False)`

## Functions

* [â€‹`block_scaled_matmul`](./block_scaled_matmul):
* [â€‹`block_scales_interleave`](./block_scales_interleave):
* [â€‹`block_scales_interleave_fp4`](./block_scales_interleave_fp4):
* [â€‹`block_scales_interleave_fp4_kernel`](./block_scales_interleave_fp4_kernel):
* [â€‹`naive_block_scaled_nvfp4_matmul`](./naive_block_scaled_nvfp4_matmul):
* [â€‹`naive_block_scaled_nvfp4_matmul_kernel`](./naive_block_scaled_nvfp4_matmul_kernel):
* [â€‹`quantize_dynamic_block_scaled`](./quantize_dynamic_block_scaled):
* [â€‹`quantize_dynamic_scaled_fp4`](./quantize_dynamic_scaled_fp4):
* [â€‹`quantize_dynamic_scaled_fp4_kernel`](./quantize_dynamic_scaled_fp4_kernel):

</section>

---

## naive_block_scaled_nvfp4_matmul

<section class='mojo-docs'>

`naive_block_scaled_nvfp4_matmul[c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, //, *, SF_VECTOR_SIZE: Int, accum_type: DType = DType.float32, transpose_b: Bool = True, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, BLOCK_DIM: Int = 16](c: LayoutTensor[c_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[a_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[b_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_scales: LayoutTensor[a_scales_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_scales: LayoutTensor[b_scales_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

</section>

---

## naive_block_scaled_nvfp4_matmul_kernel

<section class='mojo-docs'>

`naive_block_scaled_nvfp4_matmul_kernel[c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, accum_type: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, a_scale_layout: Layout, b_scale_layout: Layout, SF_VECTOR_SIZE: Int, transpose_b: Bool = True, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, c_layout, MutAnyOrigin], a: LayoutTensor[a_type, a_layout, MutAnyOrigin], b: LayoutTensor[b_type, b_layout, MutAnyOrigin], a_scales: LayoutTensor[a_scales_type, a_scale_layout, MutAnyOrigin], b_scales: LayoutTensor[b_scales_type, b_scale_layout, MutAnyOrigin])`

</section>

---

## quantize_dynamic_block_scaled

<section class='mojo-docs'>

`quantize_dynamic_block_scaled[out_dtype: DType, scales_dtype: DType, in_dtype: DType, //, *, SF_VECTOR_SIZE: Int, target: StringSlice[StaticConstantOrigin] = "cpu"](output_device: NDBuffer[out_dtype, 2, MutAnyOrigin, shape], scales_device: NDBuffer[scales_dtype, 5, MutAnyOrigin, shape], input_device: NDBuffer[in_dtype, 2, MutAnyOrigin, shape], tensor_sf: Float32, ctx: DeviceContext)`

</section>

---

## quantize_dynamic_scaled_fp4

<section class='mojo-docs'>

`quantize_dynamic_scaled_fp4[out_dtype: DType, scales_dtype: DType, in_dtype: DType, output_layout: Layout, scales_layout: Layout, input_layout: Layout, //, *, SF_VECTOR_SIZE: Int = 16, num_max_threads: Int = 512](ctx: DeviceContext, output: LayoutTensor[out_dtype, output_layout, MutAnyOrigin], scales: LayoutTensor[scales_dtype, scales_layout, MutAnyOrigin], input: LayoutTensor[in_dtype, input_layout, MutAnyOrigin], num_cols: Int, num_cols_padded: Int, tensor_sf: Float32 = 1)`

</section>

---

## quantize_dynamic_scaled_fp4_kernel

<section class='mojo-docs'>

`quantize_dynamic_scaled_fp4_kernel[out_dtype: DType, scales_dtype: DType, in_dtype: DType, output_layout: Layout, scales_layout: Layout, input_layout: Layout, *, SF_VECTOR_SIZE: Int = 16, ELEMENTS_PER_THREAD: Int = 8, num_max_threads: Int = 512](output: LayoutTensor[out_dtype, output_layout, MutAnyOrigin], scales: LayoutTensor[scales_dtype, scales_layout, MutAnyOrigin], input: LayoutTensor[in_dtype, input_layout, MutAnyOrigin], num_cols: Int, num_cols_padded: Int, tensor_sf: Float32)`

</section>

---

## cast_f4e2m1x2_to_fp16x2

<section class='mojo-docs'>

`cast_f4e2m1x2_to_fp16x2(x: UInt8) -> SIMD[DType.float16, 2]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## cast_fp32_to_fp4e2m1

<section class='mojo-docs'>

`cast_fp32_to_fp4e2m1[width: Int, //](x: SIMD[DType.float32, width]) -> UInt32`

**Returns:**

`UInt32`

</section>

---

## cast_fp_to_fp4e2m1

<section class='mojo-docs'>

`cast_fp_to_fp4e2m1[dtype: DType, width: Int, //](x: SIMD[dtype, width]) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## cast_uint_to_fp4e2m1

<section class='mojo-docs'>

`cast_uint_to_fp4e2m1[in_dtype: DType, in_width: Int, //, *, out_dtype: DType, out_width: Int](x: SIMD[in_dtype, in_width]) -> SIMD[out_dtype, out_width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## convert_ref_scales_to_mxfp8_format

<section class='mojo-docs'>

`convert_ref_scales_to_mxfp8_format[ref_scales_type: DType, scales_type: DType, *, REF_BLOCK_SIZE: Int, SF_VECTOR_SIZE: Int](m: ValOrDim[dim], n: ValOrDim[dim], k: ValOrDim[dim], ref_a_scales: NDBuffer[ref_scales_type, 2, origin, shape, strides], ref_b_scales: NDBuffer[ref_scales_type, 2, origin, shape, strides], a_scales: NDBuffer[scales_type, 5, origin, shape, strides], b_scales: NDBuffer[scales_type, 5, origin, shape, strides])`

</section>

---

## get_batched_scale_factor

<section class='mojo-docs'>

`get_batched_scale_factor[scales_dtype: DType, scales_layout: Layout, //, SF_VECTOR_SIZE: Int](scales_tensor: LayoutTensor[scales_dtype, scales_layout, MutAnyOrigin], batch_idx: Int, row_idx: Int, col_idx: Int) -> Scalar[scales_dtype]`

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar)

</section>

---

## get_scale_factor

<section class='mojo-docs'>

`get_scale_factor[scales_dtype: DType, scales_layout: Layout, //, SF_VECTOR_SIZE: Int](scales_tensor: LayoutTensor[scales_dtype, scales_layout, MutAnyOrigin], row_idx: Int, col_idx: Int) -> Scalar[scales_dtype]`

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar)

</section>

---

## fp4_utils

<section class='mojo-docs'>

## `comptime` values

### `E2M1_TO_FLOAT32`

`comptime E2M1_TO_FLOAT32 = SIMD[DType.float32, 16](0, 0.5, 1, 1.5, 2, 3, 4, 6, -0.0, -0.5, -1, -1.5, -2, -3, -4, -6, Tuple[]())`

### `MXFP4_SF_DTYPE`

`comptime MXFP4_SF_DTYPE = DType.float8_e8m0fnu`

### `MXFP4_SF_VECTOR_SIZE`

`comptime MXFP4_SF_VECTOR_SIZE = 32`

### `MXFP8_SF_DTYPE`

`comptime MXFP8_SF_DTYPE = DType.float8_e8m0fnu`

### `MXFP8_SF_VECTOR_SIZE`

`comptime MXFP8_SF_VECTOR_SIZE = 32`

### `NVFP4_SF_DTYPE`

`comptime NVFP4_SF_DTYPE = DType.float8_e4m3fn`

### `NVFP4_SF_VECTOR_SIZE`

`comptime NVFP4_SF_VECTOR_SIZE = 16`

### `SF_ATOM_K`

`comptime SF_ATOM_K = 4`

### `SF_ATOM_M`

`comptime SF_ATOM_M = Tuple[Int, Int](VariadicPack[True, origin_of(), True, Movable, Int, Int](32, 4))`

### `SF_MN_GROUP_SIZE`

`comptime SF_MN_GROUP_SIZE = ((load_from_mem SF_ATOM_M.__getitem__[Int, Int, 0]()) * (load_from_mem SF_ATOM_M.__getitem__[Int, Int, 1]()))`

## Functions

* [â€‹`cast_f4e2m1x2_to_fp16x2`](./cast_f4e2m1x2_to_fp16x2):
* [â€‹`cast_fp32_to_fp4e2m1`](./cast_fp32_to_fp4e2m1):
* [â€‹`cast_fp_to_fp4e2m1`](./cast_fp_to_fp4e2m1):
* [â€‹`cast_uint_to_fp4e2m1`](./cast_uint_to_fp4e2m1):
* [â€‹`convert_ref_scales_to_mxfp8_format`](./convert_ref_scales_to_mxfp8_format):
* [â€‹`get_batched_scale_factor`](./get_batched_scale_factor):
* [â€‹`get_scale_factor`](./get_scale_factor):
* [â€‹`set_batched_scale_factor`](./set_batched_scale_factor):
* [â€‹`set_scale_factor`](./set_scale_factor):

</section>

---

## set_batched_scale_factor

<section class='mojo-docs'>

`set_batched_scale_factor[scales_dtype: DType, scales_layout: Layout, //, SF_VECTOR_SIZE: Int](scales_tensor: LayoutTensor[scales_dtype, scales_layout, MutAnyOrigin], batch_idx: Int, row_idx: Int, col_idx: Int, scale_value: Scalar[scales_dtype])`

</section>

---

## set_scale_factor

<section class='mojo-docs'>

`set_scale_factor[scales_dtype: DType, scales_layout: Layout, //, SF_VECTOR_SIZE: Int](scales_tensor: LayoutTensor[scales_dtype, scales_layout, MutAnyOrigin], row_idx: Int, col_idx: Int, scale_value: Scalar[scales_dtype])`

</section>

---

## batched_quantize_dynamic_scaled_fp8

<section class='mojo-docs'>

`batched_quantize_dynamic_scaled_fp8[out_dtype: DType, in_dtype: DType, scales_dtype: DType, //, input_fn: fn[width: Int](batch: Int, row: Int, col: Int) capturing -> SIMD[in_dtype, width], group_size_or_per_token: Int, num_cols: Int](scaled_output: NDBuffer[out_dtype, 3, MutAnyOrigin], scales: NDBuffer[scales_dtype, 3, MutAnyOrigin], scale_ub: Float32, ctx: DeviceContext, num_rows: Int, batch_size: Int)`

</section>

---

## batched_quantize_fp8_kernel

<section class='mojo-docs'>

`batched_quantize_fp8_kernel[out_type: DType, scales_type: DType, in_type: DType, input_fn: fn[width: Int](batch: Int, row: Int, col: Int) capturing -> SIMD[in_type, width], warps_per_block: Int, group_size: Int](output: NDBuffer[out_type, 3, MutAnyOrigin], scales: NDBuffer[scales_type, 3, MutAnyOrigin], scale_ub: Scalar[scales_type])`

</section>

---

## blockwise_scaled_fp8_with_epilogue

<section class='mojo-docs'>

`blockwise_scaled_fp8_with_epilogue[c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, //, *, scales_granularity_mnk: IndexList[3], BLOCK_DIM: Int = 16, transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, accum_type: DType = get_accum_type[c_type]()](c: LayoutTensor[c_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[a_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[b_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_scales: LayoutTensor[a_scales_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_scales: LayoutTensor[b_scales_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

Our sm100 blockwise scaled fp8 matmul kernel still does not support fusion of elementwise operations. This is a temporary implementation that uses our sm100 blockwise scaled fp8 matmul kernel and dispatch a separate epilogue kernel to apply the elementwise operations. For non B200 GPUs, we use the naive blockwise scaled fp8 matmul which support normal epilogue natively.

</section>

---

## convert_e4m3fn_to_e4m3fnuz

<section class='mojo-docs'>

`convert_e4m3fn_to_e4m3fnuz(input_buffer: LayoutTensor[DType.float8_e4m3fn, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output_buffer: LayoutTensor[DType.float8_e4m3fnuz, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContext)`

Convert E4M3FN weights to E4M3FNUZ format for AMD GPU compatibility.

This conversion handles the key differences between E4M3FN and E4M3FNUZ:

1. The bit pattern 10000000 (-128) represents zero in E4M3FN but NaN in E4M3FNUZ

**Args:**

* â€‹input\_buffer ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input tensor in E4M3FN format.
* â€‹output\_buffer ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output tensor to store E4M3FNUZ format.
* â€‹context ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): Device context for kernel execution.

</section>

---

## fp8_quantization

<section class='mojo-docs'>

## `comptime` values

### `logger`

`comptime logger = Logger[DEFAULT_LEVEL](stdout, "", False)`

## Functions

* [â€‹`batched_quantize_dynamic_scaled_fp8`](./batched_quantize_dynamic_scaled_fp8):
* [â€‹`batched_quantize_fp8_kernel`](./batched_quantize_fp8_kernel):
* [â€‹`blockwise_scaled_fp8_with_epilogue`](./blockwise_scaled_fp8_with_epilogue): Our sm100 blockwise scaled fp8 matmul kernel still does not support fusion of elementwise operations. This is a temporary implementation that uses our sm100 blockwise scaled fp8 matmul kernel and dispatch a separate epilogue kernel to apply the elementwise operations. For non B200 GPUs, we use the naive blockwise scaled fp8 matmul which support normal epilogue natively.
* [â€‹`convert_e4m3fn_to_e4m3fnuz`](./convert_e4m3fn_to_e4m3fnuz): Convert E4M3FN weights to E4M3FNUZ format for AMD GPU compatibility.
* [â€‹`matmul_dynamic_scaled_fp8`](./matmul_dynamic_scaled_fp8):
* [â€‹`naive_blockwise_scaled_fp8_grouped_matmul`](./naive_blockwise_scaled_fp8_grouped_matmul):
* [â€‹`naive_blockwise_scaled_fp8_grouped_matmul_kernel`](./naive_blockwise_scaled_fp8_grouped_matmul_kernel):
* [â€‹`naive_blockwise_scaled_fp8_matmul`](./naive_blockwise_scaled_fp8_matmul):
* [â€‹`naive_blockwise_scaled_fp8_matmul_kernel`](./naive_blockwise_scaled_fp8_matmul_kernel):
* [â€‹`quantize_dynamic_scaled_fp8`](./quantize_dynamic_scaled_fp8):
* [â€‹`quantize_fp8_kernel`](./quantize_fp8_kernel):
* [â€‹`quantize_static_scaled_fp8`](./quantize_static_scaled_fp8):

</section>

---

## matmul_dynamic_scaled_fp8

<section class='mojo-docs'>

`matmul_dynamic_scaled_fp8[c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, //, input_scale_granularity: StringSlice[StaticConstantOrigin], weight_scale_granularity: StringSlice[StaticConstantOrigin], m_scale_granularity: Int, n_scale_granularity: Int, k_scale_granularity: Int, transpose_b: Bool = False, target: StringSlice[StaticConstantOrigin] = "cpu"](c: NDBuffer[c_type, 2, origin, shape], a: NDBuffer[a_type, 2, origin, shape], b: NDBuffer[b_type, 2, origin, shape], a_scales: NDBuffer[a_scales_type, 2, origin, shape], b_scales: NDBuffer[b_scales_type, 2, origin, shape], ctx: DeviceContext)`

</section>

---

## naive_blockwise_scaled_fp8_grouped_matmul

<section class='mojo-docs'>

`naive_blockwise_scaled_fp8_grouped_matmul[c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, a_offsets_type: DType, expert_ids_type: DType, c_layout: Layout, a_layout: Layout, b_layout: Layout, a_scale_layout: Layout, b_scale_layout: Layout, a_offsets_layout: Layout, expert_ids_layout: Layout, //, BLOCK_DIM_N: Int = 32, BLOCK_DIM_M: Int = 16, transpose_b: Bool = True, scales_granularity_mnk: OptionalReg[IndexList[3]] = None, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, c_layout, MutAnyOrigin], a: LayoutTensor[a_type, a_layout, MutAnyOrigin], b: LayoutTensor[b_type, b_layout, MutAnyOrigin], a_scales: LayoutTensor[a_scales_type, a_scale_layout, MutAnyOrigin], b_scales: LayoutTensor[b_scales_type, b_scale_layout, MutAnyOrigin], a_offsets: LayoutTensor[a_offsets_type, a_offsets_layout, MutAnyOrigin], expert_ids: LayoutTensor[expert_ids_type, expert_ids_layout, MutAnyOrigin], max_num_tokens_per_expert: Int, num_active_experts: Int, ctx: DeviceContext)`

</section>

---

## naive_blockwise_scaled_fp8_grouped_matmul_kernel

<section class='mojo-docs'>

`naive_blockwise_scaled_fp8_grouped_matmul_kernel[c_layout: Layout, a_layout: Layout, b_layout: Layout, a_scale_layout: Layout, b_scale_layout: Layout, a_offsets_layout: Layout, expert_ids_layout: Layout, c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, a_offsets_type: DType, expert_ids_type: DType, accum_type: DType, transpose_b: Bool = True, scales_granularity_mnk: OptionalReg[IndexList[3]] = None, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, c_layout, MutAnyOrigin], a: LayoutTensor[a_type, a_layout, MutAnyOrigin], b: LayoutTensor[b_type, b_layout, MutAnyOrigin], a_offsets: LayoutTensor[a_offsets_type, a_offsets_layout, MutAnyOrigin], expert_ids: LayoutTensor[expert_ids_type, expert_ids_layout, MutAnyOrigin], a_scales: LayoutTensor[a_scales_type, a_scale_layout, MutAnyOrigin], b_scales: LayoutTensor[b_scales_type, b_scale_layout, MutAnyOrigin])`

</section>

---

## naive_blockwise_scaled_fp8_matmul

<section class='mojo-docs'>

`naive_blockwise_scaled_fp8_matmul[c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, //, *, BLOCK_DIM: Int = 16, transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, accum_type: DType = get_accum_type[c_type](), scales_granularity_mnk: OptionalReg[IndexList[3]] = None](c: LayoutTensor[c_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[a_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[b_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_scales: LayoutTensor[a_scales_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_scales: LayoutTensor[b_scales_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

`naive_blockwise_scaled_fp8_matmul[c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, c_shape: DimList, a_shape: DimList, b_shape: DimList, a_scale_shape: DimList, b_scale_shape: DimList, //, *, BLOCK_DIM: Int = 16, transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, accum_type: DType = get_accum_type[c_type](), scales_granularity_mnk: OptionalReg[IndexList[3]] = None](c_device: NDBuffer[c_type, 2, origin, c_shape], a_device: NDBuffer[a_type, 2, origin, a_shape], b_device: NDBuffer[b_type, 2, origin, b_shape], a_scales_device: NDBuffer[a_scales_type, 2, origin, a_scale_shape], b_scales_device: NDBuffer[b_scales_type, 2, origin, b_scale_shape], ctx: DeviceContext)`

</section>

---

## naive_blockwise_scaled_fp8_matmul_kernel

<section class='mojo-docs'>

`naive_blockwise_scaled_fp8_matmul_kernel[c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, accum_type: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, a_scale_layout: Layout, b_scale_layout: Layout, BLOCK_DIM: Int, transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, scales_granularity_mnk: OptionalReg[IndexList[3]] = None](c: LayoutTensor[c_type, c_layout, MutAnyOrigin], a: LayoutTensor[a_type, a_layout, MutAnyOrigin], b: LayoutTensor[b_type, b_layout, MutAnyOrigin], a_scales: LayoutTensor[a_scales_type, a_scale_layout, MutAnyOrigin], b_scales: LayoutTensor[b_scales_type, b_scale_layout, MutAnyOrigin])`

</section>

---

## quantize_dynamic_scaled_fp8

<section class='mojo-docs'>

`quantize_dynamic_scaled_fp8[out_dtype: DType, in_dtype: DType, scales_dtype: DType, //, input_fn: fn[width: Int](row: Int, col: Int) capturing -> SIMD[in_dtype, width], group_size_or_per_token: Int, num_cols: Int](scaled_output: NDBuffer[out_dtype, 2, MutAnyOrigin], scales: NDBuffer[scales_dtype, 2, MutAnyOrigin], scale_ub: Float32, ctx: DeviceContext, num_rows: Int)`

</section>

---

## quantize_fp8_kernel

<section class='mojo-docs'>

`quantize_fp8_kernel[out_type: DType, scales_type: DType, in_type: DType, input_fn: fn[width: Int](row: Int, col: Int) capturing -> SIMD[in_type, width], warps_per_block: Int, group_size: Int](output: NDBuffer[out_type, 2, MutAnyOrigin], scales: NDBuffer[scales_type, 2, MutAnyOrigin], scale_ub: Scalar[scales_type])`

</section>

---

## quantize_static_scaled_fp8

<section class='mojo-docs'>

`quantize_static_scaled_fp8[out_dtype: DType, in_dtype: DType, scale_is_inverted: Bool = True](out_buffer: NDBuffer[out_dtype, 2, origin, shape, strides], in_buffer: NDBuffer[in_dtype, 2, origin, shape, strides], scale: Float32, context: DeviceContext)`

</section>

---

## GEMVAlgorithm

<section class='mojo-docs'>

`struct GEMVAlgorithm`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `GEMV_KERNEL`

`comptime GEMV_KERNEL = GEMVAlgorithm(0)`

### `GEMV_KERNEL_VECTOR`

`comptime GEMV_KERNEL_VECTOR = GEMVAlgorithm(1)`

### `GEMV_SPLIT_K`

`comptime GEMV_SPLIT_K = GEMVAlgorithm(2)`

### `GEVM_KERNEL`

`comptime GEVM_KERNEL = GEMVAlgorithm(4)`

### `GEVM_KERNEL_VECTOR`

`comptime GEVM_KERNEL_VECTOR = GEMVAlgorithm(3)`

### `MATMUL_NAIVE`

`comptime MATMUL_NAIVE = GEMVAlgorithm(5)`

## Methods

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__is__`

`__is__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__isnot__`

`__isnot__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__str__`

`__str__(self) -> String`

Returns the string representation of this algorithm.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): String: A human-readable string representation of the algorithm.

### `write_to`

`write_to(self, mut writer: T)`

</section>

---

## gemv

<section class='mojo-docs'>

`gemv[parallelize: Bool, c_size: Dim, c_type: DType, a_shape: DimList, a_type: DType, b_size: Dim, b_type: DType, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c_buf: NDBuffer[c_type, 1, origin, c_size], a_buf: NDBuffer[a_type, 2, origin, a_shape], b_buf: NDBuffer[b_type, 1, origin, b_size])`

</section>

---

## gemv_gpu

<section class='mojo-docs'>

`gemv_gpu[transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, pdl_level: PDLLevel = PDLLevel()](c: NDBuffer[dtype, 2, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive], a: NDBuffer[dtype, 2, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive], b: NDBuffer[dtype, 2, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive], ctx: DeviceContext)`

</section>

---

## gemv_gpu_dispatch

<section class='mojo-docs'>

`gemv_gpu_dispatch[transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, pdl_level: PDLLevel = PDLLevel()](kernel_func: GEMVAlgorithm, c: NDBuffer[dtype, 2, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive], a: NDBuffer[dtype, 2, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive], b: NDBuffer[dtype, 2, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive], ctx: DeviceContext)`

</section>

---

## gemv_kernel

<section class='mojo-docs'>

`gemv_kernel[c_type: DType, a_type: DType, b_type: DType, *, transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, s_type: DType = get_accum_type[c_type](), pdl_level: PDLLevel = PDLLevel()](c: LegacyUnsafePointer[Scalar[c_type]], a: LegacyUnsafePointer[Scalar[a_type]], b: LegacyUnsafePointer[Scalar[b_type]], m: Int, n: Int, k: Int)`

</section>

---

## gemv_kernel_vector

<section class='mojo-docs'>

`gemv_kernel_vector[c_type: DType, a_type: DType, b_type: DType, c_layout: Layout, a_layout: Layout, b_layout: Layout, *, simd_width: UInt, transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, s_type: DType = get_accum_type[c_type](), pdl_level: PDLLevel = PDLLevel()](c: LayoutTensor[c_type, c_layout, MutAnyOrigin], a: LayoutTensor[a_type, a_layout, MutAnyOrigin], b: LayoutTensor[b_type, b_layout, MutAnyOrigin], m: Int, n: Int, k: Int)`

</section>

---

## gemv_split_k

<section class='mojo-docs'>

`gemv_split_k[c_type: DType, a_type: DType, b_type: DType, c_layout: Layout, a_layout: Layout, b_layout: Layout, simd_width: UInt, tile_m: UInt, tile_n: UInt, num_threads: UInt, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, s_type: DType = get_accum_type[c_type](), check_bounds: Bool = True, pdl_level: PDLLevel = PDLLevel()](output: LayoutTensor[c_type, c_layout, MutAnyOrigin], act: LayoutTensor[a_type, a_layout, MutAnyOrigin], weight: LayoutTensor[b_type, b_layout, MutAnyOrigin], m: Int, n: Int, k: Int)`

GEMV with tiling in K dimension. Assuming the B (weight) matrix is transposed i.e. row major N x K, this kernel implements a vector (1 x K) times a matrix (N x K). The impl can actually handle M > 1 but it's only optimal for tiny M. We use it for M = 1 only.

</section>

---

## gevm_kernel

<section class='mojo-docs'>

`gevm_kernel[c_type: DType, a_type: DType, b_type: DType, *, tile_size: Int, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, s_type: DType = get_accum_type[c_type](), pdl_level: PDLLevel = PDLLevel()](c: LegacyUnsafePointer[Scalar[c_type]], a: LegacyUnsafePointer[Scalar[a_type]], b: LegacyUnsafePointer[Scalar[b_type]], m: Int, n: Int, k: Int)`

</section>

---

## gemv (Gemv)

<section class='mojo-docs'>

## `comptime` values

### `logger`

`comptime logger = Logger[DEFAULT_LEVEL](stdout, "", False)`

## Structs

* [â€‹`GEMVAlgorithm`](./GEMVAlgorithm):

## Functions

* [â€‹`gemv`](./gemv):
* [â€‹`gemv_gpu`](./gemv_gpu):
* [â€‹`gemv_gpu_dispatch`](./gemv_gpu_dispatch):
* [â€‹`gemv_kernel`](./gemv_kernel):
* [â€‹`gemv_kernel_vector`](./gemv_kernel_vector):
* [â€‹`gemv_split_k`](./gemv_split_k): GEMV with tiling in K dimension. Assuming the B (weight) matrix is transposed i.e. row major N x K, this kernel implements a vector (1 x K) times a matrix (N x K). The impl can actually handle M > 1 but it's only optimal for tiny M. We use it for M = 1 only.
* [â€‹`gevm_kernel`](./gevm_kernel):
* [â€‹`log_shape`](./log_shape):
* [â€‹`naive_gemv`](./naive_gemv):
* [â€‹`reverse_idx`](./reverse_idx):

</section>

---

## log_shape

<section class='mojo-docs'>

`log_shape[has_mode_1: Bool, has_mode_2: Bool, name: String](mode_1: Int, mode_2: Int)`

</section>

---

## naive_gemv

<section class='mojo-docs'>

`naive_gemv[c_size: Dim, a_shape: DimList, b_size: Dim, dtype: DType](c_buf: NDBuffer[dtype, 1, origin, c_size], a_buf: NDBuffer[dtype, 2, origin, a_shape], b_buf: NDBuffer[dtype, 1, origin, b_size])`

</section>

---

## reverse_idx

<section class='mojo-docs'>

`reverse_idx[transpose: Bool](x: Int, y: Int) -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## dispatch_amd_matmul_by_block_shape

<section class='mojo-docs'>

`dispatch_amd_matmul_by_block_shape[c_type: DType, a_type: DType, b_type: DType, transpose_b: Bool, N: Int, K: Int, launcher_fn: fn[config: MatmulConfig[a_type, b_type, c_type, transpose_b]]() raises capturing -> None, default_block_tile_shape: IndexList[3], use_heuristic: Bool = False](M: Int, ctx: DeviceContext)`

Dispatches to the best kernel configuration based on runtime M dimension.

</section>

---

## grouped_matmul

<section class='mojo-docs'>

`grouped_matmul[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, //, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: NDBuffer[c_type, 2, MutAnyOrigin, c_shape], a: NDBuffer[a_type, 2, MutAnyOrigin, a_shape], b: NDBuffer[b_type, 3, MutAnyOrigin, b_shape], a_offsets: NDBuffer[DType.uint32, 1, MutAnyOrigin], expert_ids: NDBuffer[DType.int32, 1, MutAnyOrigin], max_num_tokens_per_expert: Int, num_active_experts: Int, ctx: DeviceContext)`

</section>

---

## grouped_matmul_amd

<section class='mojo-docs'>

`grouped_matmul_amd[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, *, transpose_b: Bool = True, block_tile_shape: IndexList[3] = Index(128, 128, 64), elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: NDBuffer[c_type, 2, MutAnyOrigin, c_shape], a: NDBuffer[a_type, 2, MutAnyOrigin, a_shape], a_offsets: NDBuffer[DType.uint32, 1, MutAnyOrigin], max_num_tokens_per_expert: Int, b: NDBuffer[b_type, 3, MutAnyOrigin, b_shape], expert_ids: NDBuffer[DType.int32, 1, MutAnyOrigin], num_active_experts: Int, ctx: DeviceContext)`

</section>

---

## grouped_matmul_amd_kernel_launcher

<section class='mojo-docs'>

`grouped_matmul_amd_kernel_launcher[c_type: DType, a_type: DType, b_type: DType, layout_c: Layout, layout_a: Layout, layout_b: Layout, transpose_b: Bool, config: MatmulConfig[a_type, b_type, c_type, transpose_b], elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c_tensor: LayoutTensor[c_type, layout_c, MutAnyOrigin], a_tensor: LayoutTensor[a_type, layout_a, MutAnyOrigin], b_tensor: LayoutTensor[b_type, layout_b, MutAnyOrigin], a_offsets: NDBuffer[DType.uint32, 1, MutAnyOrigin], expert_ids: NDBuffer[DType.int32, 1, MutAnyOrigin], num_active_experts: Int)`

</section>

---

## grouped_matmul_kernel_sm100

<section class='mojo-docs'>

`grouped_matmul_kernel_sm100[a_type: DType, b_type: DType, c_type: DType, a_layout: Layout, b_layout: Layout, a_tile_layout: Layout, b_tile_layout: Layout, c_layout: Layout, block_tile_shape: IndexList[3], mma_shape: IndexList[3], a_desc_layout: Layout, b_desc_layout: Layout, a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_NONE, transpose_b: Bool = True, num_threads: Int = 128, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](a_tma_op: TMATensorTile[a_type, a_tile_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_tile_layout, b_desc_layout], a_offsets: NDBuffer[DType.uint32, 1, MutAnyOrigin], expert_ids: NDBuffer[DType.int32, 1, MutAnyOrigin], c: LayoutTensor[c_type, c_layout, MutAnyOrigin], num_iters: Int)`

</section>

---

## grouped_matmul_sm100

<section class='mojo-docs'>

`grouped_matmul_sm100[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, //, *, transpose_b: Bool = True, mma_shape: IndexList[3] = Index(64, 128, 16), block_tile_shape: IndexList[3] = Index(64, 128, 64), elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: NDBuffer[c_type, 2, MutAnyOrigin, c_shape], a: NDBuffer[a_type, 2, MutAnyOrigin, a_shape], a_offsets: NDBuffer[DType.uint32, 1, MutAnyOrigin], max_num_tokens_per_expert: Int, b: NDBuffer[b_type, 3, MutAnyOrigin, b_shape], expert_ids: NDBuffer[DType.int32, 1, MutAnyOrigin], num_active_experts: Int, ctx: DeviceContext)`

</section>

---

## grouped_matmul_vendor

<section class='mojo-docs'>

`grouped_matmul_vendor[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, *, transpose_b: Bool = True, use_tf32: Bool = False](c: NDBuffer[c_type, 2, MutAnyOrigin, c_shape], a: NDBuffer[a_type, 2, MutAnyOrigin, a_shape], b: NDBuffer[b_type, 3, MutAnyOrigin, b_shape], a_offsets: NDBuffer[DType.uint32, 1, MutAnyOrigin], expert_ids: NDBuffer[DType.int32, 1, MutAnyOrigin], max_num_tokens_per_expert: Int, num_active_experts: Int, ctx: DeviceContext)`

</section>

---

## grouped_matmul (Grouped_matmul)

<section class='mojo-docs'>

## Functions

* [â€‹`dispatch_amd_matmul_by_block_shape`](./dispatch_amd_matmul_by_block_shape): Dispatches to the best kernel configuration based on runtime M dimension.
* [â€‹`grouped_matmul`](./grouped_matmul):
* [â€‹`grouped_matmul_amd`](./grouped_matmul_amd):
* [â€‹`grouped_matmul_amd_kernel_launcher`](./grouped_matmul_amd_kernel_launcher):
* [â€‹`grouped_matmul_kernel_sm100`](./grouped_matmul_kernel_sm100):
* [â€‹`grouped_matmul_sm100`](./grouped_matmul_sm100):
* [â€‹`grouped_matmul_vendor`](./grouped_matmul_vendor):
* [â€‹`naive_epilogue`](./naive_epilogue):
* [â€‹`naive_epilogue_kernel`](./naive_epilogue_kernel):
* [â€‹`naive_grouped_matmul`](./naive_grouped_matmul):
* [â€‹`naive_grouped_matmul_kernel`](./naive_grouped_matmul_kernel):

</section>

---

## naive_epilogue

<section class='mojo-docs'>

`naive_epilogue[c_type: DType, c_shape: DimList, *, elementwise_lambda_fn: elementwise_epilogue_type](c: NDBuffer[c_type, 2, MutAnyOrigin, c_shape], ctx: DeviceContext)`

</section>

---

## naive_epilogue_kernel

<section class='mojo-docs'>

`naive_epilogue_kernel[c_type: DType, c_shape: DimList, *, elementwise_lambda_fn: elementwise_epilogue_type](c: NDBuffer[c_type, 2, MutAnyOrigin, c_shape])`

</section>

---

## naive_grouped_matmul

<section class='mojo-docs'>

`naive_grouped_matmul[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, //, *, transpose_b: Bool = True, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: NDBuffer[c_type, 2, MutAnyOrigin, c_shape], a: NDBuffer[a_type, 2, MutAnyOrigin, a_shape], b: NDBuffer[b_type, 3, MutAnyOrigin, b_shape], a_offsets: NDBuffer[DType.uint32, 1, MutAnyOrigin], expert_ids: NDBuffer[DType.int32, 1, MutAnyOrigin], max_num_tokens_per_expert: Int, num_active_experts: Int, ctx: DeviceContext)`

</section>

---

## naive_grouped_matmul_kernel

<section class='mojo-docs'>

`naive_grouped_matmul_kernel[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, *, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: NDBuffer[c_type, 2, MutAnyOrigin, c_shape], a: NDBuffer[a_type, 2, MutAnyOrigin, a_shape], b: NDBuffer[b_type, 3, MutAnyOrigin, b_shape], a_offsets: NDBuffer[DType.uint32, 1, MutAnyOrigin], expert_ids: NDBuffer[DType.int32, 1, MutAnyOrigin])`

</section>

---

## WarpRole

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct WarpRole`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `Epilogue`

`comptime Epilogue = WarpRole(3)`

### `MainLoad`

`comptime MainLoad = WarpRole(4)`

### `Mma`

`comptime Mma = WarpRole(5)`

## Methods

### `__eq__`

`__eq__(self, other: UInt) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ge__`

`__ge__(self, other: UInt) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_main_load`

`static is_main_load() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_mma`

`static is_mma() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_epilogue`

`static is_epilogue() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## blackwell_tma_umma_warp_specialized_kernel

<section class='mojo-docs'>

`blackwell_tma_umma_warp_specialized_kernel[a_type: DType, b_type: DType, c_type: DType, expert_m: Int, a_layout: Layout, b_layout: Layout, c_layout: Layout, c_tensor_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, c_desc_layout: Layout, block_tile_shape: IndexList[3], mma_shape: IndexList[3], cluster_shape: StaticTuple[Int32, 3], num_pipeline_stages: UInt, num_accum_pipeline_stages: UInt, num_output_stages: UInt = 2, output_tile_shape: IndexList[2] = Index(128, 32), transpose_b: Bool = True, a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, cta_group: Int = 2, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, transpose_c: Bool = False](num_active_experts: Int, a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], expert_ids: NDBuffer[DType.int32, 1, MutAnyOrigin], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], b_offsets: NDBuffer[DType.uint32, 1, MutAnyOrigin], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], c: LayoutTensor[c_type, c_tensor_layout, MutAnyOrigin], mnk: StaticTuple[UInt32, 3])`

</section>

---

## consumer_main_loop

<section class='mojo-docs'>

`consumer_main_loop[accum_type: DType, c_type: DType, a_type: DType, b_type: DType, a_smem_layout: Layout, b_smem_layout: Layout, a_swizzle: TensorMapSwizzle, b_swizzle: TensorMapSwizzle, transpose_b: Bool, pipeline_stages: Int, /, *, block_tile_shape: IndexList[3], mma_shape: IndexList[3], cta_group: Int = 1, cluster_shape: IndexList[3] = Index(1, 1, 1)](tmem_addr: UInt32, a_smem_iter: LayoutTensorIter[a_type, a_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], b_smem_iter: LayoutTensorIter[b_type, b_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], mma_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], tma_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], consumer_phase: PipelineState[pipeline_stages], mma_op: MmaOpSM100_SS[c_type, a_type, b_type, block_tile_shape, mma_shape, accum_type=accum_type, cta_group=cta_group, cluster_shape=cluster_shape, a_swizzle=a_swizzle, b_swizzle=b_swizzle, transpose_b=transpose_b], elect_one_warp: Bool, iter_idx: UInt32)`

</section>

---

## grouped_matmul_sm100_persistent

<section class='mojo-docs'>

`grouped_matmul_sm100_persistent[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, transpose_b: Bool, *, config: MatmulConfig[a_type, b_type, c_type, transpose_b], cta_group: Int = 1, num_pipeline_stages: Optional[UInt] = None, a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: NDBuffer[c_type, 2, MutAnyOrigin, c_shape], a: NDBuffer[a_type, 2, MutAnyOrigin, a_shape], a_offsets: NDBuffer[DType.uint32, 1, MutAnyOrigin], max_num_tokens_per_expert: Int, b: NDBuffer[b_type, 3, MutAnyOrigin, b_shape], expert_ids: NDBuffer[DType.int32, 1, MutAnyOrigin], num_active_experts: Int, ctx: DeviceContext)`

</section>

---

## grouped_matmul_sm100 (Grouped_matmul_sm100)

<section class='mojo-docs'>

## Structs

* [â€‹`WarpRole`](./WarpRole):

## Functions

* [â€‹`blackwell_tma_umma_warp_specialized_kernel`](./blackwell_tma_umma_warp_specialized_kernel):
* [â€‹`consumer_main_loop`](./consumer_main_loop):
* [â€‹`grouped_matmul_sm100_persistent`](./grouped_matmul_sm100_persistent):
* [â€‹`load_AB`](./load_AB):
* [â€‹`multi_stage_store_C`](./multi_stage_store_C):
* [â€‹`stsm_helper`](./stsm_helper):
* [â€‹`zero_output`](./zero_output):

</section>

---

## load_AB

<section class='mojo-docs'>

`load_AB[a_type: DType, b_type: DType, a_layout: Layout, b_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, a_smem_layout: Layout, b_smem_layout: Layout, num_pipeline_stages: UInt, /, *, block_tile_shape: IndexList[3], mma_shape: IndexList[3], cta_group: Int = 1](expert_ids: NDBuffer[DType.int32, 1, MutAnyOrigin], a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], a_smem: LayoutTensorIter[a_type, a_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], b_smem: LayoutTensorIter[b_type, b_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], mma_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], tma_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], producer_phase: PipelineState[Int(num_pipeline_stages)], peer_cta_coord: Tuple[UInt, UInt, UInt], work_tile_coord: Tuple[UInt, UInt], a_multicast_mask: UInt16, b_multicast_mask: UInt16, iter_idx: UInt32, elect_one_cta: Bool, scheduler: TileScheduler[static_MN=static_MN, tile_shape=tile_shape, cluster=cluster, cta_group=cta_group, swizzle=swizzle, swapAB=swapAB])`

</section>

---

## multi_stage_store_C

<section class='mojo-docs'>

`multi_stage_store_C[c_type: DType, c_smem_layout: Layout, c_layout: Layout, c_tensor_layout: Layout, c_desc_layout: Layout, num_accum_pipeline_stages: UInt, /, *, accum_type: DType, block_tile_shape: IndexList[3], mma_shape: IndexList[3], stage_stride_cols: UInt, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, cta_group: Int = 1, num_output_warps: UInt = 4, max_tmem_cols: UInt = 512, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, transpose_c: Bool = False](c_iter: LayoutTensorIter[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], c: LayoutTensor[c_type, c_tensor_layout, MutAnyOrigin], accum_pipeline_consumer_state: PipelineState[Int(num_accum_pipeline_stages)], accum_full_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], accum_empty_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], tmem_addr: UInt32, work_tile_coord: Tuple[UInt, UInt], group_end_idx: UInt32, elect_one_warp: Bool, M: UInt32, N: UInt32)`

</section>

---

## stsm_helper

<section class='mojo-docs'>

`stsm_helper[swizzle: Swizzle, transpose_c: Bool = False](vec: SIMD[dtype, size], dst: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## zero_output

<section class='mojo-docs'>

`zero_output[c_type: DType, c_layout: Layout, *, output_tile_shape: IndexList[2]](c: LayoutTensor[c_type, c_layout, MutAnyOrigin], coord: Tuple[UInt32, UInt32], group_end_idx: UInt32)`

</section>

---

## blackwell_gmm_tma_umma_warp_specialized_blockwise_fp8_kernel

<section class='mojo-docs'>

`blackwell_gmm_tma_umma_warp_specialized_blockwise_fp8_kernel[a_type: DType, b_type: DType, c_type: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, c_tensor_layout: Layout, a_scales_tile_layout: Layout, a_scales_type: DType, a_offsets_layout: Layout, b_scales_type: DType, b_scales_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, c_desc_layout: Layout, a_scales_desc_layout: Layout, transpose_b: Bool, config: MatmulConfig[a_type, b_type, c_type, transpose_b], num_pipeline_stages: UInt, cluster_shape: StaticTuple[Int32, 3], expert_n: Int, expert_ids_layout: Layout, b_scales_n: Int](num_active_experts: Int, a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], c: LayoutTensor[c_type, c_tensor_layout, MutAnyOrigin], a_scales_tma_op: TMATensorTile[a_scales_type, a_scales_tile_layout, a_scales_desc_layout], a_offsets: LayoutTensor[DType.uint32, a_offsets_layout, MutAnyOrigin], num_iters: UInt, b_scales: LayoutTensor[b_scales_type, b_scales_layout, MutAnyOrigin], expert_ids: LayoutTensor[DType.int32, expert_ids_layout, MutAnyOrigin], problem_shape: StaticTuple[Int32, 3])`

</section>

---

## grouped_matmul_dynamic_scaled_fp8

<section class='mojo-docs'>

`grouped_matmul_dynamic_scaled_fp8[c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, a_offsets_type: DType, expert_ids_type: DType, //, input_scale_granularity: StringSlice[StaticConstantOrigin], weight_scale_granularity: StringSlice[StaticConstantOrigin], m_scale_granularity: Int, n_scale_granularity: Int, k_scale_granularity: Int, transpose_b: Bool = False, tokens_padded_per_expert: Bool = False, target: StringSlice[StaticConstantOrigin] = "cpu"](c: NDBuffer[c_type, 2, MutAnyOrigin, shape], a: NDBuffer[a_type, 2, MutAnyOrigin, shape], b: NDBuffer[b_type, 3, MutAnyOrigin, shape], a_scales: NDBuffer[a_scales_type, 2, MutAnyOrigin, shape], b_scales: NDBuffer[b_scales_type, 3, MutAnyOrigin, shape], a_offsets: NDBuffer[a_offsets_type, 1, MutAnyOrigin, shape], expert_ids: NDBuffer[expert_ids_type, 1, MutAnyOrigin, shape], max_num_tokens_per_expert: Int, num_active_experts: Int, ctx: DeviceContext)`

</section>

---

## grouped_matmul_sm100_blockwise_scaled_fp8

<section class='mojo-docs'>

`grouped_matmul_sm100_blockwise_scaled_fp8[a_layout: Layout, b_layout: Layout, c_layout: Layout, a_scales_layout: Layout, b_scales_layout: Layout, a_offsets_layout: Layout, expert_ids_layout: Layout, c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, a_offsets_type: DType, expert_ids_type: DType, transpose_b: Bool, //, *, config: MatmulConfig[a_type, b_type, c_type, transpose_b], elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, c_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[a_type, a_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[b_type, b_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_scales: LayoutTensor[a_scales_type, a_scales_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_scales: LayoutTensor[b_scales_type, b_scales_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_offsets: LayoutTensor[a_offsets_type, a_offsets_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], expert_ids: LayoutTensor[expert_ids_type, expert_ids_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], max_num_tokens_per_expert: Int, num_active_experts: Int, ctx: DeviceContext)`

</section>

---

## grouped_matmul_sm100_blockwise_scaled_fp8_persistent

<section class='mojo-docs'>

`grouped_matmul_sm100_blockwise_scaled_fp8_persistent[a_layout: Layout, b_layout: Layout, c_layout: Layout, a_scales_layout: Layout, b_scales_layout: Layout, a_offsets_layout: Layout, expert_ids_layout: Layout, c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, a_offsets_type: DType, expert_ids_type: DType, transpose_b: Bool, //, *, config: MatmulConfig[a_type, b_type, c_type, transpose_b], elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, c_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[a_type, a_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[b_type, b_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_scales: LayoutTensor[a_scales_type, a_scales_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_scales: LayoutTensor[b_scales_type, b_scales_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_offsets: LayoutTensor[a_offsets_type, a_offsets_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], expert_ids: LayoutTensor[expert_ids_type, expert_ids_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], max_num_tokens_per_expert: Int, num_active_experts: Int, ctx: DeviceContext)`

</section>

---

## grouped_matmul_sm100_blockwise_fp8

<section class='mojo-docs'>

## `comptime` values

### `logger`

`comptime logger = Logger[DEFAULT_LEVEL](stdout, "", False)`

## Functions

* [â€‹`blackwell_gmm_tma_umma_warp_specialized_blockwise_fp8_kernel`](./blackwell_gmm_tma_umma_warp_specialized_blockwise_fp8_kernel):
* [â€‹`grouped_matmul_dynamic_scaled_fp8`](./grouped_matmul_dynamic_scaled_fp8):
* [â€‹`grouped_matmul_sm100_blockwise_scaled_fp8`](./grouped_matmul_sm100_blockwise_scaled_fp8):
* [â€‹`grouped_matmul_sm100_blockwise_scaled_fp8_persistent`](./grouped_matmul_sm100_blockwise_scaled_fp8_persistent):
* [â€‹`load_AB`](./load_AB):
* [â€‹`matmul_sm100_grouped_blockwise_scaled_fp8_1d2d_kernel`](./matmul_sm100_grouped_blockwise_scaled_fp8_1d2d_kernel):
* [â€‹`multi_stage_reg_epilogue`](./multi_stage_reg_epilogue):
* [â€‹`promote_accumulators`](./promote_accumulators):

</section>

---

## load_AB (Grouped_matmul_sm100_blockwise_fp8)

<section class='mojo-docs'>

`load_AB[a_type: DType, b_type: DType, a_scales_type: DType, a_layout: Layout, b_layout: Layout, a_scales_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, a_scales_desc_layout: Layout, a_smem_layout: Layout, b_smem_layout: Layout, a_scales_smem_layout: Layout, num_pipeline_stages: UInt, expert_ids_layout: Layout, /, *, block_tile_shape: IndexList[3], mma_shape: IndexList[3], cta_group: Int = 1](a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], a_scales_tma_op: TMATensorTile[a_scales_type, a_scales_layout, a_scales_desc_layout], a_smem: LayoutTensorIter[a_type, a_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], b_smem: LayoutTensorIter[b_type, b_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], a_scales_smem: LayoutTensorIter[a_scales_type, a_scales_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], load_mma_pipeline: ProducerConsumerPipeline[Int(num_pipeline_stages)], peer_cta_coord: Tuple[UInt, UInt, UInt], work_tile_coord: Tuple[UInt, UInt], a_multicast_mask: UInt16, b_multicast_mask: UInt16, iter_idx: UInt, elect_one_cta: Bool, scheduler: TileScheduler[static_MN=static_MN, tile_shape=tile_shape, cluster=cluster, cta_group=cta_group, swizzle=swizzle, swapAB=swapAB], expert_ids: LayoutTensor[DType.int32, expert_ids_layout, MutAnyOrigin])`

</section>

---

## matmul_sm100_grouped_blockwise_scaled_fp8_1d2d_kernel

<section class='mojo-docs'>

`matmul_sm100_grouped_blockwise_scaled_fp8_1d2d_kernel[a_type: DType, b_type: DType, c_type: DType, a_scales_type: DType, b_scales_type: DType, accum_type: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, a_offsets_layout: Layout, expert_ids_layout: Layout, a_scales_layout: Layout, b_scales_layout: Layout, a_tile_layout: Layout, b_tile_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, block_tile_shape: IndexList[3], mma_shape: IndexList[3], transpose_b: Bool = True, cluster_shape: StaticTuple[Int32, 3] = StaticTuple[Int32, 3](1, 1, 1), a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, num_threads: UInt = 128, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](a_tma_op: TMATensorTile[a_type, a_tile_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_tile_layout, b_desc_layout], a_offsets: LayoutTensor[DType.uint32, a_offsets_layout, MutAnyOrigin], expert_ids: LayoutTensor[DType.int32, expert_ids_layout, MutAnyOrigin], c: LayoutTensor[c_type, c_layout, MutAnyOrigin], a_scales: LayoutTensor[a_scales_type, a_scales_layout, MutAnyOrigin], b_scales: LayoutTensor[b_scales_type, b_scales_layout, MutAnyOrigin], num_iters: UInt)`

</section>

---

## multi_stage_reg_epilogue

<section class='mojo-docs'>

`multi_stage_reg_epilogue[c_smem_layout: Layout, c_layout: Layout, c_desc_layout: Layout, accum_type: DType, accum_layout: Layout, c_tensor_layout: Layout, /, *, c_type: DType, block_tile_shape: IndexList[3], mma_shape: IndexList[3], is_lower_frag_required: Bool, cta_group: Int, num_output_warps: UInt, c_swizzle: TensorMapSwizzle](c_upper_main_tile: LayoutTensor[accum_type, accum_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_lower_main_tile: LayoutTensor[accum_type, accum_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_iter: LayoutTensorIter[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], c: LayoutTensor[c_type, c_tensor_layout, MutAnyOrigin], c_coord: Tuple[UInt, UInt], elect_one_warp: Bool, group_end_idx: UInt32)`

</section>

---

## promote_accumulators

<section class='mojo-docs'>

`promote_accumulators[pipeline_stages: UInt, num_accum_pipeline_stages: UInt, accum_type: DType, accum_layout: Layout, a_scales_type: DType, b_scales_type: DType, b_scales_layout: Layout, a_scales_smem_layout: Layout, expert_ids_layout: Layout, /, *, block_tile_shape: IndexList[3], mma_shape: IndexList[3], cta_group: Int, CLUSTER_SIZE: Int32, is_lower_frag_required: Bool, num_output_warps: UInt](b_scales: LayoutTensor[b_scales_type, b_scales_layout, MutAnyOrigin], b_scales_n: Int, a_scales_smem_iter: LayoutTensorIter[a_scales_type, a_scales_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], c_upper_main_tile: LayoutTensor[accum_type, accum_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_lower_main_tile: LayoutTensor[accum_type, accum_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mma_output_pipeline: ProducerConsumerPipeline[Int(num_accum_pipeline_stages)], tmem_addr: UInt32, load_mma_pipeline: ProducerConsumerPipeline[Int(pipeline_stages)], work_tile_coord: Tuple[UInt, UInt], elect_one_warp: Bool, stage_stride_cols: UInt, k_iter: UInt, problem_shape: StaticTuple[Int32, 3], expert_ids: LayoutTensor[DType.int32, expert_ids_layout, MutAnyOrigin], scheduler: TileScheduler[static_MN=static_MN, tile_shape=tile_shape, cluster=cluster, cta_group=cta_group, swizzle=swizzle, swapAB=swapAB])`

</section>

---

## RasterOrder

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct RasterOrder`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `AlongM`

`comptime AlongM = RasterOrder(1)`

### `AlongN`

`comptime AlongN = RasterOrder(0)`

## Methods

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## TileScheduler

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileScheduler[offsets_layout: Layout, //, *, static_MN: Int, tile_shape: IndexList[3], cluster: IndexList[3] = Index(1, 1, 1), cta_group: Int = 1, swizzle: Bool = False, swapAB: Bool = True]`

## Fields

* â€‹num\_active\_experts (`Int`):
* â€‹group\_offsets (`LayoutTensor[DType.uint32, offsets_layout, MutAnyOrigin]`):
* â€‹current\_iter (`Int32`):
* â€‹current\_group\_idx (`UInt32`):
* â€‹current\_dynamic\_dim\_cumsum (`UInt32`):
* â€‹block\_idx\_start (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `cta_group_tile_shape`

`comptime cta_group_tile_shape = Index((tile_shape.__getitem__[3, DType.int64, Int](0) * cta_group), (tile_shape.__getitem__[3, DType.int64, Int](1) * cta_group))`

### `div_dynamic_block`

`comptime div_dynamic_block = FastDiv[DType.uint32](TileScheduler[static_MN=static_MN, tile_shape=tile_shape, cluster=cluster, cta_group=cta_group, swizzle=swizzle, swapAB=swapAB].cta_group_tile_shape.__getitem__[2, DType.int64, Int](TileScheduler[static_MN=static_MN, tile_shape=tile_shape, cluster=cluster, cta_group=cta_group, swizzle=swizzle, swapAB=swapAB].dynamic_dim))`

### `dynamic_dim`

`comptime dynamic_dim = 1 if swapAB else 0`

### `kNum1DBlocksPerGroup`

`comptime kNum1DBlocksPerGroup = 16`

### `num_static_dim_blocks`

`comptime num_static_dim_blocks = SIMD[DType.uint32, 1](ceildiv(static_MN, tile_shape.__getitem__[3, DType.int64, Int](TileScheduler[static_MN=static_MN, tile_shape=tile_shape, cluster=cluster, cta_group=cta_group, swizzle=swizzle, swapAB=swapAB].static_dim)))`

### `static_dim`

`comptime static_dim = 0 if swapAB else 1`

## Methods

### `__init__`

`__init__(num_active_experts: Int, group_offsets: LayoutTensor[DType.uint32, offsets_layout, MutAnyOrigin]) -> Self`

### `fetch_next_work`

`fetch_next_work(mut self) -> WorkInfo`

**Returns:**

`WorkInfo`

</section>

---

## WorkInfo

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct WorkInfo`

## Fields

* â€‹m (`UInt32`):
* â€‹n (`UInt32`):
* â€‹is\_valid\_tile (`Bool`):
* â€‹terminate (`Bool`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__() -> Self`

### `is_valid`

`is_valid(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_done`

`is_done(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

</section>

---

## grouped_matmul_tile_scheduler

<section class='mojo-docs'>

## Structs

* [â€‹`RasterOrder`](./RasterOrder):
* [â€‹`TileScheduler`](./TileScheduler):
* [â€‹`WorkInfo`](./WorkInfo):

</section>

---

## linalg

<section class='mojo-docs'>

Provides CPU and GPU implementations of linear algebra functions.

## Packages

* [â€‹`arch`](./arch/): Provides architecture specific utility functions.
* [â€‹`matmul`](./matmul/): Provides the backend implementation for matmuls.

## Modules

* [â€‹`accumulate`](./accumulate/):
* [â€‹`bmm`](./bmm/):
* [â€‹`distributed_matmul`](./distributed_matmul/):
* [â€‹`dual_gemm`](./dual_gemm/):
* [â€‹`fp4_quantization`](./fp4_quantization/):
* [â€‹`fp4_utils`](./fp4_utils/):
* [â€‹`fp8_quantization`](./fp8_quantization/):
* [â€‹`gemv`](./gemv/):
* [â€‹`grouped_matmul`](./grouped_matmul/):
* [â€‹`grouped_matmul_sm100`](./grouped_matmul_sm100/):
* [â€‹`grouped_matmul_sm100_blockwise_fp8`](./grouped_matmul_sm100_blockwise_fp8/):
* [â€‹`grouped_matmul_tile_scheduler`](./grouped_matmul_tile_scheduler/):
* [â€‹`lora`](./lora/):
* [â€‹`matrix_band_part`](./matrix_band_part/): The module implements matrix band part functions.
* [â€‹`packing`](./packing/):
* [â€‹`qr_factorization`](./qr_factorization/):
* [â€‹`structuring`](./structuring/):
* [â€‹`transpose`](./transpose/): The module implements Transpose functions.
* [â€‹`utils`](./utils/):
* [â€‹`utils_gpu`](./utils_gpu/):

</section>

---

## lora

<section class='mojo-docs'>

## Functions

* [â€‹`shrink_qkv_permute_3mn_sm100`](./shrink_qkv_permute_3mn_sm100): LoRA shrink GMM with planar Q/K/V output on SM100.

</section>

---

## shrink_qkv_permute_3mn_sm100

<section class='mojo-docs'>

`shrink_qkv_permute_3mn_sm100[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList](c_lora: NDBuffer[c_type, 3, MutAnyOrigin, c_shape], a: NDBuffer[a_type, 2, MutAnyOrigin, a_shape], b: NDBuffer[b_type, 3, MutAnyOrigin, b_shape], a_offsets: NDBuffer[DType.uint32, 1, MutAnyOrigin], expert_ids: NDBuffer[DType.int32, 1, MutAnyOrigin], max_num_tokens_per_expert: Int, num_active_experts: Int, ctx: DeviceContext)`

LoRA shrink GMM with planar Q/K/V output on SM100.

Performs the LoRA 'shrink' grouped matmul for routed tokens:
computes `[M, K] @ [G, 3N, K]^T` per active expert, then **permutes**
the flat `[M, 3N]` result into a planar layout `[3, M, N]` (Q, K, V)
using an elementwise epilogue, while reusing the same storage.

**Constraints:**

* c\_lora must be rank 3 with static first dimension B == 3.
* a must be rank 2 with trailing dimension K that matches b\[..., K].
* b must be rank 3 with shape (G, 3N, K).
* The temporary 2D view of c\_lora is (M, 3N) in row-major order and
  **aliases the same storage** as c\_lora.
* a\_offsets is non-decreasing with a\_offsets\[0] == 0 and
  a\_offsets\[num\_active\_experts] == M.
* expert\_ids\[i] âˆˆ \[0, G) for valid experts; kernel may treat -1 as inactive.
* The epilogue assumes `N % vector_width == 0` for aligned vector stores.

**Args:**

* â€‹c\_lora ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): Output tensor with planar Q/K/V layout, shape (3, M, N).
  Backed by row-major storage, used both as a 3D view and as a
  temporary 2D view (M, 3N) during compute.
* â€‹a ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): Routed activation matrix, shape (M, K).
* â€‹b ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): Shrink weights per expert, shape (G, 3N, K).
* â€‹a\_offsets ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): Inclusive prefix sums of tokens per (active) expert,
  length (num\_experts + 1). Defines per-expert \[start, end) in A/C.
* â€‹expert\_ids ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): Expert indices for the active groups, length â‰¥ num\_active\_experts.
* â€‹max\_num\_tokens\_per\_expert ([`Int`](/mojo/std/builtin/int/Int)): Upper bound on tokens for any active expert.
* â€‹num\_active\_experts ([`Int`](/mojo/std/builtin/int/Int)): Number of experts participating in this call.
* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): DeviceContext used for enqueues and synchronization.

</section>

---

## apple_batched_matmul

<section class='mojo-docs'>

`apple_batched_matmul[*, transpose_b: Bool = False, elementwise_epilogue_fn: OptionalReg[fn[c_type: DType, width: Int, rank: Int, *, alignment: Int = 1](IndexList[rank], SIMD[c_type, width]) capturing -> None] = None](c: NDBuffer[dtype, rank, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive], a: NDBuffer[dtype, rank, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive], b: NDBuffer[dtype, rank, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive])`

</section>

---

## apple_gemv

<section class='mojo-docs'>

`apple_gemv[*, b_packed: Bool, transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: NDBuffer[dtype, 2, origin, shape], a: NDBuffer[dtype, 2, origin, shape], b: NDBuffer[dtype, 2, origin, shape])`

</section>

---

## apple_matmul

<section class='mojo-docs'>

`apple_matmul[*, transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](cblas_gemm_fn: cblas_gemm_type, c: NDBuffer[dtype, rank, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive], a: NDBuffer[dtype, rank, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive], b: NDBuffer[dtype, rank, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive])`

`apple_matmul[*, transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: NDBuffer[dtype, rank, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive], a: NDBuffer[dtype, rank, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive], b: NDBuffer[dtype, rank, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive])`

</section>

---

## get_cblas_f32_function

<section class='mojo-docs'>

`get_cblas_f32_function() -> cblas_gemm_type`

**Returns:**

[`cblas_gemm_type`](/mojo/kernels/linalg/matmul/cpu/apple_accelerate/#cblas_gemm_type)

</section>

---

## apple_accelerate

<section class='mojo-docs'>

## `comptime` values

### `APPLE_ACCELERATE`

`comptime APPLE_ACCELERATE = _Global["APPLE_ACCELERATE", _init_dylib, _on_error_msg]`

### `cblas_gemm_type`

`comptime cblas_gemm_type = fn(_CBLASOrder, _CBLASTranspose, _CBLASTranspose, Int32, Int32, Int32, Float32, UnsafePointer[Float32, ImmutAnyOrigin], Int32, UnsafePointer[Float32, ImmutAnyOrigin], Int32, Float32, UnsafePointer[Float32, MutAnyOrigin], Int32) -> None`

### `LIB_ACC_PATH`

`comptime LIB_ACC_PATH = "/System/Library/Frameworks/Accelerate.framework/Accelerate"`

## Functions

* [â€‹`apple_batched_matmul`](./apple_batched_matmul):
* [â€‹`apple_gemv`](./apple_gemv):
* [â€‹`apple_matmul`](./apple_matmul):
* [â€‹`get_cblas_f32_function`](./get_cblas_f32_function):
* [â€‹`use_apple_accelerate_lib`](./use_apple_accelerate_lib):

</section>

---

## use_apple_accelerate_lib

<section class='mojo-docs'>

`use_apple_accelerate_lib[c_type: DType, a_type: DType, b_type: DType]() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## Inner_matmul_default

<section class='mojo-docs'>

`struct Inner_matmul_default`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`InnerMatmulKernel`](/mojo/kernels/linalg/matmul/cpu/impl/InnerMatmulKernel),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__inner_matmul__`

`__inner_matmul__[kernel_rows: Int, kernel_cols: Int, simd_size: Int](self, c: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_packed: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], global_offset: GemmShape, global_bound: GemmShape, tile_n_k: IndexList[2], skip_boundary_check: Bool)`

Utility function on the inner loop. Run the inner kernel on the whole (kernel\_rows, TileN, TileK) tile.

</section>

---

## default

<section class='mojo-docs'>

## Structs

* [â€‹`Inner_matmul_default`](./Inner_matmul_default):

</section>

---

## Inner_matmul_i8mm

<section class='mojo-docs'>

`struct Inner_matmul_i8mm`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`InnerMatmulKernel`](/mojo/kernels/linalg/matmul/cpu/impl/InnerMatmulKernel),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__inner_matmul__`

`__inner_matmul__[kernel_rows: Int, kernel_cols: Int, simd_size: Int](self, c: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_packed: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], global_offset: GemmShape, global_bound: GemmShape, tile_n_k: IndexList[2], skip_boundary_check: Bool)`

Utility function on the inner loop. Run the inner kernel on the whole (kernel\_rows2, TileN, TileK) tile.

</section>

---

## LoadStore_i8mm

<section class='mojo-docs'>

`struct LoadStore_i8mm[dtype: DType, simd_size: Int, single_row: Bool, tile_rows: Int, tile_columns: Int]`

## Fields

* â€‹output\_tile (`_Accumulator[dtype, tile_rows, LoadStore_i8mm[dtype, simd_size, single_row, tile_rows, tile_columns].num_simd_cols, simd_size]`):
* â€‹skip\_boundary\_check (`Bool`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `num_simd_cols`

`comptime num_simd_cols = (tile_columns // simd_size)`

## Methods

### `__init__`

`__init__(out self, skip_boundary_check: Bool)`

</section>

---

## i8mm

<section class='mojo-docs'>

## Structs

* [â€‹`Inner_matmul_i8mm`](./Inner_matmul_i8mm):
* [â€‹`LoadStore_i8mm`](./LoadStore_i8mm):

</section>

---

## InnerMatmulKernel

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__copyinit__` is trivial.

The implementation of `__copyinit__` is considered to be trivial if:

* The struct has a compiler-generated trivial `__copyinit__` and all its fields
  have a trivial `__copyinit__` method.

In practice, it means the value can be copied by copying the bits from
one location to another without side effects.

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__moveinit__` is trivial.

The implementation of `__moveinit__` is considered to be trivial if:

* The struct has a compiler-generated `__moveinit__` and all its fields
  have a trivial `__moveinit__` method.

In practice, it means the value can be moved by moving the bits from
one location to another without side effects.

## Required methods

### `__copyinit__`

`__copyinit__(out self: _Self, existing: _Self, /)`

Create a new instance of the value by copying an existing one.

**Args:**

* â€‹existing (`_Self`): The value to copy.

**Returns:**

`_Self`

### `__moveinit__`

`__moveinit__(out self: _Self, deinit existing: _Self, /)`

Create a new instance of the value by moving the value of another.

**Args:**

* â€‹existing (`_Self`): The value to move.

**Returns:**

`_Self`

## Provided methods

### `__inner_matmul__`

`__inner_matmul__[kernel_rows: Int, kernel_cols: Int, simd_size: Int](self: _Self, c: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_packed: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], global_offset: GemmShape, global_bound: GemmShape, tile_n_k: IndexList[2], skip_boundary_check: Bool)`

### `copy`

`copy(self: _Self) -> _Self`

Explicitly construct a copy of self.

**Returns:**

`_Self`: A copy of this value.

</section>

---

## TiledMatmul

<section class='mojo-docs'>

`struct TiledMatmul[a_mut: Bool, b_mut: Bool, //, config: KernelConfig, transpose_b: Bool, b_packed: Bool, elementwise_epilogue_enabled: Bool, kernel_id: InnerKernelID, a_type: DType, a_shape: DimList, a_origin: Origin[mut=a_mut], b_type: DType, b_shape: DimList, b_origin: Origin[mut=b_mut], c_type: DType, c_shape: DimList, c_origin: MutOrigin, algorithm: InnerMatmulKernel]`

Tiled matmul implementation integrating packing, inner loop and tile partitions.

TODO: add tag based implementation dispatch.
TODO: add fusion hooks.

## Fields

* â€‹alg (`algorithm`):
* â€‹c (`NDBuffer[c_type, 2, c_origin, c_shape]`):
* â€‹a (`NDBuffer[a_type, 2, a_origin, a_shape]`):
* â€‹b (`NDBuffer[b_type, 2, b_origin, b_shape]`):
* â€‹tile\_n\_k (`IndexList[2]`):
* â€‹global\_tile\_offset (`GemmShape`):
* â€‹global\_tile\_shape (`GemmShape`):
* â€‹b\_tile\_generator (`BTileGenerator[config, a_type, b_type, c_type, b_shape, transpose_b, b_packed, b_origin]`):
* â€‹elementwise\_epilogue\_fn (`fn(GemmShape, GemmShape) escaping -> None`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = fn(GemmShape, GemmShape) escaping -> None.__copyinit__is_trivial if True if True if True if True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial else True if algorithm.__copyinit__is_trivial else algorithm.__copyinit__is_trivial`

### `__del__is_trivial`

`comptime __del__is_trivial = fn(GemmShape, GemmShape) escaping -> None.__del__is_trivial if True if True if True if True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if True if algorithm.__del__is_trivial else algorithm.__del__is_trivial else True if algorithm.__del__is_trivial else algorithm.__del__is_trivial`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = fn(GemmShape, GemmShape) escaping -> None.__moveinit__is_trivial if True if True if True if True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial else True if algorithm.__moveinit__is_trivial else algorithm.__moveinit__is_trivial`

</section>

---

## elementwise_epilogue_c_tile

<section class='mojo-docs'>

`elementwise_epilogue_c_tile[simd_width: Int, dtype: DType, origin: MutOrigin, c_shape: DimList, func: fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None](offset: GemmShape, tile_len: GemmShape, c: NDBuffer[dtype, 2, origin, c_shape])`

</section>

---

## impl

<section class='mojo-docs'>

## Structs

* [â€‹`TiledMatmul`](./TiledMatmul): Tiled matmul implementation integrating packing, inner loop and tile partitions.

## Traits

* [â€‹`InnerMatmulKernel`](./InnerMatmulKernel):

## Functions

* [â€‹`elementwise_epilogue_c_tile`](./elementwise_epilogue_c_tile):
* [â€‹`matmul`](./matmul):
* [â€‹`tiled_matmul_run`](./tiled_matmul_run): Interface function to run tiled matmul on a given sub-tile.

</section>

---

## matmul

<section class='mojo-docs'>

`matmul[*, transpose_b: Bool = False, b_packed: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, saturated_vnni: Bool = False, single_thread_blocking_override: Bool = False](c: NDBuffer[dtype, 2, origin, shape], a: NDBuffer[dtype, 2, origin, shape], b: NDBuffer[dtype, 2, origin, shape], kernel_type_m: Int, num_threads: Int = -1)`

</section>

---

## tiled_matmul_run

<section class='mojo-docs'>

`tiled_matmul_run[config: KernelConfig, transpose_b: Bool, b_packed: Bool, simd_size: Int, elementwise_epilogue_enabled: Bool, kernel_id: InnerKernelID, algorithm: InnerMatmulKernel](alg: algorithm, c: NDBuffer[dtype, 2, origin, shape], a: NDBuffer[dtype, 2, origin, shape], b: NDBuffer[dtype, 2, origin, shape], elementwise_epilogue_fn: fn(GemmShape, GemmShape) escaping -> None, global_tile_shape: GemmShape, global_tile_offset: GemmShape)`

Interface function to run tiled matmul on a given sub-tile.

**Args:**

* â€‹alg (`algorithm`): InnerMatmulKernel algorithm for microkernel.
* â€‹c ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): Pre-allocated buffer space for result.
* â€‹a ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): Operand A of the matmul.
* â€‹b ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): Operand B of the mamtul.
* â€‹elementwise\_epilogue\_fn (\[`fn(GemmShape, GemmShape) escaping -> None`]\(/mojo/kernels/linalg/matmul/cpu/impl/fn(GemmShape, GemmShape) escaping -> None)): The elementwise epilogue function.
* â€‹global\_tile\_shape ([`GemmShape`](/mojo/kernels/linalg/utils/GemmShape)): Tile shape this call will process.
* â€‹global\_tile\_offset ([`GemmShape`](/mojo/kernels/linalg/utils/GemmShape)): Tile offset on the original buffer.

</section>

---

## cpu (Cpu)

<section class='mojo-docs'>

Provides the CPU backend implementations for matmuls.

## Modules

* [â€‹`apple_accelerate`](./apple_accelerate/):
* [â€‹`default`](./default/):
* [â€‹`i8mm`](./i8mm/):
* [â€‹`impl`](./impl/):
* [â€‹`neon`](./neon/):
* [â€‹`vnni`](./vnni/):

</section>

---

## Inner_matmul_neon

<section class='mojo-docs'>

`struct Inner_matmul_neon`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`InnerMatmulKernel`](/mojo/kernels/linalg/matmul/cpu/impl/InnerMatmulKernel),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__inner_matmul__`

`__inner_matmul__[kernel_rows: Int, kernel_cols: Int, simd_size: Int](self, c: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_packed: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], global_offset: GemmShape, global_bound: GemmShape, tile_n_k: IndexList[2], skip_boundary_check: Bool)`

Utility function on the inner loop. Run the inner kernel on the whole (kernel\_rows, TileN, TileK) tile.

</section>

---

## neon

<section class='mojo-docs'>

## Structs

* [â€‹`Inner_matmul_neon`](./Inner_matmul_neon):

</section>

---

## Inner_matmul_vnni

<section class='mojo-docs'>

`struct Inner_matmul_vnni[saturated_vnni: Bool]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`InnerMatmulKernel`](/mojo/kernels/linalg/matmul/cpu/impl/InnerMatmulKernel),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__inner_matmul__`

`__inner_matmul__[kernel_rows: Int, kernel_cols: Int, simd_size: Int](self, c: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_packed: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], global_offset: GemmShape, global_bound: GemmShape, tile_n_k: IndexList[2], skip_boundary_check: Bool)`

Utility function on the inner loop. Run the inner kernel on the whole (kernel\_rows, TileN, TileK) tile.

</section>

---

## vnni

<section class='mojo-docs'>

## Structs

* [â€‹`Inner_matmul_vnni`](./Inner_matmul_vnni):

</section>

---

## amd

<section class='mojo-docs'>

Provides the AMD GPU backend implementations for matmuls.

## Modules

* [â€‹`matmul`](./matmul/):
* [â€‹`pingpong_kernel`](./pingpong_kernel/):
* [â€‹`ring_buffer`](./ring_buffer/): Ring Buffer implementation for producer-consumer synchronization in GPU kernels.
* [â€‹`ring_buffer_traits`](./ring_buffer_traits/): Trait definitions and utilities for ring buffer synchronization strategies.
* [â€‹`structured`](./structured/):
* [â€‹`warp_spec_matmul`](./warp_spec_matmul/): AMD Warp-Specialized Matrix Multiplication

</section>

---

## MMATileBuffers

<section class='mojo-docs'>

`struct MMATileBuffers[mut: Bool, dtype: DType, layout: Layout, origin: Origin[mut=mut], address_space: AddressSpace, element_layout: Layout, layout_int_type: DType, linear_idx_type: DType, masked: Bool, alignment: Int, //, _dtype: DType, /, smem_layout: Layout, reg_tile_layout: Layout, tensor_type: AnyStruct[LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]], thread_layout: Layout, warp_rows: Int, warp_cols: Int, swizzle: Swizzle]`

Manages memory for a single matrix (A or B) in GEMM computation.

This struct encapsulates all memory handling for a matrix, including:

* Shared memory allocation and tiling
* Register buffer allocation
* Data movement between memory levels (DRAMâ†’localâ†’shared)

## Fields

* â€‹smem\_tile (`MMATileBuffers[_dtype, smem_layout, reg_tile_layout, tensor_type, thread_layout, warp_rows, warp_cols, swizzle].SMemTileType`):
* â€‹smem\_warp\_tile (`LayoutTensor[_dtype, LayoutTensor._compute_tile_layout[True, _dtype, smem_layout, MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(smem_layout, AddressSpace.SHARED), _get_index_type(smem_layout, AddressSpace.SHARED), False, align_of[_dtype](), warp_rows, warp_cols]()[0], MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=_get_layout_type(smem_layout, AddressSpace.SHARED), linear_idx_type=_get_index_type(smem_layout, AddressSpace.SHARED), masked=_tile_is_masked[smem_layout, warp_rows, warp_cols]()]`):
* â€‹load\_reg\_tile (`MMATileBuffers[_dtype, smem_layout, reg_tile_layout, tensor_type, thread_layout, warp_rows, warp_cols, swizzle].MMARegTileType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `MMARegTileType`

`comptime MMARegTileType = LayoutTensor[_dtype, reg_tile_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `SMemTileType`

`comptime SMemTileType = LayoutTensor[_dtype, smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED]`

## Methods

### `__init__`

`__init__(out self, tensor: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], warp_idx: Int, warp_k_idx: Int, block_idx: Int)`

Initialize memory regions for a matrix based on warp coordinates.

**Args:**

* â€‹tensor ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The tensor to load from global memory.
* â€‹warp\_idx ([`Int`](/mojo/std/builtin/int/Int)): The warp index within the computation grid (used for MMA operations).
* â€‹warp\_k\_idx ([`Int`](/mojo/std/builtin/int/Int)): The warp index within the computation grid (used for MMA operations).
* â€‹block\_idx ([`Int`](/mojo/std/builtin/int/Int)): The block index within the computation grid (used for warp tiling).

### `copy_to_smem`

`copy_to_smem(self)`

Copy data from thread-local memory to shared memory.

Uses structured thread cooperation to efficiently transfer data.

</section>

---

## MmaOpAMD

<section class='mojo-docs'>

`struct MmaOpAMD[out_type: DType, in_type: DType, shape: IndexList[3], transpose_b: Bool, k_group_size: Int, num_k_tiles: Int, num_m_mmas: Int, num_n_mmas: Int, out_frag_size: Int, swizzle: Swizzle]`

## Fields

* â€‹out\_reg\_tile (`MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].OutRegTileType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `alignment`

`comptime alignment = align_of[SIMD[in_type, MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width]]()`

### `out_reg_layout`

`comptime out_reg_layout = Layout.row_major((num_m_mmas * num_n_mmas), out_frag_size)`

### `OutRegTileType`

`comptime OutRegTileType = LayoutTensor[out_type, MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].out_reg_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `reg_tile_layout`

`comptime reg_tile_layout[num_mmas: Int] = Layout.row_major((num_mmas * num_k_tiles), MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width)`

#### Parameters

* â€‹num\_mmas ([`Int`](/mojo/std/builtin/int/Int)):

### `RegTileType`

`comptime RegTileType[num_mmas: Int] = LayoutTensor[in_type, Layout.row_major((num_mmas * num_k_tiles), MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width), MutAnyOrigin, address_space=AddressSpace.LOCAL]`

#### Parameters

* â€‹num\_mmas ([`Int`](/mojo/std/builtin/int/Int)):

### `simd_width`

`comptime simd_width = simd_width_of[in_type]()`

### `tensor_core_mma`

`comptime tensor_core_mma = TiledTensorCore[out_type, in_type, shape, k_group_size, transpose_b]()`

## Methods

### `__init__`

`__init__(out self)`

### `a_reg_tile`

`a_reg_tile(self, k_tile_idx: Int) -> LayoutTensor[in_type, LayoutTensor._compute_tile_layout[True, in_type, Layout.row_major((num_m_mmas * num_k_tiles), MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width), MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(Layout.row_major((num_m_mmas * num_k_tiles), MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width), AddressSpace.LOCAL), _get_index_type(Layout.row_major((num_m_mmas * num_k_tiles), MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width), AddressSpace.LOCAL), False, align_of[in_type](), num_m_mmas, simd_width_of[in_type]()]()[0], MutAnyOrigin, address_space=AddressSpace.LOCAL, layout_int_type=_get_layout_type(Layout.row_major((num_m_mmas * num_k_tiles), MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width), AddressSpace.LOCAL), linear_idx_type=_get_index_type(Layout.row_major((num_m_mmas * num_k_tiles), MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width), AddressSpace.LOCAL), masked=_tile_is_masked[Layout.row_major((num_m_mmas * num_k_tiles), MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width), num_m_mmas, simd_width_of[in_type]()]()]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `b_reg_tile`

`b_reg_tile(self, k_tile_idx: Int) -> LayoutTensor[in_type, LayoutTensor._compute_tile_layout[True, in_type, Layout.row_major((num_n_mmas * num_k_tiles), MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width), MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(Layout.row_major((num_n_mmas * num_k_tiles), MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width), AddressSpace.LOCAL), _get_index_type(Layout.row_major((num_n_mmas * num_k_tiles), MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width), AddressSpace.LOCAL), False, align_of[in_type](), num_n_mmas, simd_width_of[in_type]()]()[0], MutAnyOrigin, address_space=AddressSpace.LOCAL, layout_int_type=_get_layout_type(Layout.row_major((num_n_mmas * num_k_tiles), MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width), AddressSpace.LOCAL), linear_idx_type=_get_index_type(Layout.row_major((num_n_mmas * num_k_tiles), MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width), AddressSpace.LOCAL), masked=_tile_is_masked[Layout.row_major((num_n_mmas * num_k_tiles), MmaOpAMD[out_type, in_type, shape, transpose_b, k_group_size, num_k_tiles, num_m_mmas, num_n_mmas, out_frag_size, swizzle].simd_width), num_n_mmas, simd_width_of[in_type]()]()]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `mma`

`mma[k_tile_idx: Int](self)`

### `load_tile_fragment`

`load_tile_fragment[k_tile_idx: Int](self, a_smem_tiles: LayoutTensor[_dtype, LayoutTensor._compute_tile_layout[True, _dtype, layout, MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(layout, AddressSpace.SHARED), _get_index_type(layout, AddressSpace.SHARED), False, align_of[_dtype](), warp_rows, warp_cols]()[0], MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=_get_layout_type(layout, AddressSpace.SHARED), linear_idx_type=_get_index_type(layout, AddressSpace.SHARED), masked=_tile_is_masked[layout, warp_rows, warp_cols]()], b_smem_tiles: LayoutTensor[_dtype, LayoutTensor._compute_tile_layout[True, _dtype, layout, MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(layout, AddressSpace.SHARED), _get_index_type(layout, AddressSpace.SHARED), False, align_of[_dtype](), warp_rows, warp_cols]()[0], MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=_get_layout_type(layout, AddressSpace.SHARED), linear_idx_type=_get_index_type(layout, AddressSpace.SHARED), masked=_tile_is_masked[layout, warp_rows, warp_cols]()])`

### `reset_accumulator`

`reset_accumulator(self)`

</section>

---

## gemm_kernel_amd

<section class='mojo-docs'>

`gemm_kernel_amd[c_type: DType, c_layout: Layout, a_type: DType, a_layout: Layout, b_type: DType, b_layout: Layout, transpose_b: Bool, c_layout_int_type: DType, a_layout_int_type: DType, b_layout_int_type: DType, c_linear_idx_type: DType, a_linear_idx_type: DType, b_linear_idx_type: DType, config: MatmulConfig[a_type, b_type, c_type, transpose_b], elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, c_layout, MutAnyOrigin, layout_int_type=c_layout_int_type, linear_idx_type=c_linear_idx_type], a: LayoutTensor[a_type, a_layout, MutAnyOrigin, layout_int_type=a_layout_int_type, linear_idx_type=a_linear_idx_type], b: LayoutTensor[b_type, b_layout, MutAnyOrigin, layout_int_type=b_layout_int_type, linear_idx_type=b_linear_idx_type])`

AMD-optimized GEMM kernel for matrix multiplication C = A \* B.

This kernel implements an efficient matrix multiplication algorithm optimized
for AMD GPUs, with hierarchical tiling and structured memory access patterns.

**Parameters:**

* â€‹c\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type for the output matrix C.
* â€‹c\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Memory layout for matrix C.
* â€‹a\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type for the input matrix A.
* â€‹a\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Memory layout for matrix A.
* â€‹b\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type for the input matrix B.
* â€‹b\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Memory layout for matrix B.
* â€‹transpose\_b ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether matrix B should be transposed.
* â€‹c\_layout\_int\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type for the integer part of matrix C.
* â€‹a\_layout\_int\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type for the integer part of matrix A.
* â€‹b\_layout\_int\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type for the integer part of matrix B.
* â€‹c\_linear\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type for the linear index of matrix C.
* â€‹a\_linear\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type for the linear index of matrix A.
* â€‹b\_linear\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type for the linear index of matrix B.
* â€‹config ([`MatmulConfig`](/mojo/kernels/linalg/utils_gpu/MatmulConfig)): GEMM configuration parameters (tile sizes, etc.).
* â€‹elementwise\_lambda\_fn ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional function to apply to output elements.

**Args:**

* â€‹c ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output matrix C (result).
* â€‹a ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input matrix A.
* â€‹b ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input matrix B (must be transposed).

</section>

---

## matmul (Matmul)

<section class='mojo-docs'>

## `comptime` values

### `SMemWarpTileType`

`comptime SMemWarpTileType[_dtype: DType, layout: Layout, warp_rows: Int, warp_cols: Int] = LayoutTensor[_dtype, LayoutTensor._compute_tile_layout[True, _dtype, layout, MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(layout, AddressSpace.SHARED), _get_index_type(layout, AddressSpace.SHARED), False, align_of[_dtype](), warp_rows, warp_cols]()[0], MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=_get_layout_type(layout, AddressSpace.SHARED), linear_idx_type=_get_index_type(layout, AddressSpace.SHARED), masked=_tile_is_masked[layout, warp_rows, warp_cols]()]`

Type alias for warp-level shared memory tiles with specified dimensions.

#### Parameters

* â€‹\_dtype ([`DType`](/std/builtin/dtype/DType)):
* â€‹layout ([`Layout`](/kernels/layout/layout/Layout)):
* â€‹warp\_rows ([`Int`](/std/builtin/int/Int)):
* â€‹warp\_cols ([`Int`](/std/builtin/int/Int)):

## Structs

* [â€‹`MmaOpAMD`](./MmaOpAMD):
* [â€‹`MMATileBuffers`](./MMATileBuffers): Manages memory for a single matrix (A or B) in GEMM computation.

## Functions

* [â€‹`gemm_kernel_amd`](./gemm_kernel_amd): AMD-optimized GEMM kernel for matrix multiplication C = A \* B.
* [â€‹`write_output_fragments`](./write_output_fragments): Write output fragments from registers to global memory with optional elementwise operations.

</section>

---

## write_output_fragments

<section class='mojo-docs'>

`write_output_fragments[c_type: DType, c_frag_size: Int, MMA_M: Int, MMA_N: Int, output_thread_layout: Layout, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c_reg_fragment: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_gmem_fragment: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], warp_tile_m: Int, warp_tile_n: Int, M: Int, N: Int)`

Write output fragments from registers to global memory with optional elementwise operations.

**Parameters:**

* â€‹c\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type for the output matrix C.
* â€‹c\_frag\_size ([`Int`](/mojo/std/builtin/int/Int)): Size of each output fragment.
* â€‹MMA\_M ([`Int`](/mojo/std/builtin/int/Int)): Matrix multiply instruction M dimension.
* â€‹MMA\_N ([`Int`](/mojo/std/builtin/int/Int)): Matrix multiply instruction N dimension.
* â€‹output\_thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Thread layout for output operations.
* â€‹elementwise\_lambda\_fn ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional elementwise operation to apply.

**Args:**

* â€‹c\_reg\_fragment ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Register fragments containing computation results.
* â€‹c\_gmem\_fragment ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Global memory fragment for output.
* â€‹warp\_tile\_m ([`Int`](/mojo/std/builtin/int/Int)): M coordinate of the warp tile.
* â€‹warp\_tile\_n ([`Int`](/mojo/std/builtin/int/Int)): N coordinate of the warp tile.
* â€‹M ([`Int`](/mojo/std/builtin/int/Int)): Total M dimension of the output matrix.
* â€‹N ([`Int`](/mojo/std/builtin/int/Int)): Total N dimension of the output matrix.

</section>

---

## AMDPingPongMatmul

<section class='mojo-docs'>

`struct AMDPingPongMatmul[a_type: DType, b_type: DType, c_type: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, config: KernelConfig, /, enable_swizzle: Bool]`

8-warp ping-pong matmul for AMD MI355X.

Warps are split into 2 groups of 4, alternating between load and compute
phases for overlapped execution. Uses double-buffered LDS with swizzled
access patterns to avoid bank conflicts.

Key features:

* load\_to\_lds for direct DRAMâ†’LDS transfer (bypasses L1/L2)
* Swizzle pattern for bank-conflict-free LDS access
* Fine-grained lgkmcnt/vmcnt waits for maximum overlap

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `accum_dtype`

`comptime accum_dtype = get_accum_type[c_type]()`

### `accum_width`

`comptime accum_width = ((AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].MMA_M * AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].MMA_N) // WARP_SIZE)`

### `BK`

`comptime BK = config.block_shape.__getitem__[3, DType.int64, Int](2)`

### `BM`

`comptime BM = config.block_shape.__getitem__[3, DType.int64, Int](0)`

### `BN`

`comptime BN = config.block_shape.__getitem__[3, DType.int64, Int](1)`

### `LGKM_PER_LOAD_A`

`comptime LGKM_PER_LOAD_A = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].quadrant_m_mmas * AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].num_k_mmas)`

### `LGKM_PER_LOAD_AB`

`comptime LGKM_PER_LOAD_AB = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].LGKM_PER_LOAD_A + AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].LGKM_PER_LOAD_B)`

### `LGKM_PER_LOAD_B`

`comptime LGKM_PER_LOAD_B = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].quadrant_n_mmas * AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].num_k_mmas)`

### `load_width`

`comptime load_width = simd_width_of[a_type]()`

### `loading_threads_4warp`

`comptime loading_threads_4warp = (4 * WARP_SIZE)`

### `loading_threads_8warp`

`comptime loading_threads_8warp = (8 * WARP_SIZE)`

### `loads_per_row`

`comptime loads_per_row = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].BK // AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].load_width)`

### `MMA_K`

`comptime MMA_K = config.mma_shape.__getitem__[3, DType.int64, Int](2)`

### `MMA_M`

`comptime MMA_M = config.mma_shape.__getitem__[3, DType.int64, Int](0)`

### `MMA_N`

`comptime MMA_N = config.mma_shape.__getitem__[3, DType.int64, Int](1)`

### `num_accums`

`comptime num_accums = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].num_m_mmas * AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].num_n_mmas)`

### `num_k_mmas`

`comptime num_k_mmas = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].WK // AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].MMA_K)`

### `num_m_mmas`

`comptime num_m_mmas = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].WM // AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].MMA_M)`

### `num_n_mmas`

`comptime num_n_mmas = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].WN // AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].MMA_N)`

### `num_warps_m`

`comptime num_warps_m = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].BM // AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].WM)`

### `num_warps_n`

`comptime num_warps_n = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].BN // AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].WN)`

### `ping_pong_stages`

`comptime ping_pong_stages = 2`

### `quadrant_m_mmas`

`comptime quadrant_m_mmas = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].num_m_mmas // 2)`

### `quadrant_n_mmas`

`comptime quadrant_n_mmas = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].num_n_mmas // 2)`

### `rows_per_iter_4warp`

`comptime rows_per_iter_4warp = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].loading_threads_4warp // AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].loads_per_row)`

### `rows_per_iter_8warp`

`comptime rows_per_iter_8warp = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].loading_threads_8warp // AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].loads_per_row)`

### `total_smem_a`

`comptime total_smem_a = ((2 * AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].BM) * AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].BK)`

### `total_smem_b`

`comptime total_smem_b = ((2 * AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].BN) * AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].BK)`

### `total_warps`

`comptime total_warps = (AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].num_warps_m * AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].num_warps_n)`

### `VMCNT_PER_LOAD_A`

`comptime VMCNT_PER_LOAD_A = ((AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].BM // 2) // AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].rows_per_iter_8warp)`

### `VMCNT_PER_LOAD_A_4WARP`

`comptime VMCNT_PER_LOAD_A_4WARP = ((AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].BM // 2) // AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].rows_per_iter_4warp)`

### `VMCNT_PER_LOAD_B`

`comptime VMCNT_PER_LOAD_B = ((AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].BN // 2) // AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].rows_per_iter_8warp)`

### `VMCNT_PER_LOAD_B_4WARP`

`comptime VMCNT_PER_LOAD_B_4WARP = ((AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].BN // 2) // AMDPingPongMatmul[a_type, b_type, c_type, a_layout, b_layout, c_layout, config, enable_swizzle].rows_per_iter_4warp)`

### `WK`

`comptime WK = config.warp_shape.__getitem__[3, DType.int64, Int](2)`

### `WM`

`comptime WM = config.warp_shape.__getitem__[3, DType.int64, Int](0)`

### `WN`

`comptime WN = config.warp_shape.__getitem__[3, DType.int64, Int](1)`

## Methods

### `validate_config`

`static validate_config()`

Validate the kernel configuration.

### `matmul_ping_pong`

`static matmul_ping_pong(a: LayoutTensor[a_type, a_layout, MutAnyOrigin], b: LayoutTensor[b_type, b_layout, MutAnyOrigin], c: LayoutTensor[c_type, c_layout, MutAnyOrigin])`

</section>

---

## KernelConfig

<section class='mojo-docs'>

`struct KernelConfig`

## Fields

* â€‹block\_shape (`IndexList[3]`):
* â€‹warp\_shape (`IndexList[3]`):
* â€‹mma\_shape (`IndexList[3]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(out self, *, block_shape: IndexList[3], warp_shape: IndexList[3], mma_shape: IndexList[3])`

### `num_threads`

`num_threads(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `write_to`

`write_to(self, mut writer: T)`

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `__repr__`

`__repr__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

</section>

---

## MmaOp

<section class='mojo-docs'>

`struct MmaOp[in_type: DType, accum_type: DType, WM: Int, WN: Int, BK: Int, MMA_M: Int, MMA_N: Int, MMA_K: Int, alignment: Int, swizzle: OptionalReg[Swizzle]]`

Encapsulates MMA register tiles and operations for matrix multiplication.

This struct manages register tiles and MMA operations for a single warp.
It processes warp-sized tiles (WM Ã— BK for A, WN Ã— BK for B) without
knowledge of the broader kernel architecture.

MmaOp accepts generic SMemTileType and validates compatibility at
compile-time via load\_lds\_fragment constraints.

Note: Several values are derived from other parameters:

* num\_m\_mmas = WM // MMA\_M
* num\_n\_mmas = WN // MMA\_N
* num\_k\_mmas = BK // MMA\_K
* load\_width = simd\_width\_of[in\_type]() (SIMD width for input type)
* accum\_width = (MMA\_M \* MMA\_N) // WARP\_SIZE (elements per thread)

Quadrant Processing:
The warp tile is divided into 4 quadrants for MMA scheduling:

* quadrant\_m\_mmas = num\_m\_mmas // 2 (M-dimension quadrant size)
* quadrant\_n\_mmas = num\_n\_mmas // 2 (N-dimension quadrant size)
  This enables efficient interleaving of loads and computes.

Thread Layout for MMA:
AMD's expected pattern: 64 threads â†’ 4 rows Ã— 16 cols (row-major)
Lane offset computed on-the-fly via lane\_id()

Swizzle Configuration:
MmaOp receives the swizzle pattern from the kernel/TileBuffers, since it's
determined by how data is loaded into LDS. MmaOp must read using
the same swizzle pattern that was used for writing.

* BF16: Swizzle(1, 5, 4) - 1 bit XOR
* FP8 16Ã—128: Swizzle(3, 4, 4) - 3 bit XOR (HipKittens st\_16x128)

## Fields

* â€‹a\_reg\_tile (`MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].ARegTileType`):
* â€‹b\_reg\_tile (`MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].BRegTileType`):
* â€‹out\_reg\_tile (`MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].OutRegTileType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `accum_width`

`comptime accum_width = ((MMA_M * MMA_N) // WARP_SIZE)`

### `ARegTileType`

`comptime ARegTileType = LayoutTensor[in_type, Layout.row_major(MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].num_m_mmas, (MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].num_k_mmas * MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].mma_frag_width)), MutAnyOrigin, address_space=AddressSpace.LOCAL, alignment=alignment]`

### `BRegTileType`

`comptime BRegTileType = LayoutTensor[in_type, Layout.row_major(MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].num_n_mmas, (MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].num_k_mmas * MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].mma_frag_width)), MutAnyOrigin, address_space=AddressSpace.LOCAL, alignment=alignment]`

### `bytes_per_frag`

`comptime bytes_per_frag = (MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].lds_frag_width * size_of[in_type]())`

### `col_groups`

`comptime col_groups = (WARP_SIZE // MMA_M)`

### `ds_reads_per_frag`

`comptime ds_reads_per_frag = ceildiv(MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].bytes_per_frag, 16)`

### `elem_swizzle`

`comptime elem_swizzle = swizzle`

### `k_loads_per_mma`

`comptime k_loads_per_mma = (MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].mma_frag_width // MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].lds_frag_width)`

### `lds_frag_width`

`comptime lds_frag_width = 16 if (eq MMA_K._mlir_value, 128) if (eq MMA_M._mlir_value, 16) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 75) else (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 75) else (eq MMA_M._mlir_value, 16) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 75) else (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 75) else MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].mma_frag_width`

### `lgkm_per_load_a`

`comptime lgkm_per_load_a = (((MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].quadrant_m_mmas * MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].num_k_mmas) * MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].k_loads_per_mma) * MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].ds_reads_per_frag)`

### `lgkm_per_load_ab`

`comptime lgkm_per_load_ab = (MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].lgkm_per_load_a + MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].lgkm_per_load_b)`

### `lgkm_per_load_b`

`comptime lgkm_per_load_b = (((MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].quadrant_n_mmas * MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].num_k_mmas) * MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].k_loads_per_mma) * MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].ds_reads_per_frag)`

### `load_width`

`comptime load_width = simd_width_of[in_type]()`

### `mma_access_layout`

`comptime mma_access_layout = Layout(IntTuple(MMA_M, MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].col_groups), IntTuple(MMA_K, MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].lds_frag_width))`

### `mma_frag_width`

`comptime mma_frag_width = ((MMA_M * MMA_K) // WARP_SIZE)`

### `num_k_mmas`

`comptime num_k_mmas = (BK // MMA_K)`

### `num_m_mmas`

`comptime num_m_mmas = (WM // MMA_M)`

### `num_n_mmas`

`comptime num_n_mmas = (WN // MMA_N)`

### `out_reg_layout`

`comptime out_reg_layout = Layout.row_major(MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].num_m_mmas, (MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].num_n_mmas * MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].accum_width))`

### `OutRegTileType`

`comptime OutRegTileType = LayoutTensor[accum_type, MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].out_reg_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, alignment=alignment]`

### `quadrant_m_mmas`

`comptime quadrant_m_mmas = (MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].num_m_mmas // 2)`

### `quadrant_m_size`

`comptime quadrant_m_size = MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].quadrant_m_mmas`

### `quadrant_n_mmas`

`comptime quadrant_n_mmas = (MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].num_n_mmas // 2)`

### `quadrant_n_size`

`comptime quadrant_n_size = (MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].quadrant_n_mmas * MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].accum_width)`

### `RegTileType`

`comptime RegTileType[num_mmas: Int] = LayoutTensor[in_type, Layout.row_major(num_mmas, (MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].num_k_mmas * MmaOp[in_type, accum_type, WM, WN, BK, MMA_M, MMA_N, MMA_K, alignment, swizzle].mma_frag_width)), MutAnyOrigin, address_space=AddressSpace.LOCAL, alignment=alignment]`

#### Parameters

* â€‹num\_mmas ([`Int`](/mojo/std/builtin/int/Int)):

### `use_fp8_16x16x128_mma`

`comptime use_fp8_16x16x128_mma = (MMA_K == 128) if (eq MMA_M._mlir_value, 16) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 75) else (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 75) else (MMA_M == 16) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 75) else (in_type == DType.float8_e4m3fn)`

### `use_fp8_32x32x64_mma`

`comptime use_fp8_32x32x64_mma = (MMA_K == 64) if (eq MMA_M._mlir_value, 32) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 75) else (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 75) else (MMA_M == 32) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 75) else (in_type == DType.float8_e4m3fn)`

## Methods

### `__init__`

`__init__(out self)`

Initialize MMA operation with register tiles.

### `reset_accumulator`

`reset_accumulator(self)`

Reset output register tile to zero.

### `load_a`

`load_a[which: Int](self, smem_tile: LayoutTensor[in_type, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Load A\[which] from LDS â†’ registers.

Accepts SMemTileType with matching dtype - layout compatibility validated
at compile-time via load\_lds\_fragment constraints.

For FP8 16Ã—16Ã—128: Uses lds\_frag\_width=16 with 2 K-iterations per MMA.
For FP8 32Ã—32Ã—64:  Uses lds\_frag\_width=32 with single load.
For BF16 16Ã—16Ã—32: Uses lds\_frag\_width=8.

### `load_b`

`load_b[which: Int](self, smem_tile: LayoutTensor[in_type, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Load B\[which] from LDS â†’ registers.

Accepts SMemTileType with matching dtype - layout compatibility validated
at compile-time via load\_lds\_fragment constraints.

For FP8 16Ã—16Ã—128: Uses lds\_frag\_width=16 with 2 K-iterations per MMA.
For FP8 32Ã—32Ã—64:  Uses lds\_frag\_width=32 with single load.
For BF16 16Ã—16Ã—32: Uses lds\_frag\_width=8.

### `mma`

`mma[which_a: Int, which_b: Int](self)`

Execute MMA operations for a quadrant of the output tile.

Accesses quadrant via .tile\[] view into the contiguous out\_reg\_tile.
Uses mma\_frag\_width for fragment sizing (4 for BF16, 8 for FP8).

Works for both BF16 and FP8 via stdlib mma() dispatch.

**Parameters:**

* â€‹which\_a ([`Int`](/mojo/std/builtin/int/Int)): A quadrant index (0 or 1).
* â€‹which\_b ([`Int`](/mojo/std/builtin/int/Int)): B quadrant index (0 or 1).

</section>

---

## TileBuffers

<section class='mojo-docs'>

`struct TileBuffers[in_type: DType, a_layout: Layout, b_layout: Layout, //, BM: Int, BN: Int, BK: Int, WM: Int, WN: Int, MMA_K: Int, num_threads: Int, alignment: Int, enable_swizzle: Bool, load_width: Int, loading_warps: Int = 8]`

Double-buffered LDS tiles and TileLoaders for ping-pong matmul.

a\_layout and b\_layout are infer-only parameters (note `//`), automatically
extracted from the input tensors passed to **init**. K is derived as an
comptime from a\_layout.shape\[1].

## Fields

* â€‹a\_mma\_tiles (`Tuple[TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].AMmaTilePair, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].AMmaTilePair]`):
* â€‹b\_mma\_tiles (`Tuple[TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].BMmaTilePair, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].BMmaTilePair]`):
* â€‹a\_load\_tiles (`Tuple[TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].AHalfTilePair, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].AHalfTilePair]`):
* â€‹b\_load\_tiles (`Tuple[TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].BHalfTilePair, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].BHalfTilePair]`):
* â€‹loader\_a (`TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].ATileLoader`):
* â€‹loader\_b (`TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].BTileLoader`):
* â€‹warp\_id\_m (`Int`):
* â€‹warp\_shift\_rows (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = False`

### `AHalfTile`

`comptime AHalfTile = LayoutTensor[in_type, Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BM, BK), MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=alignment]`

### `AHalfTilePair`

`comptime AHalfTilePair = Tuple[TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].AHalfTile, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].AHalfTile]`

### `AMmaTile`

`comptime AMmaTile = LayoutTensor[in_type, LayoutTensor._compute_tile_layout[True, in_type, Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BM, BK), MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BM, BK), AddressSpace.SHARED), _get_index_type(Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BM, BK), AddressSpace.SHARED), False, alignment, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].mma_tile_m, BK]()[0], MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=_get_layout_type(Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BM, BK), AddressSpace.SHARED), linear_idx_type=_get_index_type(Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BM, BK), AddressSpace.SHARED), masked=_tile_is_masked[Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BM, BK), TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].mma_tile_m, BK](), alignment=alignment]`

### `AMmaTilePair`

`comptime AMmaTilePair = Tuple[TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].AMmaTile, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].AMmaTile]`

### `ATileLoader`

`comptime ATileLoader = TileLoaderLDS[in_type, a_layout, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_tile_layout, loading_warps, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].byte_swizzle, load_width, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].use_fp8_row_major]`

### `BHalfTile`

`comptime BHalfTile = LayoutTensor[in_type, Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BN, BK), MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=alignment]`

### `BHalfTilePair`

`comptime BHalfTilePair = Tuple[TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].BHalfTile, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].BHalfTile]`

### `BMmaTile`

`comptime BMmaTile = LayoutTensor[in_type, LayoutTensor._compute_tile_layout[True, in_type, LayoutTensor._compute_tile_layout[True, in_type, Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BN, BK), MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BN, BK), AddressSpace.SHARED), _get_index_type(Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BN, BK), AddressSpace.SHARED), False, alignment, WN, BK]()[0], MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BN, BK), AddressSpace.SHARED), _get_index_type(Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BN, BK), AddressSpace.SHARED), _tile_is_masked[Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BN, BK), WN, BK](), alignment, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].mma_tile_n, BK]()[0], MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=_get_layout_type(Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BN, BK), AddressSpace.SHARED), linear_idx_type=_get_index_type(Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BN, BK), AddressSpace.SHARED), masked=_tile_is_masked[Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BN, BK), WN, BK]() if _tile_is_masked[Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BN, BK), WN, BK]() else _tile_is_masked[LayoutTensor._compute_tile_layout[True, in_type, Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BN, BK), MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BN, BK), AddressSpace.SHARED), _get_index_type(Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BN, BK), AddressSpace.SHARED), False, alignment, WN, BK]()[0], TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].mma_tile_n, BK](), alignment=alignment]`

### `BMmaTilePair`

`comptime BMmaTilePair = Tuple[TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].BMmaTile, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].BMmaTile]`

### `BTileLoader`

`comptime BTileLoader = TileLoaderLDS[in_type, b_layout, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_tile_layout, loading_warps, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].byte_swizzle, load_width, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].use_fp8_row_major]`

### `byte_swizzle`

`comptime byte_swizzle = OptionalReg[Swizzle](Swizzle(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].swizzle_log_tile, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].swizzle_base, 4)) if enable_swizzle else OptionalReg[Swizzle]()`

### `elem_size`

`comptime elem_size = size_of[in_type]()`

### `elements_per_warp`

`comptime elements_per_warp = (WARP_SIZE * load_width)`

### `frag_bytes`

`comptime frag_bytes = (TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].lds_frag_width * size_of[in_type]())`

### `half_BM`

`comptime half_BM = (BM // 2)`

### `half_BN`

`comptime half_BN = (BN // 2)`

### `half_tile_layout`

`comptime half_tile_layout = Layout.row_major(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_BM, BK)`

### `HalfTile`

`comptime HalfTile[rows: Int] = LayoutTensor[in_type, Layout.row_major(rows, BK), MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=alignment]`

#### Parameters

* â€‹rows ([`Int`](/mojo/std/builtin/int/Int)):

### `K`

`comptime K = a_layout.shape[1].value()`

### `lds_frag_width`

`comptime lds_frag_width = 16 if (eq MMA_K._mlir_value, 128) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 75) else (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 75) else TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].mma_frag_width`

### `loading_threads`

`comptime loading_threads = (loading_warps * WARP_SIZE)`

### `loads_per_row`

`comptime loads_per_row = (BK // load_width)`

### `mma_frag_width`

`comptime mma_frag_width = ((16 * MMA_K) // WARP_SIZE)`

### `mma_tile_m`

`comptime mma_tile_m = (WM // 2)`

### `mma_tile_n`

`comptime mma_tile_n = (WN // 2)`

### `rows_per_iter_4warp`

`comptime rows_per_iter_4warp = ((4 * WARP_SIZE) // TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].loads_per_row)`

### `rows_per_load_iteration`

`comptime rows_per_load_iteration = (TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].loading_threads // TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].loads_per_row)`

### `rows_per_warp`

`comptime rows_per_warp = (TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].elements_per_warp // BK)`

### `smem_ptr`

`comptime smem_ptr = LegacyUnsafePointer[Scalar[in_type], address_space=AddressSpace.SHARED]`

### `SMemTile`

`comptime SMemTile[rows: Int, cols: Int] = LayoutTensor[in_type, Layout.row_major(rows, cols), MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=alignment]`

#### Parameters

* â€‹rows ([`Int`](/mojo/std/builtin/int/Int)):
* â€‹cols ([`Int`](/mojo/std/builtin/int/Int)):

### `swizzle_base`

`comptime swizzle_base = log2_floor(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].frag_bytes) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 75) else (log2_floor((TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].swizzle_subtile_cols // 2)) + log2_floor(TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].elem_size))`

### `swizzle_log_tile`

`comptime swizzle_log_tile = (log2_floor((MMA_K // 32)) + 1)`

### `swizzle_shift`

`comptime swizzle_shift = 4`

### `swizzle_subtile_cols`

`comptime swizzle_subtile_cols = (4 * load_width)`

### `TileLoader`

`comptime TileLoader[src_layout: Layout] = TileLoaderLDS[in_type, src_layout, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].half_tile_layout, loading_warps, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].byte_swizzle, load_width, TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].use_fp8_row_major]`

#### Parameters

* â€‹src\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)):

### `total_warps`

`comptime total_warps = 8`

### `use_fp8_row_major`

`comptime use_fp8_row_major = (in_type == DType.float8_e4m3fn)`

### `use_split_k`

`comptime use_split_k = (MMA_K == 128) if (eq #pop.dtype_to_ui8<#lit.struct.extract<:!lit.struct<@std::@builtin::@dtype::@DType> in_type, "_mlir_value">>, 75) else (in_type == DType.float8_e4m3fn)`

### `vmcnt_per_load_a`

`comptime vmcnt_per_load_a = ((BM // 2) // TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].rows_per_load_iteration)`

### `vmcnt_per_load_a_4warp`

`comptime vmcnt_per_load_a_4warp = ((BM // 2) // TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].rows_per_iter_4warp)`

### `vmcnt_per_load_ab`

`comptime vmcnt_per_load_ab = (TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].vmcnt_per_load_a + TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].vmcnt_per_load_b)`

### `vmcnt_per_load_b`

`comptime vmcnt_per_load_b = ((BN // 2) // TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].rows_per_load_iteration)`

### `vmcnt_per_load_b_4warp`

`comptime vmcnt_per_load_b_4warp = ((BN // 2) // TileBuffers[BM, BN, BK, WM, WN, MMA_K, num_threads, alignment, enable_swizzle, load_width, loading_warps].rows_per_iter_4warp)`

## Methods

### `__init__`

`__init__(out self, a: LayoutTensor[in_type, a_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[dtype, b_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], block_row: Int, block_col: Int, warp_id: Int, warp_id_m: Int, warp_id_n: Int, lane_id: Int)`

Initialize LDS tiles and loaders. Layouts inferred from a and b tensors.

### `load_a`

`load_a[stage: Int, which: Int, *, k: Int](self)`

Load A\[stage]\[which] from global to LDS using all 8 warps.

### `load_b`

`load_b[stage: Int, which: Int, *, k: Int](self)`

Load B\[stage]\[which] from global to LDS using all 8 warps.

### `load_a_as_group`

`load_a_as_group[stage: Int, target_group: Int, *, k: Int](self, caller_group: Int)`

Load A\[stage]\[target\_group] from global to LDS using 4 warps.

### `load_b_as_group`

`load_b_as_group[stage: Int, which: Int, *, k: Int](self, caller_group: Int, loading_group: Int)`

Load B\[stage]\[which] from global to LDS using 4 warps.

</section>

---

## TileLoaderLDS

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileLoaderLDS[dtype: DType, src_layout: Layout, src_tile_layout: Layout, num_loading_warps: Int, swizzle: OptionalReg[Swizzle] = OptionalReg[Swizzle](), load_width: Int = simd_width_of[dtype](), use_full_tile_width: Bool = False]`

Cooperative globalâ†’LDS tile loader with swizzle support.

Loads tiles from global memory to LDS using AMDBufferResource which provides
automatic out-of-bounds clamping to zero - critical for partial block support.

Loading Modes (controlled by use\_full\_tile\_width):

* False (default): Interleaved layout. Each warp handles 32-col subtile.
  Used for BF16 where MMA\_K (32) < BK (64).
* True: Row-major layout. Each source row maps 1:1 to LDS row.
  Used for FP8 where MMA\_K == BK, enabling correct partial block handling.

## Fields

* â€‹buffer (`AMDBufferResource`):
* â€‹thread\_row (`Int`):
* â€‹thread\_col (`Int`):
* â€‹warp\_id (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `elements_per_warp`

`comptime elements_per_warp = (TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].threads_per_warp * load_width)`

### `loading_threads`

`comptime loading_threads = (num_loading_warps * TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].threads_per_warp)`

### `loads_per_row`

`comptime loads_per_row = (TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].tile_cols // load_width)`

### `num_iterations`

`comptime num_iterations = (TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].tile_rows // TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].rows_per_iteration)`

### `num_warp_cols`

`comptime num_warp_cols = (TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].tile_cols // TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].subtile_cols)`

### `num_warp_rows`

`comptime num_warp_rows = (num_loading_warps // TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].num_warp_cols)`

### `rows_per_iteration`

`comptime rows_per_iteration = (TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].loading_threads // TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].loads_per_row)`

### `rows_per_warp`

`comptime rows_per_warp = (TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].elements_per_warp // TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].tile_cols)`

### `stride`

`comptime stride = src_layout.shape[1].value()`

### `subtile_cols`

`comptime subtile_cols = TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].tile_cols if use_full_tile_width else 32`

### `thread_rows`

`comptime thread_rows = (WARP_SIZE // TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].threads_per_row)`

### `threads_per_row`

`comptime threads_per_row = (TileLoaderLDS[dtype, src_layout, src_tile_layout, num_loading_warps, swizzle, load_width, use_full_tile_width].subtile_cols // load_width)`

### `threads_per_warp`

`comptime threads_per_warp = WARP_SIZE`

### `tile_cols`

`comptime tile_cols = src_tile_layout.shape[1].value()`

### `tile_rows`

`comptime tile_rows = src_tile_layout.shape[0].value()`

## Methods

### `__init__`

`__init__(src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], warp_id: Int, lane_id: Int) -> Self`

Pre-compute thread position with swizzle inversion for bank-conflict-free reads.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Source tensor (tile view for BF16, full tensor for FP8).
* â€‹warp\_id ([`Int`](/mojo/std/builtin/int/Int)): Warp ID within the block.
* â€‹lane\_id ([`Int`](/mojo/std/builtin/int/Int)): Lane ID within the warp.

### `load_tile`

`load_tile[dst_layout: Layout, //](self, dst: LayoutTensor[dtype, dst_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src_row: Int, src_col: Int)`

Load a tile from source coordinates to LDS.

Combines pre-computed thread position with source coordinates.
Uses the buffer resource stored at init time.
Only warps 0 to num\_loading\_warps-1 participate; others return immediately.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination LDS tile.
* â€‹src\_row ([`Int`](/mojo/std/builtin/int/Int)): Starting row in source tensor.
* â€‹src\_col ([`Int`](/mojo/std/builtin/int/Int)): Starting column in source tensor (typically k\_offset).

</section>

---

## pingpong_kernel

<section class='mojo-docs'>

## Structs

* [â€‹`AMDPingPongMatmul`](./AMDPingPongMatmul): 8-warp ping-pong matmul for AMD MI355X.
* [â€‹`KernelConfig`](./KernelConfig):
* [â€‹`MmaOp`](./MmaOp): Encapsulates MMA register tiles and operations for matrix multiplication.
* [â€‹`TileBuffers`](./TileBuffers): Double-buffered LDS tiles and TileLoaders for ping-pong matmul.
* [â€‹`TileLoaderLDS`](./TileLoaderLDS): Cooperative globalâ†’LDS tile loader with swizzle support.

## Functions

* [â€‹`load_lds_fragment`](./load_lds_fragment): Load LDS â†’ registers with MMA access pattern.
* [â€‹`make_mma_swizzle`](./make_mma_swizzle): Create swizzle pattern for MMA LDS access.
* [â€‹`ping_pong_matmul`](./ping_pong_matmul):

</section>

---

## load_lds_fragment

<section class='mojo-docs'>

`load_lds_fragment[dtype: DType, smem_layout: Layout, smem_element_layout: Layout, frag_layout: Layout, frag_element_layout: Layout, //, mma_access_layout: Layout, swizzle: OptionalReg[Swizzle] = OptionalReg[Swizzle]()](smem_tile: LayoutTensor[dtype, smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=smem_element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], reg_frag: LayoutTensor[dtype, frag_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=frag_element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Load LDS â†’ registers with MMA access pattern.

Why mma\_access\_layout differs from the globalâ†’LDS thread layout:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layout          â”‚ Purpose              â”‚ Constraint                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ load\_thread     â”‚ Global â†’ LDS write   â”‚ Coalesced global reads     â”‚
â”‚ mma\_access      â”‚ LDS â†’ Registers read â”‚ AMD WMMA hardware pattern  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

mma\_access\_layout encodes how AMD's WMMA instruction expects data:

* Lane decomposition: (lane % 16, lane // 16) = (col\_group, row\_group)
* Offset computation: col\_group \* 32 + row\_group \* 8

Using RuntimeLayout ensures compile-time evaluation (no GPU heap alloc).

Layout compatibility requirements:

* mma\_access\_layout must map exactly WARP\_SIZE (64) threads
* smem must have enough elements for: num\_iterations \* WARP\_SIZE \* frag\_width
* frag must store: num\_iterations \* frag\_width elements

</section>

---

## make_mma_swizzle

<section class='mojo-docs'>

`make_mma_swizzle[dtype: DType, MMA_M: Int, MMA_K: Int]() -> Swizzle`

Create swizzle pattern for MMA LDS access.

AMD MI355X have 64 LDS banks Ã— 4 bytes each. Without swizzling,
the MMA thread access pattern causes 4-way bank conflicts. The swizzle
XORs high-order address bits into the bank selection bits to distribute
accesses across banks.

Swizzle parameters:

* log\_tile: Number of bits to XOR, scales with MMA\_K
* base: Log2 of read granularity in bytes (lds\_frag\_width \* elem\_size)
* shift: Fixed at 4 for AMD LDS bank geometry

Configuration examples:
BF16 16Ã—16Ã—32:  lds\_frag=8  bytes=16  â†’ Swizzle(1, 4, 4)
FP8  16Ã—16Ã—128: lds\_frag=16 bytes=16  â†’ Swizzle(3, 4, 4)
FP8  32Ã—32Ã—64:  lds\_frag=32 bytes=32  â†’ Swizzle(2, 5, 4)

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Element data type (affects byte size).
* â€‹MMA\_M ([`Int`](/mojo/std/builtin/int/Int)): M dimension of MMA instruction.
* â€‹MMA\_K ([`Int`](/mojo/std/builtin/int/Int)): K dimension of MMA instruction.

**Returns:**

[`Swizzle`](/mojo/kernels/layout/swizzle/Swizzle): Swizzle pattern for bank-conflict-free LDS access.

</section>

---

## ping_pong_matmul

<section class='mojo-docs'>

`ping_pong_matmul[a_type: DType, b_type: DType, c_type: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, //, enable_swizzle: Bool = True](a_device_tensor: LayoutTensor[a_type, a_layout, origin], b_device_tensor: LayoutTensor[b_type, b_layout, origin], c_device_tensor: LayoutTensor[c_type, c_layout, origin], ctx: DeviceContext)`

</section>

---

## ConsumerTile

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ConsumerTile[dtype: DType, layout: Layout, pipeline_stages: Int, block_rows: Int, block_cols: Int, warp_rows: Int, warp_cols: Int, reads_per_warp_block: Int, tile_buffers: Int, sync_strategy_type: SyncStrategy, //, origin: MutOrigin, ring_buffer_type: AnyStruct[RingBuffer[dtype, layout, pipeline_stages, block_rows, block_cols, warp_rows, warp_cols, reads_per_warp_block, tile_buffers, sync_strategy_type]], warps_computed_per_consumer: Int]`

Context manager for consumer access to a single ring buffer tile.

## Fields

* â€‹consumer\_view\_ptr (`ConsumerTile[origin, ring_buffer_type, warps_computed_per_consumer].ConsumerViewPtrType`):
* â€‹stage (`Int`):
* â€‹consumer\_iteration (`Int`):
* â€‹warp\_tile\_idx (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ConsumerViewPtrType`

`comptime ConsumerViewPtrType = Pointer[ConsumerTile[origin, ring_buffer_type, warps_computed_per_consumer].ConsumerViewType, origin]`

### `ConsumerViewType`

`comptime ConsumerViewType = ConsumerView[origin, ring_buffer_type, warps_computed_per_consumer]`

## Methods

### `__init__`

`__init__(consumer_view_ptr: Pointer[ConsumerTile[origin, ring_buffer_type, warps_computed_per_consumer].ConsumerViewType, origin], stage: Int, consumer_iteration: Int, warp_tile_idx: Int) -> Self`

### `__enter__`

`__enter__(mut self) -> ring_buffer_type.WarpTileTupleType`

Acquire the tile for use.

**Returns:**

`ring_buffer_type.WarpTileTupleType`

### `__exit__`

`__exit__(mut self)`

Release the tile back to producers.

</section>

---

## ConsumerView

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ConsumerView[dtype: DType, layout: Layout, pipeline_stages: Int, block_rows: Int, block_cols: Int, warp_rows: Int, warp_cols: Int, reads_per_warp_block: Int, tile_buffers: Int, sync_strategy_type: SyncStrategy, //, origin: MutOrigin, ring_buffer_type: AnyStruct[RingBuffer[dtype, layout, pipeline_stages, block_rows, block_cols, warp_rows, warp_cols, reads_per_warp_block, tile_buffers, sync_strategy_type]], warps_computed_per_consumer: Int]`

Consumer view of the unified ring buffer.

## Fields

* â€‹ring\_buffer\_ptr (`ConsumerView[origin, ring_buffer_type, warps_computed_per_consumer].RingBufferPtrType`):
* â€‹phases (`StaticTuple[Int32, (pipeline_stages * warps_computed_per_consumer)]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ConsumerTileType`

`comptime ConsumerTileType = ConsumerTile[origin, ring_buffer_type, warps_computed_per_consumer]`

### `RingBufferPtrType`

`comptime RingBufferPtrType = Pointer[ring_buffer_type, origin]`

## Methods

### `__init__`

`__init__(ring_buffer_ptr: Pointer[ring_buffer_type, origin]) -> Self`

### `__enter__`

`__enter__(mut self) -> Self`

Context manager entry.

### `__exit__`

`__exit__(mut self)`

Context manager exit.

### `acquire_tiles`

`acquire_tiles(mut self, stage: Int, consumer_iteration: Int, warp_tile_idx: Int) -> ring_buffer_type.WarpTileTupleType`

Acquire tiles for reading by this consumer.

**Args:**

* â€‹stage ([`Int`](/mojo/std/builtin/int/Int)): Pipeline stage to read from.
* â€‹consumer\_iteration ([`Int`](/mojo/std/builtin/int/Int)): Which iteration this consumer is on (0 to warps\_computed\_per\_consumer-1).
* â€‹warp\_tile\_idx ([`Int`](/mojo/std/builtin/int/Int)): Which tile this consumer wants to read.

**Returns:**

`ring_buffer_type.WarpTileTupleType`

### `release_tiles`

`release_tiles(mut self, stage: Int, warp_tile_idx: Int)`

Signal to producers that tile is free.

### `get_tile`

`get_tile(mut self, stage: Int, consumer_iteration: Int, warp_tile_idx: Int) -> ConsumerView[origin, ring_buffer_type, warps_computed_per_consumer].ConsumerTileType`

Get a context manager for accessing a tile.

**Args:**

* â€‹stage ([`Int`](/mojo/std/builtin/int/Int)): Pipeline stage.
* â€‹consumer\_iteration ([`Int`](/mojo/std/builtin/int/Int)): Current iteration of this consumer.
* â€‹warp\_tile\_idx ([`Int`](/mojo/std/builtin/int/Int)): Which tile to access.

**Returns:**

`ConsumerView`

</section>

---

## ProducerTile

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ProducerTile[dtype: DType, layout: Layout, pipeline_stages: Int, block_rows: Int, block_cols: Int, warp_rows: Int, warp_cols: Int, reads_per_warp_block: Int, tile_buffers: Int, sync_strategy_type: SyncStrategy, //, origin: MutOrigin, ring_buffer_type: AnyStruct[RingBuffer[dtype, layout, pipeline_stages, block_rows, block_cols, warp_rows, warp_cols, reads_per_warp_block, tile_buffers, sync_strategy_type]], warps_processed_per_producer: Int]`

Context manager for producer access to a single ring buffer tile.

## Fields

* â€‹producer\_view\_ptr (`ProducerTile[origin, ring_buffer_type, warps_processed_per_producer].ProducerViewPtrType`):
* â€‹stage (`Int`):
* â€‹producer\_iteration (`Int`):
* â€‹warp\_tile\_idx (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ProducerViewPtrType`

`comptime ProducerViewPtrType = Pointer[ProducerTile[origin, ring_buffer_type, warps_processed_per_producer].ProducerViewType, origin]`

### `ProducerViewType`

`comptime ProducerViewType = ProducerView[origin, ring_buffer_type, warps_processed_per_producer]`

## Methods

### `__init__`

`__init__(producer_view_ptr: Pointer[ProducerTile[origin, ring_buffer_type, warps_processed_per_producer].ProducerViewType, origin], stage: Int, producer_iteration: Int, warp_tile_idx: Int) -> Self`

### `__enter__`

`__enter__(mut self) -> ring_buffer_type.WarpTileTupleType`

Acquire the tile for use.

**Returns:**

`ring_buffer_type.WarpTileTupleType`

### `__exit__`

`__exit__(mut self)`

Release the tile back to consumers.

</section>

---

## ProducerView

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ProducerView[dtype: DType, layout: Layout, pipeline_stages: Int, block_rows: Int, block_cols: Int, warp_rows: Int, warp_cols: Int, reads_per_warp_block: Int, tile_buffers: Int, sync_strategy_type: SyncStrategy, //, origin: MutOrigin, ring_buffer_type: AnyStruct[RingBuffer[dtype, layout, pipeline_stages, block_rows, block_cols, warp_rows, warp_cols, reads_per_warp_block, tile_buffers, sync_strategy_type]], warps_processed_per_producer: Int]`

Producer view of the unified ring buffer.

## Fields

* â€‹ring\_buffer\_ptr (`ProducerView[origin, ring_buffer_type, warps_processed_per_producer].RingBufferPtrType`):
* â€‹phases (`StaticTuple[Int32, (pipeline_stages * warps_processed_per_producer)]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ProducerTileType`

`comptime ProducerTileType = ProducerTile[origin, ring_buffer_type, warps_processed_per_producer]`

### `RingBufferPtrType`

`comptime RingBufferPtrType = Pointer[ring_buffer_type, origin]`

## Methods

### `__init__`

`__init__(ring_buffer_ptr: Pointer[ring_buffer_type, origin]) -> Self`

### `__enter__`

`__enter__(mut self) -> Self`

Context manager entry.

### `__exit__`

`__exit__(mut self)`

Context manager exit.

### `acquire_tiles`

`acquire_tiles(mut self, stage: Int, producer_iteration: Int, warp_tile_idx: Int) -> ring_buffer_type.WarpTileTupleType`

Acquire tiles for writing by this producer.

**Args:**

* â€‹stage ([`Int`](/mojo/std/builtin/int/Int)): Pipeline stage to write to.
* â€‹producer\_iteration ([`Int`](/mojo/std/builtin/int/Int)): Which iteration this producer is on (`0` to
  `warps_processed_per_producer - 1`).
* â€‹warp\_tile\_idx ([`Int`](/mojo/std/builtin/int/Int)): Which tile this producer is responsible for.

**Returns:**

`ring_buffer_type.WarpTileTupleType`

### `release_tiles`

`release_tiles(mut self, stage: Int, warp_tile_idx: Int)`

Signal to consumers that tile is ready.

### `get_tile`

`get_tile(mut self, stage: Int, warp_tile_idx: Int, producer_iteration: Int) -> ProducerView[origin, ring_buffer_type, warps_processed_per_producer].ProducerTileType`

Get a context manager for accessing a tile.

**Args:**

* â€‹stage ([`Int`](/mojo/std/builtin/int/Int)): Pipeline stage.
* â€‹warp\_tile\_idx ([`Int`](/mojo/std/builtin/int/Int)): Which tile to access.
* â€‹producer\_iteration ([`Int`](/mojo/std/builtin/int/Int)): Current iteration of this producer.

**Returns:**

`ProducerView`

</section>

---

## RingBuffer

<section class='mojo-docs'>

`struct RingBuffer[dtype: DType, layout: Layout, pipeline_stages: Int, block_rows: Int, block_cols: Int, warp_rows: Int, warp_cols: Int, reads_per_warp_block: Int, tile_buffers: Int, sync_strategy_type: SyncStrategy]`

Ring buffer for coordinating producer-consumer warps in matrix multiplication.

## Parameters

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of elements.
* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Memory layout for shared memory tiles.
* â€‹pipeline\_stages ([`Int`](/mojo/std/builtin/int/Int)): Number of stages for software pipelining.
* â€‹block\_rows ([`Int`](/mojo/std/builtin/int/Int)): Number of rows in block-level tiles.
* â€‹block\_cols ([`Int`](/mojo/std/builtin/int/Int)): Number of columns in block-level tiles.
* â€‹warp\_rows ([`Int`](/mojo/std/builtin/int/Int)): Number of rows in warp-level tiles.
* â€‹warp\_cols ([`Int`](/mojo/std/builtin/int/Int)): Number of columns in warp-level tiles.
* â€‹reads\_per\_warp\_block ([`Int`](/mojo/std/builtin/int/Int)): How many consumer warps read each tile.
* â€‹tile\_buffers ([`Int`](/mojo/std/builtin/int/Int)): Number of separate tile buffers (usually 1).
* â€‹sync\_strategy\_type ([`SyncStrategy`](/mojo/kernels/linalg/matmul/gpu/amd/ring_buffer_traits/SyncStrategy)): Synchronization strategy (SingleCounterSync or SplitCounterSync).

## Fields

* â€‹smem\_buffers (`RingBuffer[dtype, layout, pipeline_stages, block_rows, block_cols, warp_rows, warp_cols, reads_per_warp_block, tile_buffers, sync_strategy_type].SMemBuffersType`):
* â€‹sync\_strategy (`sync_strategy_type`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = sync_strategy_type.__del__is_trivial`

### `block_warps`

`comptime block_warps = (block_rows // warp_rows)`

### `SMemBuffersType`

`comptime SMemBuffersType = StaticTuple[SMemBuffer[dtype, layout, pipeline_stages, block_rows, block_cols, warp_rows, warp_cols], tile_buffers]`

### `SmemBufferType`

`comptime SmemBufferType = SMemBuffer[dtype, layout, pipeline_stages, block_rows, block_cols, warp_rows, warp_cols]`

### `total_tiles`

`comptime total_tiles = (RingBuffer[dtype, layout, pipeline_stages, block_rows, block_cols, warp_rows, warp_cols, reads_per_warp_block, tile_buffers, sync_strategy_type].block_warps * pipeline_stages)`

### `WarpTileTupleType`

`comptime WarpTileTupleType = StaticTuple[LayoutTensor[dtype, LayoutTensor._compute_tile_layout[True, dtype, LayoutTensor._compute_tile_layout[True, dtype, pipeline_layout[layout, pipeline_stages](), MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), _get_index_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), False, 128, block_rows, block_cols]()[0], MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), _get_index_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), _tile_is_masked[pipeline_layout[layout, pipeline_stages](), block_rows, block_cols](), 128, warp_rows, warp_cols]()[0], MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=_get_layout_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), linear_idx_type=_get_index_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), masked=_tile_is_masked[pipeline_layout[layout, pipeline_stages](), block_rows, block_cols]() if _tile_is_masked[pipeline_layout[layout, pipeline_stages](), block_rows, block_cols]() else _tile_is_masked[LayoutTensor._compute_tile_layout[True, dtype, pipeline_layout[layout, pipeline_stages](), MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), _get_index_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), False, 128, block_rows, block_cols]()[0], warp_rows, warp_cols](), alignment=128], tile_buffers]`

### `WarpTileType`

`comptime WarpTileType = RingBuffer[dtype, layout, pipeline_stages, block_rows, block_cols, warp_rows, warp_cols, reads_per_warp_block, tile_buffers, sync_strategy_type].SmemBufferType.WarpTileType`

## Methods

### `__init__`

`__init__(out self)`

### `get_tiles`

`get_tiles(self, stage: Int, warp_tile_idx: Int) -> RingBuffer[dtype, layout, pipeline_stages, block_rows, block_cols, warp_rows, warp_cols, reads_per_warp_block, tile_buffers, sync_strategy_type].WarpTileTupleType`

Get tiles from shared memory.

**Returns:**

`RingBuffer`

### `producer`

`producer[warps_processed_per_producer: Int](mut self) -> ProducerView[self, RingBuffer[dtype, layout, pipeline_stages, block_rows, block_cols, warp_rows, warp_cols, reads_per_warp_block, tile_buffers, sync_strategy_type], warps_processed_per_producer]`

Create a producer view of this ring buffer.

**Returns:**

`ProducerView`

### `consumer`

`consumer[warps_computed_per_consumer: Int](mut self) -> ConsumerView[self, RingBuffer[dtype, layout, pipeline_stages, block_rows, block_cols, warp_rows, warp_cols, reads_per_warp_block, tile_buffers, sync_strategy_type], warps_computed_per_consumer]`

Create a consumer view of this ring buffer.

**Returns:**

`ConsumerView`

### `get_staged_idx`

`get_staged_idx(self, tile_idx: Int, stage: Int) -> Int`

Get the staged index for a tile and stage.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `wait_producer_acquire`

`wait_producer_acquire(self, tile_idx: Int, stage: Int, phase: Int32)`

Producer waits to acquire a tile.

### `signal_producer_release`

`signal_producer_release(mut self, tile_idx: Int, stage: Int)`

Producer signals it has released a tile.

### `wait_consumer_acquire`

`wait_consumer_acquire(self, tile_idx: Int, stage: Int, phase: Int32)`

Consumer waits to acquire a tile.

### `signal_consumer_release`

`signal_consumer_release(mut self, tile_idx: Int, stage: Int)`

Consumer signals it has released a tile.

### `get_producer_phase_increment`

`get_producer_phase_increment(self) -> Int32`

Get the phase increment for producers.

**Returns:**

[`Int32`](/mojo/std/builtin/simd/#int32)

### `get_consumer_phase_increment`

`get_consumer_phase_increment(self) -> Int32`

Get the phase increment for consumers.

**Returns:**

[`Int32`](/mojo/std/builtin/simd/#int32)

</section>

---

## ring_buffer

<section class='mojo-docs'>

Ring Buffer implementation for producer-consumer synchronization in GPU kernels.

This ring buffer coordinates data transfer between producer warps (loading from global memory)
and consumer warps (performing computation) through shared memory tiles.

Key features:

* Configurable synchronization strategies via the SyncStrategy trait
* Pipeline stages for overlapping data transfer and computation
* Context managers for automatic acquire/release of tiles
* Phase-based synchronization to prevent data races

## Structs

* [â€‹`ConsumerTile`](./ConsumerTile): Context manager for consumer access to a single ring buffer tile.
* [â€‹`ConsumerView`](./ConsumerView): Consumer view of the unified ring buffer.
* [â€‹`ProducerTile`](./ProducerTile): Context manager for producer access to a single ring buffer tile.
* [â€‹`ProducerView`](./ProducerView): Producer view of the unified ring buffer.
* [â€‹`RingBuffer`](./RingBuffer): Ring buffer for coordinating producer-consumer warps in matrix multiplication.

</section>

---

## SingleCounterSync

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SingleCounterSync[pipeline_stages: Int, block_rows: Int, warp_rows: Int, reads_per_warp_block: Int]`

Single counter synchronization strategy.

Uses one atomic counter per tile that tracks both producer and consumer progress.
This is simpler but has higher contention as all warps compete for the same counter.

Phase progression:

* Each phase advances by (writes\_per\_warp\_block + reads\_per\_warp\_block)
* Producers wait for phase N, increment counter by 1
* Consumers wait for phase N+1, increment counter by 1

## Fields

* â€‹sync\_counter (`SingleCounterSync[pipeline_stages, block_rows, warp_rows, reads_per_warp_block].SyncCounterArray`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`SyncStrategy`](/mojo/kernels/linalg/matmul/gpu/amd/ring_buffer_traits/SyncStrategy)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `block_warps`

`comptime block_warps = (block_rows // warp_rows)`

### `SyncCounterArray`

`comptime SyncCounterArray = SMemArrayType[Int32, SingleCounterSync[pipeline_stages, block_rows, warp_rows, reads_per_warp_block].total_tiles]`

### `total_tiles`

`comptime total_tiles = (SingleCounterSync[pipeline_stages, block_rows, warp_rows, reads_per_warp_block].block_warps * pipeline_stages)`

### `writes_per_warp_block`

`comptime writes_per_warp_block = 1`

## Methods

### `__init__`

`__init__() -> Self`

Initialize with internally allocated sync counter.

### `get_staged_idx`

`get_staged_idx(self, tile_idx: Int, stage: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `wait_producer_acquire`

`wait_producer_acquire(self, tile_idx: Int, stage: Int, phase: Int32)`

### `signal_producer_release`

`signal_producer_release(mut self, tile_idx: Int, stage: Int)`

### `wait_consumer_acquire`

`wait_consumer_acquire(self, tile_idx: Int, stage: Int, phase: Int32)`

### `signal_consumer_release`

`signal_consumer_release(mut self, tile_idx: Int, stage: Int)`

### `get_producer_phase_increment`

`get_producer_phase_increment(self) -> Int32`

**Returns:**

[`Int32`](/mojo/std/builtin/simd/#int32)

### `get_consumer_phase_increment`

`get_consumer_phase_increment(self) -> Int32`

**Returns:**

[`Int32`](/mojo/std/builtin/simd/#int32)

</section>

---

## SplitCounterSync

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SplitCounterSync[pipeline_stages: Int, block_rows: Int, warp_rows: Int, reads_per_warp_block: Int]`

Split counter synchronization strategy.

Uses separate producer and consumer counters per tile to reduce atomic contention.
Producers only write to producer counters, consumers only write to consumer counters.

Phase progression:

* Producer phase advances by reads\_per\_warp\_block (waits for N consumers)
* Consumer phase advances by writes\_per\_warp\_block (waits for 1 producer)
* This asymmetry reflects the 1-producer-to-N-consumers relationship

## Fields

* â€‹producer\_counters (`SplitCounterSync[pipeline_stages, block_rows, warp_rows, reads_per_warp_block].ProducerCounterArray`):
* â€‹consumer\_counters (`SplitCounterSync[pipeline_stages, block_rows, warp_rows, reads_per_warp_block].ConsumerCounterArray`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`SyncStrategy`](/mojo/kernels/linalg/matmul/gpu/amd/ring_buffer_traits/SyncStrategy)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `block_warps`

`comptime block_warps = (block_rows // warp_rows)`

### `ConsumerCounterArray`

`comptime ConsumerCounterArray = SMemArrayType[Int32, SplitCounterSync[pipeline_stages, block_rows, warp_rows, reads_per_warp_block].total_tiles]`

### `ProducerCounterArray`

`comptime ProducerCounterArray = SMemArrayType[Int32, SplitCounterSync[pipeline_stages, block_rows, warp_rows, reads_per_warp_block].total_tiles]`

### `total_tiles`

`comptime total_tiles = (SplitCounterSync[pipeline_stages, block_rows, warp_rows, reads_per_warp_block].block_warps * pipeline_stages)`

### `writes_per_warp_block`

`comptime writes_per_warp_block = 1`

## Methods

### `__init__`

`__init__() -> Self`

Initialize with internally allocated producer and consumer counters.

### `get_staged_idx`

`get_staged_idx(self, tile_idx: Int, stage: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `wait_producer_acquire`

`wait_producer_acquire(self, tile_idx: Int, stage: Int, phase: Int32)`

Producer waits on consumer counter.

### `signal_producer_release`

`signal_producer_release(mut self, tile_idx: Int, stage: Int)`

Producer increments producer counter.

### `wait_consumer_acquire`

`wait_consumer_acquire(self, tile_idx: Int, stage: Int, phase: Int32)`

Consumer waits on producer counter.

### `signal_consumer_release`

`signal_consumer_release(mut self, tile_idx: Int, stage: Int)`

Consumer increments consumer counter by 1.

### `get_producer_phase_increment`

`get_producer_phase_increment(self) -> Int32`

Producer phase advances by reads\_per\_warp\_block.

**Returns:**

[`Int32`](/mojo/std/builtin/simd/#int32)

### `get_consumer_phase_increment`

`get_consumer_phase_increment(self) -> Int32`

Consumer phase advances by writes\_per\_warp\_block.

**Returns:**

[`Int32`](/mojo/std/builtin/simd/#int32)

</section>

---

## SyncStrategy

<section class='mojo-docs'>

Interface for synchronization strategies between producers and consumers.

All methods have the same signature regardless of the specific implementation,
allowing the RingBuffer to be parameterized with any conforming strategy.

Phase tracking ensures producers and consumers access different tiles:

* Producers wait until consumers have finished with a tile (phase N)
* Consumers wait until producers have filled a tile (phase N+1)

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

## Required methods

### `__init__`

`__init__() -> _Self`

Initialize with internally allocated sync counter.

**Returns:**

`_Self`

### `get_staged_idx`

`get_staged_idx(self: _Self, tile_idx: Int, stage: Int) -> Int`

Convert tile index and stage to a flat index in the counter arrays.

**Args:**

* â€‹tile\_idx ([`Int`](/mojo/std/builtin/int/Int)): Index of the tile within a stage (0 to block\_warps-1).
* â€‹stage ([`Int`](/mojo/std/builtin/int/Int)): Pipeline stage (0 to pipeline\_stages-1).

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): Flat index for accessing synchronization counters.

### `wait_producer_acquire`

`wait_producer_acquire(self: _Self, tile_idx: Int, stage: Int, phase: Int32)`

Producer waits until it can write to the specified tile.

Blocks until all consumers have finished reading from this tile
(counter >= phase).

### `signal_producer_release`

`signal_producer_release(mut self: _Self, tile_idx: Int, stage: Int)`

Producer signals that it has finished writing to the tile.

Increments the appropriate counter to notify waiting consumers.

### `wait_consumer_acquire`

`wait_consumer_acquire(self: _Self, tile_idx: Int, stage: Int, phase: Int32)`

Consumer waits until it can read from the specified tile.

Blocks until producer has finished writing to this tile
(counter >= phase).

### `signal_consumer_release`

`signal_consumer_release(mut self: _Self, tile_idx: Int, stage: Int)`

Consumer signals that it has finished reading from the tile.

Increments the appropriate counter to notify waiting producers.

### `get_producer_phase_increment`

`get_producer_phase_increment(self: _Self) -> Int32`

Returns how much to advance the producer phase after each acquisition.

This determines when producers can reuse a tile after consumers finish.

**Returns:**

[`Int32`](/mojo/std/builtin/simd/#int32)

### `get_consumer_phase_increment`

`get_consumer_phase_increment(self: _Self) -> Int32`

Returns how much to advance the consumer phase after each acquisition.

This determines when consumers can read a tile after producers finish.

**Returns:**

[`Int32`](/mojo/std/builtin/simd/#int32)

</section>

---

## increment_counter_if_first_thread

<section class='mojo-docs'>

`increment_counter_if_first_thread(counter: UnsafePointer[Int32, origin, address_space=AddressSpace.SHARED], increment: Int32)`

Atomically increment counter, but only from the first thread in warp.

</section>

---

## ring_buffer_traits

<section class='mojo-docs'>

Trait definitions and utilities for ring buffer synchronization strategies.

This module provides:

* SyncStrategy trait: Interface for producer-consumer synchronization protocols
* SingleCounterSync: Uses a single atomic counter per tile (original RingBuffer behavior)
* SplitCounterSync: Uses separate producer/consumer counters to reduce contention
* Atomic utility functions for thread-safe counter operations

## Structs

* [â€‹`SingleCounterSync`](./SingleCounterSync): Single counter synchronization strategy.
* [â€‹`SplitCounterSync`](./SplitCounterSync): Split counter synchronization strategy.

## Traits

* [â€‹`SyncStrategy`](./SyncStrategy): Interface for synchronization strategies between producers and consumers.

## Functions

* [â€‹`increment_counter_if_first_thread`](./increment_counter_if_first_thread): Atomically increment counter, but only from the first thread in warp.
* [â€‹`wait_for_counter`](./wait_for_counter): Spin-wait until counter reaches threshold.

</section>

---

## wait_for_counter

<section class='mojo-docs'>

`wait_for_counter(counter: UnsafePointer[Int32, origin, address_space=AddressSpace.SHARED], threshold: Int32)`

Spin-wait until counter reaches threshold.

</section>

---

## AMDSharedMemoryBarrier

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct AMDSharedMemoryBarrier`

## Fields

* â€‹\_\_repr (`Int32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `initialize`

`initialize(ref [MutAnyOrigin, 3] self)`

### `value`

`value(ref [3] self) -> Int32`

**Returns:**

[`Int32`](/mojo/std/builtin/simd/#int32)

### `increment`

`increment(ref [MutAnyOrigin, 3] self, warp_id: Int)`

### `wait_until_greater_or_equal_to`

`wait_until_greater_or_equal_to(ref [3] self, v: Int32)`

</section>

---

## AMDWarpSharedMemoryBarrier

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct AMDWarpSharedMemoryBarrier[size: Int]`

## Fields

* â€‹\_\_repr (`StaticTuple[Int32, size]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `initialize`

`initialize(ref [MutAnyOrigin, 3] self)`

### `value`

`value(ref [3] self) -> Int32`

**Returns:**

[`Int32`](/mojo/std/builtin/simd/#int32)

### `increment`

`increment(ref [MutAnyOrigin, 3] self, warp_id: Int)`

### `wait_until_greater_or_equal_to`

`wait_until_greater_or_equal_to(ref [3] self, v: Int32)`

</section>

---

## AmdTileOperator

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct AmdTileOperator[InType: DType, OutType: DType, warp_block_layout_a: Layout, warp_block_layout_b: Layout, mma_shape: IndexList[3], swizzle: OptionalReg[Swizzle] = None, transpose_b: Bool = True]`

Manages tensor core operations for matrix multiplication on AMD GPUs.

This operator handles loading matrix fragments from shared memory to registers
and performing matrix multiply-accumulate operations using tensor cores.

Requirements:
\- warp\_block\_layout\_a.shape\[0] must be divisible by mma\_shape\[0]
\- warp\_block\_layout\_b.shape\[0] must be divisible by mma\_shape\[1]
\- warp\_block\_layout\_a.shape\[1] must be divisible by mma\_shape\[2]
\- warp\_block\_layout\_b.shape\[1] must be divisible by mma\_shape\[2]
\- The K dimension must align such that num\_k\_tiles is divisible by k\_group\_size

## Parameters

* â€‹InType ([`DType`](/mojo/std/builtin/dtype/DType)): Input data type.
* â€‹OutType ([`DType`](/mojo/std/builtin/dtype/DType)): Output data type.
* â€‹warp\_block\_layout\_a ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout for matrix A warp tiles.
* â€‹warp\_block\_layout\_b ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout for matrix B warp tiles.
* â€‹mma\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): Shape of the MMA operation \[M, N, K].
* â€‹swizzle ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional swizzle pattern for memory access.
* â€‹transpose\_b ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether matrix B is transposed.

## Fields

* â€‹out\_reg\_tile (`AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].OutRegTileType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ARegTileType`

`comptime ARegTileType = LayoutTensor[InType, Layout.row_major(((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_a) * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width), MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `BRegTileType`

`comptime BRegTileType = LayoutTensor[InType, Layout.row_major(((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_b) * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width), MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `k_group_size_a`

`comptime k_group_size_a = (AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width // num_matrix_reg[mma_shape.__getitem__[3, DType.int64, Int](0), mma_shape.__getitem__[3, DType.int64, Int](2)]())`

### `k_group_size_b`

`comptime k_group_size_b = (AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width // num_matrix_reg[mma_shape.__getitem__[3, DType.int64, Int](1), mma_shape.__getitem__[3, DType.int64, Int](2)]())`

### `k_tile_fragment_index`

`comptime k_tile_fragment_index[k_tile_idx: Int] = (k_tile_idx % AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_a)`

#### Parameters

* â€‹k\_tile\_idx ([`Int`](/mojo/std/builtin/int/Int)):

### `k_tile_group_index`

`comptime k_tile_group_index[k_tile_idx: Int] = (k_tile_idx // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_a)`

#### Parameters

* â€‹k\_tile\_idx ([`Int`](/mojo/std/builtin/int/Int)):

### `num_k_tiles`

`comptime num_k_tiles = (AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].WK // mma_shape.__getitem__[3, DType.int64, Int](2))`

### `num_m_mmas`

`comptime num_m_mmas = (product(warp_block_layout_a.shape[0]) // mma_shape.__getitem__[3, DType.int64, Int](0))`

### `num_n_mmas`

`comptime num_n_mmas = (product(warp_block_layout_b.shape[0]) // mma_shape.__getitem__[3, DType.int64, Int](1))`

### `out_frag_size`

`comptime out_frag_size = ((mma_shape.__getitem__[3, DType.int64, Int](0) * mma_shape.__getitem__[3, DType.int64, Int](1)) // WARP_SIZE)`

### `OutRegTileFragmentType`

`comptime OutRegTileFragmentType = LayoutTensor[OutType, LayoutTensor._compute_tile_layout[True, OutType, Layout.row_major((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), num_matrix_reg[mma_shape.__getitem__[3, DType.int64, Int](0), mma_shape.__getitem__[3, DType.int64, Int](1)]()), MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(Layout.row_major((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), num_matrix_reg[mma_shape.__getitem__[3, DType.int64, Int](0), mma_shape.__getitem__[3, DType.int64, Int](1)]()), AddressSpace.LOCAL), _get_index_type(Layout.row_major((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), num_matrix_reg[mma_shape.__getitem__[3, DType.int64, Int](0), mma_shape.__getitem__[3, DType.int64, Int](1)]()), AddressSpace.LOCAL), False, align_of[SIMD[InType, AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width]](), (AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), num_matrix_reg[mma_shape.__getitem__[3, DType.int64, Int](0), mma_shape.__getitem__[3, DType.int64, Int](1)]()]()[0], MutAnyOrigin, address_space=AddressSpace.LOCAL, layout_int_type=_get_layout_type(Layout.row_major((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), num_matrix_reg[mma_shape.__getitem__[3, DType.int64, Int](0), mma_shape.__getitem__[3, DType.int64, Int](1)]()), AddressSpace.LOCAL), linear_idx_type=_get_index_type(Layout.row_major((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), num_matrix_reg[mma_shape.__getitem__[3, DType.int64, Int](0), mma_shape.__getitem__[3, DType.int64, Int](1)]()), AddressSpace.LOCAL), masked=_tile_is_masked[Layout.row_major((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), num_matrix_reg[mma_shape.__getitem__[3, DType.int64, Int](0), mma_shape.__getitem__[3, DType.int64, Int](1)]()), (AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), num_matrix_reg[mma_shape.__getitem__[3, DType.int64, Int](0), mma_shape.__getitem__[3, DType.int64, Int](1)]()](), alignment=align_of[SIMD[InType, AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width]]()]`

### `OutRegTileType`

`comptime OutRegTileType = LayoutTensor[OutType, Layout.row_major((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), num_matrix_reg[mma_shape.__getitem__[3, DType.int64, Int](0), mma_shape.__getitem__[3, DType.int64, Int](1)]()), MutAnyOrigin, address_space=AddressSpace.LOCAL, alignment=align_of[SIMD[InType, AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width]]()]`

### `simd_width`

`comptime simd_width = simd_width_of[InType]()`

### `tensor_core`

`comptime tensor_core = TensorCore[OutType, InType, mma_shape, transpose_b]()`

### `total_k_tiles`

`comptime total_k_tiles = AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles`

### `WK`

`comptime WK = product(warp_block_layout_a.shape[1])`

## Methods

### `__init__`

`__init__() -> Self`

### `a_reg_tile`

`a_reg_tile(self, k_tile_idx: Int) -> LayoutTensor[InType, LayoutTensor._compute_tile_layout[True, InType, Layout.row_major(((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_a) * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width), MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(Layout.row_major(((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_a) * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width), AddressSpace.LOCAL), _get_index_type(Layout.row_major(((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_a) * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width), AddressSpace.LOCAL), False, align_of[InType](), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas, AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width]()[0], MutAnyOrigin, address_space=AddressSpace.LOCAL, layout_int_type=_get_layout_type(Layout.row_major(((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_a) * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width), AddressSpace.LOCAL), linear_idx_type=_get_index_type(Layout.row_major(((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_a) * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width), AddressSpace.LOCAL), masked=_tile_is_masked[Layout.row_major(((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_a) * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_m_mmas, AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width]()]`

Get A register tile for a specific K tile.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `b_reg_tile`

`b_reg_tile(self, k_tile_idx: Int) -> LayoutTensor[InType, LayoutTensor._compute_tile_layout[True, InType, Layout.row_major(((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_b) * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width), MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(Layout.row_major(((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_b) * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width), AddressSpace.LOCAL), _get_index_type(Layout.row_major(((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_b) * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width), AddressSpace.LOCAL), False, align_of[InType](), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas, AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width]()[0], MutAnyOrigin, address_space=AddressSpace.LOCAL, layout_int_type=_get_layout_type(Layout.row_major(((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_b) * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width), AddressSpace.LOCAL), linear_idx_type=_get_index_type(Layout.row_major(((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_b) * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width), AddressSpace.LOCAL), masked=_tile_is_masked[Layout.row_major(((AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_k_tiles // AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].k_group_size_b) * AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width), AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].num_n_mmas, AmdTileOperator[InType, OutType, warp_block_layout_a, warp_block_layout_b, mma_shape, swizzle, transpose_b].simd_width]()]`

Get B register tile for a specific K tile.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `reset_accumulator`

`reset_accumulator(self)`

Reset the accumulator to zero for a new tile computation.

### `load_tile_fragment`

`load_tile_fragment[k_tile_idx: Int](self, smem_tile_a: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], smem_tile_b: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Load fragments from shared memory to registers for a specific K tile.

**Parameters:**

* â€‹k\_tile\_idx ([`Int`](/mojo/std/builtin/int/Int)): K-tile index (0 to total\_k\_tiles-1).

**Args:**

* â€‹smem\_tile\_a ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Shared memory tile for matrix A.
* â€‹smem\_tile\_b ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Shared memory tile for matrix B.

### `mma_compute`

`mma_compute[k_tile_idx: Int](self)`

Perform matrix multiply-accumulate for a specific K tile.

This method assumes fragments are already loaded via load\_tile\_fragment.

**Parameters:**

* â€‹k\_tile\_idx ([`Int`](/mojo/std/builtin/int/Int)): K-tile index (0 to total\_k\_tiles-1).

</section>

---

## Enum

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

## Required methods

### `value`

`value(self: _Self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

## Provided methods

### `__eq__`

`__eq__(self: _Self, other: _Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self: _Self, other: _Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__is__`

`__is__(self: _Self, other: _Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__isnot__`

`__isnot__(self: _Self, other: _Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## MMAConfig

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MMAConfig[InType: DType, OutType: DType, mma_shape: IndexList[3], transpose_b: Bool = True]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `k_group_size_a`

`comptime k_group_size_a = (MMAConfig[InType, OutType, mma_shape, transpose_b].simd_width // MMAConfig[InType, OutType, mma_shape, transpose_b].registers_per_thread_a)`

### `k_group_size_b`

`comptime k_group_size_b = (MMAConfig[InType, OutType, mma_shape, transpose_b].simd_width // MMAConfig[InType, OutType, mma_shape, transpose_b].registers_per_thread_b)`

### `mma`

`comptime mma = TensorCore[OutType, InType, mma_shape, transpose_b]()`

### `registers_per_thread_a`

`comptime registers_per_thread_a = num_matrix_reg[mma_shape.__getitem__[3, DType.int64, Int](0), mma_shape.__getitem__[3, DType.int64, Int](2)]()`

### `registers_per_thread_b`

`comptime registers_per_thread_b = num_matrix_reg[mma_shape.__getitem__[3, DType.int64, Int](1), mma_shape.__getitem__[3, DType.int64, Int](2)]()`

### `simd_width`

`comptime simd_width = simd_width_of[InType]()`

## Methods

### `adjusted_mma_k_shape_a`

`static adjusted_mma_k_shape_a() -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `adjusted_mma_k_shape_b`

`static adjusted_mma_k_shape_b() -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## SMemBuffer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SMemBuffer[dtype: DType, layout: Layout, pipeline_stages: Int, BM: Int, BN: Int, WM: Int, WN: Int]`

Manages shared memory and returns 2D tile slices of the buffer.

## Fields

* â€‹buffer (`SMemBuffer[dtype, layout, pipeline_stages, BM, BN, WM, WN].SMemTileType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `BlockTileType`

`comptime BlockTileType = LayoutTensor[dtype, LayoutTensor._compute_tile_layout[True, dtype, pipeline_layout[layout, pipeline_stages](), MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), _get_index_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), False, 128, BM, BN]()[0], MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=_get_layout_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), linear_idx_type=_get_index_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), masked=_tile_is_masked[pipeline_layout[layout, pipeline_stages](), BM, BN](), alignment=128]`

### `SMemTileType`

`comptime SMemTileType = LayoutTensor[dtype, pipeline_layout[layout, pipeline_stages](), MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128]`

### `WarpTileType`

`comptime WarpTileType = LayoutTensor[dtype, LayoutTensor._compute_tile_layout[True, dtype, LayoutTensor._compute_tile_layout[True, dtype, pipeline_layout[layout, pipeline_stages](), MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), _get_index_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), False, 128, BM, BN]()[0], MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), _get_index_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), _tile_is_masked[pipeline_layout[layout, pipeline_stages](), BM, BN](), 128, WM, WN]()[0], MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=_get_layout_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), linear_idx_type=_get_index_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), masked=_tile_is_masked[pipeline_layout[layout, pipeline_stages](), BM, BN]() if _tile_is_masked[pipeline_layout[layout, pipeline_stages](), BM, BN]() else _tile_is_masked[LayoutTensor._compute_tile_layout[True, dtype, pipeline_layout[layout, pipeline_stages](), MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), _get_index_type(pipeline_layout[layout, pipeline_stages](), AddressSpace.SHARED), False, 128, BM, BN]()[0], WM, WN](), alignment=128]`

## Methods

### `__init__`

`__init__() -> Self`

### `get_tile`

`get_tile(self, stage: Int) -> SMemBuffer[dtype, layout, pipeline_stages, BM, BN, WM, WN].BlockTileType`

**Returns:**

`SMemBuffer`

</section>

---

## ThreadRole

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ThreadRole`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Enum`](/mojo/kernels/linalg/matmul/gpu/amd/structured/Enum),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `CONSUMER`

`comptime CONSUMER = ThreadRole(1)`

### `PRODUCER`

`comptime PRODUCER = ThreadRole(0)`

### `PRODUCER_CONSUMER`

`comptime PRODUCER_CONSUMER = ThreadRole(2)`

## Methods

### `value`

`value(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `__str__`

`__str__(self) -> String`

Returns the string representation of this algorithm.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): String: A human-readable string representation of the algorithm.

### `write_to`

`write_to[W: Writer](self, mut writer: W)`

</section>

---

## structured

<section class='mojo-docs'>

## Structs

* [â€‹`AMDSharedMemoryBarrier`](./AMDSharedMemoryBarrier):
* [â€‹`AmdTileOperator`](./AmdTileOperator): Manages tensor core operations for matrix multiplication on AMD GPUs.
* [â€‹`AMDWarpSharedMemoryBarrier`](./AMDWarpSharedMemoryBarrier):
* [â€‹`MMAConfig`](./MMAConfig):
* [â€‹`SMemBuffer`](./SMemBuffer): Manages shared memory and returns 2D tile slices of the buffer.
* [â€‹`ThreadRole`](./ThreadRole):

## Traits

* [â€‹`Enum`](./Enum):

## Functions

* [â€‹`pipeline_layout`](./pipeline_layout):

</section>

---

## pipeline_layout

<section class='mojo-docs'>

`pipeline_layout[layout: Layout, pipeline_stages: Int]() -> Layout`

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout)

</section>

---

## determine_thread_role

<section class='mojo-docs'>

`determine_thread_role[producer_a_warps: Int, producer_b_warps: Int]() -> Tuple[ThreadRole, Int]`

Returns (role, consumer\_warp\_id within role group).

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

</section>

---

## get_producer_warp_thread_layout

<section class='mojo-docs'>

`get_producer_warp_thread_layout[k_tile_size: Int, simd_width: Int, block_rows: Int, block_cols: Int]() -> Layout`

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout)

</section>

---

## warp_spec_matmul

<section class='mojo-docs'>

AMD Warp-Specialized Matrix Multiplication

Architecture Overview:

* Producer warps: Load tiles from global to shared memory
  * A producers: Load MÃ—K tiles from matrix A
  * B producers: Load NÃ—K tiles from matrix B
* Consumer warps: Perform matrix multiplication using shared memory tiles
* Ring buffer: Coordinates producer-consumer synchronization with barriers

Data Flow:

1. Producers load tiles into shared memory stages
2. Barriers ensure data is ready before consumers access it
3. Consumers compute partial results and accumulate
4. Final results written back to global memory

Memory Layout:

* Shared memory is divided into pipeline stages for overlapping
* Each stage contains block tiles that are further divided into warp tiles
* Swizzling may be applied to avoid bank conflicts

Ring Buffer Configuration:

* Uses SingleCounterSync strategy by default (single atomic counter per tile)
* Can be changed to SplitCounterSync in the RingBuffer type aliases for reduced contention
* The trait-based design allows easy experimentation with different sync strategies

## `comptime` values

### `GlobalTensor`

`comptime GlobalTensor[dtype: DType, layout: Layout] = LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.GLOBAL]`

#### Parameters

* â€‹dtype ([`DType`](/std/builtin/dtype/DType)):
* â€‹layout ([`Layout`](/kernels/layout/layout/Layout)):

## Functions

* [â€‹`determine_thread_role`](./determine_thread_role): Returns (role, consumer\_warp\_id within role group).
* [â€‹`get_producer_warp_thread_layout`](./get_producer_warp_thread_layout):
* [â€‹`lgkm_wait`](./lgkm_wait):
* [â€‹`run_producer`](./run_producer): Generic producer function for loading matrix tiles from global to shared memory.
* [â€‹`smem_tile_layout`](./smem_tile_layout):
* [â€‹`validate_config`](./validate_config): Validates the configuration parameters for the matrix multiplication kernel.
* [â€‹`warp_specialized_matmul`](./warp_specialized_matmul):
* [â€‹`warp_specialized_matmul_kernel`](./warp_specialized_matmul_kernel):

</section>

---

## lgkm_wait

<section class='mojo-docs'>

`lgkm_wait()`

</section>

---

## run_producer

<section class='mojo-docs'>

`run_producer[dtype: DType, layout: Layout, block_rows: Int, block_cols: Int, warp_rows: Int, warp_cols: Int, producer_warps: Int, pipeline_stages: Int, k_tile_size: Int, simd_width: Int, warps_processed_per_producer: Int, tile_count: Int, swizzle: OptionalReg[Swizzle]](matrix: LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.GLOBAL], mut ring_buffer: RingBuffer[dtype, layout, pipeline_stages, block_rows, block_cols, warp_rows, warp_cols, reads_per_warp_block, tile_buffers, sync_strategy_type], warp_id: UInt, block_idx_dim: Int)`

Generic producer function for loading matrix tiles from global to shared memory.

</section>

---

## smem_tile_layout

<section class='mojo-docs'>

`smem_tile_layout[k_tile_size: Int, block_rows: Int, block_cols: Int]() -> Layout`

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout)

</section>

---

## validate_config

<section class='mojo-docs'>

`validate_config[BM: Int, BN: Int, BK: Int, WM: Int, WN: Int, WK: Int, m_warps: Int, n_warps: Int, producer_a: Int, producer_b: Int, consumer: Int]()`

Validates the configuration parameters for the matrix multiplication kernel.

</section>

---

## warp_specialized_matmul

<section class='mojo-docs'>

`warp_specialized_matmul[M: Int, N: Int, K: Int, BM: Int, BN: Int, BK: Int, WM: Int, WN: Int, WK: Int, a_producer_warps: Int, b_producer_warps: Int, consumer_warps: Int, pipeline_stages: Int = 1](a_device_tensor: LayoutTensor[DType.bfloat16, Layout.row_major(M, K), origin], b_device_tensor: LayoutTensor[DType.bfloat16, Layout.row_major(N, K), origin], c_device_tensor: LayoutTensor[DType.float32, Layout.row_major(M, N), origin], ctx: DeviceContext)`

</section>

---

## warp_specialized_matmul_kernel

<section class='mojo-docs'>

`warp_specialized_matmul_kernel[in_type: DType, out_type: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, BM: Int, BN: Int, BK: Int, WM: Int, WN: Int, WK: Int, a_producer_warps: Int, b_producer_warps: Int, consumer_warps: Int, pipeline_stages: Int](a: LayoutTensor[in_type, a_layout, MutAnyOrigin, address_space=AddressSpace.GLOBAL], b: LayoutTensor[in_type, b_layout, MutAnyOrigin, address_space=AddressSpace.GLOBAL], c: LayoutTensor[out_type, c_layout, MutAnyOrigin, address_space=AddressSpace.GLOBAL])`

</section>

---

## gpu

<section class='mojo-docs'>

## `comptime` values

### `logger`

`comptime logger = Logger[DEFAULT_LEVEL](stdout, "", False)`

## Packages

* [â€‹`amd`](./amd/): Provides the AMD GPU backend implementations for matmuls.
* [â€‹`sm100`](./sm100/): Provides the Nvidia Blackwell backend implementations for matmuls.
* [â€‹`sm100_structured`](./sm100_structured/): SM100 Structured Matmul - Refactored with encapsulated pipeline management.
* [â€‹`sm80`](./sm80/): Provides the CPU Hopper backend implementations for matmuls.
* [â€‹`sm90`](./sm90/): Provides the Nvidia Hopper backend implementations for matmuls.

## Modules

* [â€‹`profiler`](./profiler/):
* [â€‹`tile_scheduler`](./tile_scheduler/):
* [â€‹`tile_scheduler_splitk`](./tile_scheduler_splitk/):

## Functions

* [â€‹`matmul_kernel`](./matmul_kernel): Matrix Multiplication using shared memory. This version loads blocks of size tile\_size x tile\_size from A and B and updates a tile\_size x tile\_size in C. The thread block should have shape (tile\_size, tile\_size, 1). Each thread is mapped one element in C. The grid should have shape (N/tile\_size, M/tile\_size, 1). N is the first dimension for coalesced access.
* [â€‹`matmul_kernel_naive`](./matmul_kernel_naive):
* [â€‹`multistage_gemm`](./multistage_gemm):
* [â€‹`split_k_reduce`](./split_k_reduce):

</section>

---

## matmul_kernel

<section class='mojo-docs'>

`matmul_kernel[c_type: DType, a_type: DType, b_type: DType, tile_size: Int, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, s_type: DType = get_accum_type[c_type]()](c_ptr: LegacyUnsafePointer[Scalar[c_type]], a_ptr: LegacyUnsafePointer[Scalar[a_type]], b_ptr: LegacyUnsafePointer[Scalar[b_type]], m: Int, n: Int, k: Int)`

Matrix Multiplication using shared memory. This version loads blocks of size tile\_size x tile\_size from A and B and updates a tile\_size x tile\_size in C. The thread block should have shape (tile\_size, tile\_size, 1). Each thread is mapped one element in C. The grid should have shape (N/tile\_size, M/tile\_size, 1). N is the first dimension for coalesced access.

</section>

---

## matmul_kernel_naive

<section class='mojo-docs'>

`matmul_kernel_naive[c_type: DType, a_type: DType, b_type: DType, c_layout: Layout, a_layout: Layout, b_layout: Layout, BLOCK_DIM: Int, transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, s_type: DType = get_accum_type[c_type]()](c: LayoutTensor[c_type, c_layout, MutAnyOrigin], a: LayoutTensor[a_type, a_layout, MutAnyOrigin], b: LayoutTensor[b_type, b_layout, MutAnyOrigin], m: Int, n: Int, k: Int)`

</section>

---

## multistage_gemm

<section class='mojo-docs'>

`multistage_gemm[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, //, *, transpose_b: Bool, config: MatmulConfig[a_type, b_type, c_type, transpose_b], elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: NDBuffer[c_type, 2, origin, c_shape], a: NDBuffer[a_type, 2, origin, a_shape], b: NDBuffer[b_type, 2, origin, b_shape], ctx: DeviceContext)`

`multistage_gemm[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, //, *, transpose_b: Bool, config: MatmulConfig[a_type, b_type, c_type, transpose_b], elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: NDBuffer[c_type, 2, origin, c_shape], a: NDBuffer[a_type, 2, origin, a_shape], b: NDBuffer[b_type, 2, origin, b_shape], runtime_config: MatmulConfig[a_type, b_type, c_type, transpose_b], ctx: DeviceContext)`

</section>

---

## BlackwellProfileWarp

<section class='mojo-docs'>

`struct BlackwellProfileWarp[load_warps: UInt32, mma_warps: UInt32, scheduler_warps: UInt32, epilogue_warps: UInt32, max_entries_per_warp: UInt32, //, WorkspaceManager: BlackwellWarpProfilingWorkspaceManager[load_warps, mma_warps, scheduler_warps, epilogue_warps, max_entries_per_warp], warp_role: UInt32 = 0]`

This struct calculates execution time for a warp/s, and writes a single entry to the workspace.

## Fields

* â€‹timeline (`Tuple[UInt64, UInt64]`):
* â€‹workspace (`Span[UInt64, MutAnyOrigin]`):
* â€‹entry\_idx (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = False`

### `__del__is_trivial`

`comptime __del__is_trivial = False`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = False`

### `enable_profiling`

`comptime enable_profiling = (max_entries_per_warp > 0)`

## Methods

### `__init__`

`__init__(out self, workspace: Span[UInt64, MutAnyOrigin], entry_idx: UInt32)`

### `__enter__`

`__enter__(mut self)`

### `__exit__`

`__exit__(mut self)`

</section>

---

## BlackwellWarpProfilingWorkspaceManager

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct BlackwellWarpProfilingWorkspaceManager[load_warps: UInt32, mma_warps: UInt32, scheduler_warps: UInt32, epilogue_warps: UInt32, max_entries_per_warp: UInt32]`

This struct manages the profiling workspace. The workspaces consists of equal sized chunks, the total number of which is equal to the total number of active SMs. Each SM chunk consists of sequences of entries, with a maximum number of entries per warp role.

Template Parameters:
load\_warps: Number of warps specialized for load operations
mma\_warps: Number of warps specialized for matrix multiply-accumulate operations
scheduler\_warps: Number of warps specialized for scheduling operations
epilogue\_warps: Number of warps specialized for epilogue operations
max\_entries\_per\_warp: Maximum number of entries per warp (common across all warp roles)

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `entries_per_sm`

`comptime entries_per_sm = max_entries_per_warp.__rmul__[DType.uint32, 1](BlackwellWarpProfilingWorkspaceManager[load_warps, mma_warps, scheduler_warps, epilogue_warps, max_entries_per_warp].total_warp_roles)`

### `header`

`comptime header = "time_start,time_end,sm_id,block_idx_x,block_idx_y,role,entry_idx\n"`

### `sm_count`

`comptime sm_count = B200.sm_count`

### `total_data_points`

`comptime total_data_points = 7`

### `total_warp_roles`

`comptime total_warp_roles = 4`

## Methods

### `get_workspace`

`static get_workspace(ctx: DeviceContext) -> Span[UInt64, MutAnyOrigin]`

**Returns:**

[`Span`](/mojo/std/memory/span/Span)

### `write_to_workspace`

`static write_to_workspace[warp_role: UInt32](sm_idx: UInt32, entry_idx: UInt32, workspace: Span[UInt64, MutAnyOrigin], timeline: Tuple[UInt64, UInt64])`

### `dump_workspace_as_csv`

`static dump_workspace_as_csv(ctx: DeviceContext, workspace: Span[UInt64, MutAnyOrigin], filename: StringSlice[StaticConstantOrigin])`

</section>

---

## profiler

<section class='mojo-docs'>

## `comptime` values

### `MatmulProfileWarp`

`comptime MatmulProfileWarp[warp_role: UInt32, max_entries_per_warp: UInt32] = BlackwellProfileWarp[BlackwellWarpProfilingWorkspaceManager[1, 1, 1, 4, max_entries_per_warp](), warp_role]`

#### Parameters

* â€‹warp\_role ([`UInt32`](/std/builtin/simd/#uint32)):
* â€‹max\_entries\_per\_warp ([`UInt32`](/std/builtin/simd/#uint32)):

### `MatmulWarpSpecializationWorkSpaceManager`

`comptime MatmulWarpSpecializationWorkSpaceManager[max_entries_per_warp: UInt32] = BlackwellWarpProfilingWorkspaceManager[1, 1, 1, 4, max_entries_per_warp]`

#### Parameters

* â€‹max\_entries\_per\_warp ([`UInt32`](/std/builtin/simd/#uint32)):

## Structs

* [â€‹`BlackwellProfileWarp`](./BlackwellProfileWarp): This struct calculates execution time for a warp/s, and writes a single entry to the workspace.
* [â€‹`BlackwellWarpProfilingWorkspaceManager`](./BlackwellWarpProfilingWorkspaceManager): This struct manages the profiling workspace. The workspaces consists of equal sized chunks, the total number of which is equal to the total number of active SMs. Each SM chunk consists of sequences of entries, with a maximum number of entries per warp role.

</section>

---

## B200BlockScaledMatmulSmem

<section class='mojo-docs'>

`struct B200BlockScaledMatmulSmem[a_type: DType, b_type: DType, c_type: DType, sfa_dtype: DType, sfb_dtype: DType, transpose_b: Bool, *, config: BlockScaledMatmulConfig[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b]]`

## Fields

* â€‹a\_smem (`InlineArray[B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].AType, B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].a_smem_size]`):
* â€‹b\_smem (`InlineArray[B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].BType, B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].b_smem_size]`):
* â€‹c\_smem (`InlineArray[B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].CType, B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].c_smem_size]`):
* â€‹sfa\_smem (`InlineArray[B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].AScalesType, B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].sfa_smem_size]`):
* â€‹sfb\_smem (`InlineArray[B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].BScalesType, B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].sfb_smem_size]`):
* â€‹tma\_mma\_mbars (`InlineArray[SharedMemBarrier, (Int(B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].num_group_pipeline_stages) * 2)]`):
* â€‹accum\_mbars (`InlineArray[SharedMemBarrier, (Int(config) * 2)]`):
* â€‹clc\_mbars\_full (`InlineArray[SharedMemBarrier, Int(config)]`):
* â€‹clc\_mbars\_empty (`InlineArray[SharedMemBarrier, Int(config)]`):
* â€‹clc\_throttle\_mbars (`InlineArray[SharedMemBarrier, (Int(config) * 2)]`):
* â€‹clc\_response (`InlineArray[UInt128, Int(config)]`):
* â€‹tmem\_dealloc\_mbar (`InlineArray[SharedMemBarrier, 1]`):
* â€‹tmem\_addr (`InlineArray[UInt32, 1]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `a_smem_size`

`comptime a_smem_size = ((B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].BM * B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].BK) * Int(config))`

### `AScalesType`

`comptime AScalesType = Scalar[sfa_dtype]`

### `AType`

`comptime AType = Scalar[a_type]`

### `b_smem_size`

`comptime b_smem_size = ((B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].BN * B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].BK) * Int(config))`

### `BK`

`comptime BK = config.block_tile_shape.__getitem__[3, DType.int64, Int](2)`

### `BM`

`comptime BM = config.block_tile_shape.__getitem__[3, DType.int64, Int](0)`

### `BN`

`comptime BN = config.block_tile_shape.__getitem__[3, DType.int64, Int](1)`

### `BScalesType`

`comptime BScalesType = Scalar[sfb_dtype]`

### `BType`

`comptime BType = Scalar[b_type]`

### `c_smem_size`

`comptime c_smem_size = ((B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].OutputM * B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].OutputN) * Int(config))`

### `CType`

`comptime CType = Scalar[c_type]`

### `MMA_K`

`comptime MMA_K = config.mma_shape.__getitem__[3, DType.int64, Int](2)`

### `MMA_M`

`comptime MMA_M = config.mma_shape.__getitem__[3, DType.int64, Int](0)`

### `MMA_N`

`comptime MMA_N = config.mma_shape.__getitem__[3, DType.int64, Int](1)`

### `num_group_pipeline_stages`

`comptime num_group_pipeline_stages = (config // config)`

### `OutputM`

`comptime OutputM = config.output_tile_shape.__getitem__[2, DType.int64, Int](0)`

### `OutputN`

`comptime OutputN = config.output_tile_shape.__getitem__[2, DType.int64, Int](1)`

### `sf_block_atom_size`

`comptime sf_block_atom_size = (((load_from_mem SF_ATOM_M.__getitem__[Int, Int, 0]()) * (load_from_mem SF_ATOM_M.__getitem__[Int, Int, 1]())) * 4)`

### `sfa_smem_size`

`comptime sfa_smem_size = (((B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].BM // SF_MN_GROUP_SIZE) * B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].sf_block_atom_size) * Int(config))`

### `sfb_smem_size`

`comptime sfb_smem_size = (((B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].MMA_N // SF_MN_GROUP_SIZE) * B200BlockScaledMatmulSmem[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b, config=config].sf_block_atom_size) * Int(config))`

</section>

---

## blackwell_block_scaled_matmul_tma_umma_warp_specialized

<section class='mojo-docs'>

`blackwell_block_scaled_matmul_tma_umma_warp_specialized[c_type: DType, c_layout: Layout, a_type: DType, a_layout: Layout, b_type: DType, b_layout: Layout, sfa_dtype: DType, sfa_layout: Layout, sfb_dtype: DType, sfb_layout: Layout, transpose_b: Bool, *, config: BlockScaledMatmulConfig[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b], elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, pdl_level: PDLLevel = PDLLevel(), max_profiled_tiles_per_SM: OptionalReg[UInt32] = None](c_tensor: LayoutTensor[c_type, c_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_tensor: LayoutTensor[a_type, a_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_tensor: LayoutTensor[b_type, b_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_scales_tensor: LayoutTensor[sfa_dtype, sfa_layout, MutAnyOrigin], b_scales_tensor: LayoutTensor[sfb_dtype, sfb_layout, MutAnyOrigin], ctx: DeviceContext)`

</section>

---

## blackwell_block_scaled_tma_umma_warp_specialized_kernel

<section class='mojo-docs'>

`blackwell_block_scaled_tma_umma_warp_specialized_kernel[a_type: DType, b_type: DType, c_type: DType, sfa_dtype: DType, sfb_dtype: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, sfa_tile_layout: Layout, sfb_tile_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, c_desc_layout: Layout, sfa_desc_layout: Layout, sfb_desc_layout: Layout, transpose_b: Bool, config: BlockScaledMatmulConfig[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b], cluster_shape: StaticTuple[Int32, 3] = StaticTuple[Int32, 3](1), elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, pdl_level: PDLLevel = PDLLevel(), max_profiled_tiles_per_SM: UInt32 = 0](a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], sfa_tma_op: TMATensorTile[sfa_dtype, sfa_tile_layout, sfa_desc_layout], sfb_tma_op: TMATensorTile[sfb_dtype, sfb_tile_layout, sfb_desc_layout], cluster_dim: StaticTuple[Int32, 3], mnk: StaticTuple[UInt32, 3], workspace: Span[UInt64, MutAnyOrigin])`

</section>

---

## consumer_main_loop (Block_scaled_matmul)

<section class='mojo-docs'>

`consumer_main_loop[accum_type: DType, c_type: DType, a_type: DType, b_type: DType, sfa_dtype: DType, sfb_dtype: DType, a_smem_layout: Layout, b_smem_layout: Layout, sfa_smem_layout: Layout, sfb_smem_layout: Layout, a_swizzle: TensorMapSwizzle, b_swizzle: TensorMapSwizzle, transpose_b: Bool, pipeline_stages: Int, /, *, block_tile_shape: IndexList[3], mma_shape: IndexList[3], SFA_NUM_COLS: Int, SFB_NUM_COLS: Int, cta_group: Int = 1, cluster_shape: IndexList[3] = Index(1, 1, 1), k_group_size: UInt = 1](tmem_addr: UInt32, sfa_tmem: UInt32, sfb_tmem: UInt32, a_smem_iter: LayoutTensorIter[a_type, a_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], b_smem_iter: LayoutTensorIter[b_type, b_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], sfa_smem_iter: LayoutTensorIter[sfa_dtype, sfa_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], sfb_smem_iter: LayoutTensorIter[sfb_dtype, sfb_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], load_mma_pipeline: ProducerConsumerPipeline[pipeline_stages], mma_op: MmaOpSM100_BlockScaled_SS[c_type, a_type, b_type, sfa_dtype, sfb_dtype, block_tile_shape, mma_shape, accum_type=accum_type, cta_group=cta_group, cluster_shape=cluster_shape, a_swizzle=a_swizzle, b_swizzle=b_swizzle, transpose_b=transpose_b], elect_one_warp: Bool, iter_idx: UInt32, k_start: UInt32)`

</section>

---

## copy_accum_to_gmem

<section class='mojo-docs'>

`copy_accum_to_gmem[c_type: DType, c_layout: Layout, c_smem_layout: Layout, c_desc_layout: Layout, num_accum_pipeline_stages: Int, /, *, repeat: Int, accum_type: DType, cta_group: Int, epilogue_dtype: DType, block_tile_shape: IndexList[3], mma_shape: IndexList[3], num_output_warps: UInt, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, transpose_c: Bool = False](c_iter: LayoutTensorIter[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], mma_output_pipeline: ProducerConsumerPipeline[num_accum_pipeline_stages], mma_output_stage: UInt32, tmem_offset: UInt32, c_coord: Tuple[UInt32, UInt32, UInt32], c_shape: Tuple[UInt32, UInt32])`

</section>

---

## copy_sf_tmem

<section class='mojo-docs'>

`copy_sf_tmem[sf_dtype: DType, sf_smem_layout: Layout, TILE_MN: Int, cta_group: Int](sf_smem: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], sf_tmem: UInt32)`

</section>

---

## block_scaled_matmul (Block_scaled_matmul)

<section class='mojo-docs'>

## Structs

* [â€‹`B200BlockScaledMatmulSmem`](./B200BlockScaledMatmulSmem):

## Functions

* [â€‹`blackwell_block_scaled_matmul_tma_umma_warp_specialized`](./blackwell_block_scaled_matmul_tma_umma_warp_specialized):
* [â€‹`blackwell_block_scaled_tma_umma_warp_specialized_kernel`](./blackwell_block_scaled_tma_umma_warp_specialized_kernel):
* [â€‹`consumer_main_loop`](./consumer_main_loop):
* [â€‹`copy_accum_to_gmem`](./copy_accum_to_gmem):
* [â€‹`copy_sf_tmem`](./copy_sf_tmem):
* [â€‹`load_AB`](./load_AB):
* [â€‹`multi_stage_store_C`](./multi_stage_store_C):

</section>

---

## load_AB (Block_scaled_matmul)

<section class='mojo-docs'>

`load_AB[a_type: DType, b_type: DType, sfa_dtype: DType, sfb_dtype: DType, a_layout: Layout, b_layout: Layout, sfa_layout: Layout, sfb_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, sfa_desc_layout: Layout, sfb_desc_layout: Layout, a_smem_layout: Layout, b_smem_layout: Layout, sfa_smem_layout: Layout, sfb_smem_layout: Layout, num_pipeline_stages: UInt, /, *, block_tile_shape: IndexList[3], mma_shape: IndexList[3], cta_group: Int = 1, k_group_size: UInt = 1](a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], sfa_tma_op: TMATensorTile[sfa_dtype, sfa_layout, sfa_desc_layout], sfb_tma_op: TMATensorTile[sfb_dtype, sfb_layout, sfb_desc_layout], a_smem: LayoutTensorIter[a_type, a_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], b_smem: LayoutTensorIter[b_type, b_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], sfa_smem: LayoutTensorIter[sfa_dtype, sfa_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], sfb_smem: LayoutTensorIter[sfb_dtype, sfb_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], load_mma_pipeline: ProducerConsumerPipeline[Int(num_pipeline_stages)], peer_cta_coord: Tuple[UInt, UInt, UInt], work_tile_coord: Tuple[UInt, UInt, UInt], a_multicast_mask: UInt16, b_multicast_mask: UInt16, iter_idx: UInt32, elect_one_cta: Bool)`

</section>

---

## multi_stage_store_C (Block_scaled_matmul)

<section class='mojo-docs'>

`multi_stage_store_C[c_type: DType, c_smem_layout: Layout, c_layout: Layout, c_desc_layout: Layout, num_accum_pipeline_stages: UInt, /, *, input_type: DType, accum_type: DType, block_tile_shape: IndexList[3], mma_shape: IndexList[3], stage_stride_cols: UInt, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, cta_group: Int = 1, num_output_warps: UInt = 4, max_tmem_cols: UInt = 512, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, transpose_c: Bool = False](c_iter: LayoutTensorIter[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], mma_output_pipeline: ProducerConsumerPipeline[Int(num_accum_pipeline_stages)], tmem_addr: UInt32, work_tile_coord: Tuple[UInt32, UInt32, UInt32], elect_one_warp: Bool, M: UInt32, N: UInt32)`

</section>

---

## blockwise_fp8

<section class='mojo-docs'>

## `comptime` values

### `logger`

`comptime logger = Logger[DEFAULT_LEVEL](stdout, "", False)`

### `smem_layout_3D`

`comptime smem_layout_3D[layout: Layout] = Layout(IntTuple(IntTuple(1), layout.shape[0].owned_copy(), layout.shape[1].owned_copy(), Tuple[]()), IntTuple(IntTuple(0), layout.stride[0].owned_copy(), layout.stride[1].owned_copy(), Tuple[]()))`

#### Parameters

* â€‹layout ([`Layout`](/kernels/layout/layout/Layout)):

## Functions

* [â€‹`matmul_sm100_blockwise_scaled_fp8`](./matmul_sm100_blockwise_scaled_fp8):
* [â€‹`matmul_sm100_blockwise_scaled_fp8_1d2d_kernel`](./matmul_sm100_blockwise_scaled_fp8_1d2d_kernel):
* [â€‹`matmul_sm100_blockwise_scaled_fp8_1d2d_wrapper`](./matmul_sm100_blockwise_scaled_fp8_1d2d_wrapper):

</section>

---

## matmul_sm100_blockwise_scaled_fp8

<section class='mojo-docs'>

`matmul_sm100_blockwise_scaled_fp8[a_layout: Layout, b_layout: Layout, c_layout: Layout, a_scales_layout: Layout, b_scales_layout: Layout, c_type: DType, a_type: DType, b_type: DType, a_scales_type: DType, b_scales_type: DType, *, transpose_b: Bool, umma_shape: IndexList[3], block_tile_shape: IndexList[3], a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, c_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[a_type, a_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[b_type, b_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_scales: LayoutTensor[a_scales_type, a_scales_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_scales: LayoutTensor[b_scales_type, b_scales_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

</section>

---

## matmul_sm100_blockwise_scaled_fp8_1d2d_kernel

<section class='mojo-docs'>

`matmul_sm100_blockwise_scaled_fp8_1d2d_kernel[a_type: DType, b_type: DType, c_type: DType, a_scales_type: DType, b_scales_type: DType, a_layout: Layout, c_layout: Layout, a_scales_layout: Layout, b_scales_layout: Layout, a_tile_layout: Layout, b_tile_layout: Layout, a_scales_tile_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, a_scales_desc_layout: Layout, block_tile_shape: IndexList[3], mma_shape: IndexList[3], transpose_b: Bool = True, cluster_shape: StaticTuple[Int32, 3] = StaticTuple[Int32, 3](1, 1, 1), a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, num_threads: UInt = 128, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](a_tma_op: TMATensorTile[a_type, a_tile_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_tile_layout, b_desc_layout], c: LayoutTensor[c_type, c_layout, MutAnyOrigin], a_scales_tma_op: TMATensorTile[a_scales_type, a_scales_tile_layout, a_scales_desc_layout], b_scales: LayoutTensor[b_scales_type, b_scales_layout, MutAnyOrigin], num_iters: UInt)`

</section>

---

## matmul_sm100_blockwise_scaled_fp8_1d2d_wrapper

<section class='mojo-docs'>

`matmul_sm100_blockwise_scaled_fp8_1d2d_wrapper[a_type: DType, b_type: DType, c_type: DType, a_scales_type: DType, b_scales_type: DType, a_layout: Layout, c_layout: Layout, a_scales_layout: Layout, b_scales_layout: Layout, a_tile_layout: Layout, b_tile_layout: Layout, a_scales_tile_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, a_scales_desc_layout: Layout, block_tile_shape: IndexList[3], mma_shape: IndexList[3], transpose_b: Bool = True, cluster_shape: StaticTuple[Int32, 3] = StaticTuple[Int32, 3](1, 1, 1), a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, num_threads: UInt = 128, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](a_tma_op: TMATensorTile[a_type, a_tile_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_tile_layout, b_desc_layout], c: LayoutTensor[c_type, c_layout, MutAnyOrigin], a_scales_tma_op: TMATensorTile[a_scales_type, a_scales_tile_layout, a_scales_desc_layout], b_scales: LayoutTensor[b_scales_type, b_scales_layout, MutAnyOrigin], num_iters: UInt)`

</section>

---

## LoadOp

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `args_type`

`comptime args_type`

### `device_type`

`comptime device_type`

Indicate the type being used on accelerator devices.

## Required methods

### `__init__`

`__init__(out self: _Self, args: _Self.args_type)`

**Returns:**

`_Self`

### `__call__`

`__call__(self: _Self, a_smem_tile: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_smem_tile: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], m: UInt32, n: UInt32, k: UInt32, ref [3] mbar: SharedMemBarrier)`

### `get_type_name`

`static get_type_name() -> String`

Gets the name of the host type (the one implementing this trait). For example, Int would return "Int", DeviceBuffer\[DType.float32] would return "DeviceBuffer\[DType.float32]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): The host type's name.

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name. For example, because DeviceBuffer's device\_type is UnsafePointer, DeviceBuffer\[DType.float32]'s get\_device\_type\_name() should return something like "UnsafePointer\[Scalar\[DType.float32]]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): The device type's name.

</section>

---

## MmaOp (Composable)

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

## Required methods

### `__call__`

`__call__(self: _Self)`

</section>

---

## OpArgs

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__copyinit__` is trivial.

The implementation of `__copyinit__` is considered to be trivial if:

* The struct has a compiler-generated trivial `__copyinit__` and all its fields
  have a trivial `__copyinit__` method.

In practice, it means the value can be copied by copying the bits from
one location to another without side effects.

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__moveinit__` is trivial.

The implementation of `__moveinit__` is considered to be trivial if:

* The struct has a compiler-generated `__moveinit__` and all its fields
  have a trivial `__moveinit__` method.

In practice, it means the value can be moved by moving the bits from
one location to another without side effects.

## Required methods

### `__copyinit__`

`__copyinit__(out self: _Self, existing: _Self, /)`

Create a new instance of the value by copying an existing one.

**Args:**

* â€‹existing (`_Self`): The value to copy.

**Returns:**

`_Self`

### `__moveinit__`

`__moveinit__(out self: _Self, deinit existing: _Self, /)`

Create a new instance of the value by moving the value of another.

**Args:**

* â€‹existing (`_Self`): The value to move.

**Returns:**

`_Self`

## Provided methods

### `copy`

`copy(self: _Self) -> _Self`

Explicitly construct a copy of self.

**Returns:**

`_Self`: A copy of this value.

</section>

---

## OutputOp

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `args_type`

`comptime args_type`

### `device_type`

`comptime device_type`

Indicate the type being used on accelerator devices.

## Required methods

### `__init__`

`__init__(out self: _Self, args: _Self.args_type)`

**Returns:**

`_Self`

### `__call__`

`__call__(self: _Self, tmem_addr: UInt32)`

### `get_type_name`

`static get_type_name() -> String`

Gets the name of the host type (the one implementing this trait). For example, Int would return "Int", DeviceBuffer\[DType.float32] would return "DeviceBuffer\[DType.float32]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): The host type's name.

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name. For example, because DeviceBuffer's device\_type is UnsafePointer, DeviceBuffer\[DType.float32]'s get\_device\_type\_name() should return something like "UnsafePointer\[Scalar\[DType.float32]]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): The device type's name.

</section>

---

## Pipeline

<section class='mojo-docs'>

`struct Pipeline[a_type: DType, b_type: DType, c_type: DType, block_tile_shape: IndexList[3], mma_shape: IndexList[3], a_swizzle: TensorMapSwizzle, b_swizzle: TensorMapSwizzle, loadop_t: LoadOp, outputop_t: OutputOp]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`PipelineOp`](/mojo/kernels/linalg/matmul/gpu/sm100/composable/PipelineOp)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `args_type`

`comptime args_type = PipelineArgs[loadop_t, outputop_t]`

## Methods

### `run`

`static run(args: PipelineArgs[loadop_t, outputop_t])`

</section>

---

## PipelineArgs

<section class='mojo-docs'>

`struct PipelineArgs[loadop_t: LoadOp, outputop_t: OutputOp]`

## Fields

* â€‹load\_args (`loadop_t.args_type`):
* â€‹output\_args (`outputop_t.args_type`):
* â€‹num\_iters (`UInt`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`OpArgs`](/mojo/kernels/linalg/matmul/gpu/sm100/composable/OpArgs)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True if outputop_t.args_type.__copyinit__is_trivial if loadop_t.args_type.__copyinit__is_trivial else loadop_t.args_type.__copyinit__is_trivial else outputop_t.args_type.__copyinit__is_trivial if loadop_t.args_type.__copyinit__is_trivial else loadop_t.args_type.__copyinit__is_trivial`

### `__del__is_trivial`

`comptime __del__is_trivial = True if outputop_t.args_type.__del__is_trivial if loadop_t.args_type.__del__is_trivial else loadop_t.args_type.__del__is_trivial else outputop_t.args_type.__del__is_trivial if loadop_t.args_type.__del__is_trivial else loadop_t.args_type.__del__is_trivial`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True if outputop_t.args_type.__moveinit__is_trivial if loadop_t.args_type.__moveinit__is_trivial else loadop_t.args_type.__moveinit__is_trivial else outputop_t.args_type.__moveinit__is_trivial if loadop_t.args_type.__moveinit__is_trivial else loadop_t.args_type.__moveinit__is_trivial`

### `device_type`

`comptime device_type = PipelineArgs[loadop_t, outputop_t]`

## Methods

### `__init__`

`__init__(out self, load_args: loadop_t.args_type, output_args: outputop_t.args_type, num_iters: UInt)`

### `get_type_name`

`static get_type_name() -> String`

Gets this type's name, for use in error messages when handing arguments to kernels.

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name, for use in error messages when handing arguments to kernels.

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

</section>

---

## PipelineOp

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `args_type`

`comptime args_type`

## Required methods

### `run`

`static run(args: _Self.args_type)`

</section>

---

## R2GOutputOp

<section class='mojo-docs'>

`struct R2GOutputOp[accum_type: DType, dtype: DType, layout: Layout, num_threads: Int, mma_shape: IndexList[3], block_tile_shape: IndexList[3], o: MutOrigin]`

## Fields

* â€‹c (`LayoutTensor[dtype, layout, o]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`OutputOp`](/mojo/kernels/linalg/matmul/gpu/sm100/composable/OutputOp)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `args_type`

`comptime args_type = STOutputOpArgs[dtype, layout, o]`

### `device_type`

`comptime device_type = R2GOutputOp[accum_type, dtype, layout, num_threads, mma_shape, block_tile_shape, o]`

## Methods

### `__init__`

`__init__(out self, args: STOutputOpArgs[dtype, layout, o])`

### `get_type_name`

`static get_type_name() -> String`

Gets this type's name, for use in error messages when handing arguments to kernels.

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name, for use in error messages when handing arguments to kernels.

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `to_kernel_args`

`static to_kernel_args(c: LayoutTensor[dtype, layout, o], ctx: DeviceContext) -> R2GOutputOp[accum_type, dtype, layout, num_threads, mma_shape, block_tile_shape, o].args_type`

**Returns:**

`R2GOutputOp`

### `__call__`

`__call__(self, tmem_addr: UInt32)`

</section>

---

## STOutputOpArgs

<section class='mojo-docs'>

`struct STOutputOpArgs[mut: Bool, //, dtype: DType, layout: Layout, sb: Origin[mut=mut]]`

## Fields

* â€‹c (`LayoutTensor[dtype, layout, sb]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`OpArgs`](/mojo/kernels/linalg/matmul/gpu/sm100/composable/OpArgs)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = False`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(out self, c: LayoutTensor[dtype, layout, sb])`

### `__copyinit__`

`__copyinit__(out self, other: Self)`

</section>

---

## TMALoadOp

<section class='mojo-docs'>

`struct TMALoadOp[a_type: DType, b_type: DType, block_tile_shape: IndexList[3], cluster_shape: IndexList[3], a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B]`

## Fields

* â€‹a\_tma\_ptr (`LegacyUnsafePointer[TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].a_tma_type]`):
* â€‹b\_tma\_ptr (`LegacyUnsafePointer[TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].b_tma_type]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`LoadOp`](/mojo/kernels/linalg/matmul/gpu/sm100/composable/LoadOp),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `a_tma_desc_layout`

`comptime a_tma_desc_layout = _tma_desc_tile_layout[a_type, 2, Index((block_tile_shape.__getitem__[3, DType.int64, Int](0) // cluster_shape.__getitem__[3, DType.int64, Int](0)), block_tile_shape.__getitem__[3, DType.int64, Int](2)), a_swizzle]()`

### `a_tma_layout`

`comptime a_tma_layout = Layout.row_major((block_tile_shape.__getitem__[3, DType.int64, Int](0) // cluster_shape.__getitem__[3, DType.int64, Int](0)), block_tile_shape.__getitem__[3, DType.int64, Int](2))`

### `a_tma_type`

`comptime a_tma_type = TMATensorTile[a_type, TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].a_tma_layout, TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].a_tma_desc_layout]`

### `args_type`

`comptime args_type = TMALoadOpArgs[a_type, b_type, TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].a_tma_layout, TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].b_tma_layout, TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].a_tma_desc_layout, TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].b_tma_desc_layout]`

### `b_tma_desc_layout`

`comptime b_tma_desc_layout = _tma_desc_tile_layout[b_type, 2, Index((block_tile_shape.__getitem__[3, DType.int64, Int](1) // cluster_shape.__getitem__[3, DType.int64, Int](1)), block_tile_shape.__getitem__[3, DType.int64, Int](2)), b_swizzle]()`

### `b_tma_layout`

`comptime b_tma_layout = Layout.row_major((block_tile_shape.__getitem__[3, DType.int64, Int](1) // cluster_shape.__getitem__[3, DType.int64, Int](1)), block_tile_shape.__getitem__[3, DType.int64, Int](2))`

### `b_tma_type`

`comptime b_tma_type = TMATensorTile[b_type, TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].b_tma_layout, TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].b_tma_desc_layout]`

### `device_type`

`comptime device_type = TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle]`

## Methods

### `__init__`

`__init__(out self, args: TMALoadOpArgs[a_type, b_type, TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].a_tma_layout, TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].b_tma_layout, TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].a_tma_desc_layout, TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].b_tma_desc_layout])`

### `get_type_name`

`static get_type_name() -> String`

Gets this type's name, for use in error messages when handing arguments to kernels.

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name, for use in error messages when handing arguments to kernels.

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `to_kernel_args`

`static to_kernel_args(a: LayoutTensor[a_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[b_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext) -> TMALoadOp[a_type, b_type, block_tile_shape, cluster_shape, a_swizzle, b_swizzle].args_type`

**Returns:**

`TMALoadOp`

### `__call__`

`__call__(self, a_smem_tile: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_smem_tile: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], m: UInt32, n: UInt32, k: UInt32, ref [3] mbar: SharedMemBarrier)`

</section>

---

## TMALoadOpArgs

<section class='mojo-docs'>

`struct TMALoadOpArgs[a_type: DType, b_type: DType, a_layout: Layout, b_layout: Layout, a_desc_layout: Layout = a_layout, b_desc_layout: Layout = b_layout]`

## Fields

* â€‹a\_tma\_op (`TMATensorTile[a_type, a_layout, a_desc_layout]`):
* â€‹b\_tma\_op (`TMATensorTile[b_type, b_layout, b_desc_layout]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`OpArgs`](/mojo/kernels/linalg/matmul/gpu/sm100/composable/OpArgs)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = False`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(out self, a: TMATensorTile[a_type, a_layout, a_desc_layout], b: TMATensorTile[b_type, b_layout, b_desc_layout])`

### `__copyinit__`

`__copyinit__(out self, other: Self)`

</section>

---

## composable

<section class='mojo-docs'>

## Structs

* [â€‹`Pipeline`](./Pipeline):
* [â€‹`PipelineArgs`](./PipelineArgs):
* [â€‹`R2GOutputOp`](./R2GOutputOp):
* [â€‹`STOutputOpArgs`](./STOutputOpArgs):
* [â€‹`TMALoadOp`](./TMALoadOp):
* [â€‹`TMALoadOpArgs`](./TMALoadOpArgs):

## Traits

* [â€‹`LoadOp`](./LoadOp):
* [â€‹`MmaOp`](./MmaOp):
* [â€‹`OpArgs`](./OpArgs):
* [â€‹`OutputOp`](./OutputOp):
* [â€‹`PipelineOp`](./PipelineOp):

## Functions

* [â€‹`matmul_kernel`](./matmul_kernel):
* [â€‹`matmul_sm100`](./matmul_sm100):

</section>

---

## matmul_kernel (Composable)

<section class='mojo-docs'>

`matmul_kernel[pipeline_t: PipelineOp](args: pipeline_t.args_type)`

</section>

---

## matmul_sm100

<section class='mojo-docs'>

`matmul_sm100[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, *, mma_shape: IndexList[3], block_tile_shape: IndexList[3], a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B](c_device: NDBuffer[c_type, 2, origin, c_shape], a_device: NDBuffer[a_type, 2, origin, a_shape], b_device: NDBuffer[b_type, 2, origin, b_shape], ctx: DeviceContext)`

</section>

---

## BlockScaledMatmulConfig

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct BlockScaledMatmulConfig[a_type: DType, b_type: DType, c_type: DType, sfa_dtype: DType, sfb_dtype: DType, transpose_b: Bool = True]`

Static configuration of GPU matmul.

## Fields

* â€‹cta\_group (`Int`):
* â€‹mma\_shape (`IndexList[3]`):
* â€‹cluster\_shape (`IndexList[3]`):
* â€‹AB\_swapped (`Bool`):
* â€‹block\_swizzle\_size (`Int`):
* â€‹raster\_order (`RasterOrder`):
* â€‹block\_tile\_shape (`IndexList[3]`):
* â€‹num\_split\_k (`Int`):
* â€‹num\_pipeline\_stages (`UInt`):
* â€‹num\_clc\_pipeline\_stages (`UInt`):
* â€‹num\_accum\_pipeline\_stages (`UInt`):
* â€‹num\_output\_stages (`UInt`):
* â€‹output\_tile\_shape (`IndexList[2]`):
* â€‹a\_swizzle (`TensorMapSwizzle`):
* â€‹b\_swizzle (`TensorMapSwizzle`):
* â€‹c\_swizzle (`TensorMapSwizzle`):
* â€‹k\_group\_size (`UInt`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Equatable`](/mojo/std/builtin/comparable/Equatable),
[`Hashable`](/mojo/std/hashlib/hash/Hashable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `accum_type`

`comptime accum_type = get_accum_type[a_type]()`

### `sf_block_atom_size`

`comptime sf_block_atom_size = (((load_from_mem SF_ATOM_M.__getitem__[Int, Int, 0]()) * (load_from_mem SF_ATOM_M.__getitem__[Int, Int, 1]())) * 4)`

## Methods

### `__init__`

`__init__(*, cta_group: Int = 2, mma_shape: IndexList[3] = get_mma_shape[a_type, BlockScaledMatmulConfig[a_type, b_type, c_type, sfa_dtype, sfb_dtype, transpose_b].accum_type](), cluster_shape: IndexList[3] = Index(2, 1, 1), AB_swapped: Bool = False, num_split_k: Int = 1, block_swizzle_size: Int = 0, raster_order: RasterOrder = RasterOrder.AlongM, k_group_size: UInt = 1, num_pipeline_stages: Optional[UInt] = None, num_accum_pipeline_stages: UInt = 2, num_clc_pipeline_stages: UInt = 2) -> Self`

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `swap_AB_type`

`swap_AB_type(self) -> BlockScaledMatmulConfig[b_type, a_type, c_type, sfa_dtype, sfb_dtype, transpose_b]`

**Returns:**

[`BlockScaledMatmulConfig`](/mojo/kernels/linalg/matmul/gpu/sm100/config/BlockScaledMatmulConfig)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

### `__repr__`

`__repr__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `__hash__`

`__hash__[H: Hasher](self, mut hasher: H)`

Updates hasher with the underlying bytes.

**Parameters:**

* â€‹H ([`Hasher`](/mojo/std/hashlib/hasher/Hasher)): The hasher type.

**Args:**

* â€‹hasher (`H`): The hasher instance.

</section>

---

## MatmulConfig

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MatmulConfig[a_type: DType, b_type: DType, c_type: DType, transpose_b: Bool = True]`

Static configuration of GPU matmul.

## Fields

* â€‹cta\_group (`Int`):
* â€‹mma\_shape (`IndexList[3]`):
* â€‹cluster\_shape (`IndexList[3]`):
* â€‹AB\_swapped (`Bool`):
* â€‹block\_swizzle\_size (`Int`):
* â€‹raster\_order (`RasterOrder`):
* â€‹block\_tile\_shape (`IndexList[3]`):
* â€‹num\_split\_k (`Int`):
* â€‹num\_pipeline\_stages (`UInt`):
* â€‹num\_clc\_pipeline\_stages (`UInt`):
* â€‹num\_accum\_pipeline\_stages (`UInt`):
* â€‹num\_output\_stages (`UInt`):
* â€‹output\_tile\_shape (`IndexList[2]`):
* â€‹a\_swizzle (`TensorMapSwizzle`):
* â€‹b\_swizzle (`TensorMapSwizzle`):
* â€‹c\_swizzle (`TensorMapSwizzle`):
* â€‹k\_group\_size (`UInt`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Equatable`](/mojo/std/builtin/comparable/Equatable),
[`Hashable`](/mojo/std/hashlib/hash/Hashable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `accum_type`

`comptime accum_type = get_accum_type[a_type]()`

## Methods

### `__init__`

`__init__(*, cta_group: Int = 2, mma_shape: IndexList[3] = get_mma_shape[a_type, MatmulConfig[a_type, b_type, c_type, transpose_b].accum_type](), cluster_shape: IndexList[3] = Index(2, 1, 1), AB_swapped: Bool = False, num_split_k: Int = 1, block_swizzle_size: Int = 0, raster_order: RasterOrder = RasterOrder.AlongM, k_group_size: UInt = 1, num_pipeline_stages: Optional[UInt] = None, num_accum_pipeline_stages: UInt = 2, num_clc_pipeline_stages: UInt = 2) -> Self`

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `swap_AB_type`

`swap_AB_type(self) -> MatmulConfig[b_type, a_type, c_type, transpose_b]`

**Returns:**

[`MatmulConfig`](/mojo/kernels/linalg/utils_gpu/MatmulConfig)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

### `__repr__`

`__repr__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `__hash__`

`__hash__[H: Hasher](self, mut hasher: H)`

Updates hasher with the underlying bytes.

**Parameters:**

* â€‹H ([`Hasher`](/mojo/std/hashlib/hasher/Hasher)): The hasher type.

**Args:**

* â€‹hasher (`H`): The hasher instance.

</section>

---

## build_configs

<section class='mojo-docs'>

`build_configs[a_type: DType, b_type: DType, c_type: DType, N: Int, K: Int, transpose_b: Bool = True]() -> Set[MatmulConfig[a_type, b_type, c_type, transpose_b]]`

**Returns:**

`Set`

</section>

---

## choose_config

<section class='mojo-docs'>

`choose_config[a_type: DType, b_type: DType, c_type: DType, transpose_b: Bool = True](M: Int, N: Int, K: Int) -> MatmulConfig[a_type, b_type, c_type, transpose_b]`

**Returns:**

[`MatmulConfig`](/mojo/kernels/linalg/utils_gpu/MatmulConfig)

</section>

---

## config

<section class='mojo-docs'>

## Structs

* [â€‹`BlockScaledMatmulConfig`](./BlockScaledMatmulConfig): Static configuration of GPU matmul.
* [â€‹`MatmulConfig`](./MatmulConfig): Static configuration of GPU matmul.

## Functions

* [â€‹`build_configs`](./build_configs):
* [â€‹`choose_config`](./choose_config):

</section>

---

## heuristic_and_outliers_dispatch

<section class='mojo-docs'>

`heuristic_and_outliers_dispatch[c_type: DType, a_type: DType, b_type: DType, //, transpose_b: Bool = True, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, pdl_level: PDLLevel = PDLLevel()](c: NDBuffer[c_type, 2, origin, shape], a: NDBuffer[a_type, 2, origin, shape], b: NDBuffer[b_type, 2, origin, shape], ctx: DeviceContext) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## dispatch

<section class='mojo-docs'>

## `comptime` values

### `DISPATCH_HIT`

`comptime DISPATCH_HIT = 1`

### `DISPATCH_MISS`

`comptime DISPATCH_MISS = 0`

### `logger`

`comptime logger = Logger[DEFAULT_LEVEL](stdout, "", False)`

## Functions

* [â€‹`heuristic_and_outliers_dispatch`](./heuristic_and_outliers_dispatch):
* [â€‹`matmul_dispatch_sm100`](./matmul_dispatch_sm100):
* [â€‹`matmul_dispatch_sm100_bf16`](./matmul_dispatch_sm100_bf16):
* [â€‹`matmul_dispatch_sm100_fp8`](./matmul_dispatch_sm100_fp8):

</section>

---

## matmul_dispatch_sm100

<section class='mojo-docs'>

`matmul_dispatch_sm100[c_type: DType, a_type: DType, b_type: DType, transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, pdl_level: PDLLevel = PDLLevel()](c: NDBuffer[c_type, 2, origin, shape], a: NDBuffer[a_type, 2, origin, shape], b: NDBuffer[b_type, 2, origin, shape], ctx: DeviceContext) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

`matmul_dispatch_sm100[c_type: DType, a_type: DType, b_type: DType, transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_lambda_wrapper: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, pdl_level: PDLLevel = PDLLevel()](c: NDBuffer[c_type, 2, origin, shape], a: NDBuffer[a_type, 2, origin, shape], b: NDBuffer[b_type, 2, origin, shape], ctx: DeviceContext)`

</section>

---

## matmul_dispatch_sm100_bf16

<section class='mojo-docs'>

`matmul_dispatch_sm100_bf16[c_type: DType, a_type: DType, b_type: DType, //, transpose_b: Bool = True, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, pdl_level: PDLLevel = PDLLevel()](c: NDBuffer[c_type, 2, origin, shape], a: NDBuffer[a_type, 2, origin, shape], b: NDBuffer[b_type, 2, origin, shape], ctx: DeviceContext) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## matmul_dispatch_sm100_fp8

<section class='mojo-docs'>

`matmul_dispatch_sm100_fp8[c_type: DType, a_type: DType, b_type: DType, //, transpose_b: Bool = True, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, pdl_level: PDLLevel = PDLLevel()](c: NDBuffer[c_type, 2, origin, shape], a: NDBuffer[a_type, 2, origin, shape], b: NDBuffer[b_type, 2, origin, shape], ctx: DeviceContext) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## sm100 (Sm100)

<section class='mojo-docs'>

Provides the Nvidia Blackwell backend implementations for matmuls.

## Modules

* [â€‹`block_scaled_matmul`](./block_scaled_matmul/):
* [â€‹`blockwise_fp8`](./blockwise_fp8/):
* [â€‹`composable`](./composable/):
* [â€‹`config`](./config/):
* [â€‹`dispatch`](./dispatch/):
* [â€‹`matmul`](./matmul/):
* [â€‹`pipeline`](./pipeline/):
* [â€‹`tile_scheduler`](./tile_scheduler/):
* [â€‹`tile_scheduler_splitk`](./tile_scheduler_splitk/):
* [â€‹`tuning_configs`](./tuning_configs/):
* [â€‹`warp_specialized_blockwise_fp8`](./warp_specialized_blockwise_fp8/):

</section>

---

## B200MatmulSmem

<section class='mojo-docs'>

`struct B200MatmulSmem[a_type: DType, b_type: DType, c_type: DType, transpose_b: Bool, *, config: MatmulConfig[a_type, b_type, c_type, transpose_b]]`

## Fields

* â€‹a\_smem (`InlineArray[B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].AType, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].a_smem_size]`):
* â€‹b\_smem (`InlineArray[B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BType, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].b_smem_size]`):
* â€‹c\_smem (`InlineArray[B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].CType, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].c_smem_size]`):
* â€‹tma\_mma\_mbars (`InlineArray[SharedMemBarrier, (Int(B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].num_group_pipeline_stages) * 2)]`):
* â€‹accum\_mbars (`InlineArray[SharedMemBarrier, (Int(config) * 2)]`):
* â€‹clc\_mbars\_full (`InlineArray[SharedMemBarrier, Int(config)]`):
* â€‹clc\_mbars\_empty (`InlineArray[SharedMemBarrier, Int(config)]`):
* â€‹clc\_throttle\_mbars (`InlineArray[SharedMemBarrier, (Int(config) * 2)]`):
* â€‹clc\_response (`InlineArray[UInt128, Int(config)]`):
* â€‹tmem\_dealloc\_mbar (`InlineArray[SharedMemBarrier, 1]`):
* â€‹tmem\_addr (`InlineArray[UInt32, 1]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `a_smem_size`

`comptime a_smem_size = ((B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BM * B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BK) * Int(config))`

### `AType`

`comptime AType = Scalar[a_type]`

### `b_smem_size`

`comptime b_smem_size = ((B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BN * B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BK) * Int(config))`

### `BK`

`comptime BK = config.block_tile_shape.__getitem__[3, DType.int64, Int](2)`

### `BM`

`comptime BM = config.block_tile_shape.__getitem__[3, DType.int64, Int](0)`

### `BN`

`comptime BN = config.block_tile_shape.__getitem__[3, DType.int64, Int](1)`

### `BType`

`comptime BType = Scalar[b_type]`

### `c_smem_size`

`comptime c_smem_size = ((B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].OutputM * B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].OutputN) * Int(config))`

### `CType`

`comptime CType = Scalar[c_type]`

### `num_group_pipeline_stages`

`comptime num_group_pipeline_stages = (config // config)`

### `OutputM`

`comptime OutputM = config.output_tile_shape.__getitem__[2, DType.int64, Int](0)`

### `OutputN`

`comptime OutputN = config.output_tile_shape.__getitem__[2, DType.int64, Int](1)`

</section>

---

## WarpRole (Matmul)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct WarpRole`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `Epilogue`

`comptime Epilogue = WarpRole(3)`

### `MainLoad`

`comptime MainLoad = WarpRole(5)`

### `Mma`

`comptime Mma = WarpRole(6)`

### `Scheduler`

`comptime Scheduler = WarpRole(4)`

## Methods

### `__eq__`

`__eq__(self, other: UInt) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ge__`

`__ge__(self, other: UInt) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_main_load`

`static is_main_load() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_mma`

`static is_mma() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_epilogue`

`static is_epilogue() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_scheduler`

`static is_scheduler() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## accum_arrive

<section class='mojo-docs'>

`accum_arrive[cta_group: Int](mma_output_pipeline: ProducerConsumerPipeline[num_stages], mma_output_stage: UInt32)`

</section>

---

## blackwell_matmul_tma_umma_warp_specialized

<section class='mojo-docs'>

`blackwell_matmul_tma_umma_warp_specialized[c_type: DType, c_layout: Layout, a_type: DType, a_layout: Layout, b_type: DType, b_layout: Layout, transpose_b: Bool, *, config: MatmulConfig[a_type, b_type, c_type, transpose_b], elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, pdl_level: PDLLevel = PDLLevel(), max_profiled_tiles_per_SM: OptionalReg[UInt32] = None](c_device: LayoutTensor[c_type, c_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_device: LayoutTensor[a_type, a_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_device: LayoutTensor[b_type, b_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

</section>

---

## blackwell_tma_umma_warp_specialized_kernel (Matmul)

<section class='mojo-docs'>

`blackwell_tma_umma_warp_specialized_kernel[a_type: DType, b_type: DType, c_type: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, c_desc_layout: Layout, transpose_b: Bool, config: MatmulConfig[a_type, b_type, c_type, transpose_b], cluster_shape: StaticTuple[Int32, 3] = StaticTuple[Int32, 3](1), elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, pdl_level: PDLLevel = PDLLevel(), max_profiled_tiles_per_SM: UInt32 = 0](a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], cluster_dim: StaticTuple[Int32, 3], mnk: StaticTuple[UInt32, 3], workspace: Span[UInt64, MutAnyOrigin])`

</section>

---

## blackwell_tma_umma_warp_specialized_split_k_kernel

<section class='mojo-docs'>

`blackwell_tma_umma_warp_specialized_split_k_kernel[a_type: DType, b_type: DType, c_type: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, reduction_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, c_desc_layout: Layout, transpose_b: Bool, config: MatmulConfig[a_type, b_type, c_type, transpose_b], cluster_shape: StaticTuple[Int32, 3] = StaticTuple[Int32, 3](1), elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, max_profiled_tiles_per_SM: UInt32 = 0](a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], reduction_tensor: LayoutTensor[MatmulConfig[a_type, b_type, c_type, transpose_b].accum_type, reduction_layout, MutAnyOrigin], lock_ptr: LegacyUnsafePointer[UInt8], cluster_dim: StaticTuple[Int32, 3], mnk: StaticTuple[UInt32, 3], workspace: Span[UInt64, MutAnyOrigin])`

</section>

---

## consumer_main_loop (Matmul)

<section class='mojo-docs'>

`consumer_main_loop[accum_type: DType, c_type: DType, a_type: DType, b_type: DType, a_smem_layout: Layout, b_smem_layout: Layout, a_swizzle: TensorMapSwizzle, b_swizzle: TensorMapSwizzle, transpose_b: Bool, pipeline_stages: Int, /, *, block_tile_shape: IndexList[3], mma_shape: IndexList[3], cta_group: Int = 1, cluster_shape: IndexList[3] = Index(1, 1, 1), k_group_size: UInt = 1](tmem_addr: UInt32, a_smem_iter: LayoutTensorIter[a_type, a_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], b_smem_iter: LayoutTensorIter[b_type, b_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], load_mma_pipeline: ProducerConsumerPipeline[pipeline_stages], mma_op: MmaOpSM100_SS[c_type, a_type, b_type, block_tile_shape, mma_shape, accum_type=accum_type, cta_group=cta_group, cluster_shape=cluster_shape, a_swizzle=a_swizzle, b_swizzle=b_swizzle, transpose_b=transpose_b], elect_one_warp: Bool, iter_idx: UInt32, k_start: UInt32)`

</section>

---

## copy_accum_to_gmem (Matmul)

<section class='mojo-docs'>

`copy_accum_to_gmem[c_type: DType, c_layout: Layout, c_smem_layout: Layout, c_desc_layout: Layout, num_accum_pipeline_stages: Int, /, *, repeat: Int, accum_type: DType, cta_group: Int, epilogue_dtype: DType, block_tile_shape: IndexList[3], mma_shape: IndexList[3], num_output_warps: UInt, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, transpose_c: Bool = False](c_iter: LayoutTensorIter[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], mma_output_pipeline: ProducerConsumerPipeline[num_accum_pipeline_stages], mma_output_stage: UInt32, tmem_offset: UInt32, c_coord: Tuple[UInt32, UInt32], c_shape: Tuple[UInt32, UInt32])`

</section>

---

## f32_frag_to_smem

<section class='mojo-docs'>

`f32_frag_to_smem[swizzle_mode: TensorMapSwizzle, stageN: UInt](vec: SIMD[dtype, size], dst: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## matmul (3)

<section class='mojo-docs'>

## `comptime` values

### `RLayout32Bits`

`comptime RLayout32Bits[layout: Layout] = RuntimeLayout[layout, element_type=DType.uint32, linear_idx_type=DType.uint32]`

#### Parameters

* â€‹layout ([`Layout`](/kernels/layout/layout/Layout)):

## Structs

* [â€‹`B200MatmulSmem`](./B200MatmulSmem):
* [â€‹`WarpRole`](./WarpRole):

## Functions

* [â€‹`accum_arrive`](./accum_arrive):
* [â€‹`blackwell_matmul_tma_umma_warp_specialized`](./blackwell_matmul_tma_umma_warp_specialized):
* [â€‹`blackwell_tma_umma_warp_specialized_kernel`](./blackwell_tma_umma_warp_specialized_kernel):
* [â€‹`blackwell_tma_umma_warp_specialized_split_k_kernel`](./blackwell_tma_umma_warp_specialized_split_k_kernel):
* [â€‹`consumer_main_loop`](./consumer_main_loop):
* [â€‹`copy_accum_to_gmem`](./copy_accum_to_gmem):
* [â€‹`f32_frag_to_smem`](./f32_frag_to_smem):
* [â€‹`load_AB`](./load_AB):
* [â€‹`matmul_sm100_fallback`](./matmul_sm100_fallback):
* [â€‹`matmul_sm100_fallback_kernel`](./matmul_sm100_fallback_kernel):
* [â€‹`multi_stage_store_C`](./multi_stage_store_C):
* [â€‹`multi_stage_store_C_split_k`](./multi_stage_store_C_split_k):
* [â€‹`register_epilogue`](./register_epilogue):
* [â€‹`shared_memory_epilogue`](./shared_memory_epilogue):
* [â€‹`shared_memory_epilogue_transpose`](./shared_memory_epilogue_transpose):
* [â€‹`stsm_helper`](./stsm_helper):

</section>

---

## load_AB (Matmul)

<section class='mojo-docs'>

`load_AB[a_type: DType, b_type: DType, a_layout: Layout, b_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, a_smem_layout: Layout, b_smem_layout: Layout, num_pipeline_stages: UInt, /, *, block_tile_shape: IndexList[3], mma_shape: IndexList[3], cta_group: Int = 1, k_group_size: UInt = 1](a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], a_smem: LayoutTensorIter[a_type, a_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], b_smem: LayoutTensorIter[b_type, b_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], load_mma_pipeline: ProducerConsumerPipeline[Int(num_pipeline_stages)], peer_cta_coord: Tuple[UInt, UInt, UInt], work_tile_coord: Tuple[UInt, UInt], a_multicast_mask: UInt16, b_multicast_mask: UInt16, iter_idx: UInt32, elect_one_cta: Bool)`

</section>

---

## matmul_sm100_fallback

<section class='mojo-docs'>

`matmul_sm100_fallback[a_layout: Layout, b_layout: Layout, c_layout: Layout, c_type: DType, a_type: DType, b_type: DType, *, transpose_b: Bool, umma_shape: IndexList[3], block_tile_shape: IndexList[3], a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, c_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[a_type, a_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[b_type, b_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

</section>

---

## matmul_sm100_fallback_kernel

<section class='mojo-docs'>

`matmul_sm100_fallback_kernel[a_type: DType, b_type: DType, c_type: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, block_tile_shape: IndexList[3], mma_shape: IndexList[3], transpose_b: Bool = True, cluster_shape: StaticTuple[Int32, 3] = StaticTuple[Int32, 3](1, 1, 1), a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, num_threads: UInt = 128, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], c: LayoutTensor[c_type, c_layout, MutAnyOrigin], num_iters: UInt)`

</section>

---

## multi_stage_store_C (Matmul)

<section class='mojo-docs'>

`multi_stage_store_C[c_type: DType, c_smem_layout: Layout, c_layout: Layout, c_desc_layout: Layout, num_accum_pipeline_stages: UInt, /, *, input_type: DType, accum_type: DType, block_tile_shape: IndexList[3], mma_shape: IndexList[3], stage_stride_cols: UInt, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, cta_group: Int = 1, num_output_warps: UInt = 4, max_tmem_cols: UInt = 512, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, transpose_c: Bool = False](c_iter: LayoutTensorIter[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], mma_output_pipeline: ProducerConsumerPipeline[Int(num_accum_pipeline_stages)], tmem_addr: UInt32, work_tile_coord: Tuple[UInt32, UInt32], elect_one_warp: Bool, M: UInt32, N: UInt32)`

</section>

---

## multi_stage_store_C_split_k

<section class='mojo-docs'>

`multi_stage_store_C_split_k[c_type: DType, c_smem_layout: Layout, c_layout: Layout, c_desc_layout: Layout, reduction_layout: Layout, num_accum_pipeline_stages: UInt, /, *, input_type: DType, accum_type: DType, block_tile_shape: IndexList[3], mma_shape: IndexList[3], stage_stride_cols: UInt, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, cta_group: Int = 1, num_output_warps: UInt = 4, max_tmem_cols: UInt = 512, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, transpose_c: Bool = False](scheduler: TileScheduler[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k], reduction_tensor: LayoutTensor[accum_type, reduction_layout, MutAnyOrigin], c_iter: LayoutTensorIter[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], mma_output_pipeline: ProducerConsumerPipeline[Int(num_accum_pipeline_stages)], tmem_addr: UInt32, work_info: WorkInfo, elect_one_warp: Bool, M: UInt32, N: UInt32)`

</section>

---

## register_epilogue

<section class='mojo-docs'>

`register_epilogue[MMA_M: UInt, data_paths: UInt, num_stages: UInt, bits: UInt, stage: UInt, stageN: UInt, compute_lambda_fn: elementwise_compute_lambda_type, num_output_warps: UInt, epilogue_dtype: DType, frag_size: UInt, repeats: UInt, transpose_c: Bool, cta_group: Int, is_lower_frag_required: Bool](mut upper_frag_casted: SIMD[epilogue_dtype, Int(frag_size)], mut lower_frag_casted: SIMD[epilogue_dtype, Int(frag_size)], c_row: UInt32, c_col: UInt32, N: UInt32)`

</section>

---

## shared_memory_epilogue

<section class='mojo-docs'>

`shared_memory_epilogue[MMA_M: UInt, data_paths: UInt, num_stages: UInt, stage: UInt, stageN: UInt, c_type: DType, shared_n: UInt, simd_size: UInt, c_smem_upper_layout: Layout, c_smem_lower_layout: Layout, swizzle: Swizzle, compute_lambda_fn: elementwise_compute_lambda_type, num_output_warps: UInt](M: UInt32, N: UInt32, c_col: UInt, c_row: UInt, c_smem_warp_tile_upper: LayoutTensor[c_type, c_smem_upper_layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_smem_warp_tile_lower: LayoutTensor[c_type, c_smem_lower_layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## shared_memory_epilogue_transpose

<section class='mojo-docs'>

`shared_memory_epilogue_transpose[stage: UInt, stageN: UInt, c_type: DType, c_smem_layout: Layout, swizzle: Swizzle, compute_lambda_fn: elementwise_compute_lambda_type, num_output_warps: UInt, warp_dim: UInt, MMA_M: Int, BN: Int, cta_group: Int](M: UInt32, N: UInt32, c_col: UInt, c_row: UInt, c_smem: LayoutTensor[c_type, c_smem_layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], warp_i: UInt, warp_j: UInt)`

</section>

---

## stsm_helper (Matmul)

<section class='mojo-docs'>

`stsm_helper[swizzle: Swizzle, stageN: UInt, transpose_c: Bool = False, swizzle_mode: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B](vec: SIMD[dtype, size], dst: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], warp_offset: UInt32 = 0)`

</section>

---

## ProducerConsumerPipeline

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ProducerConsumerPipeline[num_stages: Int]`

A producer-consumer pipeline using shared memory barriers to enforce synchronization (between producer and consumer warps).

This struct is commonly used with warp specialization to pipeline operations
between two warps/warpgroups with data dependencies.

## Parameters

* â€‹num\_stages ([`Int`](/mojo/std/builtin/int/Int)): The number of pipeline stages.

## Fields

* â€‹full (`MbarPtr`):
* â€‹empty (`MbarPtr`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]) -> Self`

Initialize the producer-consumer pipeline with default phases.

**Args:**

* â€‹ptr ([`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)): Pointer to shared memory barriers.

### `wait_producer`

`wait_producer(self)`

Consumer waits for producer.

### `wait_consumer`

`wait_consumer(self)`

Producer waits for consumer.

### `producer_mbar`

`producer_mbar(self, stage: UInt32) -> MbarPtr`

Get the producer barrier for a specific stage.

**Args:**

* â€‹stage ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The pipeline stage.

**Returns:**

`MbarPtr`: The shared memory barrier that the producer signals.

### `consumer_mbar`

`consumer_mbar(self, stage: UInt32) -> MbarPtr`

Get the consumer barrier for a specific stage.

**Args:**

* â€‹stage ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The pipeline stage.

**Returns:**

`MbarPtr`: The shared memory barrier that the consumer signals.

### `producer_stage`

`producer_stage(self) -> UInt32`

Get the current producer stage index.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): The current stage index for the producer (0 to num\_stages-1).

### `consumer_stage`

`consumer_stage(self) -> UInt32`

Get the current consumer stage index.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): The current stage index for the consumer (0 to num\_stages-1).

### `consumer_step`

`consumer_step(mut self)`

Advance the consumer to the next pipeline stage.

Increments the consumer stage and wraps to 0 when reaching num\_stages,
toggling the phase bit on wrap-around.
Only switch phase at end of pipeline because we assume all barriers
are at the same consumer/producer phase before checked. Once checked,
the execution moves to next barrier.

### `producer_step`

`producer_step(mut self)`

Advance the producer to the next pipeline stage.

Increments the producer stage and wraps to 0 when reaching num\_stages,
toggling the phase bit on wrap-around.

### `smem_bytes`

`static smem_bytes() -> UInt32`

Calculate the shared memory bytes required for pipeline barriers.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): The total number of bytes needed for all pipeline barriers
(2 \* num\_stages barriers).

### `init_mbars`

`init_mbars(self, producer_arrive_count: Int32, consumer_arrive_count: Int32)`

Initialize the smem barriers for the producer and consumer.

This function must be called by a single thread and must be called before any the pipeline object is used.

**Args:**

* â€‹producer\_arrive\_count ([`Int32`](/mojo/std/builtin/simd/#int32)): The number of threads that will arrive at the barrier marking data as produced.
* â€‹consumer\_arrive\_count ([`Int32`](/mojo/std/builtin/simd/#int32)): The number of threads that will arrive at the barrier marking data as consumed.

</section>

---

## pipeline (Pipeline)

<section class='mojo-docs'>

## `comptime` values

### `MbarPtr`

`comptime MbarPtr = LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]`

## Structs

* [â€‹`ProducerConsumerPipeline`](./ProducerConsumerPipeline): A producer-consumer pipeline using shared memory barriers to enforce synchronization (between producer and consumer warps).

</section>

---

## TileScheduler (Tile_scheduler)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileScheduler[num_stages: Int, cluster_shape: IndexList[3, element_type=DType.uint32] = Index[dtype=DType.uint32](1, 1, 1), rasterize_order: RasterOrder = RasterOrder.AlongM, block_swizzle_size: Int = 8]`

## Fields

* â€‹cluster\_dim (`StaticTuple[Int32, 3]`):
* â€‹log\_cluster\_dim\_m (`FastDiv[DType.uint32]`):
* â€‹log\_cluster\_dim\_n (`FastDiv[DType.uint32]`):
* â€‹log\_cluster\_dim\_k (`FastDiv[DType.uint32]`):
* â€‹clc\_response (`LegacyUnsafePointer[UInt128, address_space=AddressSpace.SHARED]`):
* â€‹full\_mbar (`LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]`):
* â€‹empty\_mbar (`LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `cluster_size`

`comptime cluster_size = ((cluster_shape.__getitem__[3, DType.uint32, Int](0) * cluster_shape.__getitem__[3, DType.uint32, Int](1)) * cluster_shape.__getitem__[3, DType.uint32, Int](2))`

### `log_cluster_k`

`comptime log_cluster_k = FastDiv[DType.uint32](cluster_shape.__getitem__[3, DType.uint32, Int](2))`

### `log_cluster_m`

`comptime log_cluster_m = FastDiv[DType.uint32](cluster_shape.__getitem__[3, DType.uint32, Int](0))`

### `log_cluster_n`

`comptime log_cluster_n = FastDiv[DType.uint32](cluster_shape.__getitem__[3, DType.uint32, Int](1))`

## Methods

### `__init__`

`__init__(cluster_dim: StaticTuple[Int32, 3], clc_response_ptr: LegacyUnsafePointer[UInt128, address_space=AddressSpace.SHARED], full_mbar_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], empty_mbar_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]) -> Self`

### `work_info_from_clc_response`

`static work_info_from_clc_response(result: LegacyUnsafePointer[UInt128, address_space=AddressSpace.SHARED]) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `work_info_from_cluster`

`static work_info_from_cluster(work_info: WorkInfo, cluster_dim: StaticTuple[Int32, 3], log_cluster_dim_m: FastDiv[DType.uint32], log_cluster_dim_n: FastDiv[DType.uint32]) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `initial_work_info`

`initial_work_info(self) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `fetch_next_work`

`fetch_next_work(self, work_info: WorkInfo, consumer_state: PipelineState[num_stages]) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `advance_to_next_work`

`advance_to_next_work(self, mut clc_state: PipelineState[num_stages]) -> PipelineState[num_stages]`

**Returns:**

[`PipelineState`](/mojo/kernels/layout/tma_async/PipelineState)

</section>

---

## WorkInfo (Tile_scheduler)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct WorkInfo`

## Fields

* â€‹m (`UInt32`):
* â€‹n (`UInt32`):
* â€‹k\_start (`UInt32`):
* â€‹is\_valid\_tile (`Bool`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `is_valid`

`is_valid(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

</section>

---

## tile_scheduler

<section class='mojo-docs'>

## Structs

* [â€‹`TileScheduler`](./TileScheduler):
* [â€‹`WorkInfo`](./WorkInfo):

</section>

---

## TileScheduler (Tile_scheduler_splitk)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileScheduler[num_stages: Int, reduction_tile_shape: IndexList[3], cluster_shape: IndexList[3, element_type=DType.uint32] = Index[dtype=DType.uint32](1, 1, 1), rasterize_order: RasterOrder = RasterOrder.AlongM, block_swizzle_size: Int = 8, num_split_k: Int = 1]`

## Fields

* â€‹locks\_ptr (`LegacyUnsafePointer[Int32]`):
* â€‹scheduler (`TileScheduler[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k].UnderlyingScheduler`):
* â€‹total\_k\_tiles (`UInt32`):
* â€‹k\_tiles\_per\_split (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `BK`

`comptime BK = reduction_tile_shape.__getitem__[3, DType.int64, Int](2)`

### `BM`

`comptime BM = reduction_tile_shape.__getitem__[3, DType.int64, Int](0)`

### `MMA_N`

`comptime MMA_N = reduction_tile_shape.__getitem__[3, DType.int64, Int](1)`

### `ROW_SIZE`

`comptime ROW_SIZE = reduction_tile_shape.__getitem__[3, DType.int64, Int](1) if (eq reduction_tile_shape.__getitem__[3, DType.int64, Int](0)._mlir_value, 128) else (TileScheduler[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k].MMA_N // 2)`

### `UnderlyingScheduler`

`comptime UnderlyingScheduler = TileScheduler[num_stages, cluster_shape, rasterize_order, block_swizzle_size]`

## Methods

### `__init__`

`__init__(cluster_dim: StaticTuple[Int32, 3], mnk: StaticTuple[UInt32, 3], clc_response_ptr: LegacyUnsafePointer[UInt128, address_space=AddressSpace.SHARED], full_mbar_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], empty_mbar_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], locks_ptr: LegacyUnsafePointer[UInt8]) -> Self`

### `convert_to_splitk_work_info`

`convert_to_splitk_work_info(self, work_info: WorkInfo) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `initial_work_info`

`initial_work_info(self) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `advance_to_next_work`

`advance_to_next_work(self, mut clc_state: PipelineState[num_stages]) -> PipelineState[num_stages]`

**Returns:**

[`PipelineState`](/mojo/kernels/layout/tma_async/PipelineState)

### `fetch_next_work`

`fetch_next_work(self, work_info: WorkInfo, consumer_state: PipelineState[num_stages]) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `is_last_split`

`is_last_split(self, work_tile_info: WorkInfo) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `output_tile_index`

`output_tile_index(self, work_info: WorkInfo) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `store_to_workspace`

`store_to_workspace[accum_type: DType, workspace_layout: Layout, /, *, do_reduction: Bool = False, write_back: Bool = False](self, tmem_addr: UInt32, reduction_workspace: LayoutTensor[accum_type, workspace_layout, origin], epilogue_thread_idx: UInt, reduction_tile_idx: UInt32)`

### `reduction`

`reduction[accum_type: DType, workspace_layout: Layout](self, reduction_workspace: LayoutTensor[accum_type, workspace_layout, origin], tmem_addr: UInt32, epilogue_thread_idx: UInt, work_info: WorkInfo) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `wait_eq`

`static wait_eq(lock_ptr: LegacyUnsafePointer[Int32], barrier_id: Int32, barrier_group_thread_idx: Int, lock_idx: UInt32, val: UInt32)`

### `wait_lt`

`static wait_lt(lock_ptr: LegacyUnsafePointer[Int32], barrier_id: Int32, barrier_group_thread_idx: Int, lock_idx: UInt32, count: UInt32)`

### `arrive_set`

`static arrive_set(lock_ptr: LegacyUnsafePointer[Int32], barrier_id: Int32, barrier_group_thread_idx: Int, lock_idx: UInt32, val: UInt32)`

</section>

---

## WorkInfo (Tile_scheduler_splitk)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct WorkInfo`

## Fields

* â€‹m (`UInt32`):
* â€‹n (`UInt32`):
* â€‹k\_start (`UInt32`):
* â€‹num\_k\_tiles (`UInt32`):
* â€‹is\_valid\_tile (`Bool`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `INVALID_WORK_INFO`

`comptime INVALID_WORK_INFO = WorkInfo(0, 0, 0, 0, False)`

## Methods

### `is_valid`

`is_valid(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_final_split`

`is_final_split(self, k_tiles_per_output_tile: UInt32) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

</section>

---

## get_num_tiles

<section class='mojo-docs'>

`get_num_tiles(problem_shape: IndexList[3], block_tile_shape: IndexList[3], cluster_shape: IndexList[2]) -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## get_required_locks_buffer_size_bytes

<section class='mojo-docs'>

`get_required_locks_buffer_size_bytes[accum_type: DType](problem_shape: IndexList[3], block_tile_shape: IndexList[3], cluster_shape: IndexList[2]) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## tile_scheduler_splitk

<section class='mojo-docs'>

## Structs

* [â€‹`TileScheduler`](./TileScheduler):
* [â€‹`WorkInfo`](./WorkInfo):

## Functions

* [â€‹`get_num_tiles`](./get_num_tiles):
* [â€‹`get_required_locks_buffer_size_bytes`](./get_required_locks_buffer_size_bytes):

</section>

---

## TuningConfigSM100

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TuningConfigSM100`

## Fields

* â€‹M (`Int`):
* â€‹M\_end (`Int`):
* â€‹N (`Int`):
* â€‹K (`Int`):
* â€‹mma\_shape (`IndexList[3]`):
* â€‹block\_tile\_shape (`IndexList[3]`):
* â€‹cluster\_shape (`IndexList[3]`):
* â€‹block\_swizzle\_size (`UInt`):
* â€‹rasterize\_order (`RasterOrder`):
* â€‹cta\_group (`Int`):
* â€‹swapAB (`Bool`):
* â€‹k\_group\_size (`UInt`):
* â€‹num\_accum\_pipeline\_stages (`UInt`):
* â€‹num\_clc\_pipeline\_stages (`UInt`):
* â€‹num\_split\_k (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`TuningConfig`](/mojo/kernels/internal_utils/dispatch_utils/TuningConfig)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(M: Int, N: Int, K: Int, mma_shape: IndexList[3], block_tile_shape: IndexList[3], cluster_shape: IndexList[3], block_swizzle_size: UInt, rasterize_order: RasterOrder, cta_group: Int = 2, swapAB: Bool = False, k_group_size: UInt = 1, num_accum_pipeline_stages: UInt = 2, num_clc_pipeline_stages: UInt = 2, num_split_k: Int = 1) -> Self`

`__init__(M: Int, M_end: Int, N: Int, K: Int, mma_shape: IndexList[3], cta_group: Int, cluster_shape: IndexList[3], block_swizzle_size: UInt, rasterize_order: RasterOrder, swapAB: Bool = False, k_group_size: UInt = 1, num_accum_pipeline_stages: UInt = 2, num_clc_pipeline_stages: UInt = 2, num_split_k: Int = 1) -> Self`

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

</section>

---

## tuning_configs

<section class='mojo-docs'>

## Structs

* [â€‹`TuningConfigSM100`](./TuningConfigSM100):

</section>

---

## blackwell_tma_umma_warp_specialized_blockwise_fp8_kernel

<section class='mojo-docs'>

`blackwell_tma_umma_warp_specialized_blockwise_fp8_kernel[a_type: DType, b_type: DType, c_type: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, a_scales_tile_layout: Layout, a_scales_type: DType, b_scales_type: DType, b_scales_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, c_desc_layout: Layout, a_scales_desc_layout: Layout, transpose_b: Bool, config: MatmulConfig[a_type, b_type, c_type, transpose_b], num_pipeline_stages: UInt, cluster_shape: StaticTuple[Int32, 3]](a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], a_scales_tma_op: TMATensorTile[a_scales_type, a_scales_tile_layout, a_scales_desc_layout], cluster_dim: StaticTuple[Int32, 3], num_iters: UInt, b_scales: LayoutTensor[b_scales_type, b_scales_layout, MutAnyOrigin], problem_shape: StaticTuple[Int32, 3])`

</section>

---

## warp_specialized_blockwise_fp8

<section class='mojo-docs'>

## Functions

* [â€‹`blackwell_tma_umma_warp_specialized_blockwise_fp8_kernel`](./blackwell_tma_umma_warp_specialized_blockwise_fp8_kernel):
* [â€‹`load_AB`](./load_AB):
* [â€‹`multi_stage_reg_epilogue`](./multi_stage_reg_epilogue):
* [â€‹`promote_accumulators`](./promote_accumulators):
* [â€‹`sm100_warp_specialized_blockwise_fp8`](./sm100_warp_specialized_blockwise_fp8):

</section>

---

## load_AB (Warp_specialized_blockwise_fp8)

<section class='mojo-docs'>

`load_AB[a_type: DType, b_type: DType, a_scales_type: DType, a_layout: Layout, b_layout: Layout, a_scales_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, a_scales_desc_layout: Layout, a_smem_layout: Layout, b_smem_layout: Layout, a_scales_smem_layout: Layout, num_pipeline_stages: UInt, /, *, block_tile_shape: IndexList[3], mma_shape: IndexList[3], cta_group: Int = 1](a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], a_scales_tma_op: TMATensorTile[a_scales_type, a_scales_layout, a_scales_desc_layout], a_smem: LayoutTensorIter[a_type, a_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], b_smem: LayoutTensorIter[b_type, b_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], a_scales_smem: LayoutTensorIter[a_scales_type, a_scales_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], load_mma_pipeline: ProducerConsumerPipeline[Int(num_pipeline_stages)], peer_cta_coord: Tuple[UInt, UInt, UInt], work_tile_coord: Tuple[UInt, UInt], a_multicast_mask: UInt16, b_multicast_mask: UInt16, iter_idx: UInt, elect_one_cta: Bool)`

</section>

---

## multi_stage_reg_epilogue (Warp_specialized_blockwise_fp8)

<section class='mojo-docs'>

`multi_stage_reg_epilogue[c_smem_layout: Layout, c_layout: Layout, c_desc_layout: Layout, accum_type: DType, accum_layout: Layout, /, *, c_type: DType, block_tile_shape: IndexList[3], mma_shape: IndexList[3], is_lower_frag_required: Bool, cta_group: Int, num_output_warps: UInt, c_swizzle: TensorMapSwizzle](c_upper_main_tile: LayoutTensor[accum_type, accum_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_lower_main_tile: LayoutTensor[accum_type, accum_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_iter: LayoutTensorIter[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], c_coord: Tuple[UInt, UInt], elect_one_warp: Bool)`

</section>

---

## promote_accumulators (Warp_specialized_blockwise_fp8)

<section class='mojo-docs'>

`promote_accumulators[pipeline_stages: UInt, num_accum_pipeline_stages: UInt, accum_type: DType, accum_layout: Layout, a_scales_type: DType, b_scales_type: DType, b_scales_layout: Layout, a_scales_smem_layout: Layout, /, *, block_tile_shape: IndexList[3], mma_shape: IndexList[3], cta_group: Int, CLUSTER_SIZE: Int32, is_lower_frag_required: Bool, num_output_warps: UInt](b_scales: LayoutTensor[b_scales_type, b_scales_layout, MutAnyOrigin], a_scales_smem_iter: LayoutTensorIter[a_scales_type, a_scales_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], c_upper_main_tile: LayoutTensor[accum_type, accum_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_lower_main_tile: LayoutTensor[accum_type, accum_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mma_output_pipeline: ProducerConsumerPipeline[Int(num_accum_pipeline_stages)], tmem_addr: UInt32, load_mma_pipeline: ProducerConsumerPipeline[Int(pipeline_stages)], work_tile_coord: Tuple[UInt, UInt], elect_one_warp: Bool, stage_stride_cols: UInt, k_iter: UInt, problem_shape: StaticTuple[Int32, 3])`

</section>

---

## sm100_warp_specialized_blockwise_fp8

<section class='mojo-docs'>

`sm100_warp_specialized_blockwise_fp8[c_type: DType, c_layout: Layout, a_type: DType, a_layout: Layout, b_type: DType, b_layout: Layout, transpose_b: Bool, a_scales_layout: Layout, b_scales_layout: Layout, a_scales_type: DType, b_scales_type: DType, *, config: MatmulConfig[a_type, b_type, c_type, transpose_b]](c: LayoutTensor[c_type, c_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[a_type, a_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[b_type, b_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_scales: LayoutTensor[a_scales_type, a_scales_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_scales: LayoutTensor[b_scales_type, b_scales_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

</section>

---

## sm100_structured

<section class='mojo-docs'>

SM100 Structured Matmul - Refactored with encapsulated pipeline management.

This module provides the same SM100 matmul functionality as the original sm100
module, but with improved code organization:

Key abstractions:

* WorkIterator/SchedulerWorkIterator: Encapsulate work iteration and pipeline state
* RingBuffer/OutputRingBuffer: Encapsulate producer-consumer synchronization
* TileLoaderTMA: Encapsulate TMA tile loading logic
* Context managers for cleaner acquire/release patterns

## Switching Implementations

### Option 1: Environment Variable (Recommended)

Set `MODULAR_USE_STRUCTURED_SM100=1` to use this implementation:

```bash
# Use original sm100 (default):
./bazelw run //max/kernels/test/gpu/linalg:test_matmul_sm100_smoke.mojo.test

# Use sm100_structured:
MODULAR_USE_STRUCTURED_SM100=1 ./bazelw run //max/kernels/test/gpu/linalg:test_matmul_sm100_smoke.mojo.test
```

### Option 2: Direct Import

```mojo
# Original:
from linalg.matmul.gpu.sm100.matmul import (
    blackwell_matmul_tma_umma_warp_specialized
)

# Structured (this module):
from linalg.matmul.gpu.sm100_structured import (
    blackwell_matmul_tma_umma_warp_specialized
)
```

See DOCS/testing\_and\_switching.md for full documentation.

## Modules

* [â€‹`matmul`](./matmul/): SM100 Matmul CPU entry points - TMA setup and kernel launch wrappers.
* [â€‹`matmul_kernels`](./matmul_kernels/): SM100 Matmul Kernel Structs - GPU kernel entry points and helpers.
* [â€‹`matmul_output`](./matmul_output/): SM100 Matmul Output Pipeline - TMEM â†’ SMEM â†’ GMEM epilogue.
* [â€‹`pipeline`](./pipeline/):
* [â€‹`ring_buffer`](./ring_buffer/): Ring buffer for SM100 producer-consumer synchronization.
* [â€‹`tile_loader`](./tile_loader/): TileLoader for SM100 matrix multiplication.
* [â€‹`tile_scheduler`](./tile_scheduler/):
* [â€‹`tile_scheduler_splitk`](./tile_scheduler_splitk/):
* [â€‹`tile_writer`](./tile_writer/): TileWriter components for SM100 matrix multiplication epilogue.

</section>

---

## blackwell_matmul_tma_umma_warp_specialized (Matmul)

<section class='mojo-docs'>

`blackwell_matmul_tma_umma_warp_specialized[c_type: DType, c_layout: Layout, a_type: DType, a_layout: Layout, b_type: DType, b_layout: Layout, transpose_b: Bool, *, config: MatmulConfig[a_type, b_type, c_type, transpose_b], elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, pdl_level: PDLLevel = PDLLevel(), max_profiled_tiles_per_SM: OptionalReg[UInt32] = None](c_device: LayoutTensor[c_type, c_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a_device: LayoutTensor[a_type, a_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_device: LayoutTensor[b_type, b_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

</section>

---

## matmul (4)

<section class='mojo-docs'>

SM100 Matmul CPU entry points - TMA setup and kernel launch wrappers.

This module contains the CPU-side code for SM100 matrix multiplication:

* TMA descriptor creation
* Kernel instantiation and launch via ctx.enqueue\_function\_checked

All GPU code (kernel structs, runtime functions) is in matmul\_kernels.mojo.

## Functions

* [â€‹`blackwell_matmul_tma_umma_warp_specialized`](./blackwell_matmul_tma_umma_warp_specialized):
* [â€‹`matmul_sm100_fallback`](./matmul_sm100_fallback):

</section>

---

## matmul_sm100_fallback (Matmul)

<section class='mojo-docs'>

`matmul_sm100_fallback[a_layout: Layout, b_layout: Layout, c_layout: Layout, c_type: DType, a_type: DType, b_type: DType, *, transpose_b: Bool, umma_shape: IndexList[3], block_tile_shape: IndexList[3], a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, c_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[a_type, a_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[b_type, b_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

</section>

---

## B200MatmulSmem (Matmul_kernels)

<section class='mojo-docs'>

`struct B200MatmulSmem[a_type: DType, b_type: DType, c_type: DType, transpose_b: Bool, *, config: MatmulConfig[a_type, b_type, c_type, transpose_b]]`

Shared memory layout for B200 SM100 matrix multiplication kernel.

This struct manages the shared memory allocation for:

* Input tiles (A and B matrices) with multi-stage pipelining
* Output tile (C matrix) for accumulation
* Synchronization barriers for producer-consumer coordination
* CLC (Cluster Launch Control) barriers and response storage
* TMEM (Tensor Memory) address and deallocation barrier

The memory is organized to support asynchronous TMA loads and efficient
bank-conflict-free access patterns for tensor core operations.

Type aliases are provided for tile types (ATile, BTile, CTile) to enable
cleaner function signatures without verbose LayoutTensor declarations.

## Fields

* â€‹a\_smem (`InlineArray[B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].AType, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].a_smem_size]`):
* â€‹b\_smem (`InlineArray[B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BType, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].b_smem_size]`):
* â€‹c\_smem (`InlineArray[B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].CType, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].c_smem_size]`):
* â€‹tma\_mma\_mbars (`InlineArray[SharedMemBarrier, (B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].num_group_pipeline_stages * 2)]`):
* â€‹accum\_mbars (`InlineArray[SharedMemBarrier, (B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].num_accum_pipeline_stages * 2)]`):
* â€‹clc\_mbars\_full (`InlineArray[SharedMemBarrier, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].num_clc_pipeline_stages]`):
* â€‹clc\_mbars\_empty (`InlineArray[SharedMemBarrier, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].num_clc_pipeline_stages]`):
* â€‹clc\_throttle\_mbars (`InlineArray[SharedMemBarrier, (B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].num_clc_pipeline_stages * 2)]`):
* â€‹clc\_response (`InlineArray[UInt128, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].num_clc_pipeline_stages]`):
* â€‹tmem\_dealloc\_mbar (`InlineArray[SharedMemBarrier, 1]`):
* â€‹tmem\_addr (`InlineArray[UInt32, 1]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `a_smem_layout`

`comptime a_smem_layout = tile_layout_k_major[a_type, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BM, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BK, config.a_swizzle]()`

### `a_smem_size`

`comptime a_smem_size = ((B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BM * B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BK) * B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].num_pipeline_stages)`

### `ATile`

`comptime ATile = LayoutTensor[a_type, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].a_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=NVIDIASharedMemoryBasePtr.alignment]`

### `ATileArray`

`comptime ATileArray = SMemTileArrayType[a_type, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].a_smem_layout, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].num_pipeline_stages, 128]`

### `AType`

`comptime AType = Scalar[a_type]`

### `b_smem_layout`

`comptime b_smem_layout = tile_layout_k_major[b_type, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BN, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BK, config.b_swizzle]() if transpose_b else tile_layout_mn_major[b_type, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BN, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BK, config.b_swizzle]()`

### `b_smem_size`

`comptime b_smem_size = ((B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BN * B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].BK) * B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].num_pipeline_stages)`

### `BK`

`comptime BK = config.block_tile_shape.__getitem__[3, DType.int64, Int](2)`

### `BM`

`comptime BM = config.block_tile_shape.__getitem__[3, DType.int64, Int](0)`

### `BN`

`comptime BN = config.block_tile_shape.__getitem__[3, DType.int64, Int](1)`

### `BTile`

`comptime BTile = LayoutTensor[b_type, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].b_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=NVIDIASharedMemoryBasePtr.alignment]`

### `BTileArray`

`comptime BTileArray = SMemTileArrayType[b_type, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].b_smem_layout, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].num_pipeline_stages, 128]`

### `BType`

`comptime BType = Scalar[b_type]`

### `c_smem_layout`

`comptime c_smem_layout = Layout.row_major(B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].OutputM, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].OutputN)`

### `c_smem_size`

`comptime c_smem_size = ((B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].OutputM * B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].OutputN) * B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].num_output_stages)`

### `CTile`

`comptime CTile = LayoutTensor[c_type, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=NVIDIASharedMemoryBasePtr.alignment]`

### `CTileArray`

`comptime CTileArray = SMemTileArrayType[c_type, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].c_smem_layout, B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].num_output_stages, 128]`

### `CType`

`comptime CType = Scalar[c_type]`

### `num_accum_pipeline_stages`

`comptime num_accum_pipeline_stages = Int(config)`

### `num_clc_pipeline_stages`

`comptime num_clc_pipeline_stages = Int(config)`

### `num_group_pipeline_stages`

`comptime num_group_pipeline_stages = (B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config].num_pipeline_stages // Int(config))`

### `num_output_stages`

`comptime num_output_stages = Int(config)`

### `num_pipeline_stages`

`comptime num_pipeline_stages = Int(config)`

### `OutputM`

`comptime OutputM = config.output_tile_shape.__getitem__[2, DType.int64, Int](0)`

### `OutputN`

`comptime OutputN = config.output_tile_shape.__getitem__[2, DType.int64, Int](1)`

### `SMM`

`comptime SMM = SharedMemoryManager[NVIDIASharedMemoryBasePtr]`

## Methods

### `ab_pipeline_size`

`static ab_pipeline_size() -> Int`

Calculate the total size of A+B tiles for all pipeline stages.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `c_output_size`

`static c_output_size() -> Int`

Calculate the size of C tiles for all output stages.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `total_tile_size`

`static total_tile_size() -> Int`

Calculate the total tile storage size (A+B+C).

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## BlackwellMatmulSM100FallbackKernel

<section class='mojo-docs'>

`struct BlackwellMatmulSM100FallbackKernel[a_type: DType, b_type: DType, c_type: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, block_tile_shape: IndexList[3], mma_shape: IndexList[3], transpose_b: Bool = True, cluster_shape: StaticTuple[Int32, 3] = StaticTuple[Int32, 3](1, 1, 1), a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, num_threads: UInt = 128, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None]`

Simple fallback matmul kernel for SM100 (B200).

This kernel is used when the warp-specialized kernel is not applicable,
such as for small problem sizes or unsupported configurations.

Unlike the main BlackwellMatmulSM100Kernel, this uses:

* Single warp approach (no warp specialization)
* Basic barrier synchronization (no CLC scheduling)
* Direct LayoutTensor output (no TMA for C)
* Simpler pipeline with single buffer

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `a_size`

`comptime a_size = BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].a_smem_layout.size()`

### `a_smem_layout`

`comptime a_smem_layout = tile_layout_k_major[a_type, BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].BM, BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].BK, a_swizzle]()`

### `accum_type`

`comptime accum_type = get_accum_type[a_type]()`

### `ATile`

`comptime ATile = LayoutTensor[a_type, BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].a_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128]`

### `b_size`

`comptime b_size = BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].b_smem_layout.size()`

### `b_smem_layout`

`comptime b_smem_layout = tile_layout_k_major[b_type, BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].BN, BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].BK, b_swizzle]() if transpose_b else tile_layout_mn_major[b_type, BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].BN, BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].BK, b_swizzle]()`

### `BK`

`comptime BK = block_tile_shape.__getitem__[3, DType.int64, Int](2)`

### `BM`

`comptime BM = block_tile_shape.__getitem__[3, DType.int64, Int](0)`

### `BN`

`comptime BN = block_tile_shape.__getitem__[3, DType.int64, Int](1)`

### `BTile`

`comptime BTile = LayoutTensor[b_type, BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].b_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128]`

### `c_frag_size`

`comptime c_frag_size = ((BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].MMA_M * BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].MMA_N) // Int(num_threads))`

### `max_tmem_cols`

`comptime max_tmem_cols = 512`

### `MMA_K`

`comptime MMA_K = mma_shape.__getitem__[3, DType.int64, Int](2)`

### `MMA_M`

`comptime MMA_M = mma_shape.__getitem__[3, DType.int64, Int](0)`

### `MMA_N`

`comptime MMA_N = mma_shape.__getitem__[3, DType.int64, Int](1)`

### `num_k_mmas`

`comptime num_k_mmas = (BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].BK // BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].MMA_K)`

### `num_m_mmas`

`comptime num_m_mmas = (BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].BM // BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].MMA_M)`

### `num_n_mmas`

`comptime num_n_mmas = (BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].BN // BlackwellMatmulSM100FallbackKernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, block_tile_shape, mma_shape, transpose_b, cluster_shape, a_swizzle, b_swizzle, num_threads, elementwise_lambda_fn].MMA_N)`

## Methods

### `validate_constraints`

`static validate_constraints()`

Validate compile-time constraints for this kernel configuration.

### `run`

`static run(a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], c: LayoutTensor[c_type, c_layout, MutAnyOrigin], num_iters: UInt)`

Run the fallback matmul kernel.

**Args:**

* â€‹a\_tma\_op ([`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)): TMA descriptor for matrix A.
* â€‹b\_tma\_op ([`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)): TMA descriptor for matrix B.
* â€‹c ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output tensor C (LayoutTensor, not TMA).
* â€‹num\_iters ([`UInt`](/mojo/std/builtin/uint/UInt)): Number of K-dimension iterations.

</section>

---

## BlackwellMatmulSM100Kernel

<section class='mojo-docs'>

`struct BlackwellMatmulSM100Kernel[a_type: DType, b_type: DType, c_type: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, c_desc_layout: Layout, transpose_b: Bool, config: MatmulConfig[a_type, b_type, c_type, transpose_b], cluster_shape: StaticTuple[Int32, 3] = StaticTuple[Int32, 3](1), elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, pdl_level: PDLLevel = PDLLevel(), max_profiled_tiles_per_SM: UInt32 = 0]`

Blackwell SM100 GEMM kernel with warp specialization.

This struct unifies all parameters and derived types for the SM100
matmul kernel, providing:

* Compile-time parameter validation
* Centralized derived type computation
* Factory methods for kernel components
* Multiple kernel entry points (standard, split-k)

The SM100 kernel uses:

* Tensor Memory (TMEM) for MMA accumulators
* Cluster Launch Control (CLC) for dynamic tile scheduling
* Warp specialization: Scheduler, TMA Load, MMA, Epilogue warps
* Software pipelining for overlapping compute and memory operations

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `a_smem_layout`

`comptime a_smem_layout = tile_layout_k_major[a_type, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].BM, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].BK, config.a_swizzle]()`

### `accum_pipeline_consumer_arv_count`

`comptime accum_pipeline_consumer_arv_count = (BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].cta_group * BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].EPILOGUE_THREADS)`

### `accum_pipeline_producer_arv_count`

`comptime accum_pipeline_producer_arv_count = 1`

### `accum_type`

`comptime accum_type = MatmulConfig[a_type, b_type, c_type, transpose_b].accum_type`

### `b_smem_layout`

`comptime b_smem_layout = tile_layout_k_major[b_type, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].BN, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].BK, config.b_swizzle]() if transpose_b else tile_layout_mn_major[b_type, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].BN, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].BK, config.b_swizzle]()`

### `BK`

`comptime BK = config.block_tile_shape.__getitem__[3, DType.int64, Int](2)`

### `BM`

`comptime BM = config.block_tile_shape.__getitem__[3, DType.int64, Int](0)`

### `BN`

`comptime BN = config.block_tile_shape.__getitem__[3, DType.int64, Int](1)`

### `clc_consumer_arv_count`

`comptime clc_consumer_arv_count = (BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].SCHEDULER_THREADS + (BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].CLUSTER_SIZE * ((BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].TMA_LOAD_THREADS + BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].MMA_THREADS) + BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].EPILOGUE_THREADS)))`

### `clc_producer_arv_count`

`comptime clc_producer_arv_count = 1`

### `clc_throttle_consumer_arv_count`

`comptime clc_throttle_consumer_arv_count = BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].SCHEDULER_THREADS`

### `clc_throttle_producer_arv_count`

`comptime clc_throttle_producer_arv_count = BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].TMA_LOAD_THREADS`

### `CLUSTER_M`

`comptime CLUSTER_M = Int.__init__[Int](config.cluster_shape.__getitem__[3, DType.int64, Int](0))`

### `CLUSTER_N`

`comptime CLUSTER_N = Int.__init__[Int](config.cluster_shape.__getitem__[3, DType.int64, Int](1))`

### `CLUSTER_SIZE`

`comptime CLUSTER_SIZE = (BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].CLUSTER_M * BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].CLUSTER_N)`

### `Context`

`comptime Context = KernelContext[BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].num_clc_pipeline_stages, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].cta_group, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].CLUSTER_M, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].CLUSTER_N]`

### `cta_group`

`comptime cta_group = config.cta_group`

### `EPILOGUE_THREADS`

`comptime EPILOGUE_THREADS = (4 * WARP_SIZE)`

### `EpilogueConf`

`comptime EpilogueConf = EpilogueConfig[BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].MMA_M, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].MMA_N, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].SmemType.c_smem_layout.shape[1].value(), BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].cta_group, False]`

### `MMA_K`

`comptime MMA_K = config.mma_shape.__getitem__[3, DType.int64, Int](2)`

### `MMA_M`

`comptime MMA_M = config.mma_shape.__getitem__[3, DType.int64, Int](0)`

### `MMA_N`

`comptime MMA_N = config.mma_shape.__getitem__[3, DType.int64, Int](1)`

### `MMA_THREADS`

`comptime MMA_THREADS = WARP_SIZE`

### `MmaOp`

`comptime MmaOp = MmaOpSM100_SS[c_type, a_type, b_type, config.block_tile_shape, config.mma_shape, accum_type=BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].accum_type, cta_group=BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].cta_group, cluster_shape=config.cluster_shape, a_swizzle=config.a_swizzle, b_swizzle=config.b_swizzle, transpose_b=transpose_b]`

### `num_accum_pipeline_stages`

`comptime num_accum_pipeline_stages = Int(config)`

### `num_clc_pipeline_stages`

`comptime num_clc_pipeline_stages = Int(config)`

### `num_group_pipeline_stages`

`comptime num_group_pipeline_stages = (BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].num_pipeline_stages // Int(config))`

### `num_k_mmas`

`comptime num_k_mmas = (BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].BK // BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].MMA_K)`

### `num_m_mmas`

`comptime num_m_mmas = (BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].BM // (BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].MMA_M // BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].cta_group))`

### `num_n_mmas`

`comptime num_n_mmas = (BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].BN // (BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].MMA_N // BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].cta_group))`

### `num_output_stages`

`comptime num_output_stages = Int(config)`

### `num_output_warps`

`comptime num_output_warps = 4`

### `num_pipeline_stages`

`comptime num_pipeline_stages = Int(config)`

### `NUM_THREADS`

`comptime NUM_THREADS = (((BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].SCHEDULER_THREADS + BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].TMA_LOAD_THREADS) + BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].MMA_THREADS) + BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].EPILOGUE_THREADS)`

### `NUM_TMEM_COLS`

`comptime NUM_TMEM_COLS = 512`

### `OutputM`

`comptime OutputM = config.output_tile_shape.__getitem__[2, DType.int64, Int](0)`

### `OutputN`

`comptime OutputN = config.output_tile_shape.__getitem__[2, DType.int64, Int](1)`

### `OutputRB`

`comptime OutputRB = OutputRingBuffer[Int(config), BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].stage_stride_cols, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].cta_group]`

### `RingBuffer`

`comptime RingBuffer = RingBuffer[a_type, b_type, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].SmemType.a_smem_layout, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].SmemType.b_smem_layout, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].SmemType.num_pipeline_stages, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].SmemType.num_group_pipeline_stages, Int(config)]`

### `Scheduler`

`comptime Scheduler = TileScheduler[BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].num_clc_pipeline_stages, Index[dtype=DType.uint32](config.cluster_shape.__getitem__[3, DType.int64, Int](0), config.cluster_shape.__getitem__[3, DType.int64, Int](1), config.cluster_shape.__getitem__[3, DType.int64, Int](2)), config.raster_order, config.block_swizzle_size]`

### `SCHEDULER_THREADS`

`comptime SCHEDULER_THREADS = WARP_SIZE`

### `SmemType`

`comptime SmemType = B200MatmulSmem[a_type, b_type, c_type, transpose_b, config=config]`

### `stage_stride_cols`

`comptime stage_stride_cols = (512 // BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].num_accum_pipeline_stages)`

### `TileLoaderTMA`

`comptime TileLoaderTMA = TileLoaderTMA[BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].SmemType.a_smem_layout, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].SmemType.b_smem_layout, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].BM, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].BN, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].BK, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].MMA_N, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].cta_group, Int(config), BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].SmemType.num_pipeline_stages, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].SmemType.num_group_pipeline_stages]`

### `TMA_LOAD_THREADS`

`comptime TMA_LOAD_THREADS = WARP_SIZE`

## Methods

### `validate_constraints`

`static validate_constraints()`

Validate parameter constraints at compile time.

### `init_barriers`

`static init_barriers(ctx: KernelContext[BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].num_clc_pipeline_stages, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].cta_group, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].CLUSTER_M, BlackwellMatmulSM100Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, a_desc_layout, b_desc_layout, c_desc_layout, transpose_b, config, cluster_shape, elementwise_compute_lambda_fn, register_based_epilogue, pdl_level, max_profiled_tiles_per_SM].CLUSTER_N], a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], tma_mma_mbars_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], accum_mbars_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], clc_throttle_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], clc_full_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], clc_empty_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], tmem_dealloc_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED])`

Initialize barriers and prefetch TMA descriptors. Called by elect\_one\_warp && elect\_one\_thread.

### `mma`

`static mma(tmem_addr: UInt32, tiles: ConsumerTiles[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size], mma_op: MmaOpSM100_SS[c_type, a_type, b_type, block_tile_shape, mma_shape, accum_type=accum_type, cta_group=cta_group, cluster_shape=cluster_shape, a_swizzle=a_swizzle, b_swizzle=b_swizzle, transpose_b=transpose_b], elect_one_warp: Bool, iter_idx: UInt32, k_start: UInt32)`

Execute MMA operations for one pipeline stage.

This is the core MMA function designed to be called within a consumer
tiles context:

```
with consumer.get_tiles() as tiles:
    Self.mma(tmem_addr, tiles, mma_op, ...)
```

**Args:**

* â€‹tmem\_addr ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Tensor memory address for accumulators.
* â€‹tiles ([`ConsumerTiles`](/mojo/kernels/linalg/matmul/gpu/sm100_structured/ring_buffer/ConsumerTiles)): ConsumerTiles context with stage, mbar, and tile arrays.
* â€‹mma\_op ([`MmaOpSM100_SS`](/mojo/kernels/linalg/arch/sm100/mma/MmaOpSM100_SS)): The MMA operation instance.
* â€‹elect\_one\_warp ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether this warp should execute.
* â€‹iter\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): K iteration index.
* â€‹k\_start ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Starting K iteration (for init\_c determination).

### `run`

`static run(a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], cluster_dim: StaticTuple[Int32, 3], mnk: StaticTuple[UInt32, 3], workspace: Span[UInt64, MutAnyOrigin])`

Main kernel entry point for SM100 matrix multiplication.

### `run_splitk`

`static run_splitk[reduction_layout: Layout](a_tma_op: TMATensorTile[a_type, a_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_layout, b_desc_layout], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], reduction_tensor: LayoutTensor[MatmulConfig[a_type, b_type, c_type, transpose_b].accum_type, reduction_layout, MutAnyOrigin], lock_ptr: LegacyUnsafePointer[UInt8], cluster_dim: StaticTuple[Int32, 3], mnk: StaticTuple[UInt32, 3], workspace: Span[UInt64, MutAnyOrigin])`

Split-K kernel entry point for better parallelism on small problems.

Split-K divides the K dimension across multiple CTAs, with each CTA
computing a partial result that is then reduced.

**Args:**

* â€‹a\_tma\_op ([`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)): TMA descriptor for matrix A.
* â€‹b\_tma\_op ([`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)): TMA descriptor for matrix B.
* â€‹c\_tma\_op ([`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)): TMA descriptor for matrix C.
* â€‹reduction\_tensor ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Workspace for partial results from each split.
* â€‹lock\_ptr ([`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)): Synchronization locks for reduction coordination.
* â€‹cluster\_dim ([`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)): Cluster dimensions.
* â€‹mnk ([`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)): Problem dimensions (M, N, K).
* â€‹workspace ([`Span`](/mojo/std/memory/span/Span)): Workspace buffer for profiling/scheduling.

</section>

---

## KernelContext

<section class='mojo-docs'>

`struct KernelContext[num_clc_pipeline_stages: Int, cta_group: Int, CLUSTER_M: Int, CLUSTER_N: Int]`

Shared kernel state: election vars, CTA coords, multicast masks, pipeline states.

## Fields

* â€‹elect\_one\_warp (`Bool`):
* â€‹elect\_one\_thread (`Bool`):
* â€‹elect\_one\_cta (`Bool`):
* â€‹is\_first\_cta\_in\_cluster (`Bool`):
* â€‹warp\_id (`UInt32`):
* â€‹rank\_m (`UInt`):
* â€‹rank\_n (`UInt`):
* â€‹peer\_cta\_coord (`Tuple[UInt, UInt, UInt]`):
* â€‹a\_multicast\_mask (`UInt16`):
* â€‹b\_multicast\_mask (`UInt16`):
* â€‹mma\_complete\_mask (`Int`):
* â€‹ptr\_tmem\_addr (`LegacyUnsafePointer[UInt32, address_space=AddressSpace.SHARED]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = False`

### `__del__is_trivial`

`comptime __del__is_trivial = False`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = False`

## Methods

### `__init__`

`__init__(out self, ptr_tmem_addr: LegacyUnsafePointer[UInt32, address_space=AddressSpace.SHARED])`

Initialize context from TMEM pointer; computes all derived state.

</section>

---

## WarpRole (Matmul_kernels)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct WarpRole`

Warp role identifiers for SM100 warp-specialized kernel.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `Epilogue`

`comptime Epilogue = WarpRole(3)`

### `MainLoad`

`comptime MainLoad = WarpRole(5)`

### `Mma`

`comptime Mma = WarpRole(6)`

### `Scheduler`

`comptime Scheduler = WarpRole(4)`

## Methods

### `__eq__`

`__eq__(self, other: UInt) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ge__`

`__ge__(self, other: UInt) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_main_load`

`static is_main_load() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_mma`

`static is_mma() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_epilogue`

`static is_epilogue() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_scheduler`

`static is_scheduler() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## consumer_main_loop (Matmul_kernels)

<section class='mojo-docs'>

`consumer_main_loop[accum_type: DType, c_type: DType, a_type: DType, b_type: DType, a_smem_layout: Layout, b_smem_layout: Layout, a_swizzle: TensorMapSwizzle, b_swizzle: TensorMapSwizzle, transpose_b: Bool, pipeline_stages: Int, /, *, block_tile_shape: IndexList[3], mma_shape: IndexList[3], cta_group: Int = 1, cluster_shape: IndexList[3] = Index(1, 1, 1), k_group_size: UInt = 1](tmem_addr: UInt32, a_smem_iter: LayoutTensorIter[a_type, a_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], b_smem_iter: LayoutTensorIter[b_type, b_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], load_mma_pipeline: ProducerConsumerPipeline[pipeline_stages], mma_op: MmaOpSM100_SS[c_type, a_type, b_type, block_tile_shape, mma_shape, accum_type=accum_type, cta_group=cta_group, cluster_shape=cluster_shape, a_swizzle=a_swizzle, b_swizzle=b_swizzle, transpose_b=transpose_b], elect_one_warp: Bool, iter_idx: UInt32, k_start: UInt32)`

Consume tiles from shared memory and execute MMA operations.

This is the public API for external callers using SMemTileIter.

</section>

---

## f32_frag_to_smem (Matmul_kernels)

<section class='mojo-docs'>

`f32_frag_to_smem[swizzle_mode: TensorMapSwizzle, stageN: UInt](vec: SIMD[dtype, size], dst: LayoutTensor[_dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## matmul_kernels

<section class='mojo-docs'>

SM100 Matmul Kernel Structs - GPU kernel entry points and helpers.

This module contains the GPU kernel structs for SM100 matmul:

* WarpRole: Warp specialization roles (MMA, Load, Scheduler, Epilogue)
* KernelContext: Common kernel state (election vars, CTA coords, masks)
* B200MatmulSmem: Shared memory layout for the kernel
* BlackwellMatmulSM100Kernel: Main kernel struct with run() and run\_splitk()
* BlackwellMatmulSM100FallbackKernel: Simple fallback kernel
* consumer\_main\_loop: MMA consumer loop (for external callers)

Output pipeline functions (copy\_accum\_to\_gmem, multi\_stage\_store\_C) are in
matmul\_output.mojo.

The kernel implements a warp-specialized architecture:

* Scheduler warp: CLC-based tile scheduling
* TMA Load warp: Async memory transfers
* MMA warp: Tensor core operations with TMEM accumulators
* Epilogue warps: Output from TMEM to GMEM (see matmul\_output.mojo)

## `comptime` values

### `RLayout32Bits`

`comptime RLayout32Bits[layout: Layout] = RuntimeLayout[layout, element_type=DType.uint32, linear_idx_type=DType.uint32]`

#### Parameters

* â€‹layout ([`Layout`](/kernels/layout/layout/Layout)):

## Structs

* [â€‹`B200MatmulSmem`](./B200MatmulSmem): Shared memory layout for B200 SM100 matrix multiplication kernel.
* [â€‹`BlackwellMatmulSM100FallbackKernel`](./BlackwellMatmulSM100FallbackKernel): Simple fallback matmul kernel for SM100 (B200).
* [â€‹`BlackwellMatmulSM100Kernel`](./BlackwellMatmulSM100Kernel): Blackwell SM100 GEMM kernel with warp specialization.
* [â€‹`KernelContext`](./KernelContext): Shared kernel state: election vars, CTA coords, multicast masks, pipeline states.
* [â€‹`WarpRole`](./WarpRole): Warp role identifiers for SM100 warp-specialized kernel.

## Functions

* [â€‹`consumer_main_loop`](./consumer_main_loop): Consume tiles from shared memory and execute MMA operations.
* [â€‹`f32_frag_to_smem`](./f32_frag_to_smem):
* [â€‹`stsm_helper`](./stsm_helper): Store a fragment to shared memory using st.matrix.

</section>

---

## stsm_helper (Matmul_kernels)

<section class='mojo-docs'>

`stsm_helper[swizzle: Swizzle, stageN: UInt, transpose_c: Bool = False, swizzle_mode: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B](vec: SIMD[dtype, size], dst: LayoutTensor[_dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], warp_offset: UInt32 = 0)`

Store a fragment to shared memory using st.matrix.

Delegates to store\_fragment\_to\_smem for non-float32 types,
and to f32\_frag\_to\_smem for float32.

</section>

---

## accum_arrive (Matmul_output)

<section class='mojo-docs'>

`accum_arrive[cta_group: Int, num_stages: Int](stage: OutputStage[num_stages])`

Signal accumulator arrival. Delegates to AccumBarrier.

**Args:**

* â€‹stage (`OutputStage`): OutputStage containing pipeline and stage index.

</section>

---

## copy_accum_to_gmem (Matmul_output)

<section class='mojo-docs'>

`copy_accum_to_gmem[c_type: DType, c_layout: Layout, c_smem_layout: Layout, c_desc_layout: Layout, num_accum_pipeline_stages: Int, num_output_stages: Int, /, *, repeat: Int, accum_type: DType, cta_group: Int, epilogue_dtype: DType, block_tile_shape: IndexList[3], mma_shape: IndexList[3], num_output_warps: UInt, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, transpose_c: Bool = False](c_tiles: SMemTileArrayType[c_type, c_smem_layout, num_output_stages, 128], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], output_stage: OutputStage[num_accum_pipeline_stages], c_coord: Tuple[UInt32, UInt32], c_shape: Tuple[UInt32, UInt32])`

Epilogue pipeline: TMEM â†’ Registers â†’ SMEM â†’ GMEM (via TMA).

**Args:**

* â€‹c\_tiles ([`SMemTileArrayType`](/mojo/kernels/linalg/structuring/SMemTileArrayType)): Shared memory tiles for output staging.
* â€‹c\_tma\_op ([`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)): TMA descriptor for C matrix.
* â€‹output\_stage (`OutputStage`): Self-contained stage with pipeline, stage index, and TMEM offset.
* â€‹c\_coord ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): (M, N) tile coordinates.
* â€‹c\_shape ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): (M, N) matrix dimensions.

</section>

---

## matmul_output

<section class='mojo-docs'>

SM100 Matmul Output Pipeline - TMEM â†’ SMEM â†’ GMEM epilogue.

This module contains the output pipeline code for SM100 matmul:

* copy\_accum\_to\_gmem: Core epilogue pipeline (TMEM â†’ Registers â†’ SMEM â†’ GMEM)
* multi\_stage\_store\_C: Output pipeline orchestration for standard matmul
* multi\_stage\_store\_C\_split\_k: Output pipeline for split-K matmul

The output pipeline handles:

* Loading accumulated results from Tensor Memory (TMEM)
* Applying optional epilogue operations (bias, activation)
* Writing to shared memory via st.matrix instructions
* Transferring to global memory via TMA async stores

## Functions

* [â€‹`accum_arrive`](./accum_arrive): Signal accumulator arrival. Delegates to AccumBarrier.
* [â€‹`copy_accum_to_gmem`](./copy_accum_to_gmem): Epilogue pipeline: TMEM â†’ Registers â†’ SMEM â†’ GMEM (via TMA).
* [â€‹`multi_stage_store_C`](./multi_stage_store_C): Orchestrate output from TMEM to GMEM via shared memory.
* [â€‹`multi_stage_store_C_split_k`](./multi_stage_store_C_split_k): Split-K output pipeline with reduction.

</section>

---

## multi_stage_store_C (Matmul_output)

<section class='mojo-docs'>

`multi_stage_store_C[c_type: DType, c_smem_layout: Layout, c_layout: Layout, c_desc_layout: Layout, num_accum_pipeline_stages: Int, num_output_stages: Int, /, *, input_type: DType, accum_type: DType, block_tile_shape: IndexList[3], mma_shape: IndexList[3], stage_stride_cols: UInt, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, cta_group: Int = 1, num_output_warps: UInt = 4, max_tmem_cols: UInt = 512, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, transpose_c: Bool = False](c_tiles: SMemTileArrayType[c_type, c_smem_layout, num_output_stages, 128], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], stage: OutputStage[num_accum_pipeline_stages], work_tile_coord: Tuple[UInt32, UInt32], elect_one_warp: Bool, M: UInt32, N: UInt32)`

Orchestrate output from TMEM to GMEM via shared memory.

**Args:**

* â€‹c\_tiles ([`SMemTileArrayType`](/mojo/kernels/linalg/structuring/SMemTileArrayType)): Shared memory tiles for output staging.
* â€‹c\_tma\_op ([`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)): TMA descriptor for C matrix.
* â€‹stage (`OutputStage`): Self-contained output stage with pipeline, stage index, and TMEM offset.
* â€‹work\_tile\_coord ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): (M, N) tile coordinates.
* â€‹elect\_one\_warp ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether this warp is elected.
* â€‹M ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Matrix M dimension.
* â€‹N ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Matrix N dimension.

</section>

---

## multi_stage_store_C_split_k (Matmul_output)

<section class='mojo-docs'>

`multi_stage_store_C_split_k[c_type: DType, c_smem_layout: Layout, c_layout: Layout, c_desc_layout: Layout, reduction_layout: Layout, num_accum_pipeline_stages: Int, num_output_stages: Int, /, *, input_type: DType, accum_type: DType, block_tile_shape: IndexList[3], mma_shape: IndexList[3], stage_stride_cols: UInt, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, cta_group: Int = 1, num_output_warps: UInt = 4, max_tmem_cols: UInt = 512, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, register_based_epilogue: Bool = True, transpose_c: Bool = False](scheduler: TileScheduler[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k], reduction_tensor: LayoutTensor[accum_type, reduction_layout, MutAnyOrigin], c_tiles: SMemTileArrayType[c_type, c_smem_layout, num_output_stages, 128], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], stage: OutputStage[num_accum_pipeline_stages], work_info: WorkInfo, elect_one_warp: Bool, M: UInt32, N: UInt32)`

Split-K output pipeline with reduction.

**Args:**

* â€‹scheduler ([`TileScheduler`](/mojo/kernels/linalg/grouped_matmul_tile_scheduler/TileScheduler)): Split-K tile scheduler for reduction.
* â€‹reduction\_tensor ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor for accumulating partial results.
* â€‹c\_tiles ([`SMemTileArrayType`](/mojo/kernels/linalg/structuring/SMemTileArrayType)): Shared memory tiles for output staging.
* â€‹c\_tma\_op ([`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)): TMA descriptor for C matrix.
* â€‹stage (`OutputStage`): Self-contained output stage with pipeline, stage index, and TMEM offset.
* â€‹work\_info ([`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)): Current work item info.
* â€‹elect\_one\_warp ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether this warp is elected.
* â€‹M ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Matrix M dimension.
* â€‹N ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Matrix N dimension.

</section>

---

## ProducerConsumerPipeline (Pipeline)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ProducerConsumerPipeline[num_stages: Int]`

A producer-consumer pipeline using shared memory barriers to enforce synchronization (between producer and consumer warps).

This struct is commonly used with warp specialization to pipeline operations
between two warps/warpgroups with data dependencies.

## Parameters

* â€‹num\_stages ([`Int`](/mojo/std/builtin/int/Int)): The number of pipeline stages.

## Fields

* â€‹full (`MbarPtr`):
* â€‹empty (`MbarPtr`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]) -> Self`

Initialize the producer-consumer pipeline with default phases.

**Args:**

* â€‹ptr ([`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)): Pointer to shared memory barriers.

### `wait_producer`

`wait_producer(self)`

Consumer waits for producer.

### `wait_consumer`

`wait_consumer(self)`

Producer waits for consumer.

### `producer_mbar`

`producer_mbar(self, stage: UInt32) -> MbarPtr`

Get the producer barrier for a specific stage.

**Args:**

* â€‹stage ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The pipeline stage.

**Returns:**

`MbarPtr`: The shared memory barrier that the producer signals.

### `consumer_mbar`

`consumer_mbar(self, stage: UInt32) -> MbarPtr`

Get the consumer barrier for a specific stage.

**Args:**

* â€‹stage ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The pipeline stage.

**Returns:**

`MbarPtr`: The shared memory barrier that the consumer signals.

### `producer_stage`

`producer_stage(self) -> UInt32`

Get the current producer stage index.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): The current stage index for the producer (0 to num\_stages-1).

### `consumer_stage`

`consumer_stage(self) -> UInt32`

Get the current consumer stage index.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): The current stage index for the consumer (0 to num\_stages-1).

### `consumer_step`

`consumer_step(mut self)`

Advance the consumer to the next pipeline stage.

Increments the consumer stage and wraps to 0 when reaching num\_stages,
toggling the phase bit on wrap-around.
Only switch phase at end of pipeline because we assume all barriers
are at the same consumer/producer phase before checked. Once checked,
the execution moves to next barrier.

### `producer_step`

`producer_step(mut self)`

Advance the producer to the next pipeline stage.

Increments the producer stage and wraps to 0 when reaching num\_stages,
toggling the phase bit on wrap-around.

### `smem_bytes`

`static smem_bytes() -> UInt32`

Calculate the shared memory bytes required for pipeline barriers.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): The total number of bytes needed for all pipeline barriers
(2 \* num\_stages barriers).

### `init_mbars`

`init_mbars(self, producer_arrive_count: Int32, consumer_arrive_count: Int32)`

Initialize the smem barriers for the producer and consumer.

This function must be called by a single thread and must be called before any the pipeline object is used.

**Args:**

* â€‹producer\_arrive\_count ([`Int32`](/mojo/std/builtin/simd/#int32)): The number of threads that will arrive at the barrier marking data as produced.
* â€‹consumer\_arrive\_count ([`Int32`](/mojo/std/builtin/simd/#int32)): The number of threads that will arrive at the barrier marking data as consumed.

### `producer_signal_and_step`

`producer_signal_and_step(mut self)`

Wait for consumer, signal production complete, and advance stage.

Combines: wait\_consumer() + arrive on producer\_mbar + producer\_step()
Used for CLC throttling in the Load warp.

### `consumer_signal_and_step`

`consumer_signal_and_step(mut self)`

Wait for producer, signal consumption complete, and advance stage.

Combines: wait\_producer() + arrive on consumer\_mbar + consumer\_step()
Used for CLC throttling in the Scheduler warp.

</section>

---

## pipeline (3)

<section class='mojo-docs'>

## `comptime` values

### `MbarPtr`

`comptime MbarPtr = LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]`

## Structs

* [â€‹`ProducerConsumerPipeline`](./ProducerConsumerPipeline): A producer-consumer pipeline using shared memory barriers to enforce synchronization (between producer and consumer warps).

</section>

---

## Consumer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct Consumer[origin: MutOrigin, a_type: DType, b_type: DType, a_tile_layout: Layout, b_tile_layout: Layout, num_pipeline_stages: Int, num_group_stages: Int, k_group_size: Int]`

Consumer view with get\_tiles() API.

## Fields

* â€‹ring\_buffer\_ptr (`Pointer[Consumer[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].RingBufferType, origin]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `RingBufferType`

`comptime RingBufferType = RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size]`

## Methods

### `__enter__`

`__enter__(mut self) -> Self`

### `__exit__`

`__exit__(mut self)`

### `get_tiles`

`get_tiles(mut self) -> ConsumerTiles[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size]`

Get the next slot with stage, barrier, and tile arrays.

Synchronization is handled internally - waits for tiles to be ready.

**Returns:**

[`ConsumerTiles`](/mojo/kernels/linalg/matmul/gpu/sm100_structured/ring_buffer/ConsumerTiles)

</section>

---

## ConsumerTiles

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ConsumerTiles[origin: MutOrigin, a_type: DType, b_type: DType, a_tile_layout: Layout, b_tile_layout: Layout, num_pipeline_stages: Int, num_group_stages: Int, k_group_size: Int]`

Context manager for consumer access with stage, barrier, and tile arrays.

Provides everything needed to process k\_group tiles in a single context:
\- stage: Current pipeline stage index
\- mbar: Barrier for synchronization (tiles ready when context entered)
\- a\_tiles: Full A tile array (index with stage \* k\_group\_size + j)
\- b\_tiles: Full B tile array (index with stage \* k\_group\_size + j)

## Fields

* â€‹ring\_buffer\_ptr (`Pointer[ConsumerTiles[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].RingBufferType, origin]`):
* â€‹stage (`UInt32`):
* â€‹mbar (`MbarPtr`):
* â€‹a\_tiles (`ConsumerTiles[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].ATileArray`):
* â€‹b\_tiles (`ConsumerTiles[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].BTileArray`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ATileArray`

`comptime ATileArray = ConsumerTiles[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].RingBufferType.ATileArray`

### `BTileArray`

`comptime BTileArray = ConsumerTiles[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].RingBufferType.BTileArray`

### `RingBufferType`

`comptime RingBufferType = RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size]`

## Methods

### `__enter__`

`__enter__(mut self) -> Self`

### `__exit__`

`__exit__(mut self)`

</section>

---

## OutputConsumerContext

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct OutputConsumerContext[origin: MutOrigin, num_stages: Int, stage_stride_cols: Int, cta_group: Int]`

Context manager for epilogue consumer access to OutputRingBuffer.

Automatically calls acquire\_for\_epilogue on enter and release\_from\_epilogue
on exit.

Usage:
with output\_rb.consumer() as stage:
\# ... read from stage.tmem\_offset, write to GMEM ...
\# release\_from\_epilogue called automatically

## Fields

* â€‹ring\_buffer\_ptr (`Pointer[OutputConsumerContext[origin, num_stages, stage_stride_cols, cta_group].RingBufferType, origin]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `RingBufferType`

`comptime RingBufferType = OutputRingBuffer[num_stages, stage_stride_cols, cta_group]`

### `Stage`

`comptime Stage = OutputStage[num_stages]`

## Methods

### `__init__`

`__init__(ring_buffer_ptr: Pointer[OutputConsumerContext[origin, num_stages, stage_stride_cols, cta_group].RingBufferType, origin]) -> Self`

### `__enter__`

`__enter__(mut self) -> OutputConsumerContext[origin, num_stages, stage_stride_cols, cta_group].Stage`

**Returns:**

`OutputConsumerContext`

### `__exit__`

`__exit__(mut self)`

</section>

---

## OutputProducerContext

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct OutputProducerContext[origin: MutOrigin, num_stages: Int, stage_stride_cols: Int, cta_group: Int]`

Context manager for MMA producer access to OutputRingBuffer.

Automatically calls acquire\_for\_mma on enter and release\_from\_mma on exit.

Usage:
with output\_rb.producer() as stage:
\# ... MMA into stage.tmem\_offset ...
\# release\_from\_mma called automatically

## Fields

* â€‹ring\_buffer\_ptr (`Pointer[OutputProducerContext[origin, num_stages, stage_stride_cols, cta_group].RingBufferType, origin]`):
* â€‹stage (`OutputProducerContext[origin, num_stages, stage_stride_cols, cta_group].Stage`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `RingBufferType`

`comptime RingBufferType = OutputRingBuffer[num_stages, stage_stride_cols, cta_group]`

### `Stage`

`comptime Stage = OutputStage[num_stages]`

## Methods

### `__init__`

`__init__(ring_buffer_ptr: Pointer[OutputProducerContext[origin, num_stages, stage_stride_cols, cta_group].RingBufferType, origin]) -> Self`

### `__enter__`

`__enter__(mut self) -> OutputProducerContext[origin, num_stages, stage_stride_cols, cta_group].Stage`

**Returns:**

`OutputProducerContext`

### `__exit__`

`__exit__(mut self)`

</section>

---

## OutputRingBuffer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct OutputRingBuffer[num_stages: Int, stage_stride_cols: Int, cta_group: Int]`

Ring buffer for MMAâ†’Epilogue output pipeline.

Manages TMEM accumulator stage synchronization between MMA warps (producer)
and Epilogue warps (consumer). Unlike RingBuffer which manages SMEM tiles,
this manages stage indices and computes TMEM offsets.

The TMEM itself is allocated separately via tcgen05\_alloc; this struct
only coordinates access to different stages within that allocation.

Template Parameters:
num\_stages: Number of accumulator pipeline stages.
stage\_stride\_cols: TMEM column stride between stages.
cta\_group: CTA group size (1 or 2) for multicast signaling.

Usage:
\# Initialize barriers once (elect\_one\_warp/elect\_one\_thread):
OutputRingBuffer\[...].init\_barriers(storage\_ptr, prod\_cnt, cons\_cnt)

```
# Create ring buffer (each warp creates its own):
var output_rb = OutputRingBuffer[...](storage_ptr, tmem_addr, mask)

# MMA warp (producer):
with output_rb.producer() as stage:
    # ... perform MMA into stage.tmem_offset ...

# Epilogue warp (consumer):
with output_rb.consumer() as stage:
    # ... read from stage.tmem_offset, write to GMEM ...
```

## Fields

* â€‹pipeline (`OutputRingBuffer[num_stages, stage_stride_cols, cta_group].Pipeline`):
* â€‹tmem\_base\_addr (`UInt32`):
* â€‹mma\_complete\_mask (`UInt16`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `Pipeline`

`comptime Pipeline = ProducerConsumerPipeline[num_stages]`

### `Stage`

`comptime Stage = OutputStage[num_stages]`

## Methods

### `__init__`

`__init__(storage_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], tmem_base_addr: UInt32, mma_complete_mask: UInt16) -> Self`

Initialize output ring buffer.

Creates pipeline internally from storage pointer. Barriers must be
initialized via init\_barriers() before first use.

**Args:**

* â€‹storage\_ptr ([`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)): Pointer to shared memory barrier storage.
* â€‹tmem\_base\_addr ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Base TMEM address for accumulators.
* â€‹mma\_complete\_mask ([`UInt16`](/mojo/std/builtin/simd/#uint16)): Multicast mask for 2-SM MMA completion signaling.

### `init_barriers`

`static init_barriers(storage_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], producer_arv_count: Int32, consumer_arv_count: Int32)`

Initialize pipeline barriers. Called once by elect\_one thread.

**Args:**

* â€‹storage\_ptr ([`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)): Pointer to shared memory barrier storage.
* â€‹producer\_arv\_count ([`Int32`](/mojo/std/builtin/simd/#int32)): Expected arrival count for producer barriers.
* â€‹consumer\_arv\_count ([`Int32`](/mojo/std/builtin/simd/#int32)): Expected arrival count for consumer barriers.

### `acquire_for_mma`

`acquire_for_mma(self) -> OutputRingBuffer[num_stages, stage_stride_cols, cta_group].Stage`

Acquire a stage for MMA computation.

Waits for the epilogue to finish with this stage, then returns
the stage info with computed TMEM offset and pipeline reference.

**Returns:**

`OutputRingBuffer`: OutputStage with stage index, TMEM offset, and pipeline for signaling.

### `release_from_mma`

`release_from_mma(mut self, stage: OutputStage[num_stages])`

Signal MMA completion and advance to next stage.

Signals the epilogue that accumulator data is ready, using either
mma\_arrive (1-SM) or mma\_arrive\_multicast (2-SM).

**Args:**

* â€‹stage ([`OutputStage`](/mojo/kernels/linalg/matmul/gpu/sm100_structured/ring_buffer/OutputStage)): The stage being released (from acquire\_for\_mma).

### `acquire_for_epilogue`

`acquire_for_epilogue(self) -> OutputRingBuffer[num_stages, stage_stride_cols, cta_group].Stage`

Acquire a stage for epilogue processing.

Waits for MMA to complete this stage, then returns the stage info.

**Returns:**

`OutputRingBuffer`: OutputStage with stage index, TMEM offset, and pipeline for signaling.

### `release_from_epilogue`

`release_from_epilogue(mut self)`

Signal epilogue completion and advance to next stage.

Signals MMA that this accumulator stage is free for reuse.

### `producer`

`producer[origin: MutOrigin, //](ref [origin] self) -> OutputProducerContext[origin, num_stages, stage_stride_cols, cta_group]`

Get a producer context for MMA warp.

Usage:
with output\_rb.producer() as stage:
\# MMA into stage.tmem\_offset
\# release\_from\_mma called automatically

**Returns:**

`OutputProducerContext`

### `consumer`

`consumer[origin: MutOrigin, //](ref [origin] self) -> OutputConsumerContext[origin, num_stages, stage_stride_cols, cta_group]`

Get a consumer context for epilogue warp.

Usage:
with output\_rb.consumer() as stage:
\# Read from stage.tmem\_offset, write to GMEM
\# release\_from\_epilogue called automatically

**Returns:**

`OutputConsumerContext`

### `get_pipeline`

`get_pipeline(self) -> OutputRingBuffer[num_stages, stage_stride_cols, cta_group].Pipeline`

Get the underlying pipeline for barrier initialization.

Note: With OutputStage now carrying the pipeline, most code no longer
needs this. It's retained for init\_barriers() which needs the raw
pipeline before any OutputStage instances exist.

**Returns:**

`OutputRingBuffer`

</section>

---

## OutputStage

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct OutputStage[num_stages: Int]`

Stage info for output pipeline.

Contains the stage index, computed TMEM offset, and a copy of the pipeline.
This makes the stage self-contained, eliminating the need to pass the
pipeline separately to functions like multi\_stage\_store\_C.

Template Parameters:
num\_stages: Number of pipeline stages (must match the OutputRingBuffer).

## Fields

* â€‹stage (`UInt32`):
* â€‹tmem\_offset (`UInt32`):
* â€‹pipeline (`OutputStage[num_stages].Pipeline`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `Pipeline`

`comptime Pipeline = ProducerConsumerPipeline[num_stages]`

## Methods

### `__init__`

`__init__(stage: UInt32, tmem_offset: UInt32, pipeline: ProducerConsumerPipeline[num_stages]) -> Self`

</section>

---

## Producer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct Producer[origin: MutOrigin, a_type: DType, b_type: DType, a_tile_layout: Layout, b_tile_layout: Layout, num_pipeline_stages: Int, num_group_stages: Int, k_group_size: Int]`

Producer view with get\_tiles() API.

## Fields

* â€‹ring\_buffer\_ptr (`Pointer[Producer[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].RingBufferType, origin]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `RingBufferType`

`comptime RingBufferType = RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size]`

## Methods

### `__enter__`

`__enter__(mut self) -> Self`

### `__exit__`

`__exit__(mut self)`

### `drain`

`drain(mut self)`

Drain all pending pipeline stages.

Prevents the CTA from exiting while a peer CTA is still working on MMA.
Waits for each consumer slot and releases it, cycling through all
num\_group\_stages stages.

### `get_tiles`

`get_tiles(mut self) -> ProducerTiles[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size]`

Get the next available slot with stage, barrier, and tile arrays.

Synchronization is handled internally - waits for slot availability.

**Returns:**

`ProducerTiles`

</section>

---

## ProducerTiles

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ProducerTiles[origin: MutOrigin, a_type: DType, b_type: DType, a_tile_layout: Layout, b_tile_layout: Layout, num_pipeline_stages: Int, num_group_stages: Int, k_group_size: Int]`

Context manager for producer access with stage, barrier, and tile arrays.

Provides everything needed to load k\_group tiles in a single context:
\- stage: Current pipeline stage index
\- barrier: Barrier for synchronization (call expect\_bytes once for all k\_group)
\- a\_tiles: Full A tile array (index with stage \* k\_group\_size + j)
\- b\_tiles: Full B tile array (index with stage \* k\_group\_size + j)

## Fields

* â€‹ring\_buffer\_ptr (`Pointer[ProducerTiles[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].RingBufferType, origin]`):
* â€‹stage (`UInt32`):
* â€‹barrier (`MbarPtr`):
* â€‹a\_tiles (`ProducerTiles[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].ATileArray`):
* â€‹b\_tiles (`ProducerTiles[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].BTileArray`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ATileArray`

`comptime ATileArray = ProducerTiles[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].RingBufferType.ATileArray`

### `BTileArray`

`comptime BTileArray = ProducerTiles[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].RingBufferType.BTileArray`

### `RingBufferType`

`comptime RingBufferType = RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size]`

## Methods

### `__enter__`

`__enter__(mut self) -> Self`

### `__exit__`

`__exit__(mut self)`

</section>

---

## RingBuffer (Ring_buffer)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct RingBuffer[a_type: DType, b_type: DType, a_tile_layout: Layout, b_tile_layout: Layout, num_pipeline_stages: Int, num_group_stages: Int, k_group_size: Int]`

Ring buffer with tile storage for SM100 producer-consumer sync.

This is the SM90-style API where tiles are stored in the ring buffer
and returned directly from get\_tiles().

Template Parameters:
a\_type: Data type for A matrix tiles.
b\_type: Data type for B matrix tiles.
a\_tile\_layout: Memory layout for A tiles.
b\_tile\_layout: Memory layout for B tiles.
num\_pipeline\_stages: Total number of tile stages.
num\_group\_stages: Number of synchronization stages.
k\_group\_size: Number of tiles per synchronization stage.

## Fields

* â€‹pipeline (`RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].Pipeline`):
* â€‹a\_tiles (`RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].ATileArray`):
* â€‹b\_tiles (`RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].BTileArray`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ATile`

`comptime ATile = RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].ATileArray.Tile`

### `ATileArray`

`comptime ATileArray = SMemTileArrayType[a_type, a_tile_layout, num_pipeline_stages, 128]`

### `BTile`

`comptime BTile = RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].BTileArray.Tile`

### `BTileArray`

`comptime BTileArray = SMemTileArrayType[b_type, b_tile_layout, num_pipeline_stages, 128]`

### `Pipeline`

`comptime Pipeline = ProducerConsumerPipeline[num_group_stages]`

## Methods

### `__init__`

`__init__(storage_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], a_tiles: SMemTileArrayType[a_type, a_tile_layout, num_pipeline_stages, 128], b_tiles: SMemTileArrayType[b_type, b_tile_layout, num_pipeline_stages, 128]) -> Self`

Initialize ring buffer from storage pointer.

Creates pipeline internally from storage pointer. Barriers must be
initialized via init\_barriers() before first use.

**Args:**

* â€‹storage\_ptr ([`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)): Pointer to shared memory barrier storage.
* â€‹a\_tiles ([`SMemTileArrayType`](/mojo/kernels/linalg/structuring/SMemTileArrayType)): A matrix tile array in shared memory.
* â€‹b\_tiles ([`SMemTileArrayType`](/mojo/kernels/linalg/structuring/SMemTileArrayType)): B matrix tile array in shared memory.

### `init_barriers`

`static init_barriers(storage_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], producer_arv_count: Int32, consumer_arv_count: Int32)`

Initialize pipeline barriers. Called once by elect\_one thread.

**Args:**

* â€‹storage\_ptr ([`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)): Pointer to shared memory barrier storage.
* â€‹producer\_arv\_count ([`Int32`](/mojo/std/builtin/simd/#int32)): Expected arrival count for producer barriers.
* â€‹consumer\_arv\_count ([`Int32`](/mojo/std/builtin/simd/#int32)): Expected arrival count for consumer barriers.

### `producer`

`producer[origin: MutOrigin](ref [origin] self) -> Producer[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size]`

Get producer view with get\_tiles() API.

**Returns:**

`Producer`

### `consumer`

`consumer[origin: MutOrigin](ref [origin] self) -> Consumer[origin, a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size]`

Get consumer view with get\_tiles() API.

**Returns:**

`Consumer`

### `get_producer_tiles`

`get_producer_tiles(mut self) -> Tuple[UInt32, MbarPtr, RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].ATileArray, RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].BTileArray]`

Wait for slot and return stage, barrier, and tile arrays.

Synchronization is handled internally - waits for consumer to release slot.
Use stage to index: tiles.a\_tiles\[stage \* k\_group\_size + j]

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple): Tuple of (stage, barrier, a\_tiles, b\_tiles).

### `enqueue_tile`

`enqueue_tile(mut self)`

Signal producer finished loading and advance stage.

### `get_tile`

`get_tile[tile_idx_in_group: Int](self, stage: UInt32) -> Tuple[RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].ATile, RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].BTile]`

Get tiles at specific index within the current k\_group.

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

### `get_consumer_tiles`

`get_consumer_tiles(mut self) -> Tuple[UInt32, MbarPtr, RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].ATileArray, RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_group_stages, k_group_size].BTileArray]`

Wait for slot and return stage, barrier, and tile arrays.

Synchronization is handled internally - waits for producer to fill slot.
Use stage to index: tiles.a\_tiles\[stage \* k\_group\_size + j]

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple): Tuple of (stage, mbar, a\_tiles, b\_tiles).

### `release_slot`

`release_slot(mut self)`

Signal consumer finished and advance stage.

### `producer_stage`

`producer_stage(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `consumer_stage`

`consumer_stage(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `producer_mbar`

`producer_mbar(self, stage: UInt32) -> MbarPtr`

**Returns:**

`MbarPtr`

### `consumer_mbar`

`consumer_mbar(self, stage: UInt32) -> MbarPtr`

**Returns:**

`MbarPtr`

</section>

---

## ring_buffer (Ring_buffer)

<section class='mojo-docs'>

Ring buffer for SM100 producer-consumer synchronization.

Provides SM90-style get\_tiles() API for TMA-MMA pipeline synchronization.

Usage:
var ring\_buffer = RingBuffer\[...]\(pipeline, a\_tiles, b\_tiles)

```
# Producer: tiles contains stage, barrier, a_tiles, b_tiles
with ring_buffer.producer() as producer:
    with producer.get_tiles() as tiles:
        load_tiles(tiles)  # Access tiles.a_tiles[stage * k_group + j]

# Consumer: tiles contains stage, mbar, a_tiles, b_tiles
with ring_buffer.consumer() as consumer:
    with consumer.get_tiles() as tiles:
        mma_tiles(tiles)  # Access tiles.a_tiles[stage * k_group + j]
```

## `comptime` values

### `MbarPtr`

`comptime MbarPtr = LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]`

## Structs

* [â€‹`Consumer`](./Consumer): Consumer view with get\_tiles() API.
* [â€‹`ConsumerTiles`](./ConsumerTiles): Context manager for consumer access with stage, barrier, and tile arrays.
* [â€‹`OutputConsumerContext`](./OutputConsumerContext): Context manager for epilogue consumer access to OutputRingBuffer.
* [â€‹`OutputProducerContext`](./OutputProducerContext): Context manager for MMA producer access to OutputRingBuffer.
* [â€‹`OutputRingBuffer`](./OutputRingBuffer): Ring buffer for MMAâ†’Epilogue output pipeline.
* [â€‹`OutputStage`](./OutputStage): Stage info for output pipeline.
* [â€‹`Producer`](./Producer): Producer view with get\_tiles() API.
* [â€‹`ProducerTiles`](./ProducerTiles): Context manager for producer access with stage, barrier, and tile arrays.
* [â€‹`RingBuffer`](./RingBuffer): Ring buffer with tile storage for SM100 producer-consumer sync.

</section>

---

## TileLoaderTMA

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileLoaderTMA[a_tma_origin: ImmutOrigin, b_tma_origin: ImmutOrigin, a_type: DType, b_type: DType, a_layout: Layout, b_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, //, a_smem_layout: Layout, b_smem_layout: Layout, BM: Int, BN: Int, BK: Int, MMA_N: Int, cta_group: Int, k_group_size: Int, num_pipeline_stages: Int, num_group_stages: Int]`

TMA-based tile loader for SM100.

Encapsulates the complete tile loading logic including:

* K-group batching (multiple tiles per barrier)
* CTA group coordination (1-SM or 2-SM cooperative loading)
* Peer CTA slicing for 2-SM MMA
* expect\_bytes management

Template Parameters:
a\_tma\_origin: Origin type for A TMA pointer.
b\_tma\_origin: Origin type for B TMA pointer.
a\_type: Data type for A matrix.
b\_type: Data type for B matrix.
a\_layout: Global memory layout for A.
b\_layout: Global memory layout for B.
a\_desc\_layout: TMA descriptor layout for A.
b\_desc\_layout: TMA descriptor layout for B.
a\_smem\_layout: Shared memory tile layout for A.
b\_smem\_layout: Shared memory tile layout for B.
BM: Block tile M dimension.
BN: Block tile N dimension.
BK: Block tile K dimension.
MMA\_N: MMA N dimension for B coordinate calculation.
cta\_group: Number of CTAs cooperating, 1 or 2.
k\_group\_size: Number of K tiles per barrier sync.
num\_pipeline\_stages: Total pipeline stages.
num\_group\_stages: Pipeline stages / k\_group\_size.

## Fields

* â€‹a\_tma\_op (`TileLoaderTMA[a_smem_layout, b_smem_layout, BM, BN, BK, MMA_N, cta_group, k_group_size, num_pipeline_stages, num_group_stages].ATmaOpPtr`):
* â€‹b\_tma\_op (`TileLoaderTMA[a_smem_layout, b_smem_layout, BM, BN, BK, MMA_N, cta_group, k_group_size, num_pipeline_stages, num_group_stages].BTmaOpPtr`):
* â€‹a\_multicast\_mask (`UInt16`):
* â€‹b\_multicast\_mask (`UInt16`):
* â€‹peer\_rank\_n (`UInt`):
* â€‹peer\_rank\_m (`UInt`):
* â€‹peer\_m\_rank (`UInt`):
* â€‹work\_m\_coord (`UInt`):
* â€‹work\_n\_coord (`UInt`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `a_expected_bytes`

`comptime a_expected_bytes = (a_smem_layout.size() * size_of[a_type]())`

### `a_tma_load_size`

`comptime a_tma_load_size = a_desc_layout.size()`

### `a_tma_rows`

`comptime a_tma_rows = a_desc_layout.shape[0].value()`

### `ATmaOp`

`comptime ATmaOp = TMATensorTile[a_type, a_layout, a_desc_layout]`

### `ATmaOpPtr`

`comptime ATmaOpPtr = Pointer[TileLoaderTMA[a_smem_layout, b_smem_layout, BM, BN, BK, MMA_N, cta_group, k_group_size, num_pipeline_stages, num_group_stages].ATmaOp, a_tma_origin]`

### `b_expected_bytes`

`comptime b_expected_bytes = (b_smem_layout.size() * size_of[b_type]())`

### `b_tma_load_size`

`comptime b_tma_load_size = b_desc_layout.size()`

### `b_tma_rows`

`comptime b_tma_rows = b_desc_layout.shape[0].value()`

### `BTmaOp`

`comptime BTmaOp = TMATensorTile[b_type, b_layout, b_desc_layout]`

### `BTmaOpPtr`

`comptime BTmaOpPtr = Pointer[TileLoaderTMA[a_smem_layout, b_smem_layout, BM, BN, BK, MMA_N, cta_group, k_group_size, num_pipeline_stages, num_group_stages].BTmaOp, b_tma_origin]`

### `expected_bytes`

`comptime expected_bytes = ((cta_group * (TileLoaderTMA[a_smem_layout, b_smem_layout, BM, BN, BK, MMA_N, cta_group, k_group_size, num_pipeline_stages, num_group_stages].a_expected_bytes + TileLoaderTMA[a_smem_layout, b_smem_layout, BM, BN, BK, MMA_N, cta_group, k_group_size, num_pipeline_stages, num_group_stages].b_expected_bytes)) * k_group_size)`

## Methods

### `__init__`

`__init__(a_tma_op: Pointer[TileLoaderTMA[a_smem_layout, b_smem_layout, BM, BN, BK, MMA_N, cta_group, k_group_size, num_pipeline_stages, num_group_stages].ATmaOp, a_tma_origin], b_tma_op: Pointer[TileLoaderTMA[a_smem_layout, b_smem_layout, BM, BN, BK, MMA_N, cta_group, k_group_size, num_pipeline_stages, num_group_stages].BTmaOp, b_tma_origin], a_multicast_mask: UInt16, b_multicast_mask: UInt16, peer_cta_coord: Tuple[UInt, UInt, UInt]) -> Self`

Initialize the TMA tile loader.

**Args:**

* â€‹a\_tma\_op ([`Pointer`](/mojo/std/memory/pointer/Pointer)): Pointer to A matrix TMA descriptor.
* â€‹b\_tma\_op ([`Pointer`](/mojo/std/memory/pointer/Pointer)): Pointer to B matrix TMA descriptor.
* â€‹a\_multicast\_mask ([`UInt16`](/mojo/std/builtin/simd/#uint16)): Multicast mask for A tiles.
* â€‹b\_multicast\_mask ([`UInt16`](/mojo/std/builtin/simd/#uint16)): Multicast mask for B tiles.
* â€‹peer\_cta\_coord ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Peer CTA coordinates (rank\_n, rank\_m, peer\_m\_rank).

### `set_work_tile`

`set_work_tile(mut self, m_coord: UInt, n_coord: UInt)`

Set the current output tile coordinates.

**Args:**

* â€‹m\_coord ([`UInt`](/mojo/std/builtin/uint/UInt)): M coordinate of the output tile.
* â€‹n\_coord ([`UInt`](/mojo/std/builtin/uint/UInt)): N coordinate of the output tile.

### `load_tiles`

`load_tiles[tiles_origin: MutOrigin, //](self, tiles: ProducerTiles[tiles_origin, a_type, b_type, a_smem_layout, b_smem_layout, num_pipeline_stages, num_group_stages, k_group_size], iter_idx: UInt32, elect_one_cta: Bool)`

Load k\_group\_size A and B tiles using TMA.

**Args:**

* â€‹tiles ([`ProducerTiles`](/mojo/kernels/linalg/matmul/gpu/sm100_structured/ring_buffer/ProducerTiles)): ProducerTiles context with stage, barrier, and tile arrays.
* â€‹iter\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): K iteration index (base index, not multiplied).
* â€‹elect\_one\_cta ([`Bool`](/mojo/std/builtin/bool/Bool)): True if this CTA should call expect\_bytes.

</section>

---

## tile_loader

<section class='mojo-docs'>

TileLoader for SM100 matrix multiplication.

Provides tile loading abstractions for efficient global-to-shared memory
transfers using TMA with support for:

* K-group batching (multiple tiles per barrier synchronization)
* CTA group coordination (1-SM or 2-SM cooperative loading)
* Multicast for cluster distribution

Usage:
var loader = TileLoaderTMA\[...]\(a\_tma\_op, b\_tma\_op, masks, peer\_coord)
loader.set\_work\_tile(m\_coord, n\_coord)

```
with producer.get_tiles() as tiles:
    loader.load_tiles(tiles, k_iter, elect_one_cta)
```

## Structs

* [â€‹`TileLoaderTMA`](./TileLoaderTMA): TMA-based tile loader for SM100.

</section>

---

## AdvanceAfterWorkContext

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct AdvanceAfterWorkContext[work_origin: MutOrigin, state_origin: MutOrigin, num_stages: Int, cluster_shape: IndexList[3, element_type=DType.uint32], rasterize_order: RasterOrder, block_swizzle_size: Int]`

Context for warps that do work THEN advance (Load/Scheduler/Epilogue).

* **enter**: Returns current work\_info for use in the block
* **exit**: Fetches next work, assigns to work\_info, steps state

## Fields

* â€‹scheduler (`AdvanceAfterWorkContext[work_origin, state_origin, num_stages, cluster_shape, rasterize_order, block_swizzle_size].SchedulerType`):
* â€‹work\_info\_ptr (`Pointer[WorkInfo, work_origin]`):
* â€‹consumer\_state\_ptr (`Pointer[PipelineState[num_stages], state_origin]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `SchedulerType`

`comptime SchedulerType = TileScheduler[num_stages, cluster_shape, rasterize_order, block_swizzle_size]`

## Methods

### `__init__`

`__init__(scheduler: TileScheduler[num_stages, cluster_shape, rasterize_order, block_swizzle_size], work_info_ptr: Pointer[WorkInfo, work_origin], consumer_state_ptr: Pointer[PipelineState[num_stages], state_origin]) -> Self`

### `__enter__`

`__enter__(self) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `__exit__`

`__exit__(mut self)`

</section>

---

## PrefetchBeforeWorkContext

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct PrefetchBeforeWorkContext[work_origin: MutOrigin]`

Context for MMA warp that prefetches BEFORE work (software pipelining).

* Construction: Fetches next work and steps state immediately
* **enter**: Returns current work\_info for use in the block
* **exit**: Assigns prefetched work to work\_info

## Fields

* â€‹work\_info\_ptr (`Pointer[WorkInfo, work_origin]`):
* â€‹next\_work (`WorkInfo`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(work_info_ptr: Pointer[WorkInfo, work_origin], next_work: WorkInfo) -> Self`

### `__enter__`

`__enter__(self) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `__exit__`

`__exit__(mut self)`

</section>

---

## SchedulerWorkIterator

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SchedulerWorkIterator[num_stages: Int, cluster_shape: IndexList[3, element_type=DType.uint32], rasterize_order: RasterOrder, block_swizzle_size: Int]`

Work iterator for Scheduler warp - owns work\_info and both pipeline states.

The Scheduler warp uniquely needs to:

1. Consume work responses (like other warps) via next()
2. Signal throttle and produce new work requests via signal\_and\_advance()
3. Drain pending requests at exit via drain()

Usage:
var sched\_iter = scheduler.scheduler\_iterator()
while sched\_iter.has\_work():
with sched\_iter.next():
sched\_iter.signal\_and\_advance()
sched\_iter.drain()

## Fields

* â€‹scheduler (`SchedulerWorkIterator[num_stages, cluster_shape, rasterize_order, block_swizzle_size].SchedulerType`):
* â€‹work\_info (`WorkInfo`):
* â€‹consumer\_state (`PipelineState[num_stages]`):
* â€‹producer\_state (`PipelineState[num_stages]`):
* â€‹throttle\_pipeline (`SchedulerWorkIterator[num_stages, cluster_shape, rasterize_order, block_swizzle_size].ThrottlePipeline`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `SchedulerType`

`comptime SchedulerType = TileScheduler[num_stages, cluster_shape, rasterize_order, block_swizzle_size]`

### `ThrottlePipeline`

`comptime ThrottlePipeline = SchedulerWorkIterator[num_stages, cluster_shape, rasterize_order, block_swizzle_size].SchedulerType.ThrottlePipeline`

## Methods

### `__init__`

`__init__(scheduler: TileScheduler[num_stages, cluster_shape, rasterize_order, block_swizzle_size], work_info: WorkInfo) -> Self`

Create scheduler iterator. Throttle pipeline from scheduler.

### `has_work`

`has_work(self) -> Bool`

Check if there is more work to process.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `next`

`next[state_origin: MutOrigin, //](ref [state_origin] self) -> AdvanceAfterWorkContext[origin_of(state_origin._mlir_origin.work_info), origin_of(state_origin._mlir_origin.consumer_state), num_stages, cluster_shape, rasterize_order, block_swizzle_size]`

Get next work item.

**Returns:**

`AdvanceAfterWorkContext`

### `signal_and_advance`

`signal_and_advance(mut self)`

Signal CLC throttle consumer and advance to next work request.

Combines two operations that always happen together in Scheduler warp:

1. Signal throttle consumer (tells Load warp we've consumed a response)
2. Issue next CLC work request (producer side)

### `drain`

`drain(mut self)`

Drain all pending CLC requests before kernel exit.

Must be called after the work loop completes to ensure all
CLC pipeline stages are properly synchronized before exit.

</section>

---

## TileScheduler (3)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileScheduler[num_stages: Int, cluster_shape: IndexList[3, element_type=DType.uint32] = Index[dtype=DType.uint32](1, 1, 1), rasterize_order: RasterOrder = RasterOrder.AlongM, block_swizzle_size: Int = 8]`

## Fields

* â€‹cluster\_dim (`StaticTuple[Int32, 3]`):
* â€‹log\_cluster\_dim\_m (`FastDiv[DType.uint32]`):
* â€‹log\_cluster\_dim\_n (`FastDiv[DType.uint32]`):
* â€‹log\_cluster\_dim\_k (`FastDiv[DType.uint32]`):
* â€‹clc\_response (`LegacyUnsafePointer[UInt128, address_space=AddressSpace.SHARED]`):
* â€‹full\_mbar (`LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]`):
* â€‹empty\_mbar (`LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]`):
* â€‹throttle\_pipeline (`TileScheduler[num_stages, cluster_shape, rasterize_order, block_swizzle_size].ThrottlePipeline`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `cluster_size`

`comptime cluster_size = ((cluster_shape.__getitem__[3, DType.uint32, Int](0) * cluster_shape.__getitem__[3, DType.uint32, Int](1)) * cluster_shape.__getitem__[3, DType.uint32, Int](2))`

### `log_cluster_k`

`comptime log_cluster_k = FastDiv[DType.uint32](cluster_shape.__getitem__[3, DType.uint32, Int](2))`

### `log_cluster_m`

`comptime log_cluster_m = FastDiv[DType.uint32](cluster_shape.__getitem__[3, DType.uint32, Int](0))`

### `log_cluster_n`

`comptime log_cluster_n = FastDiv[DType.uint32](cluster_shape.__getitem__[3, DType.uint32, Int](1))`

### `ThrottlePipeline`

`comptime ThrottlePipeline = ProducerConsumerPipeline[num_stages]`

## Methods

### `__init__`

`__init__(cluster_dim: StaticTuple[Int32, 3], clc_response_ptr: LegacyUnsafePointer[UInt128, address_space=AddressSpace.SHARED], full_mbar_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], empty_mbar_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], throttle_storage_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]) -> Self`

### `init_throttle_barriers`

`static init_throttle_barriers(storage_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], producer_arv_count: Int32, consumer_arv_count: Int32)`

Initialize throttle pipeline barriers. Called once by elect\_one thread.

**Args:**

* â€‹storage\_ptr ([`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)): Pointer to shared memory barrier storage.
* â€‹producer\_arv\_count ([`Int32`](/mojo/std/builtin/simd/#int32)): Expected arrival count for producer barriers.
* â€‹consumer\_arv\_count ([`Int32`](/mojo/std/builtin/simd/#int32)): Expected arrival count for consumer barriers.

### `work_info_from_clc_response`

`static work_info_from_clc_response(result: LegacyUnsafePointer[UInt128, address_space=AddressSpace.SHARED]) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `work_info_from_cluster`

`static work_info_from_cluster(work_info: WorkInfo, cluster_dim: StaticTuple[Int32, 3], log_cluster_dim_m: FastDiv[DType.uint32], log_cluster_dim_n: FastDiv[DType.uint32]) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `initial_work_info`

`initial_work_info(self) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `fetch_next_work`

`fetch_next_work(self, work_info: WorkInfo, consumer_state: PipelineState[num_stages]) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `advance_after_work`

`advance_after_work[work_origin: MutOrigin, state_origin: MutOrigin, //](self, ref [work_origin] work_info: WorkInfo, ref [state_origin] consumer_state: PipelineState[num_stages]) -> AdvanceAfterWorkContext[work_origin, state_origin, num_stages, cluster_shape, rasterize_order, block_swizzle_size]`

Context for warps that do work THEN advance (Load/Scheduler/Epilogue).

Usage:
with scheduler.advance\_after\_work(work\_info, state) as current:
do\_work(current)
syncwarp()
\# After: work\_info updated, state stepped

**Returns:**

`AdvanceAfterWorkContext`

### `prefetch_before_work`

`prefetch_before_work[work_origin: MutOrigin, //](self, ref [work_origin] work_info: WorkInfo, mut consumer_state: PipelineState[num_stages]) -> PrefetchBeforeWorkContext[work_origin]`

Context for MMA warp that prefetches BEFORE work (software pipelining).

Fetches next work and steps state IMMEDIATELY (before the with block).

Usage:
with scheduler.prefetch\_before\_work(work\_info, state) as current:
do\_mma(current)  # Uses current, not prefetched
\# After: work\_info updated to prefetched value

**Returns:**

`PrefetchBeforeWorkContext`

### `work_iterator`

`work_iterator(self) -> WorkIterator[num_stages, cluster_shape, rasterize_order, block_swizzle_size]`

Create a per-warp work iterator with internally managed state.

Each warp should create its own work iterator. The iterator owns
work\_info, pipeline state, and throttle internally.

Usage:
var work\_iter = scheduler.work\_iterator()
while work\_iter.has\_work():
with work\_iter.next() as current:
work\_iter.throttle\_signal(ctx.is\_first\_cta\_in\_cluster)
do\_work(current)

**Returns:**

`WorkIterator`

### `scheduler_iterator`

`scheduler_iterator(self) -> SchedulerWorkIterator[num_stages, cluster_shape, rasterize_order, block_swizzle_size]`

Create iterator for Scheduler warp (owns work\_info and both pipeline states).

The Scheduler warp uniquely needs to both consume work responses and
produce new work requests. This iterator owns everything internally.

Usage:
var sched\_iter = scheduler.scheduler\_iterator()
while sched\_iter.has\_work():
with sched\_iter.next():
sched\_iter.signal\_and\_advance()
sched\_iter.drain()

**Returns:**

`SchedulerWorkIterator`

### `advance_to_next_work`

`advance_to_next_work(self, mut clc_state: PipelineState[num_stages]) -> PipelineState[num_stages]`

**Returns:**

[`PipelineState`](/mojo/kernels/layout/tma_async/PipelineState)

</section>

---

## WorkInfo (3)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct WorkInfo`

## Fields

* â€‹m (`UInt32`):
* â€‹n (`UInt32`):
* â€‹k\_start (`UInt32`):
* â€‹is\_valid\_tile (`Bool`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `is_valid`

`is_valid(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

</section>

---

## WorkIterator

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct WorkIterator[num_stages: Int, cluster_shape: IndexList[3, element_type=DType.uint32], rasterize_order: RasterOrder, block_swizzle_size: Int]`

Per-warp work iterator that owns work\_info and pipeline state.

Each warp creates its own WorkIterator which internally manages both
the current work item and the CLC pipeline consumer state. Throttle
pipeline is obtained from the scheduler.

Usage:
var work\_iter = scheduler.work\_iterator()
while work\_iter.has\_work():
with work\_iter.next() as current:
work\_iter.throttle\_signal(ctx.is\_first\_cta\_in\_cluster)
do\_work(current)

## Fields

* â€‹scheduler (`WorkIterator[num_stages, cluster_shape, rasterize_order, block_swizzle_size].SchedulerType`):
* â€‹work\_info (`WorkInfo`):
* â€‹consumer\_state (`PipelineState[num_stages]`):
* â€‹throttle\_pipeline (`WorkIterator[num_stages, cluster_shape, rasterize_order, block_swizzle_size].ThrottlePipeline`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `SchedulerType`

`comptime SchedulerType = TileScheduler[num_stages, cluster_shape, rasterize_order, block_swizzle_size]`

### `ThrottlePipeline`

`comptime ThrottlePipeline = WorkIterator[num_stages, cluster_shape, rasterize_order, block_swizzle_size].SchedulerType.ThrottlePipeline`

## Methods

### `__init__`

`__init__(scheduler: TileScheduler[num_stages, cluster_shape, rasterize_order, block_swizzle_size], work_info: WorkInfo) -> Self`

Create work iterator with initial work\_info. Throttle from scheduler.

### `has_work`

`has_work(self) -> Bool`

Check if there is more work to process.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `next`

`next[state_origin: MutOrigin, //](ref [state_origin] self) -> AdvanceAfterWorkContext[origin_of(state_origin._mlir_origin.work_info), origin_of(state_origin._mlir_origin.consumer_state), num_stages, cluster_shape, rasterize_order, block_swizzle_size]`

Get next work item (advance AFTER work pattern).

**Returns:**

`AdvanceAfterWorkContext`

### `next_prefetch`

`next_prefetch[state_origin: MutOrigin, //](ref [state_origin] self) -> PrefetchBeforeWorkContext[origin_of(state_origin._mlir_origin.work_info)]`

Get next work item with prefetch (advance BEFORE work pattern).

**Returns:**

`PrefetchBeforeWorkContext`

### `throttle_signal`

`throttle_signal(mut self, is_first_cta_in_cluster: Bool)`

Signal CLC throttle if this is the first CTA in cluster.

The Load warp acts as producer for CLC throttle, signaling that it has
started processing a new work item. This prevents the scheduler from
getting too far ahead.

**Args:**

* â€‹is\_first\_cta\_in\_cluster ([`Bool`](/mojo/std/builtin/bool/Bool)): Only first CTA signals to avoid duplicates.

</section>

---

## tile_scheduler (Tile_scheduler)

<section class='mojo-docs'>

## Structs

* [â€‹`AdvanceAfterWorkContext`](./AdvanceAfterWorkContext): Context for warps that do work THEN advance (Load/Scheduler/Epilogue).
* [â€‹`PrefetchBeforeWorkContext`](./PrefetchBeforeWorkContext): Context for MMA warp that prefetches BEFORE work (software pipelining).
* [â€‹`SchedulerWorkIterator`](./SchedulerWorkIterator): Work iterator for Scheduler warp - owns work\_info and both pipeline states.
* [â€‹`TileScheduler`](./TileScheduler):
* [â€‹`WorkInfo`](./WorkInfo):
* [â€‹`WorkIterator`](./WorkIterator): Per-warp work iterator that owns work\_info and pipeline state.

</section>

---

## AdvanceAfterWorkContextSplitK

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct AdvanceAfterWorkContextSplitK[work_origin: MutOrigin, state_origin: MutOrigin, num_stages: Int, reduction_tile_shape: IndexList[3], cluster_shape: IndexList[3, element_type=DType.uint32], rasterize_order: RasterOrder, block_swizzle_size: Int, num_split_k: Int]`

Context for warps that do work THEN advance (Load/Scheduler/Epilogue).

## Fields

* â€‹scheduler (`AdvanceAfterWorkContextSplitK[work_origin, state_origin, num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k].SchedulerType`):
* â€‹work\_info\_ptr (`Pointer[WorkInfo, work_origin]`):
* â€‹consumer\_state\_ptr (`Pointer[PipelineState[num_stages], state_origin]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `SchedulerType`

`comptime SchedulerType = TileScheduler[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k]`

## Methods

### `__init__`

`__init__(scheduler: TileScheduler[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k], work_info_ptr: Pointer[WorkInfo, work_origin], consumer_state_ptr: Pointer[PipelineState[num_stages], state_origin]) -> Self`

### `__enter__`

`__enter__(self) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `__exit__`

`__exit__(mut self)`

</section>

---

## PrefetchBeforeWorkContextSplitK

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct PrefetchBeforeWorkContextSplitK[work_origin: MutOrigin]`

Context for MMA warp that prefetches BEFORE work (software pipelining).

## Fields

* â€‹work\_info\_ptr (`Pointer[WorkInfo, work_origin]`):
* â€‹next\_work (`WorkInfo`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(work_info_ptr: Pointer[WorkInfo, work_origin], next_work: WorkInfo) -> Self`

### `__enter__`

`__enter__(self) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `__exit__`

`__exit__(mut self)`

</section>

---

## SchedulerWorkIteratorSplitK

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SchedulerWorkIteratorSplitK[num_stages: Int, reduction_tile_shape: IndexList[3], cluster_shape: IndexList[3, element_type=DType.uint32], rasterize_order: RasterOrder, block_swizzle_size: Int, num_split_k: Int]`

Work iterator for Scheduler warp (split-K) - owns work\_info and both states. Throttle pipeline is obtained from the scheduler.

## Fields

* â€‹scheduler (`SchedulerWorkIteratorSplitK[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k].SchedulerType`):
* â€‹work\_info (`WorkInfo`):
* â€‹consumer\_state (`PipelineState[num_stages]`):
* â€‹producer\_state (`PipelineState[num_stages]`):
* â€‹throttle\_pipeline (`SchedulerWorkIteratorSplitK[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k].ThrottlePipeline`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `SchedulerType`

`comptime SchedulerType = TileScheduler[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k]`

### `ThrottlePipeline`

`comptime ThrottlePipeline = SchedulerWorkIteratorSplitK[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k].SchedulerType.ThrottlePipeline`

## Methods

### `__init__`

`__init__(scheduler: TileScheduler[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k], work_info: WorkInfo) -> Self`

Create scheduler iterator. Throttle pipeline from scheduler.

### `has_work`

`has_work(self) -> Bool`

Check if there is more work to process.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `next`

`next[state_origin: MutOrigin, //](ref [state_origin] self) -> AdvanceAfterWorkContextSplitK[origin_of(state_origin._mlir_origin.work_info), origin_of(state_origin._mlir_origin.consumer_state), num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k]`

Get next work item.

**Returns:**

`AdvanceAfterWorkContextSplitK`

### `signal_and_advance`

`signal_and_advance(mut self)`

Signal CLC throttle consumer and advance to next work request.

### `drain`

`drain(mut self)`

Drain all pending CLC requests before kernel exit.

</section>

---

## TileScheduler (4)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileScheduler[num_stages: Int, reduction_tile_shape: IndexList[3], cluster_shape: IndexList[3, element_type=DType.uint32] = Index[dtype=DType.uint32](1, 1, 1), rasterize_order: RasterOrder = RasterOrder.AlongM, block_swizzle_size: Int = 8, num_split_k: Int = 1]`

## Fields

* â€‹locks\_ptr (`LegacyUnsafePointer[Int32]`):
* â€‹scheduler (`TileScheduler[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k].UnderlyingScheduler`):
* â€‹total\_k\_tiles (`UInt32`):
* â€‹k\_tiles\_per\_split (`UInt32`):
* â€‹throttle\_pipeline (`TileScheduler[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k].ThrottlePipeline`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `BK`

`comptime BK = reduction_tile_shape.__getitem__[3, DType.int64, Int](2)`

### `BM`

`comptime BM = reduction_tile_shape.__getitem__[3, DType.int64, Int](0)`

### `MMA_N`

`comptime MMA_N = reduction_tile_shape.__getitem__[3, DType.int64, Int](1)`

### `ROW_SIZE`

`comptime ROW_SIZE = reduction_tile_shape.__getitem__[3, DType.int64, Int](1) if (eq reduction_tile_shape.__getitem__[3, DType.int64, Int](0)._mlir_value, 128) else (TileScheduler[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k].MMA_N // 2)`

### `ThrottlePipeline`

`comptime ThrottlePipeline = TileScheduler[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k].UnderlyingScheduler.ThrottlePipeline`

### `UnderlyingScheduler`

`comptime UnderlyingScheduler = TileScheduler[num_stages, cluster_shape, rasterize_order, block_swizzle_size]`

## Methods

### `__init__`

`__init__(cluster_dim: StaticTuple[Int32, 3], mnk: StaticTuple[UInt32, 3], clc_response_ptr: LegacyUnsafePointer[UInt128, address_space=AddressSpace.SHARED], full_mbar_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], empty_mbar_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], throttle_storage_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], locks_ptr: LegacyUnsafePointer[UInt8]) -> Self`

### `init_throttle_barriers`

`static init_throttle_barriers(storage_ptr: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], producer_arv_count: Int32, consumer_arv_count: Int32)`

Initialize throttle pipeline barriers. Called once by elect\_one thread.

### `convert_to_splitk_work_info`

`convert_to_splitk_work_info(self, work_info: WorkInfo) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `initial_work_info`

`initial_work_info(self) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `advance_to_next_work`

`advance_to_next_work(self, mut clc_state: PipelineState[num_stages]) -> PipelineState[num_stages]`

**Returns:**

[`PipelineState`](/mojo/kernels/layout/tma_async/PipelineState)

### `fetch_next_work`

`fetch_next_work(self, work_info: WorkInfo, consumer_state: PipelineState[num_stages]) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `advance_after_work`

`advance_after_work[work_origin: MutOrigin, state_origin: MutOrigin, //](self, ref [work_origin] work_info: WorkInfo, ref [state_origin] consumer_state: PipelineState[num_stages]) -> AdvanceAfterWorkContextSplitK[work_origin, state_origin, num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k]`

Context for warps that do work THEN advance (Load/Scheduler/Epilogue).

Usage:
with scheduler.advance\_after\_work(work\_info, state) as current:
do\_work(current)
syncwarp()
\# After: work\_info updated, state stepped

**Returns:**

`AdvanceAfterWorkContextSplitK`

### `prefetch_before_work`

`prefetch_before_work[work_origin: MutOrigin, //](self, ref [work_origin] work_info: WorkInfo, mut consumer_state: PipelineState[num_stages]) -> PrefetchBeforeWorkContextSplitK[work_origin]`

Context for MMA warp that prefetches BEFORE work (software pipelining).

Fetches next work and steps state IMMEDIATELY (before the with block).

Usage:
with scheduler.prefetch\_before\_work(work\_info, state) as current:
do\_mma(current)  # Uses current, not prefetched
\# After: work\_info updated to prefetched value

**Returns:**

`PrefetchBeforeWorkContextSplitK`

### `work_iterator`

`work_iterator(self) -> WorkIteratorSplitK[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k]`

Create a per-warp work iterator that owns work\_info internally. Throttle pipeline is obtained from the scheduler.

**Returns:**

`WorkIteratorSplitK`

### `scheduler_iterator`

`scheduler_iterator(self) -> SchedulerWorkIteratorSplitK[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k]`

Create iterator for Scheduler warp (owns work\_info and both states). Throttle pipeline is obtained from the scheduler.

**Returns:**

`SchedulerWorkIteratorSplitK`

### `is_last_split`

`is_last_split(self, work_tile_info: WorkInfo) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `output_tile_index`

`output_tile_index(self, work_info: WorkInfo) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `store_to_workspace`

`store_to_workspace[accum_type: DType, workspace_layout: Layout, /, *, do_reduction: Bool = False, write_back: Bool = False](self, tmem_addr: UInt32, reduction_workspace: LayoutTensor[accum_type, workspace_layout, origin], epilogue_thread_idx: UInt, reduction_tile_idx: UInt32)`

### `reduction`

`reduction[accum_type: DType, workspace_layout: Layout](self, reduction_workspace: LayoutTensor[accum_type, workspace_layout, origin], tmem_addr: UInt32, epilogue_thread_idx: UInt, work_info: WorkInfo) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `wait_eq`

`static wait_eq(lock_ptr: LegacyUnsafePointer[Int32], barrier_id: Int32, barrier_group_thread_idx: Int, lock_idx: UInt32, val: UInt32)`

### `wait_lt`

`static wait_lt(lock_ptr: LegacyUnsafePointer[Int32], barrier_id: Int32, barrier_group_thread_idx: Int, lock_idx: UInt32, count: UInt32)`

### `arrive_set`

`static arrive_set(lock_ptr: LegacyUnsafePointer[Int32], barrier_id: Int32, barrier_group_thread_idx: Int, lock_idx: UInt32, val: UInt32)`

</section>

---

## WorkInfo (4)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct WorkInfo`

## Fields

* â€‹m (`UInt32`):
* â€‹n (`UInt32`):
* â€‹k\_start (`UInt32`):
* â€‹num\_k\_tiles (`UInt32`):
* â€‹is\_valid\_tile (`Bool`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `INVALID_WORK_INFO`

`comptime INVALID_WORK_INFO = WorkInfo(0, 0, 0, 0, False)`

## Methods

### `is_valid`

`is_valid(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_final_split`

`is_final_split(self, k_tiles_per_output_tile: UInt32) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

</section>

---

## WorkIteratorSplitK

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct WorkIteratorSplitK[num_stages: Int, reduction_tile_shape: IndexList[3], cluster_shape: IndexList[3, element_type=DType.uint32], rasterize_order: RasterOrder, block_swizzle_size: Int, num_split_k: Int]`

Per-warp work iterator for split-K that owns work\_info and pipeline state. Throttle pipeline is obtained from the scheduler.

## Fields

* â€‹scheduler (`WorkIteratorSplitK[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k].SchedulerType`):
* â€‹work\_info (`WorkInfo`):
* â€‹consumer\_state (`PipelineState[num_stages]`):
* â€‹throttle\_pipeline (`WorkIteratorSplitK[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k].ThrottlePipeline`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `SchedulerType`

`comptime SchedulerType = TileScheduler[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k]`

### `ThrottlePipeline`

`comptime ThrottlePipeline = WorkIteratorSplitK[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k].SchedulerType.ThrottlePipeline`

## Methods

### `__init__`

`__init__(scheduler: TileScheduler[num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k], work_info: WorkInfo) -> Self`

Create work iterator. Throttle pipeline from scheduler.

### `has_work`

`has_work(self) -> Bool`

Check if there is more work to process.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `next`

`next[state_origin: MutOrigin, //](ref [state_origin] self) -> AdvanceAfterWorkContextSplitK[origin_of(state_origin._mlir_origin.work_info), origin_of(state_origin._mlir_origin.consumer_state), num_stages, reduction_tile_shape, cluster_shape, rasterize_order, block_swizzle_size, num_split_k]`

Get next work item (advance AFTER work pattern).

**Returns:**

`AdvanceAfterWorkContextSplitK`

### `next_prefetch`

`next_prefetch[state_origin: MutOrigin, //](ref [state_origin] self) -> PrefetchBeforeWorkContextSplitK[origin_of(state_origin._mlir_origin.work_info)]`

Get next work item with prefetch (advance BEFORE work pattern).

**Returns:**

`PrefetchBeforeWorkContextSplitK`

### `throttle_signal`

`throttle_signal(mut self, is_first_cta_in_cluster: Bool)`

Signal CLC throttle if this is the first CTA in cluster.

**Args:**

* â€‹is\_first\_cta\_in\_cluster ([`Bool`](/mojo/std/builtin/bool/Bool)): Only first CTA signals to avoid duplicates.

</section>

---

## get_num_tiles (Tile_scheduler_splitk)

<section class='mojo-docs'>

`get_num_tiles(problem_shape: IndexList[3], block_tile_shape: IndexList[3], cluster_shape: IndexList[2]) -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## get_required_locks_buffer_size_bytes (Tile_scheduler_splitk)

<section class='mojo-docs'>

`get_required_locks_buffer_size_bytes[accum_type: DType](problem_shape: IndexList[3], block_tile_shape: IndexList[3], cluster_shape: IndexList[2]) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## tile_scheduler_splitk (Tile_scheduler_splitk)

<section class='mojo-docs'>

## Structs

* [â€‹`AdvanceAfterWorkContextSplitK`](./AdvanceAfterWorkContextSplitK): Context for warps that do work THEN advance (Load/Scheduler/Epilogue).
* [â€‹`PrefetchBeforeWorkContextSplitK`](./PrefetchBeforeWorkContextSplitK): Context for MMA warp that prefetches BEFORE work (software pipelining).
* [â€‹`SchedulerWorkIteratorSplitK`](./SchedulerWorkIteratorSplitK): Work iterator for Scheduler warp (split-K) - owns work\_info and both states. Throttle pipeline is obtained from the scheduler.
* [â€‹`TileScheduler`](./TileScheduler):
* [â€‹`WorkInfo`](./WorkInfo):
* [â€‹`WorkIteratorSplitK`](./WorkIteratorSplitK): Per-warp work iterator for split-K that owns work\_info and pipeline state. Throttle pipeline is obtained from the scheduler.

## Functions

* [â€‹`get_num_tiles`](./get_num_tiles):
* [â€‹`get_required_locks_buffer_size_bytes`](./get_required_locks_buffer_size_bytes):

</section>

---

## AccumBarrier

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct AccumBarrier[cta_group: Int]`

Helper for accumulator pipeline barrier operations.

Handles the different arrival patterns for single-CTA vs 2-CTA groups.

Template Parameters:
cta\_group: Number of CTAs cooperating (1 or 2).

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `arrive`

`static arrive(pipeline: ProducerConsumerPipeline[num_stages], stage: UInt32)`

Signal accumulator arrival.

**Args:**

* â€‹pipeline ([`ProducerConsumerPipeline`](/mojo/kernels/linalg/matmul/gpu/sm100/pipeline/ProducerConsumerPipeline)): The MMA output pipeline.
* â€‹stage ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Current pipeline stage.

</section>

---

## AccumTile

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct AccumTile[dtype: DType, size: Int]`

Accumulator tile holding upper and lower fragment data.

SM100 accumulators in TMEM are stored as two halves (upper 16 rows,
lower 16 rows). This struct represents the complete tile being written.

This is the SM100 equivalent of SM90's RegTileType - the data being
written by the tile writer.

Template Parameters:
dtype: Data type of the fragments (typically epilogue\_dtype).
size: Number of elements per fragment.

## Fields

* â€‹upper (`SIMD[dtype, size]`):
* â€‹lower (`SIMD[dtype, size]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(upper: SIMD[dtype, size], lower: SIMD[dtype, size]) -> Self`

Create an accumulator tile from upper and lower fragments.

</section>

---

## EpilogueApplier

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct EpilogueApplier[MMA_M: Int, stageN: Int, num_stages: Int, repeats: Int, cta_group: Int, transpose_c: Bool]`

Apply element-wise epilogue operations on register fragments.

Computes global coordinates for each element and applies a lambda function.
Handles different MMA layouts (A/B/D/F) and transpose modes.

Template Parameters:
MMA\_M: MMA M dimension.
stageN: Stage width in elements.
num\_stages: Number of output stages.
repeats: Number of repetitions per load.
cta\_group: Number of CTAs cooperating (1 or 2).
transpose\_c: Whether output is transposed.

## Fields

* â€‹coords (`EpilogueApplier[MMA_M, stageN, num_stages, repeats, cta_group, transpose_c].Coords`):
* â€‹warp\_id (`UInt32`):
* â€‹lane\_id (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `Coords`

`comptime Coords = FragmentCoords[stageN, repeats]`

## Methods

### `__init__`

`__init__(warp_id: UInt32, lane_id: UInt32) -> Self`

Initialize the epilogue applier.

**Args:**

* â€‹warp\_id ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Warp ID within the CTA.
* â€‹lane\_id ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Lane ID within the warp.

### `compute_staged_coords`

`compute_staged_coords(self, stage: UInt32, c_row: UInt32, c_col: UInt32) -> Tuple[UInt32, UInt32]`

Compute staged row and column coordinates.

**Args:**

* â€‹stage ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Current stage index.
* â€‹c\_row ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Base row coordinate.
* â€‹c\_col ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Base column coordinate.

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple): Tuple of (staged\_row, staged\_col).

### `apply_to_fragment`

`apply_to_fragment[epilogue_dtype: DType, frag_size: Int, compute_lambda_fn: elementwise_compute_lambda_type](self, mut frag: SIMD[epilogue_dtype, frag_size], staged_row: UInt32, staged_col: UInt32, is_upper: Bool)`

Apply epilogue lambda to a fragment.

**Args:**

* â€‹frag ([`SIMD`](/mojo/std/builtin/simd/SIMD)): Fragment to apply epilogue to (modified in place).
* â€‹staged\_row ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Staged row coordinate.
* â€‹staged\_col ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Staged column coordinate.
* â€‹is\_upper ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether this is the upper or lower fragment.

### `apply_to_both_fragments`

`apply_to_both_fragments[epilogue_dtype: DType, frag_size: Int, compute_lambda_fn: elementwise_compute_lambda_type, is_lower_frag_required: Bool](self, mut upper_frag: SIMD[epilogue_dtype, frag_size], mut lower_frag: SIMD[epilogue_dtype, frag_size], stage: UInt32, c_row: UInt32, c_col: UInt32) -> Tuple[SIMD[epilogue_dtype, frag_size], SIMD[epilogue_dtype, frag_size]]`

Apply epilogue lambda to both upper and lower fragments.

This is the main entry point for register-based epilogue, replacing
the standalone register\_epilogue function.

**Args:**

* â€‹upper\_frag ([`SIMD`](/mojo/std/builtin/simd/SIMD)): Upper fragment to apply epilogue to.
* â€‹lower\_frag ([`SIMD`](/mojo/std/builtin/simd/SIMD)): Lower fragment to apply epilogue to.
* â€‹stage ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Current stage index.
* â€‹c\_row ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Base row coordinate.
* â€‹c\_col ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Base column coordinate.

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple): Tuple of (modified upper\_frag, modified lower\_frag).

</section>

---

## EpilogueConfig

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct EpilogueConfig[MMA_M: Int, MMA_N: Int, stageN: Int, cta_group: Int, transpose_c: Bool]`

Configuration for epilogue stage computations.

Computes the number of stages and other parameters needed for
the output epilogue based on MMA and CTA configuration.

Template Parameters:
MMA\_M: MMA M dimension.
MMA\_N: MMA N dimension.
stageN: Stage width in elements.
cta\_group: Number of CTAs cooperating (1 or 2).
transpose\_c: Whether output is transposed.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `bits`

`comptime bits = 256`

### `cg1_num_stages`

`comptime cg1_num_stages = (MMA_N // stageN)`

### `cg2_num_stages`

`comptime cg2_num_stages = (MMA_N // stageN) if (eq MMA_M._mlir_value, 256) else ((MMA_N // stageN) // 2)`

### `data_paths`

`comptime data_paths = 16`

### `fragment_size`

`comptime fragment_size = 4`

### `is_lower_frag_required`

`comptime is_lower_frag_required = (MMA_M == 64) if (eq cta_group._mlir_value, 1) else (cta_group == 1).__bool__().__invert__()`

### `num_stages`

`comptime num_stages = (MMA_N // stageN) if (eq MMA_M._mlir_value, 256) else ((MMA_N // stageN) // 2) if (eq cta_group._mlir_value, 2) else EpilogueConfig[MMA_M, MMA_N, stageN, cta_group, transpose_c].cg1_num_stages`

</section>

---

## FragmentCoords

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct FragmentCoords[stageN: Int, repeats: Int]`

Compute coordinates for fragment elements in tensor memory layout.

Based on tcgen05 matrix fragment layout (16x256b):
<https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-matrix-fragments-shape-16256b>

Template Parameters:
stageN: Stage width in elements.
repeats: Number of repetitions for wider loads.

## Fields

* â€‹top\_upper (`StaticTuple[UInt32, 2]`):
* â€‹bottom\_upper (`StaticTuple[UInt32, 2]`):
* â€‹top\_lower (`StaticTuple[UInt32, 2]`):
* â€‹bottom\_lower (`StaticTuple[UInt32, 2]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `load_width`

`comptime load_width = 2`

### `threads_per_row`

`comptime threads_per_row = ((stageN // repeats) // 2)`

## Methods

### `__init__`

`__init__(lane_id: UInt32) -> Self`

Initialize fragment coordinates based on lane ID.

**Args:**

* â€‹lane\_id ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Lane ID within the warp.

</section>

---

## OutputStageWriter

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct OutputStageWriter[c_type: DType, c_smem_layout: Layout, MMA_M: Int, MMA_N: Int, stageN: Int, cta_group: Int, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, transpose_c: Bool = False]`

Orchestrate writing a single output stage.

Coordinates TMEM read, optional epilogue, st.matrix to SMEM, and TMA store.

Template Parameters:
c\_type: Output data type.
c\_smem\_layout: Shared memory tile layout.
MMA\_M: MMA M dimension.
MMA\_N: MMA N dimension.
stageN: Stage width in elements.
cta\_group: Number of CTAs cooperating.
c\_swizzle: TMA swizzle mode.
transpose\_c: Whether output is transposed.

## Fields

* â€‹st\_writer (`OutputStageWriter[c_type, c_smem_layout, MMA_M, MMA_N, stageN, cta_group, c_swizzle, transpose_c].StWriter`):
* â€‹warp\_id (`UInt32`):
* â€‹lane\_id (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `Config`

`comptime Config = EpilogueConfig[MMA_M, MMA_N, stageN, cta_group, transpose_c]`

### `StWriter`

`comptime StWriter = StMatrixWriter[c_type, c_smem_layout, stageN, c_swizzle, transpose_c]`

## Methods

### `__init__`

`__init__(warp_id: UInt32, lane_id: UInt32) -> Self`

Initialize the output stage writer.

**Args:**

* â€‹warp\_id ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Warp ID within the CTA.
* â€‹lane\_id ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Lane ID within the warp.

### `write_upper_fragment`

`write_upper_fragment[frag_size: Int, epilogue_dtype: DType](self, frag: SIMD[epilogue_dtype, frag_size], dst: LayoutTensor[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], warp_offset: UInt32 = 0)`

Write the upper fragment to shared memory.

**Args:**

* â€‹frag ([`SIMD`](/mojo/std/builtin/simd/SIMD)): Upper fragment (already cast to epilogue\_dtype).
* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination shared memory tile.
* â€‹warp\_offset ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Additional warp-based offset for transpose mode.

### `write_lower_fragment`

`write_lower_fragment[frag_size: Int, epilogue_dtype: DType](self, frag: SIMD[epilogue_dtype, frag_size], dst: LayoutTensor[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], warp_offset: UInt32 = 0)`

Write the lower fragment to shared memory.

**Args:**

* â€‹frag ([`SIMD`](/mojo/std/builtin/simd/SIMD)): Lower fragment (already cast to epilogue\_dtype).
* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination shared memory tile.
* â€‹warp\_offset ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Additional warp-based offset for transpose mode.

</section>

---

## SMemEpilogueWriter

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SMemEpilogueWriter[c_type: DType, c_smem_layout: Layout, num_output_stages: Int, //, epilogue_dtype: DType, BM: Int, BN: Int, MMA_M: Int, MMA_N: Int, cta_group: Int, num_output_warps: Int, c_swizzle: TensorMapSwizzle, transpose_c: Bool, is_lower_frag_required: Bool, num_stages: Int, simd_size: Int, stage: Int, rep_frag_size: Int, compute_lambda_fn: elementwise_compute_lambda_type]`

Write accumulator tile to SMEM and apply element-wise epilogue lambda.

This writer handles the SMEM-based epilogue path when register\_based\_epilogue=False.
Inferred from c\_tiles: c\_type, c\_smem\_layout, num\_output\_stages.
Derived from layout: stageN, stage\_contiguous\_size.

## Fields

* â€‹warp\_id (`UInt32`):
* â€‹c\_tiles (`SMemEpilogueWriter[epilogue_dtype, BM, BN, MMA_M, MMA_N, cta_group, num_output_warps, c_swizzle, transpose_c, is_lower_frag_required, num_stages, simd_size, stage, rep_frag_size, compute_lambda_fn].CTileArray`):
* â€‹M (`UInt32`):
* â€‹N (`UInt32`):
* â€‹c\_row (`UInt32`):
* â€‹c\_col (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `barrier_threads`

`comptime barrier_threads = (num_output_warps * WARP_SIZE)`

### `CTileArray`

`comptime CTileArray = SMemTileArrayType[c_type, c_smem_layout, num_output_stages, 128]`

### `data_paths`

`comptime data_paths = 16`

### `N_dim`

`comptime N_dim = 0 if transpose_c else 1`

### `stage_contiguous_size`

`comptime stage_contiguous_size = c_smem_layout.shape[1].value()`

### `stageN`

`comptime stageN = c_smem_layout.shape[SMemEpilogueWriter[epilogue_dtype, BM, BN, MMA_M, MMA_N, cta_group, num_output_warps, c_swizzle, transpose_c, is_lower_frag_required, num_stages, simd_size, stage, rep_frag_size, compute_lambda_fn].N_dim].value()`

### `swizzle`

`comptime swizzle = make_swizzle[c_type, c_swizzle]()`

### `swizzle_width`

`comptime swizzle_width = (c_swizzle.bytes() // size_of[c_type]())`

### `Tile`

`comptime Tile = AccumTile[epilogue_dtype, rep_frag_size]`

## Methods

### `__init__`

`__init__(warp_id: UInt32, c_tiles: SMemTileArrayType[c_type, c_smem_layout, num_output_stages, 128], c_shape: Tuple[UInt32, UInt32], c_coord: Tuple[UInt32, UInt32]) -> Self`

Initialize the SMEM epilogue writer.

### `write_tile`

`write_tile(self, tile: AccumTile[epilogue_dtype, rep_frag_size])`

Write accumulator tile to SMEM and apply epilogue lambda.

</section>

---

## StMatrixConfig

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct StMatrixConfig[c_type: DType, stageN: Int, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, transpose_c: Bool = False]`

Configuration for st.matrix store operations.

Computes the various constants needed for st.matrix operations
based on the output tile configuration.

Template Parameters:
c\_type: Output data type (e.g., bfloat16).
stageN: Stage width in elements.
c\_swizzle: TMA swizzle mode.
transpose\_c: Whether output is transposed.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `stmtx_simd_width`

`comptime stmtx_simd_width = 4 if (eq (stageN % 16)._mlir_value, 0) else 2`

### `stsmx_lane_size`

`comptime stsmx_lane_size = (16 // size_of[c_type]())`

### `stsmx_row_size`

`comptime stsmx_row_size = (32 // size_of[c_type]()) if (eq (stageN % 16)._mlir_value, 0) else (16 // size_of[c_type]())`

### `swizzle_width`

`comptime swizzle_width = (c_swizzle.bytes() // size_of[c_type]())`

## Methods

### `make_swizzle`

`static make_swizzle() -> Swizzle`

Create the swizzle pattern for st.matrix operations.

**Returns:**

[`Swizzle`](/mojo/kernels/layout/swizzle/Swizzle): Swizzle instance for the configured swizzle mode.

</section>

---

## StMatrixCoords

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct StMatrixCoords[MMA_M: Int, MMA_N: Int, stageN: Int, cta_group: Int, transpose_c: Bool]`

Compute coordinates for st.matrix operations.

Encapsulates the complex coordinate calculations needed for storing
accumulator fragments to shared memory.

Template Parameters:
MMA\_M: MMA M dimension.
MMA\_N: MMA N dimension.
stageN: Stage N dimension (width of each output tile).
cta\_group: Number of CTAs cooperating (1 or 2).
transpose\_c: Whether output is transposed.

## Fields

* â€‹warp\_id (`UInt32`):
* â€‹lane\_id (`UInt32`):
* â€‹c\_row (`UInt32`):
* â€‹c\_col (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(warp_id: UInt32, lane_id: UInt32, c_row: UInt32, c_col: UInt32) -> Self`

Initialize coordinate calculator.

**Args:**

* â€‹warp\_id ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Warp ID within the CTA.
* â€‹lane\_id ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Lane ID within the warp.
* â€‹c\_row ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Base row coordinate in global memory.
* â€‹c\_col ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Base column coordinate in global memory.

### `staged_row`

`staged_row(self, stage: UInt32, num_stages: UInt32) -> UInt32`

Compute the staged row coordinate.

**Args:**

* â€‹stage ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Current stage index.
* â€‹num\_stages ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Total number of stages.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): Row coordinate for the current stage.

### `staged_col`

`staged_col(self, stage: UInt32, num_stages: UInt32) -> UInt32`

Compute the staged column coordinate.

**Args:**

* â€‹stage ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Current stage index.
* â€‹num\_stages ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Total number of stages.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): Column coordinate for the current stage.

### `smem_coord_m`

`smem_coord_m(self) -> UInt32`

Compute shared memory M coordinate for TMA store.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): M coordinate in shared memory tile.

</section>

---

## StMatrixWriter

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct StMatrixWriter[c_type: DType, c_smem_layout: Layout, stageN: Int, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, transpose_c: Bool = False]`

Write register fragments to shared memory using st.matrix.

Handles the complex swizzling and addressing required for efficient
shared memory writes from WGMMA accumulator fragments.

Template Parameters:
c\_type: Output data type.
c\_smem\_layout: Shared memory tile layout.
stageN: Stage width in elements.
c\_swizzle: TMA swizzle mode.
transpose\_c: Whether output is transposed.

## Fields

* â€‹swizzle (`Swizzle`):
* â€‹lane\_id (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `Config`

`comptime Config = StMatrixConfig[c_type, stageN, c_swizzle, transpose_c]`

### `shape0`

`comptime shape0 = c_smem_layout.shape[1].value() if (xor transpose_c._mlir_value, True) else c_smem_layout.shape[0].value()`

### `stride0`

`comptime stride0 = c_smem_layout.stride[0].value()`

### `stride1`

`comptime stride1 = c_smem_layout.stride[1].value()`

### `stsmx_tile_offset`

`comptime stsmx_tile_offset = (StMatrixWriter[c_type, c_smem_layout, stageN, c_swizzle, transpose_c].stride0 if transpose_c else StMatrixWriter[c_type, c_smem_layout, stageN, c_swizzle, transpose_c].stride1 * StMatrixWriter[c_type, c_smem_layout, stageN, c_swizzle, transpose_c].Config.stsmx_row_size)`

## Methods

### `__init__`

`__init__(lane_id: UInt32) -> Self`

Initialize the st.matrix writer.

**Args:**

* â€‹lane\_id ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Lane ID within the warp.

### `compute_lane_offset`

`compute_lane_offset(self) -> UInt32`

Compute the base lane offset for st.matrix.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): Lane offset in shared memory.

### `write_fragment`

`write_fragment[frag_size: Int](self, frag: SIMD[dtype, frag_size], dst: LayoutTensor[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], warp_offset: UInt32 = 0)`

Write a fragment to shared memory using st.matrix.

**Args:**

* â€‹frag ([`SIMD`](/mojo/std/builtin/simd/SIMD)): Source fragment (typically from TMEM load).
* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination shared memory tile.
* â€‹warp\_offset ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Additional warp-based offset for transpose mode.

</section>

---

## TMAStoreCoords

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TMAStoreCoords[BM: Int, BN: Int, MMA_M: Int, MMA_N: Int, stageN: Int, cta_group: Int, c_smem_shape0: Int, stage: Int]`

Compute TMA store coordinates and warp election for SM100 epilogue.

Encapsulates the complex coordinate computation logic for TMA stores,
including cta\_group-specific branching and warp election.

Template Parameters:
BM: Block M dimension.
BN: Block N dimension.
MMA\_M: MMA M dimension.
MMA\_N: MMA N dimension.
stageN: Stage width in elements.
cta\_group: Number of CTAs cooperating (1 or 2).
c\_smem\_shape0: Shape\[0] of shared memory tile layout.
stage: Current output stage index.

## Fields

* â€‹coord\_m (`UInt`):
* â€‹coord\_n (`UInt`):
* â€‹elect\_one\_warp (`Bool`):
* â€‹c\_smem\_coord\_m (`UInt`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `CG1_TMA_BM`

`comptime CG1_TMA_BM = c_smem_shape0`

### `CG2_TMA_BM`

`comptime CG2_TMA_BM = c_smem_shape0 if (eq MMA_M._mlir_value, 256) else BM`

### `stage_n_offset`

`comptime stage_n_offset = (stage * stageN)`

### `TMA_BM`

`comptime TMA_BM = c_smem_shape0 if (eq MMA_M._mlir_value, 256) else BM if (eq cta_group._mlir_value, 2) else TMAStoreCoords[BM, BN, MMA_M, MMA_N, stageN, cta_group, c_smem_shape0, stage].CG1_TMA_BM`

## Methods

### `__init__`

`__init__(c_coord: Tuple[UInt32, UInt32], warp_id: UInt32) -> Self`

Compute all TMA store coordinates.

**Args:**

* â€‹c\_coord ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Output tile coordinates (m\_tile, n\_tile).
* â€‹warp\_id ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Current warp ID.

</section>

---

## TMAStoreExecutor

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TMAStoreExecutor[c_type: DType, c_smem_layout: Layout, BM: Int, BN: Int, MMA_M: Int, MMA_N: Int, stageN: Int, stage_contiguous_size: Int, cta_group: Int, c_swizzle: TensorMapSwizzle, transpose_c: Bool, is_lower_frag_required: Bool]`

Execute TMA store from shared memory to global memory with proper tiling.

Encapsulates all the complex SMEM tiling/reshaping logic for TMA stores.
Handles 3 distinct paths based on transpose\_c, cta\_group, and MMA\_M:

1. transpose\_c + cta\_group==2 + MMA\_M==128: Split reshape
2. transpose\_c + other: Loop over swizzle-width tiles
3. non-transpose: Simple tile selection

Template Parameters:
c\_type: Output data type.
c\_smem\_layout: Shared memory layout for C tile.
BM: Block M dimension.
BN: Block N dimension.
MMA\_M: MMA M dimension.
MMA\_N: MMA N dimension.
stageN: Stage width in elements.
stage\_contiguous\_size: Contiguous size in SMEM layout.
cta\_group: Number of CTAs cooperating (1 or 2).
c\_swizzle: TensorMap swizzle mode.
transpose\_c: Whether output is transposed.
is\_lower\_frag\_required: Whether lower fragment is used.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `c_smem_shape0`

`comptime c_smem_shape0 = c_smem_layout.shape[0].value()`

### `CG1_TMA_BM`

`comptime CG1_TMA_BM = TMAStoreExecutor[c_type, c_smem_layout, BM, BN, MMA_M, MMA_N, stageN, stage_contiguous_size, cta_group, c_swizzle, transpose_c, is_lower_frag_required].c_smem_shape0`

### `CG2_TMA_BM`

`comptime CG2_TMA_BM = c_smem_layout.shape[0].value() if (eq MMA_M._mlir_value, 256) else BM`

### `num_c_smem_tiles`

`comptime num_c_smem_tiles = ((128 // TMAStoreExecutor[c_type, c_smem_layout, BM, BN, MMA_M, MMA_N, stageN, stage_contiguous_size, cta_group, c_swizzle, transpose_c, is_lower_frag_required].swizzle_width) // 1 if is_lower_frag_required else 2)`

### `swizzle_width`

`comptime swizzle_width = (c_swizzle.bytes() // size_of[c_type]())`

### `TMA_BM`

`comptime TMA_BM = c_smem_layout.shape[0].value() if (eq MMA_M._mlir_value, 256) else BM if (eq cta_group._mlir_value, 2) else TMAStoreExecutor[c_type, c_smem_layout, BM, BN, MMA_M, MMA_N, stageN, stage_contiguous_size, cta_group, c_swizzle, transpose_c, is_lower_frag_required].CG1_TMA_BM`

## Methods

### `execute`

`static execute[c_layout: Layout, c_desc_layout: Layout](c_smem_tile: LayoutTensor[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], store_coords: TMAStoreCoords[BM, BN, MMA_M, MMA_N, stageN, cta_group, TMAStoreExecutor[c_type, c_smem_layout, BM, BN, MMA_M, MMA_N, stageN, stage_contiguous_size, cta_group, c_swizzle, transpose_c, is_lower_frag_required].c_smem_shape0, stage], c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], warp_id: UInt32, lane: UInt32)`

Execute TMA store with appropriate tiling for the configuration.

**Args:**

* â€‹c\_smem\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Source shared memory tile.
* â€‹store\_coords ([`TMAStoreCoords`](/mojo/kernels/linalg/matmul/gpu/sm100_structured/tile_writer/TMAStoreCoords)): Precomputed TMA store coordinates.
* â€‹c\_tma\_op ([`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)): TMA tensor tile for async store operations.
* â€‹warp\_id ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Current warp ID.
* â€‹lane ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Current lane ID within warp.

</section>

---

## TMEMFragment

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TMEMFragment[accum_type: DType, epilogue_type: DType, frag_size: Int]`

Accumulator fragment pair from tensor memory.

SM100 TMEM stores data in upper/lower fragment pairs due to the
physical layout of tensor memory datapaths.

Template Parameters:
accum\_type: Accumulator data type (e.g., float32).
epilogue\_type: Epilogue data type after casting (e.g., bfloat16).
frag\_size: Number of elements per fragment.

## Fields

* â€‹upper (`SIMD[accum_type, frag_size]`):
* â€‹lower (`SIMD[accum_type, frag_size]`):
* â€‹has\_lower (`Bool`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(has_lower: Bool = True) -> Self`

Initialize empty fragments.

**Args:**

* â€‹has\_lower ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether lower fragment is needed (based on MMA config).

### `cast_upper`

`cast_upper(self) -> SIMD[epilogue_type, frag_size]`

Cast upper fragment to epilogue type.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): Upper fragment cast to epilogue\_type.

### `cast_lower`

`cast_lower(self) -> SIMD[epilogue_type, frag_size]`

Cast lower fragment to epilogue type.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): Lower fragment cast to epilogue\_type.

</section>

---

## TMEMReader

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TMEMReader[accum_type: DType, data_paths: Int = 16, bits: Int = 256, repeat: Int = 4]`

Load accumulator fragments from tensor memory (TMEM).

SM100 Blackwell GPUs have dedicated tensor memory for MMA accumulators.
This struct encapsulates the tcgen05\_ld operations.

Template Parameters:
accum\_type: Accumulator data type.
data\_paths: Number of datapaths (always 16 for SM100).
bits: Bits per load (always 256 for SM100).
repeat: Number of repetitions for wider loads.

## Fields

* â€‹base\_addr (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `frag_size`

`comptime frag_size = (((data_paths * (bits // 32)) // 32) * repeat)`

### `lower_offset`

`comptime lower_offset = 1048576`

## Methods

### `__init__`

`__init__(base_addr: UInt32) -> Self`

Initialize TMEM reader.

**Args:**

* â€‹base\_addr ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Base tensor memory address for the accumulator.

### `stage_addr`

`stage_addr(self, stage: Int, stageN: Int) -> UInt32`

Compute TMEM address for a given stage.

**Args:**

* â€‹stage ([`Int`](/mojo/std/builtin/int/Int)): Stage index.
* â€‹stageN ([`Int`](/mojo/std/builtin/int/Int)): Stage width in elements.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): TMEM address for the stage.

</section>

---

## TMEMToSMemWriter

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TMEMToSMemWriter[c_type: DType, accum_type: DType, c_smem_layout: Layout, BM: Int, BN: Int, MMA_M: Int, MMA_N: Int, stageN: Int, cta_group: Int, num_output_warps: Int, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, transpose_c: Bool = False]`

Write TMEM accumulator fragments to shared memory for SM100.

This is the SM100-specific equivalent of SM90's FragmentToSMemWriter.
Key difference: SM100 accumulators live in Tensor Memory (TMEM),
not registers, so we need tcgen05\_ld to load them first.

Handles three tile reshaping cases:

1. transpose\_c + is\_lower\_frag\_required: 2 warps share swizzle blocks
2. transpose\_c + !is\_lower\_frag\_required: 4 warps, upper only
3. !transpose\_c: Simple row-major tiling

Template Parameters:
c\_type: Output data type (e.g., bfloat16).
accum\_type: Accumulator data type (e.g., float32).
c\_smem\_layout: Shared memory tile layout.
BM: Block M dimension.
BN: Block N dimension.
MMA\_M: MMA M dimension.
MMA\_N: MMA N dimension.
stageN: Stage N dimension.
cta\_group: Number of CTAs cooperating (1 or 2).
num\_output\_warps: Number of warps participating in output.
c\_swizzle: TMA swizzle mode.
transpose\_c: Whether output is transposed.

## Fields

* â€‹warp\_id (`UInt32`):
* â€‹lane\_id (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `Config`

`comptime Config = EpilogueConfig[MMA_M, MMA_N, stageN, cta_group, transpose_c]`

### `data_paths`

`comptime data_paths = 16`

### `stage_contiguous_size`

`comptime stage_contiguous_size = c_smem_layout.shape[1].value()`

### `swizzle`

`comptime swizzle = make_swizzle[c_type, c_swizzle]()`

### `swizzle_width`

`comptime swizzle_width = (c_swizzle.bytes() // size_of[c_type]())`

## Methods

### `__init__`

`__init__(warp_id: UInt32, lane_id: UInt32) -> Self`

Initialize the TMEM to SMEM writer.

**Args:**

* â€‹warp\_id ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Warp ID within the CTA.
* â€‹lane\_id ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Lane ID within the warp.

### `write_stage`

`write_stage[repeat: Int, bits: Int = 256](self, tmem_addr: UInt32, stage: Int, c_smem_tile: LayoutTensor[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128])`

Write a single stage from TMEM to shared memory with tile reshaping.

Automatically handles the correct tile reshaping based on transpose\_c
and is\_lower\_frag\_required configuration.

Template Parameters:
repeat: Repeat factor for fragment loading.
bits: TMEM bits width (default 256).

**Args:**

* â€‹tmem\_addr ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Base tensor memory address.
* â€‹stage ([`Int`](/mojo/std/builtin/int/Int)): Current stage index.
* â€‹c\_smem\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Base shared memory tile (will be reshaped internally).

### `write_fragments`

`write_fragments[repeat: Int](self, upper_frag: SIMD[c_type, (4 * repeat)], lower_frag: SIMD[c_type, (4 * repeat)], c_smem_tile: LayoutTensor[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128])`

Write pre-loaded fragments to shared memory with tile reshaping.

Use this when fragments are loaded separately (e.g., with load\_tmem\_fragments)
and need to be written after applying register-based epilogue.

Template Parameters:
repeat: Repeat factor matching the fragment size.

**Args:**

* â€‹upper\_frag ([`SIMD`](/mojo/std/builtin/simd/SIMD)): Upper fragment (already casted to c\_type).
* â€‹lower\_frag ([`SIMD`](/mojo/std/builtin/simd/SIMD)): Lower fragment (already casted to c\_type, ignored if not needed).
* â€‹c\_smem\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Base shared memory tile (will be reshaped internally).

</section>

---

## tile_writer

<section class='mojo-docs'>

TileWriter components for SM100 matrix multiplication epilogue.

This module provides modular components for the output pipeline:

1. **TMAStoreWriter**: TMA async store from shared memory to global memory
2. **StMatrixWriter**: Register to shared memory via st.matrix instructions
3. **TMEMReader**: Load accumulator data from tensor memory to registers
4. **EpilogueApplier**: Apply element-wise operations on fragments

The SM100 epilogue pipeline flows as:
TMEM (accumulators) â†’ Registers â†’ SMEM â†’ GMEM (via TMA)

Usage:
\# TMA store from shared memory to global memory
var tma\_writer = TMAStoreWriter[...](c_tma_op)
tma\_writer.store\_tile(c\_smem\_tile, (n\_coord, m\_coord))

## `comptime` values

### `RLayout32Bits`

`comptime RLayout32Bits[layout: Layout] = RuntimeLayout[layout, element_type=DType.uint32, linear_idx_type=DType.uint32]`

#### Parameters

* â€‹layout ([`Layout`](/kernels/layout/layout/Layout)):

### `ThreadwiseStoreWriter`

`comptime ThreadwiseStoreWriter = TileWriterThreadwise[?, ?, ?]`

### `TMAStoreWriter`

`comptime TMAStoreWriter = TileWriterTMA`

## Structs

* [â€‹`AccumBarrier`](./AccumBarrier): Helper for accumulator pipeline barrier operations.
* [â€‹`AccumTile`](./AccumTile): Accumulator tile holding upper and lower fragment data.
* [â€‹`EpilogueApplier`](./EpilogueApplier): Apply element-wise epilogue operations on register fragments.
* [â€‹`EpilogueConfig`](./EpilogueConfig): Configuration for epilogue stage computations.
* [â€‹`FragmentCoords`](./FragmentCoords): Compute coordinates for fragment elements in tensor memory layout.
* [â€‹`OutputStageWriter`](./OutputStageWriter): Orchestrate writing a single output stage.
* [â€‹`SMemEpilogueWriter`](./SMemEpilogueWriter): Write accumulator tile to SMEM and apply element-wise epilogue lambda.
* [â€‹`StMatrixConfig`](./StMatrixConfig): Configuration for st.matrix store operations.
* [â€‹`StMatrixCoords`](./StMatrixCoords): Compute coordinates for st.matrix operations.
* [â€‹`StMatrixWriter`](./StMatrixWriter): Write register fragments to shared memory using st.matrix.
* [â€‹`TMAStoreCoords`](./TMAStoreCoords): Compute TMA store coordinates and warp election for SM100 epilogue.
* [â€‹`TMAStoreExecutor`](./TMAStoreExecutor): Execute TMA store from shared memory to global memory with proper tiling.
* [â€‹`TMEMFragment`](./TMEMFragment): Accumulator fragment pair from tensor memory.
* [â€‹`TMEMReader`](./TMEMReader): Load accumulator fragments from tensor memory (TMEM).
* [â€‹`TMEMToSMemWriter`](./TMEMToSMemWriter): Write TMEM accumulator fragments to shared memory for SM100.

## Functions

* [â€‹`load_tmem_fragments`](./load_tmem_fragments): Load upper and lower fragments from TMEM and cast to epilogue type.
* [â€‹`shared_memory_epilogue`](./shared_memory_epilogue): Apply element-wise epilogue to non-transposed shared memory tile.
* [â€‹`shared_memory_epilogue_transpose`](./shared_memory_epilogue_transpose): Apply element-wise epilogue to transposed shared memory tile.
* [â€‹`store_fragment_to_smem`](./store_fragment_to_smem): Store a fragment to shared memory using st.matrix.
* [â€‹`tma_store_with_pipeline`](./tma_store_with_pipeline): Perform TMA store with pipelined commit and wait.
* [â€‹`tma_wait_pipelined`](./tma_wait_pipelined): Wait for TMA stores with pipelining.

</section>

---

## load_tmem_fragments

<section class='mojo-docs'>

`load_tmem_fragments[accum_type: DType, epilogue_type: DType, frag_size: Int, is_lower_required: Bool, data_paths: Int = 16, bits: Int = 256, repeat: Int = 1](tmem_addr: UInt32) -> Tuple[SIMD[epilogue_type, (frag_size * repeat)], SIMD[epilogue_type, (frag_size * repeat)]]`

Load upper and lower fragments from TMEM and cast to epilogue type.

This encapsulates the common pattern of loading accumulator data from
tensor memory, waiting for completion, and casting to output type.

Template Parameters:
accum\_type: Accumulator data type (e.g., float32).
epilogue\_type: Output data type after casting (e.g., bfloat16).
frag\_size: Base fragment size per warp.
is\_lower\_required: Whether lower fragment is needed.
data\_paths: TMEM data paths (default 16).
bits: TMEM bits width (default 256).
repeat: Repeat factor for larger fragments.

**Args:**

* â€‹tmem\_addr ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Tensor memory address for this stage.

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple): Tuple of (upper\_casted, lower\_casted) SIMD fragments.

</section>

---

## shared_memory_epilogue (Tile_writer)

<section class='mojo-docs'>

`shared_memory_epilogue[MMA_M: UInt, data_paths: UInt, num_stages: UInt, stage: UInt, stageN: UInt, c_type: DType, shared_n: UInt, simd_size: UInt, c_smem_upper_layout: Layout, c_smem_lower_layout: Layout, swizzle: Swizzle, compute_lambda_fn: elementwise_compute_lambda_type, num_output_warps: UInt](M: UInt32, N: UInt32, c_col: UInt, c_row: UInt, c_smem_warp_tile_upper: LayoutTensor[c_type, c_smem_upper_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_smem_warp_tile_lower: LayoutTensor[c_type, c_smem_lower_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Apply element-wise epilogue to non-transposed shared memory tile.

Handles the non-transpose case for SMEM-based epilogue. Processes upper
and lower fragments separately with proper coordinate mapping.

Template Parameters:
MMA\_M: MMA M dimension.
data\_paths: Number of data paths (typically 16).
num\_stages: Total number of output stages.
stage: Current output stage index.
stageN: Stage width in elements.
c\_type: Output data type.
shared\_n: Shared memory N dimension.
simd\_size: SIMD width for vectorized access.
c\_smem\_upper\_layout: Layout for upper fragment tile.
c\_smem\_lower\_layout: Layout for lower fragment tile.
swizzle: Swizzle pattern for SMEM access.
compute\_lambda\_fn: Element-wise compute function.
num\_output\_warps: Number of warps participating.

**Args:**

* â€‹M ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Output M dimension.
* â€‹N ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Output N dimension.
* â€‹c\_col ([`UInt`](/mojo/std/builtin/uint/UInt)): Base column coordinate.
* â€‹c\_row ([`UInt`](/mojo/std/builtin/uint/UInt)): Base row coordinate.
* â€‹c\_smem\_warp\_tile\_upper ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Upper fragment shared memory tile.
* â€‹c\_smem\_warp\_tile\_lower ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Lower fragment shared memory tile.

</section>

---

## shared_memory_epilogue_transpose (Tile_writer)

<section class='mojo-docs'>

`shared_memory_epilogue_transpose[stage: UInt, stageN: UInt, c_type: DType, c_smem_layout: Layout, swizzle: Swizzle, compute_lambda_fn: elementwise_compute_lambda_type, num_output_warps: UInt, warp_dim: UInt, MMA_M: Int, BN: Int, cta_group: Int](M: UInt32, N: UInt32, c_col: UInt, c_row: UInt, c_smem: LayoutTensor[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], warp_i: UInt, warp_j: UInt)`

Apply element-wise epilogue to transposed shared memory tile.

Handles the transpose\_c case for SMEM-based epilogue. Supports two warp
configurations based on warp\_dim parameter.

Template Parameters:
stage: Current output stage index.
stageN: Stage width in elements.
c\_type: Output data type.
c\_smem\_layout: Shared memory tile layout.
swizzle: Swizzle pattern for SMEM access.
compute\_lambda\_fn: Element-wise compute function.
num\_output\_warps: Number of warps participating.
warp\_dim: Warp dimension configuration (1 or 2).
MMA\_M: MMA M dimension.
BN: Block N dimension.
cta\_group: Number of CTAs cooperating.

**Args:**

* â€‹M ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Output M dimension.
* â€‹N ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Output N dimension.
* â€‹c\_col ([`UInt`](/mojo/std/builtin/uint/UInt)): Base column coordinate.
* â€‹c\_row ([`UInt`](/mojo/std/builtin/uint/UInt)): Base row coordinate.
* â€‹c\_smem ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Shared memory tile.
* â€‹warp\_i ([`UInt`](/mojo/std/builtin/uint/UInt)): Warp index i.
* â€‹warp\_j ([`UInt`](/mojo/std/builtin/uint/UInt)): Warp index j.

</section>

---

## store_fragment_to_smem

<section class='mojo-docs'>

`store_fragment_to_smem[swizzle: Swizzle, stageN: Int, transpose_c: Bool = False, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B](vec: SIMD[dtype, size], dst: LayoutTensor[_dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], warp_offset: UInt32 = 0)`

Store a fragment to shared memory using st.matrix.

This function provides a static interface compatible with stsm\_helper,
delegating to the underlying st.matrix operations.

Template Parameters:
swizzle: Pre-computed swizzle pattern.
stageN: Stage width in elements.
transpose\_c: Whether output is transposed.
c\_swizzle: TMA swizzle mode (for configuration).

**Args:**

* â€‹vec ([`SIMD`](/mojo/std/builtin/simd/SIMD)): Source SIMD fragment.
* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination shared memory tile.
* â€‹warp\_offset ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Additional warp-based offset for transpose mode.

</section>

---

## tma_store_with_pipeline

<section class='mojo-docs'>

`tma_store_with_pipeline[c_type: DType, c_layout: Layout, c_desc_layout: Layout, is_last_stage: Bool](c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout], src: LayoutTensor[c_type, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=128], coords: Tuple[UInt, UInt])`

Perform TMA store with pipelined commit and wait.

Encapsulates the common SM100 output pattern:

1. fence\_async\_view\_proxy()
2. async\_store()
3. commit\_group()
4. wait\_group() with pipelining

Template Parameters:
c\_type: Output data type.
c\_layout: Global memory layout for C.
c\_desc\_layout: TMA descriptor layout for C.
is\_last\_stage: If True, wait for all; else keep 1 in flight.

**Args:**

* â€‹c\_tma\_op ([`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)): TMA tensor tile descriptor.
* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Source shared memory tile.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Destination coordinates in global memory.

</section>

---

## tma_wait_pipelined

<section class='mojo-docs'>

`tma_wait_pipelined[c_type: DType, c_layout: Layout, c_desc_layout: Layout, is_last_stage: Bool](c_tma_op: TMATensorTile[c_type, c_layout, c_desc_layout])`

Wait for TMA stores with pipelining.

For SM100 output pipeline:

* Non-last stages: Keep 1 store in flight for pipelining
* Last stage: Wait for all stores to complete

Template Parameters:
c\_type: Output data type.
c\_layout: Global memory layout for C.
c\_desc\_layout: TMA descriptor layout for C.
is\_last\_stage: If True, wait for all; else keep 1 in flight.

**Args:**

* â€‹c\_tma\_op ([`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)): TMA tensor tile descriptor.

</section>

---

## create_matmul_configs_ampere

<section class='mojo-docs'>

`create_matmul_configs_ampere[key: String, a_type: DType, b_type: DType, c_type: DType, transpose_b: Bool]() -> MatmulConfig[a_type, b_type, c_type, transpose_b]`

**Returns:**

[`MatmulConfig`](/mojo/kernels/linalg/utils_gpu/MatmulConfig)

</section>

---

## get_dispatch_table

<section class='mojo-docs'>

`get_dispatch_table[a_type: DType, b_type: DType, c_type: DType, transpose_b: Bool]() -> Dict[String, MatmulConfig[a_type, b_type, c_type, transpose_b], default_comp_time_hasher]`

**Returns:**

`Dict`

</section>

---

## dispatch (Dispatch)

<section class='mojo-docs'>

## Functions

* [â€‹`create_matmul_configs_ampere`](./create_matmul_configs_ampere):
* [â€‹`get_dispatch_table`](./get_dispatch_table):

</section>

---

## sm80

<section class='mojo-docs'>

Provides the CPU Hopper backend implementations for matmuls.

## Modules

* [â€‹`dispatch`](./dispatch/):

</section>

---

## MatmulConfig (Config)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MatmulConfig[a_type: DType, b_type: DType, c_type: DType, transpose_b: Bool = True]`

Static configuration of SM90 GPU matmul.

## Fields

* â€‹block\_tile\_shape (`IndexList[3]`):
* â€‹mma\_shape (`IndexList[3]`):
* â€‹cluster\_shape (`IndexList[3]`):
* â€‹num\_pipeline\_stages (`UInt`):
* â€‹num\_k\_partitions (`UInt`):
* â€‹num\_consumer (`UInt`):
* â€‹partitioned\_multicast (`Bool`):
* â€‹k\_group\_size (`UInt`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Equatable`](/mojo/std/builtin/comparable/Equatable),
[`Hashable`](/mojo/std/hashlib/hash/Hashable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(M: Int, N: Int, K: Int, num_k_partitions: UInt = 1, partitioned_multicast: Bool = False, pdl_level: PDLLevel = PDLLevel.OFF, k_groups: OptionalReg[UInt] = None, consumer_groups: OptionalReg[Int] = None) -> Self`

Initialize MatmulConfig by computing optimal values from M, N, K.

**Args:**

* â€‹M ([`Int`](/mojo/std/builtin/int/Int)): The M dimension of the matmul.
* â€‹N ([`Int`](/mojo/std/builtin/int/Int)): The N dimension of the matmul.
* â€‹K ([`Int`](/mojo/std/builtin/int/Int)): The K dimension of the matmul.
* â€‹num\_k\_partitions ([`UInt`](/mojo/std/builtin/uint/UInt)): Number of K partitions.
* â€‹partitioned\_multicast ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to use partitioned multicast.
* â€‹pdl\_level ([`PDLLevel`](/mojo/std/gpu/primitives/grid_controls/PDLLevel)): PDL level for grid controls.
* â€‹k\_groups ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): How many pipeline (loads and stores) are grouped together.
* â€‹consumer\_groups ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): The number of consumer groups.

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `pdl_level`

`pdl_level(self) -> PDLLevel`

**Returns:**

[`PDLLevel`](/mojo/std/gpu/primitives/grid_controls/PDLLevel)

### `to_base_config`

`to_base_config(self) -> MatmulConfig[a_type, b_type, c_type, transpose_b]`

Convert to base MatmulConfig from utils\_gpu.

**Returns:**

[`MatmulConfig`](/mojo/kernels/linalg/utils_gpu/MatmulConfig)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

### `__repr__`

`__repr__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `__hash__`

`__hash__[H: Hasher](self, mut hasher: H)`

Updates hasher with the underlying bytes.

**Parameters:**

* â€‹H ([`Hasher`](/mojo/std/hashlib/hasher/Hasher)): The hasher type.

**Args:**

* â€‹hasher (`H`): The hasher instance.

</section>

---

## build_configs (Config)

<section class='mojo-docs'>

`build_configs[a_type: DType, b_type: DType, c_type: DType, N: Int, K: Int, transpose_b: Bool = True, num_k_partitions: UInt = 1, partitioned_multicast: Bool = False, pdl_level: PDLLevel = PDLLevel.OFF, k_groups: OptionalReg[UInt] = None, consumer_groups: OptionalReg[Int] = None]() -> Set[MatmulConfig[a_type, b_type, c_type, transpose_b]]`

**Returns:**

`Set`

</section>

---

## config (Config)

<section class='mojo-docs'>

## Structs

* [â€‹`MatmulConfig`](./MatmulConfig): Static configuration of SM90 GPU matmul.

## Functions

* [â€‹`build_configs`](./build_configs):

</section>

---

## dispatch (3)

<section class='mojo-docs'>

## `comptime` values

### `DISPATCH_HIT`

`comptime DISPATCH_HIT = 1`

### `DISPATCH_MISS`

`comptime DISPATCH_MISS = 0`

### `llama_405b_fp8_list`

`comptime llama_405b_fp8_list = List[TuningConfigSM90](TuningConfigSM90(64, 16384, 2048, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(64, 128, 128), 8, Index(1, 1, 1), 1, False, OptionalReg[IndexList[2]](Index(128, 1)), MatmulSchedule.DS_SCHEDULER, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(128, 16384, 2048, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(128, 128, 128), 4, Index(1, 1, 1), 2, True, OptionalReg[IndexList[2]](Index(H100.sm_count, 1)), MatmulSchedule.DS_SCHEDULER, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(256, 16384, 2048, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(128, 128, 128), 4, Index(1, 1, 1), 2, True, OptionalReg[IndexList[2]](Index(H100.sm_count, 1)), MatmulSchedule.DS_SCHEDULER, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(512, 16384, 2048, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(128, 128, 128), 4, Index(1, 1, 1), 2, True, OptionalReg[IndexList[2]](Index(H100.sm_count, 1)), MatmulSchedule.DS_SCHEDULER, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(1024, 16384, 2048, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(128, 128, 128), 4, Index(1, 1, 1), 2, True, OptionalReg[IndexList[2]](Index(H100.sm_count, 1)), MatmulSchedule.DS_SCHEDULER, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(MAX_M, 16384, 2048, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(128, 128, 128), 4, Index(2, 1, 1), 2, True, OptionalReg[IndexList[2]](Index(8, (H100 // 8))), MatmulSchedule.TILE2D, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(64, 2304, 16384, IndexList[3, DType.int64](64, 48, 32, Tuple[]()), Index(64, 48, 128), 8, Index(1, 1, 1), 1, False, OptionalReg[IndexList[2]](Index(H100.sm_count, 1)), MatmulSchedule.DS_SCHEDULER, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(128, 2304, 16384, IndexList[3, DType.int64](64, 48, 32, Tuple[]()), Index(64, 48, 128), 8, Index(1, 1, 1), 1, False, OptionalReg[IndexList[2]](Index(H100.sm_count, 1)), MatmulSchedule.DS_SCHEDULER, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(256, 2304, 16384, IndexList[3, DType.int64](64, 96, 32, Tuple[]()), Index(64, 96, 128), 4, Index(1, 1, 1), 1, False, OptionalReg[IndexList[2]](Index(H100.sm_count, 1)), MatmulSchedule.DS_SCHEDULER, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(512, 2304, 16384, IndexList[3, DType.int64](64, 144, 32, Tuple[]()), Index(128, 144, 128), 4, Index(1, 1, 1), 2, False, OptionalReg[IndexList[2]](Index(H100.sm_count, 1)), MatmulSchedule.DS_SCHEDULER, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(1024, 2304, 16384, IndexList[3, DType.int64](64, 144, 32, Tuple[]()), Index(128, 144, 128), 4, Index(1, 1, 1), 2, False, OptionalReg[IndexList[2]](Index(H100.sm_count, 1)), MatmulSchedule.DS_SCHEDULER, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(2048, 2304, 16384, IndexList[3, DType.int64](64, 144, 32, Tuple[]()), Index(128, 144, 128), 4, Index(2, 1, 1), 2, True, OptionalReg[IndexList[2]](Index(16, 8)), MatmulSchedule.TILE2D, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(MAX_M, 2304, 16384, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(128, 128, 128), 4, Index(2, 1, 1), 2, True, OptionalReg[IndexList[2]](None), MatmulSchedule.TILE2D, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(64, 13312, 16384, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(64, 128, 128), 8, Index(1, 1, 1), 1, False, OptionalReg[IndexList[2]](Index(128, 1)), MatmulSchedule.DS_SCHEDULER, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(128, 13312, 16384, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(128, 128, 128), 4, Index(1, 1, 1), 2, True, OptionalReg[IndexList[2]](None), MatmulSchedule.NONE, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(256, 13312, 16384, IndexList[3, DType.int64](64, 208, 32, Tuple[]()), Index(128, 208, 128), 4, Index(1, 2, 1), 2, True, OptionalReg[IndexList[2]](None), MatmulSchedule.NONE, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(512, 13312, 16384, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(128, 128, 128), 4, Index(1, 1, 1), 2, True, OptionalReg[IndexList[2]](None), MatmulSchedule.NONE, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(1024, 13312, 16384, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(128, 128, 128), 4, Index(1, 1, 1), 2, True, OptionalReg[IndexList[2]](None), MatmulSchedule.NONE, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(MAX_M, 13312, 16384, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(128, 128, 128), 4, Index(2, 1, 1), 2, True, OptionalReg[IndexList[2]](Index(8, (H100 // 8))), MatmulSchedule.TILE2D, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(64, 16384, 6656, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(64, 128, 128), 8, Index(1, 1, 1), 1, False, OptionalReg[IndexList[2]](Index(128, 1)), MatmulSchedule.DS_SCHEDULER, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(1024, 16384, 6656, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(128, 128, 128), 4, Index(1, 1, 1), 2, True, OptionalReg[IndexList[2]](None), MatmulSchedule.NONE, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(MAX_M, 16384, 6656, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(128, 128, 128), 4, Index(2, 1, 1), 2, True, OptionalReg[IndexList[2]](Index(8, (H100 // 8))), MatmulSchedule.TILE2D, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), Tuple[]())`

### `llama_405b_fp8_table`

`comptime llama_405b_fp8_table = Table[TuningConfigSM90](llama_405b_fp8_list, "llama_405b_fp8")`

### `llama_8b_fp8_list`

`comptime llama_8b_fp8_list = List[TuningConfigSM90](TuningConfigSM90(128, -1, -1, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(64, 128, 128), 8, Index(1, 1, 1), 1, True, OptionalReg[IndexList[2]](None), MatmulSchedule.NONE, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(1024, -1, -1, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(128, 128, 128), 6, Index(1, 1, 1), 2, True, OptionalReg[IndexList[2]](None), MatmulSchedule.NONE, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), TuningConfigSM90(MAX_M, -1, -1, IndexList[3, DType.int64](64, 128, 32, Tuple[]()), Index(128, 128, 128), 6, Index(2, 1, 1), 2, True, OptionalReg[IndexList[2]](Index(8, (H100 // 8))), MatmulSchedule.TILE2D, OptionalReg[Int](None), OptionalReg[RasterOrder](None)), Tuple[]())`

### `llama_8b_fp8_table`

`comptime llama_8b_fp8_table = Table[TuningConfigSM90](llama_8b_fp8_list, "llama_8b_fp8")`

### `logger`

`comptime logger = Logger[DEFAULT_LEVEL](stdout, "", False)`

### `MAX_M`

`comptime MAX_M = Int.MAX`

## Functions

* [â€‹`matmul_dispatch_sm90`](./matmul_dispatch_sm90):
* [â€‹`matmul_dispatch_sm90_bf16_fp32`](./matmul_dispatch_sm90_bf16_fp32):
* [â€‹`matmul_dispatch_sm90_fp8`](./matmul_dispatch_sm90_fp8):

</section>

---

## matmul_dispatch_sm90

<section class='mojo-docs'>

`matmul_dispatch_sm90[c_type: DType, a_type: DType, b_type: DType, transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, pdl_level: PDLLevel = PDLLevel()](c: NDBuffer[c_type, 2, origin, shape], a: NDBuffer[a_type, 2, origin, shape], b: NDBuffer[b_type, 2, origin, shape], ctx: DeviceContext) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## matmul_dispatch_sm90_bf16_fp32

<section class='mojo-docs'>

`matmul_dispatch_sm90_bf16_fp32[c_type: DType, a_type: DType, b_type: DType, //, transpose_b: Bool = True, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, pdl_level: PDLLevel = PDLLevel()](c: NDBuffer[c_type, 2, origin, shape], a: NDBuffer[a_type, 2, origin, shape], b: NDBuffer[b_type, 2, origin, shape], ctx: DeviceContext) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## matmul_dispatch_sm90_fp8

<section class='mojo-docs'>

`matmul_dispatch_sm90_fp8[c_type: DType, a_type: DType, b_type: DType, //, transpose_b: Bool = True, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, pdl_level: PDLLevel = PDLLevel()](c: NDBuffer[c_type, 2, origin, shape], a: NDBuffer[a_type, 2, origin, shape], b: NDBuffer[b_type, 2, origin, shape], ctx: DeviceContext) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## default_config_sm90

<section class='mojo-docs'>

`default_config_sm90[a_type: DType, b_type: DType, c_type: DType, transpose_b: Bool, wgmma_shape: IndexList[3]]() -> MatmulConfig[a_type, b_type, c_type, transpose_b]`

**Returns:**

[`MatmulConfig`](/mojo/kernels/linalg/utils_gpu/MatmulConfig)

</section>

---

## grouped_matmul_sm90

<section class='mojo-docs'>

`grouped_matmul_sm90[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, //, *, transpose_b: Bool = True, wgmma_shape: IndexList[3] = Index(64, 256, 16), config: MatmulConfig[a_type, b_type, c_type, transpose_b] = default_config_sm90[a_type, b_type, c_type, transpose_b, wgmma_shape](), elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: NDBuffer[c_type, 2, MutAnyOrigin, c_shape], a: NDBuffer[a_type, 2, MutAnyOrigin, a_shape], a_offsets: NDBuffer[DType.uint32, 1, MutAnyOrigin], max_num_tokens_per_expert: Int, b: NDBuffer[b_type, 3, MutAnyOrigin, b_shape], expert_ids: NDBuffer[DType.int32, 1, MutAnyOrigin], num_active_experts: Int, ctx: DeviceContext)`

</section>

---

## grouped_matmul (3)

<section class='mojo-docs'>

## Functions

* [â€‹`default_config_sm90`](./default_config_sm90):
* [â€‹`grouped_matmul_sm90`](./grouped_matmul_sm90):

</section>

---

## sm90

<section class='mojo-docs'>

Provides the Nvidia Hopper backend implementations for matmuls.

## Modules

* [â€‹`config`](./config/):
* [â€‹`dispatch`](./dispatch/):
* [â€‹`grouped_matmul`](./grouped_matmul/):
* [â€‹`matmul`](./matmul/):
* [â€‹`matmul_kernel_persistent`](./matmul_kernel_persistent/):
* [â€‹`matmul_kernels`](./matmul_kernels/):
* [â€‹`matmul_output`](./matmul_output/):
* [â€‹`ring_buffer`](./ring_buffer/): Ring buffer implementation for producer-consumer synchronization in GPU kernels.
* [â€‹`testbed`](./testbed/):
* [â€‹`tile_loader`](./tile_loader/): TileLoader module for efficient tile loading in GPU matrix multiplication.
* [â€‹`tile_writer`](./tile_writer/): TileWriter module for efficient tile writing in GPU matrix multiplication.
* [â€‹`tuning_configs`](./tuning_configs/):

</section>

---

## matmul (5)

<section class='mojo-docs'>

## `comptime` values

### `logger`

`comptime logger = Logger[DEFAULT_LEVEL](stdout, "", False)`

## Functions

* [â€‹`warp_specialize_gemm_with_multicasting`](./warp_specialize_gemm_with_multicasting): Unified dispatcher for all matmul kernel variants.
* [â€‹`warp_specialize_gemm_with_multicasting_splitk`](./warp_specialize_gemm_with_multicasting_splitk):

</section>

---

## warp_specialize_gemm_with_multicasting

<section class='mojo-docs'>

`warp_specialize_gemm_with_multicasting[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, *, transpose_b: Bool, config: MatmulConfig[a_type, b_type, c_type, transpose_b], grid_shape: OptionalReg[IndexList[2]] = None, use_tma_store: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, schedule: MatmulSchedule = MatmulSchedule.NONE, hilbert_swizzle: Bool = False, splits: Int = 0, raster_order: RasterOrder = RasterOrder.AlongM](c_device: NDBuffer[c_type, 2, origin, c_shape], a_device: NDBuffer[a_type, 2, origin, a_shape], b_device: NDBuffer[b_type, 2, origin, b_shape], ctx: DeviceContext)`

Unified dispatcher for all matmul kernel variants.

</section>

---

## warp_specialize_gemm_with_multicasting_splitk

<section class='mojo-docs'>

`warp_specialize_gemm_with_multicasting_splitk[c_type: DType, c_shape: DimList, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, *, transpose_b: Bool, config: MatmulConfig[a_type, b_type, c_type, transpose_b], splits: Int, raster_order: RasterOrder, use_tma_store: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None](c_device: NDBuffer[c_type, 2, origin, c_shape], a_device: NDBuffer[a_type, 2, origin, a_shape], b_device: NDBuffer[b_type, 2, origin, b_shape], ctx: DeviceContext)`

</section>

---

## matmul_kernel_persistent

<section class='mojo-docs'>

</section>

---

## HopperMatmulSM90Kernel

<section class='mojo-docs'>

`struct HopperMatmulSM90Kernel[a_type: DType, b_type: DType, c_type: DType, a_layout: Layout, b_layout: Layout, c_layout: Layout, c_smem_layout: Layout, block_tile_shape: IndexList[3], wgmma_shape: IndexList[3], cluster_shape: StaticTuple[Int32, 3], num_pipeline_stages: Int, num_threads: Int = 128, transpose_b: Bool = True, a_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, b_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, c_swizzle: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_NONE, partitioned_multicast: Bool = False, use_tma_store: Bool = False, promotion_frequency: Int = 1, pdl_level: PDLLevel = PDLLevel(), elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, hilbert_swizzle: Bool = False, k_group_size: Int = 1]`

Hopper SM90 Matrix Multiplication kernel optimized for NVIDIA H100 GPUs.

This kernel implements a highly optimized matrix multiplication (GEMM) using:

* Tensor Memory Accelerator (TMA) for efficient global-to-shared memory transfers
* Warp Group Matrix Multiply Accumulate (WGMMA) instructions for tensor cores
* Multi-stage software pipelining for overlapping compute and memory operations
* Producer-consumer model with separate warp groups for loading and computing

Template Parameters:
a\_type, b\_type, c\_type: Data types for input and output matrices
a\_layout, b\_layout, c\_layout: Memory layouts for matrices
c\_smem\_layout: Shared memory layout for output tile
block\_tile\_shape: Tile dimensions \[M, N, K] processed by each thread block
wgmma\_shape: Dimensions for each WGMMA instruction \[M, N, K]
cluster\_shape: Thread block cluster dimensions for distributed shared memory
num\_pipeline\_stages: Number of stages in the software pipeline (typically 3-7)
num\_threads: Number of threads per block (must be multiple of 128)
transpose\_b: Whether B matrix is transposed (required to be True)
a\_swizzle, b\_swizzle: Memory swizzling for bank-conflict-free access
c\_swizzle: Swizzling for output writes
partitioned\_multicast: Enable partitioned multicast for large tiles
use\_tma\_store: Use TMA for storing output (vs regular stores)
promotion\_frequency: How often to promote FP8 accumulation to higher precision
pdl\_level: Programmatic Dependency Launch (PDL) level
elementwise\_lambda\_fn: Optional epilogue function
elementwise\_compute\_lambda\_fn: Optional compute function
hilbert\_swizzle: Use Hilbert curve for thread block scheduling

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `a_smem_layout`

`comptime a_smem_layout = tile_layout_k_major[a_type, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].BM, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].BK, a_swizzle]()`

### `accum_type`

`comptime accum_type = get_accum_type[a_type]()`

### `AccumRegTileType`

`comptime AccumRegTileType = LayoutTensor[HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].accum_type, Layout.row_major((HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_m_mmas * HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_n_mmas), HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].c_frag_size), MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `adjusted_num_pipeline_stages`

`comptime adjusted_num_pipeline_stages = (num_pipeline_stages // k_group_size)`

### `b_smem_layout`

`comptime b_smem_layout = tile_layout_k_major[b_type, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].BN, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].BK, b_swizzle]()`

### `BK`

`comptime BK = block_tile_shape.__getitem__[3, DType.int64, Int](2)`

### `BM`

`comptime BM = block_tile_shape.__getitem__[3, DType.int64, Int](0)`

### `BN`

`comptime BN = block_tile_shape.__getitem__[3, DType.int64, Int](1)`

### `c_frag_size`

`comptime c_frag_size = ((wgmma_shape.__getitem__[3, DType.int64, Int](0) * wgmma_shape.__getitem__[3, DType.int64, Int](1)) // 128)`

### `cluster_size`

`comptime cluster_size = Int.__init__[Int32](((cluster_shape.__getitem__[3, Int](0) * cluster_shape.__getitem__[3, Int](1)) * cluster_shape.__getitem__[3, Int](2)))`

### `num_consumer`

`comptime num_consumer = ((num_threads // 128) - 1)`

### `num_consumer_threads`

`comptime num_consumer_threads = (HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_consumer * 128)`

### `num_m_mmas`

`comptime num_m_mmas = ((HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].BM // wgmma_shape.__getitem__[3, DType.int64, Int](0)) // HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_consumer)`

### `num_n_mmas`

`comptime num_n_mmas = (HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].BN // wgmma_shape.__getitem__[3, DType.int64, Int](1))`

### `RingBuffer`

`comptime RingBuffer[tma_transfer: Bool = True] = RingBuffer[a_type, b_type, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].a_smem_layout, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].b_smem_layout, num_pipeline_stages, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_consumer, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].cluster_size, tma_transfer, k_group_size]`

#### Parameters

* â€‹tma\_transfer ([`Bool`](/mojo/std/builtin/bool/Bool)):

### `RingBufferConsumer`

`comptime RingBufferConsumer[origin: MutOrigin, tma_transfer: Bool] = RingBufferConsumer[origin, RingBuffer[a_type, b_type, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].a_smem_layout, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].b_smem_layout, num_pipeline_stages, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_consumer, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].cluster_size, tma_transfer, k_group_size]]`

#### Parameters

* â€‹origin ([`MutOrigin`](/mojo/std/builtin/type_aliases/#mutorigin)):
* â€‹tma\_transfer ([`Bool`](/mojo/std/builtin/bool/Bool)):

### `SMem`

`comptime SMem = HopperMatmulSM90Kernel_SMem[a_type, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].a_smem_layout, b_type, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].b_smem_layout, c_type, c_smem_layout, num_pipeline_stages, k_group_size]`

### `WgmmaOp`

`comptime WgmmaOp = TensorCoreAsync[HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].accum_type, a_type, b_type, wgmma_shape, a_swizzle, b_swizzle, transpose_b]`

## Methods

### `validate_constraints`

`static validate_constraints()`

Validate common constraints for all kernel variants.

### `pipeline_init`

`static pipeline_init()`

Initialize pipeline synchronization barriers.

This function ensures that all pipeline initialization (barriers, shared memory)
is visible to all thread blocks in the cluster before proceeding. This is
critical for correct producer-consumer synchronization.

For multi-cluster configurations, uses fence and cluster sync.
For single block, uses a simple barrier.

### `finalize_kernel`

`static finalize_kernel()`

Common finalization for all kernel variants.

### `multicast_mask`

`static multicast_mask(rank_m: UInt, rank_n: UInt) -> Tuple[Int32, Int32]`

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

### `common_kernel_init`

`static common_kernel_init() -> Tuple[UInt, UInt, UInt, UInt, UInt, Bool]`

Common initialization for all kernel variants.

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple): Tuple of (warp\_group\_idx, warp\_group\_thread\_idx,
rank\_m, rank\_n, warp\_id, lane\_predicate).

### `build_ring_buffer`

`static build_ring_buffer[tma_transfer: Bool = True](smem: HopperMatmulSM90Kernel_SMem[a_type, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].a_smem_layout, b_type, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].b_smem_layout, c_type, c_smem_layout, num_pipeline_stages, k_group_size], warp_group_thread_idx: UInt) -> RingBuffer[a_type, b_type, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].a_smem_layout, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].b_smem_layout, num_pipeline_stages, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_consumer, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].cluster_size, tma_transfer, k_group_size]`

Create ring buffer for producer-consumer synchronization.

**Returns:**

[`RingBuffer`](/mojo/kernels/linalg/matmul/gpu/amd/ring_buffer/RingBuffer)

### `setup_producer`

`static setup_producer() -> Int`

Setup producer warp group by deallocating registers.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): Number of registers deallocated.

### `setup_consumer`

`static setup_consumer(warp_group_idx: UInt) -> Tuple[UInt, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].AccumRegTileType, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].AccumRegTileType]`

Setup consumer warp group.

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple): Tuple of (local\_warp\_group\_idx, c\_reg\_tile, final\_c\_reg\_tile).

### `get_block_swizzle`

`static get_block_swizzle(lut_ptr: LegacyUnsafePointer[UInt32] = LegacyUnsafePointer[UInt32, AddressSpace.GENERIC, True, MutAnyOrigin]()) -> IndexList[2, element_type=DType.uint32]`

Calculate block swizzle for better L2 cache locality.

**Args:**

* â€‹lut\_ptr ([`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)): Lookup table for Hilbert curve block scheduling (optional).

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): Swizzled block indices.

### `consumer_output`

`static consumer_output[custom_elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = elementwise_lambda_fn](c_tma_op: TMATensorTile[c_type, layout, desc_layout], c: LayoutTensor[c_type, layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_tile: LayoutTensor[c_type, c_smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=NVIDIASharedMemoryBasePtr.alignment], output_reg_tile: LayoutTensor[HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].accum_type, Layout.row_major((HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_m_mmas * HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_n_mmas), HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].c_frag_size), MutAnyOrigin, address_space=AddressSpace.LOCAL], warp_group_thread_idx: UInt, local_warp_group_idx: UInt, local_thread_idx: UInt, block_y: Int, block_x: Int)`

Handle consumer output by writing GEMM results to global memory.

### `build_tma_loaders`

`static build_tma_loaders[a_tile_layout: Layout, b_tile_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, //](a_tma_op: TMATensorTile[a_type, a_tile_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_tile_layout, b_desc_layout], rank_m: UInt, rank_n: UInt) -> Tuple[TileLoaderTMA[a_tma_op, a_type, a_tile_layout, a_desc_layout, BK=UInt(HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].BK), cluster_size=cluster_shape.__getitem__[3, Int](0), use_partitioned_multicast=partitioned_multicast], TileLoaderTMA[b_tma_op, b_type, b_tile_layout, b_desc_layout, BK=UInt(HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].BK), cluster_size=cluster_shape.__getitem__[3, Int](1), use_partitioned_multicast=partitioned_multicast]]`

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

### `build_cpasync_loaders`

`static build_cpasync_loaders[k_align: Int, vector_size: Int = (k_align // size_of[a_type]()), num_threads_per_row: Int = (HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].BK // vector_size), thread_layout: Layout = Layout.row_major((WARPGROUP_SIZE // num_threads_per_row), num_threads_per_row)](a: LayoutTensor[a_type, a_layout, MutAnyOrigin], b: LayoutTensor[b_type, b_layout, MutAnyOrigin]) -> Tuple[TileLoaderCPAsync[a_type, a_layout, thread_layout, a_swizzle, vector_size], TileLoaderCPAsync[b_type, b_layout, thread_layout, b_swizzle, vector_size]]`

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

### `producer_main_loop`

`static producer_main_loop[a_loader_type: TileLoader, b_loader_type: TileLoader, //, num_k_iters: Int](m_coord: UInt, n_coord: UInt, k_coord: UInt, a_loader: a_loader_type, b_loader: b_loader_type, mut ring_buffer: RingBuffer[a_loader_type._dtype, b_loader_type._dtype, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size])`

Polymorphic A and B Tile Loader, works with both TMA and CPAsync.

### `run`

`static run[a_tile_layout: Layout, b_tile_layout: Layout, c_tma_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, c_desc_layout: Layout](a_tma_op: TMATensorTile[a_type, a_tile_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_tile_layout, b_desc_layout], c_tma_op: TMATensorTile[c_type, c_tma_layout, c_desc_layout], a: LayoutTensor[a_type, a_layout, MutAnyOrigin], b: LayoutTensor[b_type, b_layout, MutAnyOrigin], c: LayoutTensor[c_type, c_layout, MutAnyOrigin], lut_ptr: LegacyUnsafePointer[UInt32])`

Main kernel entry point for matrix multiplication.

This kernel implements a producer-consumer pattern where:

* One warp group (producer) loads tiles from global memory using TMA
* Multiple warp groups (consumers) perform matrix multiplication using tensor cores

The kernel uses software pipelining to overlap memory transfers with computation,
achieving high throughput on Hopper GPUs.

**Args:**

* â€‹a\_tma\_op ([`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)): TMA descriptor for matrix A.
* â€‹b\_tma\_op ([`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)): TMA descriptor for matrix B.
* â€‹c\_tma\_op ([`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)): TMA descriptor for matrix C.
* â€‹a ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input matrix A.
* â€‹b ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input matrix B.
* â€‹c ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output matrix C.
* â€‹lut\_ptr ([`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)): Lookup table for Hilbert curve block scheduling (optional).

### `run_splitk`

`static run_splitk[a_tile_layout: Layout, b_tile_layout: Layout, c_tma_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, c_desc_layout: Layout, splits: Int, raster_order: RasterOrder](a_tma_op: TMATensorTile[a_type, a_tile_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_tile_layout, b_desc_layout], c_tma_op: TMATensorTile[c_type, c_tma_layout, c_desc_layout], c: LayoutTensor[c_type, c_layout, MutAnyOrigin], workspace_buffer: NDBuffer[HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].accum_type, 3, MutAnyOrigin], locks_ptr: LegacyUnsafePointer[UInt8], problem_shape: IndexList[3])`

Split-K variant of the kernel for better load balancing on small problems.

### `run_grouped`

`static run_grouped[a_tile_layout: Layout, b_tile_layout: Layout, c_tile_layout: Layout, a_desc_layout: Layout, b_desc_layout: Layout, c_desc_layout: Layout](a_tma_op: TMATensorTile[a_type, a_tile_layout, a_desc_layout], b_tma_op: TMATensorTile[b_type, b_tile_layout, b_desc_layout], c_tma_op: TMATensorTile[c_type, c_tile_layout, c_desc_layout], a_offsets: NDBuffer[DType.uint32, 1, MutAnyOrigin], expert_ids: NDBuffer[DType.int32, 1, MutAnyOrigin], c: LayoutTensor[c_type, c_layout, MutAnyOrigin])`

Grouped matmul variant for MoE (Mixture of Experts) models.

This variant handles multiple experts where each expert processes a subset of tokens.
The a\_offsets array indicates token boundaries for each expert.

### `consumer_main_loop`

`static consumer_main_loop[ring_buffer_origin: MutOrigin, //, num_k_iters: Int](wgmma_op: TensorCoreAsync[HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].accum_type, a_type, b_type, wgmma_shape, a_swizzle, b_swizzle, transpose_b], local_warp_group_idx: UInt, final_c_reg_tile: LayoutTensor[HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].accum_type, Layout.row_major((HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_m_mmas * HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_n_mmas), HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].c_frag_size), MutAnyOrigin, address_space=AddressSpace.LOCAL], c_reg_tile: LayoutTensor[HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].accum_type, Layout.row_major((HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_m_mmas * HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_n_mmas), HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].c_frag_size), MutAnyOrigin, address_space=AddressSpace.LOCAL], mut ring_buffer: RingBufferConsumer[ring_buffer_origin, RingBuffer[a_type, b_type, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].a_smem_layout, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].b_smem_layout, num_pipeline_stages, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_consumer, HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].cluster_size, tma_transfer, k_group_size]])`

Main computation loop for consumer warp groups.

This function implements the core matrix multiplication using tensor cores.
It consumes tiles from the ring buffer and accumulates results using WGMMA
(Warp Group Matrix Multiply Accumulate) instructions.

For FP8 data types, it periodically promotes intermediate results to higher
precision to maintain accuracy.

**Args:**

* â€‹wgmma\_op ([`TensorCoreAsync`](/mojo/kernels/layout/tensor_core_async/TensorCoreAsync)): Tensor core operator for matrix multiplication.
* â€‹local\_warp\_group\_idx ([`UInt`](/mojo/std/builtin/uint/UInt)): Index of this consumer warp group (0-based).
* â€‹final\_c\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Final accumulation register tile (for FP8 promotion).
* â€‹c\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Working accumulation register tile.
* â€‹ring\_buffer ([`RingBufferConsumer`](/mojo/kernels/linalg/matmul/gpu/sm90/matmul_kernels/#ringbufferconsumer)): Consumer handle for synchronized tile access.

### `promote_to_cuda_cores`

`static promote_to_cuda_cores(c_reg_tile: LayoutTensor[HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].accum_type, Layout.row_major((HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_m_mmas * HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_n_mmas), HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].c_frag_size), MutAnyOrigin, address_space=AddressSpace.LOCAL], final_c_reg_tile: LayoutTensor[HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].accum_type, Layout.row_major((HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_m_mmas * HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_n_mmas), HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].c_frag_size), MutAnyOrigin, address_space=AddressSpace.LOCAL])`

Promote FP8 accumulation to higher precision using CUDA cores.

When using FP8 data types, tensor cores accumulate in limited precision.
To maintain accuracy over many accumulations, we periodically add the
intermediate results to a higher-precision accumulator using CUDA cores.

This technique is commonly used in production libraries like cuBLAS to
achieve both high performance (from FP8 tensor cores) and good accuracy.

**Args:**

* â€‹c\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Current accumulation from tensor cores.
* â€‹final\_c\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Higher-precision accumulator (updated in place).

### `wgmma`

`static wgmma(wgmma_op: TensorCoreAsync[HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].accum_type, a_type, b_type, wgmma_shape, a_swizzle, b_swizzle, transpose_b], local_warp_group_idx: UInt, a_tile: LayoutTensor[a_type, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_tile: LayoutTensor[b_type, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c_reg_tile: LayoutTensor[HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].accum_type, Layout.row_major((HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_m_mmas * HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].num_n_mmas), HopperMatmulSM90Kernel[a_type, b_type, c_type, a_layout, b_layout, c_layout, c_smem_layout, block_tile_shape, wgmma_shape, cluster_shape, num_pipeline_stages, num_threads, transpose_b, a_swizzle, b_swizzle, c_swizzle, partitioned_multicast, use_tma_store, promotion_frequency, pdl_level, elementwise_lambda_fn, elementwise_compute_lambda_fn, hilbert_swizzle, k_group_size].c_frag_size), MutAnyOrigin, address_space=AddressSpace.LOCAL])`

</section>

---

## HopperMatmulSM90Kernel_SMem

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct HopperMatmulSM90Kernel_SMem[a_type: DType, a_layout: Layout, b_type: DType, b_layout: Layout, c_type: DType, c_layout: Layout, num_pipeline_stages: Int, k_group_size: Int]`

Shared memory layout for Hopper SM90 matrix multiplication kernel.

This struct manages the shared memory allocation for:

* Input tiles (A and B matrices) with multi-stage pipelining
* Output tile (C matrix) for accumulation
* Synchronization barriers for producer-consumer coordination

The memory is organized to support asynchronous loads and efficient
bank-conflict-free access patterns for tensor core operations.

## Fields

* â€‹a\_tiles (`HopperMatmulSM90Kernel_SMem[a_type, a_layout, b_type, b_layout, c_type, c_layout, num_pipeline_stages, k_group_size].ATileArray`):
* â€‹b\_tiles (`HopperMatmulSM90Kernel_SMem[a_type, a_layout, b_type, b_layout, c_type, c_layout, num_pipeline_stages, k_group_size].BTileArray`):
* â€‹c\_tile (`HopperMatmulSM90Kernel_SMem[a_type, a_layout, b_type, b_layout, c_type, c_layout, num_pipeline_stages, k_group_size].CTile`):
* â€‹full\_mbar (`HopperMatmulSM90Kernel_SMem[a_type, a_layout, b_type, b_layout, c_type, c_layout, num_pipeline_stages, k_group_size].PipelineBarrier`):
* â€‹empty\_mbar (`HopperMatmulSM90Kernel_SMem[a_type, a_layout, b_type, b_layout, c_type, c_layout, num_pipeline_stages, k_group_size].PipelineBarrier`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ATileArray`

`comptime ATileArray = SMemTileArrayType[a_type, a_layout, num_pipeline_stages, NVIDIASharedMemoryBasePtr.alignment]`

### `BTileArray`

`comptime BTileArray = SMemTileArrayType[b_type, b_layout, num_pipeline_stages, NVIDIASharedMemoryBasePtr.alignment]`

### `CTile`

`comptime CTile = LayoutTensor[c_type, c_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=NVIDIASharedMemoryBasePtr.alignment]`

### `PipelineBarrier`

`comptime PipelineBarrier = SMemArrayType[SharedMemBarrier, (num_pipeline_stages // k_group_size)]`

### `SMM`

`comptime SMM = SharedMemoryManager[NVIDIASharedMemoryBasePtr]`

## Methods

### `__init__`

`__init__() -> Self`

### `pipeline_storage_size`

`static pipeline_storage_size() -> Int`

Calculate the memory size for all pipeline stages.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `output_storage_size`

`static output_storage_size() -> Int`

Calculate the memory size for output tile.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `storage_size`

`static storage_size() -> Int`

Calculate the total storage size.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## find_K_alignment_upto_16B

<section class='mojo-docs'>

`find_K_alignment_upto_16B(row_bytes_arg: Int) -> Int`

Find alignment among 1B, 2B, 4B, 16B based on the row's bytes.

This function determines the largest power-of-2 alignment (up to 16 bytes)
that evenly divides the given row size. This is used to determine the
optimal vector size for cp.async operations when K dimension alignment
doesn't meet TMA requirements.

**Args:**

* â€‹row\_bytes\_arg ([`Int`](/mojo/std/builtin/int/Int)): Number of bytes in a row (K \* sizeof(element)).

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): Alignment in bytes (1, 2, 4, 8, or 16).

</section>

---

## matmul_kernels (Matmul_kernels)

<section class='mojo-docs'>

## Structs

* [â€‹`HopperMatmulSM90Kernel`](./HopperMatmulSM90Kernel): Hopper SM90 Matrix Multiplication kernel optimized for NVIDIA H100 GPUs.
* [â€‹`HopperMatmulSM90Kernel_SMem`](./HopperMatmulSM90Kernel_SMem): Shared memory layout for Hopper SM90 matrix multiplication kernel.

## Functions

* [â€‹`find_K_alignment_upto_16B`](./find_K_alignment_upto_16B): Find alignment among 1B, 2B, 4B, 16B based on the row's bytes.

</section>

---

## MatmulTileWriter

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MatmulTileWriter[dtype: DType, layout: Layout, address_space: AddressSpace, element_layout: Layout, layout_int_type: DType, linear_idx_type: DType, masked: Bool, alignment: Int, smem_tile_layout: Layout, //, *, BM: Int, BN: Int, swizzle: TensorMapSwizzle, wgmma_shape: IndexList[3], num_consumer: Int = 1, use_tma_store: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None]`

## Fields

* â€‹tensor (`MatmulTileWriter[BM=BM, BN=BN, swizzle=swizzle, wgmma_shape=wgmma_shape, num_consumer=num_consumer, use_tma_store=use_tma_store, elementwise_lambda_fn=elementwise_lambda_fn, elementwise_compute_lambda_fn=elementwise_compute_lambda_fn].CTensorType`):
* â€‹smem\_tile (`LayoutTensor[dtype, smem_tile_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128]`):
* â€‹warp\_group\_thread\_idx (`UInt`):
* â€‹local\_warp\_group\_idx (`UInt`):
* â€‹local\_thread\_idx (`UInt`):
* â€‹block\_y (`Int`):
* â€‹block\_x (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `CTensorType`

`comptime CTensorType = LayoutTensor[dtype, layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

### `frag_size`

`comptime frag_size = ((wgmma_shape.__getitem__[3, DType.int64, Int](0) * wgmma_shape.__getitem__[3, DType.int64, Int](1)) // WARPGROUP_SIZE)`

### `lambda_type`

`comptime lambda_type = fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], mut SIMD[dtype, width]) capturing -> None`

### `N`

`comptime N = layout.shape[1].value()`

### `num_consumer_threads`

`comptime num_consumer_threads = (num_consumer * WARPGROUP_SIZE)`

### `num_m_mmas`

`comptime num_m_mmas = ((BM // wgmma_shape.__getitem__[3, DType.int64, Int](0)) // num_consumer)`

### `num_n_mmas`

`comptime num_n_mmas = (BN // wgmma_shape.__getitem__[3, DType.int64, Int](1))`

### `simd_size`

`comptime simd_size = simd_width_of[dtype]()`

### `WG_BM`

`comptime WG_BM = smem_tile_layout.shape[0].value()`

### `WG_BN`

`comptime WG_BN = smem_tile_layout.shape[1].value()`

## Methods

### `__init__`

`__init__(tensor: LayoutTensor[dtype, layout, MutAnyOrigin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], smem_tile: LayoutTensor[dtype, smem_tile_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], warp_group_thread_idx: UInt, local_warp_group_idx: UInt, local_thread_idx: UInt, block_y: Int, block_x: Int) -> Self`

### `write_tile`

`write_tile[tma_layout: Layout, desc_layout: Layout, accum_type: DType, reg_tile_layout: Layout, //](self, tma_op: TMATensorTile[dtype, tma_layout, desc_layout], reg_tile: LayoutTensor[accum_type, reg_tile_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL])`

Write output from registers to global memory.

Selects optimized st.matrix path for bf16 when constraints are met,
otherwise uses general register-to-global path.

</section>

---

## matmul_output (Matmul_output)

<section class='mojo-docs'>

## Structs

* [â€‹`MatmulTileWriter`](./MatmulTileWriter):

</section>

---

## ConsumerTiles (Ring_buffer)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ConsumerTiles[a_type: DType, b_type: DType, a_tile_layout: Layout, b_tile_layout: Layout, num_pipeline_stages: Int, num_consumers: Int, cluster_size: Int, tma_transfer: Bool, k_group_size: Int, //, origin: MutOrigin, ring_buffer_type: AnyStruct[RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size]]]`

Context manager for consumer access to ring buffer tiles.

This struct provides safe access to a single tile slot in the ring buffer
for consumers to read. It tracks the read index and automatically releases
the slot when exiting the context.

## Fields

* â€‹ring\_buffer\_ptr (`ConsumerTiles[origin, ring_buffer_type].RingBufferPtrType`):
* â€‹read\_idx (`UInt32`):
* â€‹a\_tile\_array (`ConsumerTiles[origin, ring_buffer_type].ATileArraySlice`):
* â€‹b\_tile\_array (`ConsumerTiles[origin, ring_buffer_type].BTileArraySlice`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ATileArraySlice`

`comptime ATileArraySlice = ring_buffer_type.ATileArraySlice`

### `BTileArraySlice`

`comptime BTileArraySlice = ring_buffer_type.BTileArraySlice`

### `RingBufferPtrType`

`comptime RingBufferPtrType = Pointer[ring_buffer_type, origin]`

## Methods

### `__init__`

`__init__(ring_buffer_ptr: Pointer[ring_buffer_type, origin]) -> Self`

### `__enter__`

`__enter__(mut self) -> Self`

### `__exit__`

`__exit__(mut self)`

</section>

---

## ProducerTiles (Ring_buffer)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ProducerTiles[a_type: DType, b_type: DType, a_tile_layout: Layout, b_tile_layout: Layout, num_pipeline_stages: Int, num_consumers: Int, cluster_size: Int, tma_transfer: Bool, k_group_size: Int, //, origin: MutOrigin, ring_buffer_type: AnyStruct[RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size]]]`

Context manager for producer access to ring buffer tiles.

This struct provides safe access to a single tile slot in the ring buffer
for the producer to fill. It automatically handles barrier synchronization
when entering and exiting the context.

## Fields

* â€‹ring\_buffer\_ptr (`ProducerTiles[origin, ring_buffer_type].RingBufferPtrType`):
* â€‹barrier (`SMemBarrier`):
* â€‹a\_tile\_array (`ProducerTiles[origin, ring_buffer_type].ATileArraySlice`):
* â€‹b\_tile\_array (`ProducerTiles[origin, ring_buffer_type].BTileArraySlice`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ATileArraySlice`

`comptime ATileArraySlice = ring_buffer_type.ATileArraySlice`

### `BTileArraySlice`

`comptime BTileArraySlice = ring_buffer_type.BTileArraySlice`

### `RingBufferPtrType`

`comptime RingBufferPtrType = Pointer[ring_buffer_type, origin]`

## Methods

### `__init__`

`__init__(ring_buffer_ptr: Pointer[ring_buffer_type, origin]) -> Self`

### `__enter__`

`__enter__(mut self) -> Self`

### `__exit__`

`__exit__(mut self)`

</section>

---

## RingBuffer (3)

<section class='mojo-docs'>

`struct RingBuffer[a_type: DType, b_type: DType, a_tile_layout: Layout, b_tile_layout: Layout, num_pipeline_stages: Int, num_consumers: Int, cluster_size: Int, tma_transfer: Bool = True, k_group_size: Int = 1]`

Ring buffer for managing pipeline synchronization between producers and consumers.

This struct encapsulates the synchronization logic for a multi-stage pipeline
with one producer and multiple consumers, supporting both single-block and
multi-cluster configurations.

The ring buffer uses two sets of barriers:

* full\_mbar: Signals when tiles are ready for consumption
* empty\_mbar: Signals when slots are available for production

Template Parameters:
a\_type: Data type for A matrix tiles
b\_type: Data type for B matrix tiles
a\_tile\_layout: Memory layout for A tiles
b\_tile\_layout: Memory layout for B tiles
num\_pipeline\_stages: Number of stages in the circular buffer
num\_consumers: Number of consumer warp groups
cluster\_size: Number of blocks in the cluster (1 for single-block)
tma\_transfer: Whether the RingBuffer is used for TMA transfers (default: True)

## Fields

* â€‹full\_mbar (`RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].PipelineBarrier`):
* â€‹empty\_mbar (`RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].PipelineBarrier`):
* â€‹read\_state (`PipelineState[RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].adjusted_num_pipeline_stages]`):
* â€‹write\_state (`PipelineState[RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].adjusted_num_pipeline_stages]`):
* â€‹warp\_group\_thread\_idx (`UInt`):
* â€‹a\_tiles (`RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].ATileArray`):
* â€‹b\_tiles (`RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].BTileArray`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `adjusted_num_pipeline_stages`

`comptime adjusted_num_pipeline_stages = (num_pipeline_stages // k_group_size)`

### `ATile`

`comptime ATile = RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].ATileArray.Tile`

### `ATileArray`

`comptime ATileArray = SMemTileArrayType[a_type, a_tile_layout, num_pipeline_stages, NVIDIASharedMemoryBasePtr.alignment]`

### `ATileArraySlice`

`comptime ATileArraySlice = SMemTileArrayType[a_type, a_tile_layout, k_group_size, NVIDIASharedMemoryBasePtr.alignment]`

### `BTile`

`comptime BTile = RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].BTileArray.Tile`

### `BTileArray`

`comptime BTileArray = SMemTileArrayType[b_type, b_tile_layout, num_pipeline_stages, NVIDIASharedMemoryBasePtr.alignment]`

### `BTileArraySlice`

`comptime BTileArraySlice = SMemTileArrayType[b_type, b_tile_layout, k_group_size, NVIDIASharedMemoryBasePtr.alignment]`

### `PipelineBarrier`

`comptime PipelineBarrier = SMemArrayType[SharedMemBarrier, RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].adjusted_num_pipeline_stages]`

### `SMM`

`comptime SMM = SharedMemoryManager[NVIDIASharedMemoryBasePtr]`

## Methods

### `__init__`

`__init__(out self, full_mbar: SMemArrayType[SharedMemBarrier, RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].adjusted_num_pipeline_stages], empty_mbar: SMemArrayType[SharedMemBarrier, RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].adjusted_num_pipeline_stages], warp_group_thread_idx: UInt, a_tiles: SMemTileArrayType[a_type, a_tile_layout, num_pipeline_stages, NVIDIASharedMemoryBasePtr.alignment], b_tiles: SMemTileArrayType[b_type, b_tile_layout, num_pipeline_stages, NVIDIASharedMemoryBasePtr.alignment])`

Initialize ring buffer with barrier pointers.

**Args:**

* â€‹full\_mbar (`SMemArrayType`): Barrier array signaling when tiles are ready.
* â€‹empty\_mbar (`SMemArrayType`): Barrier array signaling when slots are empty.
* â€‹warp\_group\_thread\_idx ([`UInt`](/mojo/std/builtin/uint/UInt)): Thread index within the warp group.
* â€‹a\_tiles ([`SMemTileArrayType`](/mojo/kernels/linalg/structuring/SMemTileArrayType)): Iterator over A matrix tile storage.
* â€‹b\_tiles ([`SMemTileArrayType`](/mojo/kernels/linalg/structuring/SMemTileArrayType)): Iterator over B matrix tile storage.

### `__enter__`

`__enter__(mut self) -> Self`

Context manager entry.

### `get_expected_bytes`

`static get_expected_bytes() -> Int`

Calculate expected bytes per pipeline stage for TMA transfers.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `get_slot`

`get_slot(mut self) -> UInt32`

Producer waits for empty buffer slot and prepares for loading.

This method blocks until the current write slot is empty (all consumers
have finished with it), then prepares the barrier for new data.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): Index of the available slot in the ring buffer.

### `get_producer_tiles`

`get_producer_tiles(mut self) -> Tuple[SMemBarrier, RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].ATileArraySlice, RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].BTileArraySlice]`

Get the next available slot for the producer to fill.

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple): Tuple of (barrier, a\_tile, b\_tile) for the producer to use.

### `enqueue_tile`

`enqueue_tile(mut self)`

Producer signals that tile loading is complete.

This handles the specific signaling pattern needed:

* For cp.async: Signal async copy arrival and barrier arrival
* For TMA: Barrier arrival is handled by hardware

After signaling, advances to the next pipeline stage.

### `get_tile`

`get_tile(mut self) -> UInt32`

Consumer waits for full buffer slot.

This method blocks until the producer has filled the current read slot.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32): Index of the available tile to consume.

### `get_consumer_tiles`

`get_consumer_tiles(mut self) -> Tuple[UInt32, RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].ATileArraySlice, RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size].BTileArraySlice]`

Consumer waits for full buffer slot and returns the tiles.

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple): Tuple of (read\_idx, a\_tile, b\_tile) for the consumer to process.

### `release_slot`

`release_slot(mut self, read_idx: UInt32)`

Consumer signals that buffer slot is empty.

This allows the producer to reuse this slot in the ring buffer.
Different arrival patterns are used for single-block vs multi-cluster.

**Args:**

* â€‹read\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Index of the slot to release.

### `consumer`

`consumer(mut self) -> RingBufferConsumer[self, RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size]]`

Create a consumer view of this ring buffer.

**Returns:**

[`RingBufferConsumer`](/mojo/kernels/linalg/matmul/gpu/sm90/matmul_kernels/#ringbufferconsumer)

### `producer`

`producer(mut self) -> RingBufferProducer[self, RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size]]`

Create a producer view of this ring buffer.

**Returns:**

`RingBufferProducer`

### `arrive_empty_barriers`

`arrive_empty_barriers(self)`

Helper to arrive at empty barriers during consumer initialization.

This is called when consumers enter their context to signal they are
ready to consume tiles. It ensures all pipeline stages start with
empty slots available for the producer.

</section>

---

## RingBufferConsumer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct RingBufferConsumer[a_type: DType, b_type: DType, a_tile_layout: Layout, b_tile_layout: Layout, num_pipeline_stages: Int, num_consumers: Int, cluster_size: Int, tma_transfer: Bool, k_group_size: Int, //, origin: MutOrigin, ring_buffer_type: AnyStruct[RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size]]]`

Consumer view of the ring buffer.

This struct provides the consumer interface to the ring buffer, allowing
consumers to wait for and access tiles loaded by the producer. It handles
the initial barrier arrival when entering the consumer context.

## Fields

* â€‹ring\_buffer\_ptr (`RingBufferConsumer[origin, ring_buffer_type].RingBufferPtrType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ATile`

`comptime ATile = ring_buffer_type.ATile`

### `BTile`

`comptime BTile = ring_buffer_type.BTile`

### `RingBufferPtrType`

`comptime RingBufferPtrType = Pointer[ring_buffer_type, origin]`

## Methods

### `__init__`

`__init__(ring_buffer_ptr: Pointer[ring_buffer_type, origin]) -> Self`

### `__enter__`

`__enter__(mut self) -> Self`

### `get_tiles`

`get_tiles(mut self) -> ConsumerTiles[origin, ring_buffer_type]`

Get a context manager for accessing the next available tile.

**Returns:**

[`ConsumerTiles`](/mojo/kernels/linalg/matmul/gpu/sm100_structured/ring_buffer/ConsumerTiles)

</section>

---

## RingBufferProducer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct RingBufferProducer[a_type: DType, b_type: DType, a_tile_layout: Layout, b_tile_layout: Layout, num_pipeline_stages: Int, num_consumers: Int, cluster_size: Int, tma_transfer: Bool, k_group_size: Int, //, origin: MutOrigin, ring_buffer_type: AnyStruct[RingBuffer[a_type, b_type, a_tile_layout, b_tile_layout, num_pipeline_stages, num_consumers, cluster_size, tma_transfer, k_group_size]]]`

Producer view of the ring buffer.

This struct provides the producer interface to the ring buffer, allowing
the producer to wait for empty slots and fill them with new tiles.

## Fields

* â€‹ring\_buffer\_ptr (`RingBufferProducer[origin, ring_buffer_type].RingBufferPtrType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ATile`

`comptime ATile = ring_buffer_type.ATile`

### `BTile`

`comptime BTile = ring_buffer_type.BTile`

### `RingBufferPtrType`

`comptime RingBufferPtrType = Pointer[ring_buffer_type, origin]`

## Methods

### `__init__`

`__init__(ring_buffer_ptr: Pointer[ring_buffer_type, origin]) -> Self`

### `__enter__`

`__enter__(mut self) -> Self`

### `get_tiles`

`get_tiles(mut self) -> ProducerTiles[origin, ring_buffer_type]`

Get a context manager for accessing the next empty tile slot.

**Returns:**

[`ProducerTiles`](/mojo/kernels/linalg/matmul/gpu/sm100_structured/ring_buffer/ProducerTiles)

</section>

---

## ring_buffer (3)

<section class='mojo-docs'>

Ring buffer implementation for producer-consumer synchronization in GPU kernels.

This module provides a ring buffer abstraction that enables efficient overlap of
memory transfers and computation in matrix multiplication kernels. The pattern
divides work between:

* Producer: One warp group that loads tiles from global to shared memory
* Consumers: Multiple warp groups that process tiles using tensor cores

The ring buffer uses barrier synchronization to coordinate access to a circular
queue of tile buffers, allowing the producer to work ahead while consumers process
previously loaded data.

Usage Example:
\# Create ring buffer during kernel initialization
var ring\_buffer = RingBuffer\[...]\(full\_mbar, empty\_mbar, ...)

```
# Producer pattern
with ring_buffer.producer() as producer:
    while has_work():
        with producer.get_tiles() as tiles:
            # Load data into tiles.a_tile and tiles.b_tile
            load_tile(tiles.a_tile, tiles.barrier)

# Consumer pattern
with ring_buffer.consumer() as consumer:
    while has_work():
        with consumer.get_tiles() as tiles:
            # Process tiles.a_tile and tiles.b_tile
            gemm(tiles.a_tile, tiles.b_tile, output)
```

## Structs

* [â€‹`ConsumerTiles`](./ConsumerTiles): Context manager for consumer access to ring buffer tiles.
* [â€‹`ProducerTiles`](./ProducerTiles): Context manager for producer access to ring buffer tiles.
* [â€‹`RingBuffer`](./RingBuffer): Ring buffer for managing pipeline synchronization between producers and consumers.
* [â€‹`RingBufferConsumer`](./RingBufferConsumer): Consumer view of the ring buffer.
* [â€‹`RingBufferProducer`](./RingBufferProducer): Producer view of the ring buffer.

</section>

---

## testbed

<section class='mojo-docs'>

## Functions

* [â€‹`test_matmul_sm90`](./test_matmul_sm90):

</section>

---

## test_matmul_sm90

<section class='mojo-docs'>

`test_matmul_sm90[a_type: DType, b_type: DType, c_type: DType, cluster_shape: IndexList[3], block_tile_shape: IndexList[3], wgmma_shape: IndexList[3], num_consumer: Int = 1, num_pipeline_stages: Int = 4, transpose_b: Bool = True, partitioned_multicast: Bool = False, grid_shape: OptionalReg[IndexList[2]] = None, use_tma_store: Bool = False, schedule: MatmulSchedule = MatmulSchedule.NONE, default_epilogue: Bool = False, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, measure_threshold: OptionalReg[Float64] = None, backend: Backend = Backend.CUBLAS, k_group_size: Int = 1](ctx: DeviceContext, m: ValOrDim[dim], n: ValOrDim[dim], k: ValOrDim[dim])`

</section>

---

## TileLoader

<section class='mojo-docs'>

Base trait for tile loading mechanisms in matrix multiplication.

This trait defines the interface for loading tiles from global memory
to shared memory, abstracting over different hardware mechanisms.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

## Required methods

### `load_tile`

`load_tile(self: _Self, dst: LayoutTensor[_Self._dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=128], mem_barrier: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], coords: Tuple[UInt, UInt])`

Load a tile from global memory to shared memory.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination tile in shared memory (must be 128-byte aligned).
* â€‹mem\_barrier ([`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)): Memory barrier for synchronization.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Tile coordinates (row, column) in the source matrix.

</section>

---

## TileLoaderCPAsync

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileLoaderCPAsync[dtype: DType, src_layout: Layout, thread_layout: Layout, swizzle_mode: TensorMapSwizzle, vector_size: Int]`

Software-based tile loader using cp.async instructions.

This loader uses CUDA's cp.async instructions for asynchronous memory
transfers with manual bounds checking and shared memory swizzling for
optimal bank conflict avoidance.

## Parameters

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the elements being loaded.
* â€‹src\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout of the source matrix in global memory.
* â€‹thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Thread arrangement for distributed copying.
* â€‹swizzle\_mode ([`TensorMapSwizzle`](/mojo/std/gpu/host/nvidia/tma/TensorMapSwizzle)): Swizzling pattern for shared memory access.
* â€‹vector\_size ([`Int`](/mojo/std/builtin/int/Int)): Number of elements loaded per thread.

## Fields

* â€‹src (`LayoutTensor[dtype, src_layout, MutAnyOrigin]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`TileLoader`](/mojo/kernels/linalg/matmul/gpu/sm90/tile_loader/TileLoader)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(src: LayoutTensor[dtype, src_layout, MutAnyOrigin]) -> Self`

Initialize the cp.async tile loader.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Source tensor in global memory.

### `load_tile`

`load_tile(self, dst: LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=128], mem_barrier: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], coords: Tuple[UInt, UInt])`

Load a tile using cp.async instructions.

Extracts a tile from the source tensor and performs an asynchronous
copy to shared memory with bounds checking and swizzling.

Note:
Unlike TMA, this method expects tile indices and handles the
conversion to element offsets internally via the tile() method.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination tile in shared memory.
* â€‹mem\_barrier ([`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)): Memory barrier for synchronization (currently unused).
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Tile indices (row\_tile, col\_tile) in the source matrix.

</section>

---

## TileLoaderTMA (Tile_loader)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileLoaderTMA[tma_origin: ImmutOrigin, dtype: DType, tile_layout: Layout, desc_layout: Layout, /, *, BK: UInt, cluster_size: Int32, use_partitioned_multicast: Bool]`

TMA-based tile loader for hardware-accelerated memory transfers.

This loader uses NVIDIA's Tensor Memory Accelerator (TMA) for efficient
2D tile transfers from global to shared memory, with optional multicast
support for multi-block clusters.

## Parameters

* â€‹tma\_origin ([`ImmutOrigin`](/mojo/std/builtin/type_aliases/#immutorigin)): Origin type for the TMA operation.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the elements being loaded.
* â€‹tile\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout of the complete tile in shared memory.
* â€‹desc\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout described by the TMA descriptor (may be smaller).
* â€‹BK ([`UInt`](/mojo/std/builtin/uint/UInt)): Block size in the K dimension (for coordinate conversion).
* â€‹cluster\_size ([`Int32`](/mojo/std/builtin/simd/#int32)): Number of blocks in the cluster (1 for no clustering).
* â€‹use\_partitioned\_multicast ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to use partitioned multicast loading.

## Fields

* â€‹tma\_op (`TileLoaderTMA[tma_origin, dtype, tile_layout, desc_layout, BK=BK, cluster_size=cluster_size, use_partitioned_multicast=use_partitioned_multicast].TMATensorTilePtr`):
* â€‹rank (`UInt`):
* â€‹multicast\_mask (`UInt16`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`TileLoader`](/mojo/kernels/linalg/matmul/gpu/sm90/tile_loader/TileLoader)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `TMATensorTilePtr`

`comptime TMATensorTilePtr = Pointer[TMATensorTile[dtype, tile_layout, desc_layout], tma_origin]`

## Methods

### `__init__`

`__init__(tma_op: Pointer[TMATensorTile[dtype, tile_layout, desc_layout], tma_origin], rank: UInt, multicast_mask: UInt16) -> Self`

Initialize the TMA tile loader.

**Args:**

* â€‹tma\_op ([`Pointer`](/mojo/std/memory/pointer/Pointer)): Pointer to the TMA tensor descriptor.
* â€‹rank ([`UInt`](/mojo/std/builtin/uint/UInt)): Rank of this block within the cluster.
* â€‹multicast\_mask ([`UInt16`](/mojo/std/builtin/simd/#uint16)): Bit mask for multicast targets.

### `load_tile`

`load_tile(self, dst: LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=128], mem_barrier: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], _coords: Tuple[UInt, UInt])`

Load a tile using TMA hardware acceleration.

Converts tile indices to element coordinates and initiates a TMA
transfer. For clusters, uses multicast to share data across blocks.

Note:
Coordinates are converted from (row, col) tile indices to
(k\_elements, row/col\_elements) for TMA's K-major ordering.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination tile in shared memory.
* â€‹mem\_barrier ([`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)): Memory barrier for synchronization.
* â€‹\_coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Tile coordinates (row\_tile\_idx, col\_tile\_idx).

</section>

---

## async_copy_with_bound_check

<section class='mojo-docs'>

`async_copy_with_bound_check[dtype: DType, src_layout: Layout, dst_layout: Layout, //, thread_layout: Layout, swizzle_mode: TensorMapSwizzle](src: LayoutTensor[dtype, src_layout, MutAnyOrigin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dst: LayoutTensor[dtype, dst_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Helper function for cp.async with boundary checking.

This method performs element-wise async copies with per-element boundary
checking. Out-of-bounds accesses are automatically zero-filled, ensuring
safe operation near matrix edges.

The method also handles shared memory swizzling to avoid bank conflicts
and maximize memory bandwidth utilization.

Template Parameters:
dtype: Data type of the elements.
src\_layout: Layout of the source tile.
dst\_layout: Layout of the destination tile.
thread\_layout: Thread arrangement for distributed copying.
swizzle\_mode: Swizzling pattern for bank conflict avoidance.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Source tensor fragment in global memory.
* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination tensor fragment in shared memory.

</section>

---

## tile_loader (Tile_loader)

<section class='mojo-docs'>

TileLoader module for efficient tile loading in GPU matrix multiplication.

This module provides utilities for loading matrix tiles from global memory to
shared memory using two different mechanisms:

1. TMA (Tensor Memory Accelerator): Hardware-accelerated loads that can efficiently
   transfer 2D tiles with multicast support for multi-block clusters.

2. cp.async: Software-based asynchronous copy instructions with manual bounds
   checking and swizzling for optimal shared memory access patterns.

The TileLoader struct abstracts these loading mechanisms to provide a unified
interface for the matmul kernel's producer threads.

## Structs

* [â€‹`TileLoaderCPAsync`](./TileLoaderCPAsync): Software-based tile loader using cp.async instructions.
* [â€‹`TileLoaderTMA`](./TileLoaderTMA): TMA-based tile loader for hardware-accelerated memory transfers.

## Traits

* [â€‹`TileLoader`](./TileLoader): Base trait for tile loading mechanisms in matrix multiplication.

## Functions

* [â€‹`async_copy_with_bound_check`](./async_copy_with_bound_check): Helper function for cp.async with boundary checking.

</section>

---

## FragmentToSMemWriter

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct FragmentToSMemWriter[c_type: DType, c_tile_layout: Layout, //, tile_n_size: Int, num_m_mmas: Int, num_consumer: Int, half_tile: Bool, WG_BM: Int, WG_BN: Int, sub_wg_id: Int]`

Writes WGMMA accumulator results from registers to shared memory using st.matrix.

Stores 16-byte fragments with swizzling to avoid bank conflicts. Sub-warp groups
divide N-dimension work, each handling a portion of WG\_BN output tiles.

## Parameters

* â€‹c\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Output data type (must be bfloat16 for st.matrix).
* â€‹c\_tile\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout of the entire shared memory region.
* â€‹tile\_n\_size ([`Int`](/mojo/std/builtin/int/Int)): Width of each output tile (typically TMA\_BN).
* â€‹num\_m\_mmas ([`Int`](/mojo/std/builtin/int/Int)): Number of MMA operations in M dimension.
* â€‹num\_consumer ([`Int`](/mojo/std/builtin/int/Int)): Number of consumer warp groups.
* â€‹half\_tile ([`Bool`](/mojo/std/builtin/bool/Bool)): Special mode for handling partial tiles.
* â€‹WG\_BM ([`Int`](/mojo/std/builtin/int/Int)): Warp group tile height.
* â€‹WG\_BN ([`Int`](/mojo/std/builtin/int/Int)): Warp group tile width.
* â€‹sub\_wg\_id ([`Int`](/mojo/std/builtin/int/Int)): Which portion of WG\_BN this instance handles.

## Fields

* â€‹c\_tile (`LayoutTensor[c_type, c_tile_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128]`):
* â€‹warp\_group\_thread\_idx (`UInt`):
* â€‹local\_warp\_group\_idx (`UInt`):
* â€‹st\_matrix\_rt\_layout (`RuntimeLayout[st_matrix_n_layout[c_type, tile_n_size, num_m_mmas, num_consumer](), element_type=DType.int32, linear_idx_type=DType.int32]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`RegTileWriter`](/mojo/kernels/linalg/matmul/gpu/sm90/tile_writer/RegTileWriter)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `st_matrix_layout`

`comptime st_matrix_layout = Layout.row_major(WG_BM, tile_n_size)`

### `st_matrix_rt_layout_type`

`comptime st_matrix_rt_layout_type = RuntimeLayout[st_matrix_n_layout[c_type, tile_n_size, num_m_mmas, num_consumer](), element_type=DType.int32, linear_idx_type=DType.int32]`

### `st_matrix_swizzle`

`comptime st_matrix_swizzle = make_ldmatrix_swizzle[c_type, tile_n_size, log2_floor((16 // size_of[c_type]()))]()`

## Methods

### `__init__`

`__init__(c_tile: LayoutTensor[c_type, c_tile_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128], warp_group_thread_idx: UInt, local_warp_group_idx: UInt) -> Self`

Initialize the fragment writer.

**Args:**

* â€‹c\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Shared memory tile to write to.
* â€‹warp\_group\_thread\_idx ([`UInt`](/mojo/std/builtin/uint/UInt)): Thread index within the warp group.
* â€‹local\_warp\_group\_idx ([`UInt`](/mojo/std/builtin/uint/UInt)): Sub-warp group index (divides N-dimension work).

### `write_tile`

`write_tile(self, c_reg_tile: LayoutTensor[_dtype, layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], coords: Tuple[UInt, UInt])`

Write accumulator tile from registers to shared memory.

**Args:**

* â€‹c\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Register tile containing MMA results.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Tile position (row\_idx, col\_idx) in output.

</section>

---

## RegTileWriter

<section class='mojo-docs'>

Base trait for tile writing mechanisms in matrix multiplication.

This trait defines the interface for writing register tiles to memory
(either shared memory or global memory).

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

## Required methods

### `write_tile`

`write_tile(self: _Self, c_reg_tile: LayoutTensor[_dtype, layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], coords: Tuple[UInt, UInt])`

Write a register tile to memory.

**Args:**

* â€‹c\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Source register tile containing accumulator values.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Tile coordinates (row, column) in the destination matrix.

</section>

---

## RegisterToGMemWriter

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct RegisterToGMemWriter[c_type: DType, dst_layout: Layout, dst_address_space: AddressSpace, dst_element_layout: Layout, dst_layout_int_type: DType, dst_linear_idx_type: DType, dst_masked: Bool, dst_alignment: Int, //, wgmma_shape: IndexList[3], num_consumer: Int, N: Int, epilogue_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, check_runtime_bounds: Bool = False]`

Writer for transferring accumulator registers directly to global memory.

This writer handles the direct copy from register tiles to global memory
tiles, with proper thread distribution and alignment. It supports optional
epilogue processing, compute lambda transformations, and bounds checking.

Note:
At most one of epilogue\_fn or compute\_lambda\_fn should be set.

## Parameters

* â€‹c\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Output data type.
* â€‹dst\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout of the destination tensor.
* â€‹dst\_address\_space ([`AddressSpace`](/mojo/std/memory/pointer/AddressSpace)): Address space of the destination tensor.
* â€‹dst\_element\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Element layout of the destination tensor.
* â€‹dst\_layout\_int\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Integer type for destination layout indices.
* â€‹dst\_linear\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Linear index type for destination tensor.
* â€‹dst\_masked ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether the destination tensor is masked.
* â€‹dst\_alignment ([`Int`](/mojo/std/builtin/int/Int)): Alignment requirement for destination tensor.
* â€‹wgmma\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): Shape of the WGMMA operation \[M, N, K].
* â€‹num\_consumer ([`Int`](/mojo/std/builtin/int/Int)): Number of consumer warp groups.
* â€‹N ([`Int`](/mojo/std/builtin/int/Int)): Matrix N dimension.
* â€‹epilogue\_fn ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional epilogue function (mutates value in place).
* â€‹compute\_lambda\_fn ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional compute lambda function (returns new value).
* â€‹check\_runtime\_bounds ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to perform bounds checking on N dimension.

## Fields

* â€‹thread\_info (`ThreadInfo`):
* â€‹dst (`RegisterToGMemWriter[wgmma_shape, num_consumer, N, epilogue_fn, compute_lambda_fn, check_runtime_bounds].DstType`):
* â€‹num\_m\_mmas (`Int`):
* â€‹tile\_coords (`OptionalReg[TileCoordinates]`):
* â€‹max\_row (`OptionalReg[UInt32]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`RegTileWriter`](/mojo/kernels/linalg/matmul/gpu/sm90/tile_writer/RegTileWriter)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `c_frag_size`

`comptime c_frag_size = ((wgmma_shape.__getitem__[3, DType.int64, Int](0) * wgmma_shape.__getitem__[3, DType.int64, Int](1)) // WARPGROUP_SIZE)`

### `DstType`

`comptime DstType = LayoutTensor[c_type, dst_layout, MutAnyOrigin, address_space=dst_address_space, element_layout=dst_element_layout, layout_int_type=dst_layout_int_type, linear_idx_type=dst_linear_idx_type, masked=dst_masked, alignment=dst_alignment]`

### `num_frag_mats`

`comptime num_frag_mats = (RegisterToGMemWriter[wgmma_shape, num_consumer, N, epilogue_fn, compute_lambda_fn, check_runtime_bounds].num_n_frag_mat * RegisterToGMemWriter[wgmma_shape, num_consumer, N, epilogue_fn, compute_lambda_fn, check_runtime_bounds].num_m_frag_mat)`

### `num_m_frag_mat`

`comptime num_m_frag_mat = ((wgmma_shape.__getitem__[3, DType.int64, Int](0) // 4) // 8)`

### `num_n_frag_mat`

`comptime num_n_frag_mat = (wgmma_shape.__getitem__[3, DType.int64, Int](1) // 8)`

## Methods

### `__init__`

`__init__(dst: LayoutTensor[c_type, dst_layout, MutAnyOrigin, address_space=dst_address_space, element_layout=dst_element_layout, layout_int_type=dst_layout_int_type, linear_idx_type=dst_linear_idx_type, masked=dst_masked, alignment=dst_alignment], warp_group_thread_idx: UInt, num_m_mmas: Int, tile_coords: OptionalReg[TileCoordinates] = None, max_row: OptionalReg[UInt32] = None) -> Self`

Initialize the register-to-global-memory writer.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination tensor in global memory.
* â€‹warp\_group\_thread\_idx ([`UInt`](/mojo/std/builtin/uint/UInt)): Thread index within the warp group.
* â€‹num\_m\_mmas ([`Int`](/mojo/std/builtin/int/Int)): Number of MMA tiles in M dimension.
* â€‹tile\_coords ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional tile coordinates for epilogue processing.
* â€‹max\_row ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional maximum valid M coordinate (for epilogue).

### `write_tile`

`write_tile(self, c_reg_tile: LayoutTensor[_dtype, layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], coords: Tuple[UInt, UInt])`

Write a single MMA tile from registers to global memory.

**Args:**

* â€‹c\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Register tile containing accumulator values.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Tile coordinates (row, column) in the destination matrix.

</section>

---

## SMemTileWriter

<section class='mojo-docs'>

Base trait for tile writing mechanisms in matrix multiplication.

This trait defines the interface for writing tiles from shared memory to global memory,
abstracting over different hardware mechanisms.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

## Required methods

### `write_tile`

`write_tile(self: _Self, src: LayoutTensor[_Self._dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=128], coords: Tuple[UInt, UInt])`

Write a tile from shared memory to global memory.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Source tile in shared memory (must be 128-byte aligned).
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Tile coordinates (row, column) in the destination matrix.

</section>

---

## ThreadInfo

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ThreadInfo`

Thread identification within the warp group.

## Fields

* â€‹warp\_id (`UInt`):
* â€‹lane\_id (`UInt`):
* â€‹lane\_row (`UInt32`):
* â€‹lane\_col (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(warp_id: UInt, lane_id: UInt, lane_row: UInt32, lane_col: UInt32) -> Self`

### `from_warp_group_idx`

`static from_warp_group_idx(warp_group_thread_idx: UInt) -> Self`

Create ThreadInfo from a warp group thread index.

**Args:**

* â€‹warp\_group\_thread\_idx ([`UInt`](/mojo/std/builtin/uint/UInt)): Thread index within the warp group.

**Returns:**

`Self`: ThreadInfo struct with computed warp\_id, lane\_id, lane\_row, and lane\_col.

</section>

---

## TileCoordinates

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileCoordinates`

Helper struct for managing tile coordinate offsets.

This struct encapsulates corner and split coordinates used in epilogue
processing and provides a clean interface for coordinate transformations.

## Fields

* â€‹corner (`IndexList[2]`):
* â€‹split (`IndexList[2]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(corner: IndexList[2], split: IndexList[2]) -> Self`

Initialize tile coordinates.

**Args:**

* â€‹corner ([`IndexList`](/mojo/std/utils/index_/IndexList)): Corner coordinates offset.
* â€‹split ([`IndexList`](/mojo/std/utils/index_/IndexList)): Split coordinates offset.

### `adjust`

`adjust(self, base_coords: IndexList[2]) -> IndexList[2]`

Add corner and split offsets to base coordinates.

**Args:**

* â€‹base\_coords ([`IndexList`](/mojo/std/utils/index_/IndexList)): Base tile coordinates.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): Adjusted coordinates with corner and split offsets applied.

</section>

---

## TileWriterTMA

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileWriterTMA[tma_origin: ImmutOrigin, dtype: DType, tma_layout: Layout, desc_layout: Layout, //]`

TMA-based tile writer for hardware-accelerated memory transfers.

This writer uses NVIDIA's Tensor Memory Accelerator (TMA) for efficient
2D tile transfers from shared to global memory.

## Parameters

* â€‹tma\_origin ([`ImmutOrigin`](/mojo/std/builtin/type_aliases/#immutorigin)): Origin type for the TMA operation.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the elements being written.
* â€‹tma\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout of the TMA tile for async store operations.
* â€‹desc\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout described by the TMA descriptor.

## Fields

* â€‹tma\_op (`TileWriterTMA.TMATensorTilePtr`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`SMemTileWriter`](/mojo/kernels/linalg/matmul/gpu/sm90/tile_writer/SMemTileWriter)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `TMATensorTilePtr`

`comptime TMATensorTilePtr = Pointer[TMATensorTile[dtype, tma_layout, desc_layout], tma_origin]`

## Methods

### `__init__`

`__init__(tma_op: Pointer[TMATensorTile[dtype, tma_layout, desc_layout], tma_origin]) -> Self`

Initialize the TMA tile writer.

**Args:**

* â€‹tma\_op ([`Pointer`](/mojo/std/memory/pointer/Pointer)): Pointer to the TMA tensor descriptor.

### `write_tile`

`write_tile(self, src: LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=128], coords: Tuple[UInt, UInt])`

Write a tile using TMA hardware acceleration.

Performs an asynchronous TMA store from shared memory to global memory.
The operation includes proper fencing and synchronization.

Note:
Coordinates are expected in (N, M) order for column-major output.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Source tile in shared memory.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Tile coordinates (col, row) in element space.

</section>

---

## TileWriterThreadwise

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileWriterThreadwise[dtype: DType, dst_layout: Layout, dst_address_space: AddressSpace, dst_element_layout: Layout, dst_layout_int_type: DType, dst_linear_idx_type: DType, dst_masked: Bool, dst_alignment: Int, //, thread_layout: Layout, simd_size: Int, half_tile: Bool = False]`

## Fields

* â€‹dst (`TileWriterThreadwise[thread_layout, simd_size, half_tile].DstType`):
* â€‹thread\_idx (`UInt`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`SMemTileWriter`](/mojo/kernels/linalg/matmul/gpu/sm90/tile_writer/SMemTileWriter)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `DstType`

`comptime DstType = LayoutTensor[dtype, dst_layout, MutAnyOrigin, address_space=dst_address_space, element_layout=dst_element_layout, layout_int_type=dst_layout_int_type, linear_idx_type=dst_linear_idx_type, masked=dst_masked, alignment=dst_alignment]`

## Methods

### `__init__`

`__init__(dst: LayoutTensor[dtype, dst_layout, MutAnyOrigin, address_space=dst_address_space, element_layout=dst_element_layout, layout_int_type=dst_layout_int_type, linear_idx_type=dst_linear_idx_type, masked=dst_masked, alignment=dst_alignment], thread_idx: UInt) -> Self`

Initialize the threadwise tile writer.

**Args:**

* â€‹dst ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination tensor in global memory.
* â€‹thread\_idx ([`UInt`](/mojo/std/builtin/uint/UInt)): Thread index within the consumer warp group.

### `write_tile`

`write_tile(self, src: LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=128], coords: Tuple[UInt, UInt])`

Write a tile using thread-distributed stores.

Each thread writes a portion of the tile with proper swizzling
for optimal memory access patterns.

**Args:**

* â€‹src ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Source tile in shared memory.
* â€‹coords ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Tile indices (row\_tile, col\_tile) in the destination matrix.

</section>

---

## tile_writer (Tile_writer)

<section class='mojo-docs'>

TileWriter module for efficient tile writing in GPU matrix multiplication.

This module provides utilities for writing tiles to memory using different
mechanisms and destinations:

1. Register â†’ Shared Memory: Uses st.matrix hardware instruction for efficient
   storage of WGMMA accumulator results to shared memory with swizzling.

2. Register â†’ Global Memory: Direct stores from register tiles to global memory
   with optional epilogue processing and bounds checking.

3. Shared Memory â†’ Global Memory: Hardware-accelerated TMA stores or regular
   stores for efficient 2D tile transfers from shared to global memory.

Two main traits abstract these writing mechanisms:

* TileWriter: For shared memory â†’ global memory transfers
* RegTileWriter: For register â†’ memory (shared or global) transfers

## Structs

* [â€‹`FragmentToSMemWriter`](./FragmentToSMemWriter): Writes WGMMA accumulator results from registers to shared memory using st.matrix.
* [â€‹`RegisterToGMemWriter`](./RegisterToGMemWriter): Writer for transferring accumulator registers directly to global memory.
* [â€‹`ThreadInfo`](./ThreadInfo): Thread identification within the warp group.
* [â€‹`TileCoordinates`](./TileCoordinates): Helper struct for managing tile coordinate offsets.
* [â€‹`TileWriterThreadwise`](./TileWriterThreadwise):
* [â€‹`TileWriterTMA`](./TileWriterTMA): TMA-based tile writer for hardware-accelerated memory transfers.

## Traits

* [â€‹`RegTileWriter`](./RegTileWriter): Base trait for tile writing mechanisms in matrix multiplication.
* [â€‹`SMemTileWriter`](./SMemTileWriter): Base trait for tile writing mechanisms in matrix multiplication.

</section>

---

## TuningConfigSM90

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TuningConfigSM90`

## Fields

* â€‹M (`Int`):
* â€‹N (`Int`):
* â€‹K (`Int`):
* â€‹mma\_shape (`IndexList[3]`):
* â€‹block\_tile\_shape (`IndexList[3]`):
* â€‹num\_pipeline\_stages (`UInt`):
* â€‹cluster\_shape (`IndexList[3]`):
* â€‹num\_consumer (`UInt`):
* â€‹partitioned\_multicast (`Bool`):
* â€‹grid\_shape (`OptionalReg[IndexList[2]]`):
* â€‹schedule (`MatmulSchedule`):
* â€‹splits (`OptionalReg[Int]`):
* â€‹raster\_order (`OptionalReg[RasterOrder]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`TuningConfig`](/mojo/kernels/internal_utils/dispatch_utils/TuningConfig)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(M: Int, N: Int, K: Int, mma_shape: IndexList[3], block_tile_shape: IndexList[3], num_pipeline_stages: UInt, cluster_shape: IndexList[3], num_consumer: UInt, partitioned_multicast: Bool, grid_shape: OptionalReg[IndexList[2]] = None, schedule: MatmulSchedule = MatmulSchedule.NONE, splits: OptionalReg[Int] = None, raster_order: OptionalReg[RasterOrder] = None) -> Self`

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

</section>

---

## tuning_configs (Tuning_configs)

<section class='mojo-docs'>

## Structs

* [â€‹`TuningConfigSM90`](./TuningConfigSM90):

</section>

---

## split_k_reduce

<section class='mojo-docs'>

`split_k_reduce[c_type: DType, work_space_type: DType, c_layout: Layout, work_space_layout: Layout, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, c_layout, origin], work_space: LayoutTensor[work_space_type, work_space_layout, origin], ctx: DeviceContext)`

</section>

---

## MatmulSchedule

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MatmulSchedule`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `DS_SCHEDULER`

`comptime DS_SCHEDULER = MatmulSchedule(3)`

### `NONE`

`comptime NONE = MatmulSchedule(0)`

### `TILE1D`

`comptime TILE1D = MatmulSchedule(1)`

### `TILE2D`

`comptime TILE2D = MatmulSchedule(2)`

## Methods

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## RasterOrder (Tile_scheduler)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct RasterOrder`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Hashable`](/mojo/std/hashlib/hash/Hashable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `AlongM`

`comptime AlongM = RasterOrder(1)`

### `AlongN`

`comptime AlongN = RasterOrder(0)`

## Methods

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

### `__hash__`

`__hash__[H: Hasher](self, mut hasher: H)`

</section>

---

## TileScheduler (5)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileScheduler[problem_shape: IndexList[3], tile_shape: IndexList[3], grid_shape: IndexList[2], cluster: IndexList[3] = Index(1, 1, 1), raster_dim: UInt32 = 1, schedule: MatmulSchedule = MatmulSchedule.TILE2D]`

## Fields

* â€‹idx (`UInt32`):
* â€‹prob\_shape (`IndexList[3]`):
* â€‹num\_waves\_m (`UInt32`):
* â€‹num\_waves\_n (`UInt32`):
* â€‹log\_num\_waves\_n (`FastDiv[DType.uint32]`):
* â€‹current\_iter (`Int`):
* â€‹num\_aligned\_m\_blocks (`UInt32`):
* â€‹num\_blocks (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `kNum1DBlocksPerGroup`

`comptime kNum1DBlocksPerGroup = 16`

### `kNumNBlocks`

`comptime kNumNBlocks = SIMD[DType.uint32, 1](ceildiv(problem_shape.__getitem__[3, DType.int64, Int](1), tile_shape.__getitem__[3, DType.int64, Int](1)))`

### `num_grids`

`comptime num_grids = SIMD[DType.uint32, 1]((grid_shape.__getitem__[2, DType.int64, Int](0) * grid_shape.__getitem__[2, DType.int64, Int](1)))`

### `wave_shape`

`comptime wave_shape = Index[dtype=DType.uint32]((tile_shape.__getitem__[3, DType.int64, Int](0) * grid_shape.__getitem__[2, DType.int64, Int](1)), (tile_shape.__getitem__[3, DType.int64, Int](1) * grid_shape.__getitem__[2, DType.int64, Int](0)))`

## Methods

### `__init__`

`__init__(prob_shape: IndexList[3]) -> Self`

### `get_current_work_info`

`get_current_work_info(mut self) -> WorkInfo`

**Returns:**

`WorkInfo`

### `advance`

`advance(mut self)`

### `fetch_next_work`

`fetch_next_work(mut self) -> WorkInfo`

**Returns:**

`WorkInfo`

### `num_output_tiles`

`num_output_tiles(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `fetch_next_work_ds`

`fetch_next_work_ds(mut self) -> WorkInfo`

**Returns:**

`WorkInfo`

</section>

---

## WorkInfo (5)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct WorkInfo`

## Fields

* â€‹m (`UInt32`):
* â€‹n (`UInt32`):
* â€‹k\_start (`UInt32`):
* â€‹num\_k\_tiles (`UInt32`):
* â€‹is\_valid\_tile (`Bool`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `INVALID_WORK_INFO`

`comptime INVALID_WORK_INFO = WorkInfo(0, 0, 0, 0, False)`

## Methods

### `__init__`

`__init__() -> Self`

### `is_valid`

`is_valid(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `is_final_split`

`is_final_split(self, k_tiles_per_output_tile: UInt32) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `get_k_start`

`get_k_start(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

</section>

---

## tile_scheduler (3)

<section class='mojo-docs'>

## Structs

* [â€‹`MatmulSchedule`](./MatmulSchedule):
* [â€‹`RasterOrder`](./RasterOrder):
* [â€‹`TileScheduler`](./TileScheduler):
* [â€‹`WorkInfo`](./WorkInfo):

</section>

---

## ReductionMode

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ReductionMode`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `Deterministic`

`comptime Deterministic = ReductionMode(0)`

### `Nondeterministic`

`comptime Nondeterministic = ReductionMode(1)`

## Methods

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## SplitKTileScheduler

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SplitKTileScheduler[problem_shape_nk: IndexList[2], tile_shape: IndexList[3], splits: UInt32, num_consumer: UInt32, num_pipeline_stages: UInt32, cluster_shape: IndexList[2], raster_order: RasterOrder, reduction_mode: ReductionMode = ReductionMode.Deterministic]`

## Fields

* â€‹prob\_shape (`IndexList[3]`):
* â€‹block\_id\_in\_cluster (`IndexList[2]`):
* â€‹blocks\_per\_problem (`UInt32`):
* â€‹current\_work\_linear\_idx (`UInt32`):
* â€‹log\_cluster\_shape\_major (`UInt32`):
* â€‹log\_cluster\_shape\_minor (`UInt32`):
* â€‹cluster\_blk\_major (`UInt32`):
* â€‹locks\_ptr (`LegacyUnsafePointer[Int32]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `k_tiles_per_output_tile`

`comptime k_tiles_per_output_tile = ceildiv(problem_shape_nk.__getitem__[2, DType.int64, Int](1), tile_shape.__getitem__[3, DType.int64, Int](2))`

### `k_tiles_per_split`

`comptime k_tiles_per_split = splits.__rfloordiv__[DType.uint32, 1](SIMD[DType.uint32, 1](ceildiv(problem_shape_nk.__getitem__[2, DType.int64, Int](1), tile_shape.__getitem__[3, DType.int64, Int](2))))`

### `log_cluster_size`

`comptime log_cluster_size = log2_floor((cluster_shape.__getitem__[2, DType.int64, Int](0) * cluster_shape.__getitem__[2, DType.int64, Int](1)))`

### `WorkTileType`

`comptime WorkTileType[dtype: DType, layout: Layout] = LayoutTensor[dtype, layout, MutAnyOrigin]`

#### Parameters

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)):
* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)):

## Methods

### `__init__`

`__init__(prob_shape: IndexList[3], block_id_in_cluster: IndexList[2], locks_ptr: LegacyUnsafePointer[UInt8]) -> Self`

### `get_sm_num`

`get_sm_num(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `get_problem_blocks_shape`

`static get_problem_blocks_shape(problem_shape: IndexList[3], dyn_tile_shape: IndexList[3], dyn_cluster_shape: IndexList[2]) -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

### `initial_work_tile_info`

`initial_work_tile_info(mut self) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `get_current_work_info`

`get_current_work_info(mut self) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `get_worktile_m_n_idx`

`get_worktile_m_n_idx(mut self, mut work_tile_info: WorkInfo, linear_tile_id: UInt32)`

### `assign_work`

`assign_work(mut self, mut work_tile_info: WorkInfo, linear_idx: UInt32)`

### `get_k_start_and_linear_tile_id`

`get_k_start_and_linear_tile_id(mut self, mut work_tile_info: WorkInfo, linear_idx: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `fetch_next_work`

`fetch_next_work(mut self, mut work_tile_info: WorkInfo) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/linalg/matmul/gpu/tile_scheduler/WorkInfo)

### `requires_reduction`

`requires_reduction(self, work_tile_info: WorkInfo) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `advance_to_next_work`

`advance_to_next_work(mut self)`

### `is_last_split`

`is_last_split(self, work_tile_info: WorkInfo) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `get_grid_shape`

`static get_grid_shape(dyn_cluster_shape: IndexList[3], dyn_raster_order: RasterOrder = RasterOrder.AlongN) -> IndexList[3]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

### `get_num_tiles`

`static get_num_tiles(problem_shape: IndexList[3], dyn_tile_shape: IndexList[3], dyn_cluster_shape: IndexList[2]) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `get_required_locks_buffer_size_bytes`

`static get_required_locks_buffer_size_bytes[accum_type: DType, dyn_num_consumer: UInt32](problem_shape: IndexList[3], dyn_tile_shape: IndexList[3], dyn_cluster_shape: IndexList[2]) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `get_linear_idx_from_m_and_n`

`get_linear_idx_from_m_and_n(self, tile_m: UInt32, tile_n: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `output_tile_index`

`output_tile_index(self, work_tile_info: WorkInfo) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `reduction`

`reduction[accum_type: DType, c_reg_layout: Layout, workspace_layout: Layout](self, reduction_workspace: LayoutTensor[accum_type, workspace_layout, MutAnyOrigin], c_reg_tile: LayoutTensor[accum_type, c_reg_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL], work_tile_info: WorkInfo, num_barriers: UInt32, warp_group_local_idx: UInt32)`

### `wait_eq`

`static wait_eq(lock_ptr: LegacyUnsafePointer[Int32], barrier_id: Int32, barrier_group_thread_idx: Int, lock_idx: UInt32, val: UInt32)`

### `wait_lt`

`static wait_lt(lock_ptr: LegacyUnsafePointer[Int32], barrier_id: Int32, barrier_group_thread_idx: Int, lock_idx: UInt32, count: UInt32)`

### `arrive_set`

`static arrive_set(lock_ptr: LegacyUnsafePointer[Int32], barrier_id: Int32, barrier_group_thread_idx: Int, lock_idx: UInt32, increment: UInt32)`

### `store_accumulator`

`store_accumulator[accum_type: DType, c_reg_layout: Layout, workspace_layout: Layout](self, reduction_workspace: LayoutTensor[accum_type, workspace_layout, MutAnyOrigin], c_reg_tile: LayoutTensor[accum_type, c_reg_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL], reduction_tile_idx: UInt32, warp_group_local_idx: UInt32, warp_group_thread_idx: UInt32)`

### `reduce_add`

`reduce_add[accum_type: DType, c_reg_layout: Layout, workspace_layout: Layout, //, *, write_back: Bool](self, reduction_workspace: LayoutTensor[accum_type, workspace_layout, MutAnyOrigin], c_reg_tile: LayoutTensor[accum_type, c_reg_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL], reduction_tile_idx: UInt32, warp_group_local_idx: UInt32, warp_group_thread_idx: UInt32)`

</section>

---

## tile_scheduler_splitk (3)

<section class='mojo-docs'>

## Structs

* [â€‹`ReductionMode`](./ReductionMode):
* [â€‹`SplitKTileScheduler`](./SplitKTileScheduler):

</section>

---

## matmul (6)

<section class='mojo-docs'>

Provides the backend implementation for matmuls.

## Packages

* [â€‹`cpu`](./cpu/): Provides the CPU backend implementations for matmuls.
* [â€‹`gpu`](./gpu/):
* [â€‹`vendor`](./vendor/): Provides the Vendor backend implementations for matmuls.

## Functions

* [â€‹`matmul`](./matmul):

</section>

---

## matmul (7)

<section class='mojo-docs'>

`matmul[transpose_a: Bool = False, transpose_b: Bool = False, b_packed: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, saturated_vnni: Bool = False, single_thread_blocking_override: Bool = False, _trace_description: StringSlice[StaticConstantOrigin] = "", target: StringSlice[StaticConstantOrigin] = "cpu"](c: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: Optional[DeviceContext])`

`matmul[transpose_a: Bool = False, transpose_b: Bool = False, b_packed: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, saturated_vnni: Bool = False, single_thread_blocking_override: Bool = False, _trace_description: StringSlice[StaticConstantOrigin] = "", target: StringSlice[StaticConstantOrigin] = "cpu"](c: NDBuffer[dtype, 2, origin, shape], a: NDBuffer[dtype, 2, origin, shape], b: NDBuffer[dtype, 2, origin, shape], ctx: DeviceContextPtr = DeviceContextPtr())`

`matmul[transpose_a: Bool = False, transpose_b: Bool = False, b_packed: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, elementwise_compute_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, saturated_vnni: Bool = False, single_thread_blocking_override: Bool = False, _trace_description: StringSlice[StaticConstantOrigin] = "", target: StringSlice[StaticConstantOrigin] = "cpu"](c: NDBuffer[dtype, 2, origin, shape], a: NDBuffer[dtype, 2, origin, shape], b: NDBuffer[dtype, 2, origin, shape], ctx: Optional[DeviceContext])`

</section>

---

## Backend

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct Backend`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Equatable`](/mojo/std/builtin/comparable/Equatable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `AUTOMATIC`

`comptime AUTOMATIC = Backend(0)`

### `CUBLAS`

`comptime CUBLAS = Backend(1)`

### `CUBLASLT`

`comptime CUBLASLT = Backend(2)`

### `HIPBLASLT`

`comptime HIPBLASLT = Backend(4)`

### `ROCBLAS`

`comptime ROCBLAS = Backend(3)`

## Methods

### `__init__`

`__init__(value: Int) -> Self`

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__is__`

`__is__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__isnot__`

`__isnot__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__int__`

`__int__(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

</section>

---

## Handle

<section class='mojo-docs'>

`struct Handle[backend: Backend = _resolve_backend[Backend.AUTOMATIC]()]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = False`

### `__del__is_trivial`

`comptime __del__is_trivial = False`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = False`

### `resolved_backend`

`comptime resolved_backend = _resolve_backend[backend]()`

### `type`

`comptime type = Variant[LegacyUnsafePointer[cublasContext], Handle, hipblasLtHandle_t]`

## Methods

### `__init__`

`__init__(out self)`

### `__is__`

`__is__(self, other: Backend) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__isnot__`

`__isnot__(self, other: Backend) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__enter__`

`__enter__(self) -> Self`

### `__exit__`

`__exit__(mut self)`

</section>

---

## blas

<section class='mojo-docs'>

## Structs

* [â€‹`Backend`](./Backend):
* [â€‹`Handle`](./Handle):

## Functions

* [â€‹`matmul`](./matmul): Matmul using the vendor BLAS library. With a global handle.

</section>

---

## matmul (Blas)

<section class='mojo-docs'>

`matmul[use_tf32: Bool = False, *, scales_type: DType = DType.invalid, a_scales_layout: Layout = Layout.row_major(-1), b_scales_layout: Layout = Layout.row_major(-1)](ctx: DeviceContext, c: NDBuffer[dtype, 2, origin, shape], a: NDBuffer[dtype, 2, origin, shape], b: NDBuffer[dtype, 2, origin, shape], *, a_scales: OptionalReg[LayoutTensor[scales_type, a_scales_layout, MutAnyOrigin]] = None, b_scales: OptionalReg[LayoutTensor[scales_type, b_scales_layout, MutAnyOrigin]] = None, c_row_major: Bool = False, transpose_a: Bool = False, transpose_b: Bool = False, alpha: Float32 = 1, beta: Float32 = 0, batch_size: Int = 1)`

Matmul using the vendor BLAS library. With a global handle.

`matmul[c_type: DType, a_type: DType, b_type: DType, c_layout: Layout, a_layout: Layout, b_layout: Layout, *, use_tf32: Bool = False, scales_type: DType = DType.invalid, a_scales_layout: Layout = Layout.row_major(-1), b_scales_layout: Layout = Layout.row_major(-1)](ctx: DeviceContext, c_tensor: LayoutTensor[c_type, c_layout, origin], a_tensor: LayoutTensor[a_type, a_layout, origin], b_tensor: LayoutTensor[b_type, b_layout, origin], *, a_scales: OptionalReg[LayoutTensor[scales_type, a_scales_layout, MutAnyOrigin]] = None, b_scales: OptionalReg[LayoutTensor[scales_type, b_scales_layout, MutAnyOrigin]] = None, c_row_major: Bool = False, transpose_a: Bool = False, transpose_b: Bool = False, alpha: Float32 = 1, beta: Float32 = 0, batch_size: Int = 1)`

`matmul[c_type: DType, a_type: DType, b_type: DType, c_layout: Layout, a_layout: Layout, b_layout: Layout, use_tf32: Bool = False, scales_type: DType = DType.invalid, a_scales_layout: Layout = Layout.row_major(-1), b_scales_layout: Layout = Layout.row_major(-1)](ctx: DeviceContext, handle: Handle[backend], c_tensor: LayoutTensor[c_type, c_layout, origin], a_tensor: LayoutTensor[a_type, a_layout, origin], b_tensor: LayoutTensor[b_type, b_layout, origin], *, a_scales: OptionalReg[LayoutTensor[scales_type, a_scales_layout, MutAnyOrigin]] = None, b_scales: OptionalReg[LayoutTensor[scales_type, b_scales_layout, MutAnyOrigin]] = None, c_row_major: Bool = False, transpose_a: Bool = False, transpose_b: Bool = False, alpha: Float32 = 1, beta: Float32 = 0, batch_size: Int = 1)`

`matmul[use_tf32: Bool = False](ctx: DeviceContext, handle: Handle[backend], c: NDBuffer[dtype, 2, origin, shape], a: NDBuffer[dtype, 2, origin, shape], b: NDBuffer[dtype, 2, origin, shape], *, c_row_major: Bool = False, transpose_a: Bool = False, transpose_b: Bool = False, alpha: Float32 = 1, beta: Float32 = 0)`

</section>

---

## vendor (Vendor)

<section class='mojo-docs'>

Provides the Vendor backend implementations for matmuls.

This backend is used for testing and evaluation.

## Modules

* [â€‹`blas`](./blas/):
* [â€‹`matmul`](./matmul/):

</section>

---

## matmul (8)

<section class='mojo-docs'>

## Functions

* [â€‹`matmul`](./matmul): This implements the matmul kernel for the Blackwell architecture. Note that we do not currently have pure mojo kernels which would utilize blackwell architectures, so in place we just call the CUBLAS library.

</section>

---

## matmul (9)

<section class='mojo-docs'>

`matmul[c_type: DType, a_type: DType, b_type: DType, //, transpose_b: Bool = False, elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None, config: OptionalReg[MatmulConfig[a_type, b_type, c_type, transpose_b]] = None](c: NDBuffer[c_type, 2, origin, shape], a: NDBuffer[a_type, 2, origin, shape], b: NDBuffer[b_type, 2, origin, shape], ctx: DeviceContext)`

This implements the matmul kernel for the Blackwell architecture. Note that we do not currently have pure mojo kernels which would utilize blackwell architectures, so in place we just call the CUBLAS library.

</section>

---

## matrix_band_part

<section class='mojo-docs'>

The module implements matrix band part functions.

## Functions

* [â€‹`matrix_band_part`](./matrix_band_part):

</section>

---

## matrix_band_part (Matrix_band_part)

<section class='mojo-docs'>

`matrix_band_part[dtype: DType, int_type: DType, cond_type: DType, rank: Int, input_0_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], simd_width: Int, single_thread_blocking_override: Bool, target: StringSlice[StaticConstantOrigin] = "cpu"](input_shape: IndexList[rank], num_lower: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], num_upper: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], exclude: LayoutTensor[cond_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr)`

</section>

---

## BTileGenerator

<section class='mojo-docs'>

`struct BTileGenerator[mut: Bool, //, config: KernelConfig, a_type: DType, b_type: DType, c_type: DType, shape: DimList, transpose_b: Bool, b_packed: Bool, origin: Origin[mut=mut]]`

Struct to encapsulate a tile of B that supports prepacking.

If b\_packed is true, calls to get\_tile will return a buffer view from B.
Otherwise, calls to get\_tile will copy a tile from B into a stack allocated
scratch buffer and return a view of that.

## Fields

* â€‹b (`NDBuffer[b_type, 2, origin, shape]`):
* â€‹b\_tile\_stack\_ptr (`LegacyUnsafePointer[Scalar[b_type]]`):
* â€‹tile\_n\_k (`IndexList[2]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `get`

`static get(b: NDBuffer[b_type, 2, origin, shape], tile_n_k: IndexList[2]) -> Self`

### `get_tile`

`get_tile[inner_size: Int](self, global_offset: GemmShape, tile_dim_nk: IndexList[2], valid_data_dim_nk: IndexList[2]) -> NDBuffer[b_type, 3, MutAnyOrigin, config.packed_shape]`

Get a packed matrix (B) tile.

valid\_data\_tile\_nk is ignored for pre-packing, where the tile is padded
to have shape of tile\_dim\_nk.

**Args:**

* â€‹global\_offset ([`GemmShape`](/mojo/kernels/linalg/utils/GemmShape)): Offset in the global M, N, K dimensions.
* â€‹tile\_dim\_nk ([`IndexList`](/mojo/std/utils/index_/IndexList)): Tile shape based on cache size and matrix dimensions.
* â€‹valid\_data\_dim\_nk ([`IndexList`](/mojo/std/utils/index_/IndexList)): The upper bounds for N and K dimensions.

**Returns:**

[`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer): A view of the packed tile.

</section>

---

## PackMatrixCols

<section class='mojo-docs'>

`struct PackMatrixCols[original_mut: Bool, //, original_shape: DimList, packed_shape: DimList, dtype: DType, simd_size: Int, column_inner_size: Int, use_vnni: Bool, use_i8mm: Bool, packed_origin: MutOrigin, original_origin: Origin[mut=original_mut]]`

Pack columns from a matrix into the mlas packed layout and extract inner vectors of columns into the packed inner dimension, e.g. extracts \[X, Y] and packs as \[Yo]\[X]\[Yi].

## Fields

* â€‹packed\_matrix (`NDBuffer[dtype, 3, packed_origin, packed_shape]`):
* â€‹original\_matrix (`NDBuffer[dtype, 2, original_origin, original_shape]`):
* â€‹global\_offset (`IndexList[2]`):
* â€‹pack\_tile\_dim (`IndexList[2]`):
* â€‹valid\_data\_dim (`IndexList[2]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `run`

`static run(packed_matrix: NDBuffer[dtype, 3, MutAnyOrigin, packed_shape], original_matrix: NDBuffer[dtype, 2, MutAnyOrigin, original_shape], global_offset: IndexList[2], pack_tile_dim: IndexList[2], valid_data_dim: IndexList[2])`

Interface function to run the packing routine. Args:     packed\_matrix(NDBuffer): pre-allocated buffer space for packed         data.     original\_matrix(NDBuffer): data buffer containing the original matrix         to pack.     global\_offset(IndexList): offset to use when indexing the         original matrix.     pack\_tile\_dim(IndexList): 2D dimension tuple describing the         size of the packed tile.     valid\_data\_dim(IndexList): 2D dimension tuple describing the         amount of valid data on the global buffer starting from the         offset.

</section>

---

## PackMatrixRows

<section class='mojo-docs'>

`struct PackMatrixRows[original_mut: Bool, //, original_shape: DimList, packed_shape: DimList, dtype: DType, simd_size: Int, row_inner_size: Int, packed_origin: MutOrigin, original_origin: Origin[mut=original_mut]]`

Pack rows from a matrix into the mlas packed layout and extract inner vectors of rows into the packed inner dimension, e.g. extract tile \[X, Y] and pack into \[Xo]\[Y]\[Xi].

## Fields

* â€‹packed\_matrix (`NDBuffer[dtype, 3, packed_origin, packed_shape]`):
* â€‹original\_matrix (`NDBuffer[dtype, 2, original_origin, original_shape]`):
* â€‹global\_offset (`IndexList[2]`):
* â€‹pack\_tile\_dim (`IndexList[2]`):
* â€‹valid\_data\_dim (`IndexList[2]`):
* â€‹valid\_simd\_dim (`IndexList[2]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `run`

`static run(packed_matrix: NDBuffer[dtype, 3, packed_origin, packed_shape], original_matrix: NDBuffer[dtype, 2, original_origin, original_shape], global_offset: IndexList[2], pack_tile_dim: IndexList[2], valid_data_dim: IndexList[2])`

Interface function to run the packing routine. Args:     packed\_matrix(NDBuffer): pre-allocated buffer space for packed         data.     original\_matrix(NDBuffer): data buffer containing the original matrix         to pack.     global\_offset(IndexList): offset to use when indexing the         original matrix.     pack\_tile\_dim(IndexList): 2D dimension tuple describing the         size of the packed tile.     valid\_data\_dim(IndexList): 2D dimension tuple describing the         amount of valid data on the global buffer starting from the         offset.

</section>

---

## packing

<section class='mojo-docs'>

## Structs

* [â€‹`BTileGenerator`](./BTileGenerator): Struct to encapsulate a tile of B that supports prepacking.
* [â€‹`PackMatrixCols`](./PackMatrixCols): Pack columns from a matrix into the mlas packed layout and extract inner vectors of columns into the packed inner dimension, e.g. extracts \[X, Y] and packs as \[Yo]\[X]\[Yi].
* [â€‹`PackMatrixRows`](./PackMatrixRows): Pack rows from a matrix into the mlas packed layout and extract inner vectors of rows into the packed inner dimension, e.g. extract tile \[X, Y] and pack into \[Xo]\[Y]\[Xi].

## Functions

* [â€‹`pack_b`](./pack_b): Utility function to pack the entire B matrix, such that each \[tile\_n // inner\_size, tile\_k, inner\_size] tile of src is contiguous in dst.
* [â€‹`pack_b_ndbuffer`](./pack_b_ndbuffer):
* [â€‹`pack_matmul_b_shape_func`](./pack_matmul_b_shape_func):
* [â€‹`pack_transposed_b_ndbuffer`](./pack_transposed_b_ndbuffer):

</section>

---

## pack_b

<section class='mojo-docs'>

`pack_b[transpose_b: Bool, simd_size: Int, inner_size: Int, a_type: DType, b_type: DType, c_type: DType, src_shape: DimList, dst_shape: DimList](dst: NDBuffer[b_type, 2, origin, dst_shape], src: NDBuffer[b_type, 2, origin, src_shape], tile_n: Int, tile_k: Int)`

Utility function to pack the entire B matrix, such that each \[tile\_n // inner\_size, tile\_k, inner\_size] tile of src is contiguous in dst.

Tiles (not tile contents) are stored in row major order, so tile\[i, j] is
tile\_n \* tile\_k bytes away from tile\[i, j+1].

</section>

---

## pack_b_ndbuffer

<section class='mojo-docs'>

`pack_b_ndbuffer[b_mut: Bool, //, a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, c_type: DType, c_shape: DimList, b_origin: Origin[mut=b_mut], output_origin: MutOrigin](b_input: NDBuffer[b_type, 2, b_origin, b_shape], output_buffer: NDBuffer[b_type, 2, output_origin])`

</section>

---

## pack_matmul_b_shape_func

<section class='mojo-docs'>

`pack_matmul_b_shape_func[a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, c_type: DType, c_shape: DimList, transpose_in_0: Bool, single_thread_blocking_override: Bool](b_input: NDBuffer[b_type, 2, origin, b_shape]) -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## pack_transposed_b_ndbuffer

<section class='mojo-docs'>

`pack_transposed_b_ndbuffer[a_type: DType, a_shape: DimList, b_type: DType, b_shape: DimList, c_type: DType, c_shape: DimList](b_input: NDBuffer[b_type, 2, origin, b_shape], output_buffer: NDBuffer[b_type, 2, origin])`

</section>

---

## apply_q

<section class='mojo-docs'>

`apply_q[dtype: DType, element_layout: Layout](sigma: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], A: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], X: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Applies the implicit Q factor stored in `A` and `sigma` after calling `qr_factorization` to the `X` matrix.

See `qr_factorization` for more details on the construction of the
Householder reflector.

</section>

---

## form_q

<section class='mojo-docs'>

`form_q[dtype: DType, element_layout: Layout](sigma: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], A: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], Q: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Forms the Q factor from the implicit Q factor stored in `A` and `sigma` after calling `qr_factorization` and stores the result in `Q`.

</section>

---

## qr_factorization

<section class='mojo-docs'>

## Functions

* [â€‹`apply_q`](./apply_q): Applies the implicit Q factor stored in `A` and `sigma` after calling `qr_factorization` to the `X` matrix.
* [â€‹`form_q`](./form_q): Forms the Q factor from the implicit Q factor stored in `A` and `sigma` after calling `qr_factorization` and stores the result in `Q`.
* [â€‹`qr_factorization`](./qr_factorization): Performs QR factorization of a matrix `A` using the Householder reflector method.

</section>

---

## qr_factorization (Qr_factorization)

<section class='mojo-docs'>

`qr_factorization[dtype: DType, element_layout: Layout](sigma: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], A: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Performs QR factorization of a matrix `A` using the Householder reflector method.

This function computes the QR factorization of matrix `A` in-place using
Householder reflections. The result is stored directly in the input matrix
`A`, with scaling factors in `sigma`. The implementation follows the LAPACK
algorithm for generating Householder reflectors in-place.

Algorithm:
The Householder reflector is defined as:
U = I - Ïƒww^H
where:
w = (x + Î½eâ‚)/Î¾
Ïƒ = Î¾/Î½
Î¾ = xâ‚€ + Î½
Î½ = sign(xâ‚€)â€–xâ€–â‚‚

```
This ensures that U^H x = -Î½eâ‚ and U^H U = I.
```

References:
\[1] Lehoucq, R. B. (1996). The computation of elementary unitary matrices.
ACM Transactions on Mathematical Software, 22(4), 393-400.
<https://www.netlib.org/lapack/lawnspdf/lawn72.pdf>
<https://library.eecs.utk.edu/files/ut-cs-94-233.pdf>

Note:
There is a typo in reference \[lawn72]. The correct result is U^H x =
-Î½eâ‚.

</section>

---

## IteratorScatterGatherAmd

<section class='mojo-docs'>

`struct IteratorScatterGatherAmd[thread_layout: Layout, num_threads: Int = thread_layout.size(), thread_scope: ThreadScope = ThreadScope.BLOCK, block_dim_count: Int = 1]`

Iterator-based AMD scatter-gather for DRAM-register data movement.

## Parameters

* â€‹thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Thread organization layout.
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): Total threads (defaults to thread\_layout size).
* â€‹thread\_scope ([`ThreadScope`](/mojo/kernels/layout/layout_tensor/ThreadScope)): Thread execution scope (block or warp).
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): Number of block dimensions.

## Fields

* â€‹buffer (`AMDBufferResource`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

## Methods

### `__init__`

`__init__(out self, tensor: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], tensor_iter: LayoutTensorIter[dtype, layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked])`

Initialize with tensor and iterator.

**Args:**

* â€‹tensor ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Layout tensor for bounds.
* â€‹tensor\_iter ([`LayoutTensorIter`](/mojo/kernels/layout/layout_tensor/LayoutTensorIter)): Iterator for AMD buffer resource.

### `copy`

`copy(self, dst_reg_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src_gmem_tile_iter: LayoutTensorIter[dtype, layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked])`

Copy DRAM to registers via iterator.

**Args:**

* â€‹dst\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination register tile.
* â€‹src\_gmem\_tile\_iter ([`LayoutTensorIter`](/mojo/kernels/layout/layout_tensor/LayoutTensorIter)): Source memory iterator.

</section>

---

## NVIDIASharedMemoryBasePtr

<section class='mojo-docs'>

`struct NVIDIASharedMemoryBasePtr[name: StringSlice[StaticConstantOrigin] = "extern_ptr_syml", memory_alignment: Int = 8]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`SharedMemoryBasePtr`](/mojo/kernels/linalg/structuring/SharedMemoryBasePtr)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `alignment`

`comptime alignment = 128`

## Methods

### `ptr`

`static ptr() -> LegacyUnsafePointer[Int8, address_space=AddressSpace.SHARED]`

**Returns:**

`LegacyUnsafePointer`

</section>

---

## SMemArrayType

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SMemArrayType[type: AnyTrivialRegType, size: Int]`

Shared memory array of fixed size.

## Parameters

* â€‹type ([`AnyTrivialRegType`](/mojo/std/builtin/type_aliases/#anytrivialregtype)): Element type.
* â€‹size ([`Int`](/mojo/std/builtin/int/Int)): Number of elements.

## Fields

* â€‹ptr (`SMemArrayType[type, size].ptr_type`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ptr_type`

`comptime ptr_type = LegacyUnsafePointer[type, address_space=AddressSpace.SHARED]`

### `storage_size`

`comptime storage_size = (size * size_of[type]())`

## Methods

### `__init__`

`__init__(unsafe_ptr: LegacyUnsafePointer[type, address_space=AddressSpace.SHARED]) -> Self`

Initialize with shared memory pointer.

**Args:**

* â€‹unsafe\_ptr (`LegacyUnsafePointer`): Shared memory pointer.

### `__getitem__`

`__getitem__[T: Intable](self, index: T) -> SMemArrayType[type, size].ptr_type`

Get a pointer to the element at index.

**Args:**

* â€‹index (`T`): Element index.

**Returns:**

`SMemArrayType`: Pointer to element.

### `len`

`static len() -> Int`

Get array length in bytes.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): Total size in bytes.

### `stack_allocation`

`static stack_allocation[alignment: Int = align_of[type]()]() -> Self`

</section>

---

## SMemTileArrayType

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SMemTileArrayType[dtype: DType, layout: Layout, num_tiles: Int, alignment: Int]`

Array of tiles in shared memory.

## Parameters

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Tile data type.
* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Tile layout configuration.
* â€‹num\_tiles ([`Int`](/mojo/std/builtin/int/Int)): Number of tiles.
* â€‹alignment ([`Int`](/mojo/std/builtin/int/Int)): Memory alignment.

## Fields

* â€‹ptr (`LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `storage_size`

`comptime storage_size = ((layout.size() * size_of[dtype]()) * num_tiles)`

### `Tile`

`comptime Tile = LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=alignment]`

## Methods

### `__init__`

`__init__[mut: Bool, //, origin: Origin[mut=mut]](unsafe_ptr: LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED, mut=mut, origin=origin]) -> Self`

Initialize with shared memory pointer.

**Args:**

* â€‹unsafe\_ptr (`LegacyUnsafePointer`): Shared memory pointer.

### `__getitem__`

`__getitem__[T: Intable](self, index: T) -> SMemTileArrayType[dtype, layout, num_tiles, alignment].Tile`

Get tile at index.

**Args:**

* â€‹index (`T`): Tile index.

**Returns:**

`SMemTileArrayType`: Tile at index.

### `slice`

`slice[length: Int](self, start: Int) -> SMemTileArrayType[dtype, layout, length, alignment]`

**Returns:**

`SMemTileArrayType`

### `stack_allocation`

`static stack_allocation() -> Self`

</section>

---

## ScatterGatherAmd

<section class='mojo-docs'>

`struct ScatterGatherAmd[thread_layout: Layout, num_threads: Int = thread_layout.size(), thread_scope: ThreadScope = ThreadScope.BLOCK, block_dim_count: Int = 1]`

AMD tile-based scatter-gather for DRAM-register data movement.

## Parameters

* â€‹thread\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Thread organization layout.
* â€‹num\_threads ([`Int`](/mojo/std/builtin/int/Int)): Total threads (defaults to thread\_layout size).
* â€‹thread\_scope ([`ThreadScope`](/mojo/kernels/layout/layout_tensor/ThreadScope)): Thread execution scope (block or warp).
* â€‹block\_dim\_count ([`Int`](/mojo/std/builtin/int/Int)): Number of block dimensions.

## Fields

* â€‹buffer (`AMDBufferResource`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

## Methods

### `__init__`

`__init__(out self, tensor: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Initialize with a tensor.

**Args:**

* â€‹tensor ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Layout tensor for AMD buffer resource creation.

### `copy`

`copy(self, dst_reg_tile: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src_gmem_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], offset: OptionalReg[UInt] = None)`

Copy DRAM to registers.

**Args:**

* â€‹dst\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination register tile.
* â€‹src\_gmem\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Source global memory tile.
* â€‹offset ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional copy offset.

`copy(self, dst_gmem_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src_reg_tile: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Copy registers to DRAM.

**Args:**

* â€‹dst\_gmem\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Destination global memory tile.
* â€‹src\_reg\_tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Source register tile.

</section>

---

## SharedMemoryBasePtr

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `alignment`

`comptime alignment`

## Required methods

### `ptr`

`static ptr() -> LegacyUnsafePointer[Int8, address_space=AddressSpace.SHARED]`

**Returns:**

`LegacyUnsafePointer`

</section>

---

## SharedMemoryManager

<section class='mojo-docs'>

`struct SharedMemoryManager[SMBP: SharedMemoryBasePtr]`

## Fields

* â€‹base\_ptr (`LegacyUnsafePointer[Int8, address_space=AddressSpace.SHARED]`):
* â€‹offset (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `Array`

`comptime Array[type: AnyTrivialRegType, size: Int] = SMemArrayType[type, size]`

#### Parameters

* â€‹type ([`AnyTrivialRegType`](/mojo/std/builtin/type_aliases/#anytrivialregtype)):
* â€‹size ([`Int`](/mojo/std/builtin/int/Int)):

### `Tile`

`comptime Tile[dtype: DType, layout: Layout] = LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=SMBP.alignment]`

#### Parameters

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)):
* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)):

### `TileArray`

`comptime TileArray[dtype: DType, layout: Layout, num_tiles: Int] = SMemTileArrayType[dtype, layout, num_tiles, SMBP.alignment]`

#### Parameters

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)):
* â€‹layout ([`Layout`](/mojo/kernels/layout/layout/Layout)):
* â€‹num\_tiles ([`Int`](/mojo/std/builtin/int/Int)):

## Methods

### `__init__`

`__init__(out self)`

Initialize the shared memory manager.

### `build`

`build[dtype: DType, layout: Layout, //, T: AnyStruct[LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=SMBP.alignment]]](mut self) -> T`

Allocate a single tile.

**Returns:**

`T`: Allocated tile.

`build[dtype: DType, layout: Layout, num_tiles: Int, //, T: AnyStruct[SMemTileArrayType[dtype, layout, num_tiles, SMBP.alignment]]](mut self) -> T`

Allocate a tile array.

**Returns:**

`T`: Allocated tile array.

`build[type: AnyTrivialRegType, size: Int, //, T: AnyStruct[SMemArrayType[type, size]]](mut self) -> T`

Allocate a regular array.

**Returns:**

`T`: Allocated array.

</section>

---

## structuring

<section class='mojo-docs'>

## `comptime` values

### `eval`

`comptime eval[T: AnyType, //, val: T] = val`

Helper alias to force evaluation of expressions at compile time.

#### Parameters

* â€‹T ([`AnyType`](/std/builtin/anytype/AnyType)):
* â€‹val (`T`):

### `NVIDIASharedMemoryManager`

`comptime NVIDIASharedMemoryManager = SharedMemoryManager[NVIDIASharedMemoryBasePtr]`

### `PipelineBarrier`

`comptime PipelineBarrier[num_pipeline_stages: Int] = SMemArrayType[SharedMemBarrier, num_pipeline_stages]`

Type alias for shared memory pipeline barrier array.

#### Parameters

* â€‹num\_pipeline\_stages ([`Int`](/std/builtin/int/Int)):

### `RegTileType`

`comptime RegTileType[_dtype: DType, layout: Layout, /, *, element_layout: Layout = Layout(IntTuple(1), IntTuple(1)), layout_int_type: DType = _get_layout_type(layout, AddressSpace.LOCAL), linear_idx_type: DType = _get_index_type(layout, AddressSpace.LOCAL), masked: Bool = False, alignment: Int = align_of[_dtype]()] = LayoutTensor[_dtype, layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Type alias for register (local memory) tile tensors.

#### Parameters

* â€‹\_dtype ([`DType`](/std/builtin/dtype/DType)):
* â€‹layout ([`Layout`](/kernels/layout/layout/Layout)):
* â€‹element\_layout ([`Layout`](/kernels/layout/layout/Layout)):
* â€‹layout\_int\_type ([`DType`](/std/builtin/dtype/DType)):
* â€‹linear\_idx\_type ([`DType`](/std/builtin/dtype/DType)):
* â€‹masked ([`Bool`](/std/builtin/bool/Bool)):
* â€‹alignment ([`Int`](/std/builtin/int/Int)):

### `SMemBarrier`

`comptime SMemBarrier = LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]`

Type alias for shared memory barrier pointer.

### `SMemPtr`

`comptime SMemPtr[type: AnyTrivialRegType] = LegacyUnsafePointer[type, address_space=AddressSpace.SHARED]`

#### Parameters

* â€‹type (`AnyTrivialRegType`):

### `SMemTileIter`

`comptime SMemTileIter[dtype: DType, layout: Layout] = LayoutTensorIter[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, alignment=128]`

#### Parameters

* â€‹dtype ([`DType`](/std/builtin/dtype/DType)):
* â€‹layout ([`Layout`](/kernels/layout/layout/Layout)):

### `SMemTileType`

`comptime SMemTileType[_dtype: DType, layout: Layout, /, *, element_layout: Layout = Layout(IntTuple(1), IntTuple(1)), layout_int_type: DType = _get_layout_type(layout, AddressSpace.SHARED), linear_idx_type: DType = _get_index_type(layout, AddressSpace.SHARED), masked: Bool = False, alignment: Int = align_of[_dtype]()] = LayoutTensor[_dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

Type alias for shared memory tile tensors.

#### Parameters

* â€‹\_dtype ([`DType`](/std/builtin/dtype/DType)):
* â€‹layout ([`Layout`](/kernels/layout/layout/Layout)):
* â€‹element\_layout ([`Layout`](/kernels/layout/layout/Layout)):
* â€‹layout\_int\_type ([`DType`](/std/builtin/dtype/DType)):
* â€‹linear\_idx\_type ([`DType`](/std/builtin/dtype/DType)):
* â€‹masked ([`Bool`](/std/builtin/bool/Bool)):
* â€‹alignment ([`Int`](/std/builtin/int/Int)):

## Structs

* [â€‹`IteratorScatterGatherAmd`](./IteratorScatterGatherAmd): Iterator-based AMD scatter-gather for DRAM-register data movement.
* [â€‹`NVIDIASharedMemoryBasePtr`](./NVIDIASharedMemoryBasePtr):
* [â€‹`ScatterGatherAmd`](./ScatterGatherAmd): AMD tile-based scatter-gather for DRAM-register data movement.
* [â€‹`SharedMemoryManager`](./SharedMemoryManager):
* [â€‹`SMemArrayType`](./SMemArrayType): Shared memory array of fixed size.
* [â€‹`SMemTileArrayType`](./SMemTileArrayType): Array of tiles in shared memory.

## Traits

* [â€‹`SharedMemoryBasePtr`](./SharedMemoryBasePtr):

</section>

---

## transpose

<section class='mojo-docs'>

The module implements Transpose functions.

## Functions

* [â€‹`transpose`](./transpose): Permute the axis of `input` based on `perms`, and place the result in `output`.
* [â€‹`transpose_2d`](./transpose_2d):
* [â€‹`transpose_3d_swap_inner`](./transpose_3d_swap_inner):
* [â€‹`transpose_3d_swap_outer`](./transpose_3d_swap_outer):
* [â€‹`transpose_4d_swap_middle`](./transpose_4d_swap_middle):
* [â€‹`transpose_inplace`](./transpose_inplace):
* [â€‹`transpose_strided`](./transpose_strided):
* [â€‹`transpose_trivial_memcpy`](./transpose_trivial_memcpy):

</section>

---

## transpose (Transpose)

<section class='mojo-docs'>

`transpose[rank: Int, dtype: DType, //](output: NDBuffer[dtype, rank, origin, shape], input: NDBuffer[dtype, rank, origin, shape], perms: LegacyUnsafePointer[Scalar[DType.index]])`

Permute the axis of `input` based on `perms`, and place the result in `output`.

Example:

```mojo
transpose(output, input, [2, 0, 1])
# guarantees output[x, y, z] = input[z, x, y]
```

**Parameters:**

* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of input and output buffers.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of buffer elements.

**Args:**

* â€‹output ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): The output buffer.
* â€‹input ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): The input buffer.
* â€‹perms (`LegacyUnsafePointer`): Permutation of the input axes.

</section>

---

## transpose_2d

<section class='mojo-docs'>

`transpose_2d[rank: Int, output_shape: DimList, input_shape: DimList, dtype: DType](output: NDBuffer[dtype, rank, origin, output_shape], input: NDBuffer[dtype, rank, origin, input_shape], perms: LegacyUnsafePointer[Scalar[DType.index]], simplified_input_shape: IndexList[rank], simplified_rank: Int, offset: Int)`

</section>

---

## transpose_3d_swap_inner

<section class='mojo-docs'>

`transpose_3d_swap_inner[rank: Int, dtype: DType, //](output: NDBuffer[dtype, rank, origin, shape], input: NDBuffer[dtype, rank, origin, shape], perms: LegacyUnsafePointer[Scalar[DType.index]], simplified_input_shape: IndexList[rank], simplified_rank: Int)`

</section>

---

## transpose_3d_swap_outer

<section class='mojo-docs'>

`transpose_3d_swap_outer[rank: Int, output_shape: DimList, input_shape: DimList, dtype: DType](output: NDBuffer[dtype, rank, origin, output_shape], input: NDBuffer[dtype, rank, origin, input_shape], perms: LegacyUnsafePointer[Scalar[DType.index]], simplified_input_shape: IndexList[rank], simplified_rank: Int)`

</section>

---

## transpose_4d_swap_middle

<section class='mojo-docs'>

`transpose_4d_swap_middle[rank: Int, dtype: DType, //](output: NDBuffer[dtype, rank, origin, shape], input: NDBuffer[dtype, rank, origin, shape, strides], perms: LegacyUnsafePointer[Scalar[DType.index]], simplified_input_shape: IndexList[rank], simplified_rank: Int)`

</section>

---

## transpose_inplace

<section class='mojo-docs'>

`transpose_inplace[rows: Int, cols: Int, dtype: DType](buf: NDBuffer[dtype, 2, origin, DimList.__init__[Int, Int](rows, cols)])`

`transpose_inplace[rows: Int, cols: Int, dtype: DType](buf: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## transpose_strided

<section class='mojo-docs'>

`transpose_strided[rank: Int, dtype: DType, //](output: NDBuffer[dtype, rank, origin, shape], input: NDBuffer[dtype, rank, origin, shape], perms: LegacyUnsafePointer[Scalar[DType.index]])`

</section>

---

## transpose_trivial_memcpy

<section class='mojo-docs'>

`transpose_trivial_memcpy[rank: Int, output_shape: DimList, input_shape: DimList, dtype: DType](output: NDBuffer[dtype, rank, origin, output_shape], input: NDBuffer[dtype, rank, origin, input_shape])`

</section>

---

## GemmShape

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct GemmShape`

Helper class to unpack gemm dimension and layout.

## Fields

* â€‹M (`Int`):
* â€‹N (`Int`):
* â€‹K (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(index: IndexList[3]) -> Self`

Constructor of a gemm shape record from a index tuple.

**Args:**

* â€‹index ([`IndexList`](/mojo/std/utils/index_/IndexList)): The int tuple containing the index(m,n,k).

### `__getitem__`

`__getitem__(self, idx: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `__setitem__`

`__setitem__(mut self, idx: Int, value: Int)`

### `__add__`

`__add__(self, rhs: Self) -> Self`

Coordinate-wise addition of two gemm shape records.

**Args:**

* â€‹rhs (`Self`): Another gemm shape record to add with.

### `__sub__`

`__sub__(self, rhs: Self) -> Self`

Coordinate-wise subtraction of two gemm shape records.

**Args:**

* â€‹rhs (`Self`): Another gemm shape record to subtract with.

### `get`

`static get[transpose_b: Bool](c: NDBuffer[dtype, 2, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive], a: NDBuffer[dtype, 2, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive], b: NDBuffer[dtype, 2, origin, shape, strides, alignment2=alignment2, address_space=address_space, exclusive=exclusive]) -> Self`

Constructor of a gemm shape record from input buffers.

M, N, and K are intentionally calculated using `a` and `c` ONLY. This
is because `b` may be padded to a multiple of the tile size if it has
been pre-packed.

**Args:**

* â€‹c ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): NDBuffer with allocated output space.
* â€‹a ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): NDBuffer containing matrix operand A.
* â€‹b ([`NDBuffer`](/mojo/kernels/buffer/buffer/NDBuffer)): NDBuffer containing matrix operand B.

`static get[transpose_b: Bool, layout_c: Layout, layout_a: Layout, layout_b: Layout](c: LayoutTensor[dtype, layout_c, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[dtype, layout_a, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[dtype, layout_b, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> Self`

Constructor of a gemm shape record from input buffers.

M, N, and K are intentionally calculated using `a` and `c` ONLY. This
is because `b` may be padded to a multiple of the tile size if it has
been pre-packed.

**Args:**

* â€‹c ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): LayoutTensor with allocated output space.
* â€‹a ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): LayoutTensor containing matrix operand A.
* â€‹b ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): LayoutTensor containing matrix operand B.

### `as_index`

`as_index(self) -> IndexList[3]`

Utility to convert the underlying data to an index tuple. So that the utilities such as elementwise add can be used.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The constructed index tuple.

</section>

---

## InnerKernelID

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct InnerKernelID`

## Fields

* â€‹value (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `DEFAULT`

`comptime DEFAULT = InnerKernelID(0)`

### `I8MM`

`comptime I8MM = InnerKernelID(3)`

### `NEON`

`comptime NEON = InnerKernelID(2)`

### `VNNI`

`comptime VNNI = InnerKernelID(1)`

## Methods

### `__eq__`

`__eq__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## KernelConfig (Utils)

<section class='mojo-docs'>

`struct KernelConfig`

Static configuration of the matmul inner kernel.

## Fields

* â€‹kernel\_rows (`Int`):
* â€‹kernel\_cols (`Int`):
* â€‹simd\_size (`Int`):
* â€‹packed\_shape (`DimList`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

## Methods

### `__init__`

`__init__(out self, *, kernel_rows: Int, kernel_cols: Int, simd_size: Int, packed_shape: DimList)`

</section>

---

## MicroKernelShape

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MicroKernelShape`

Record describing the inner kernel shape.

## Fields

* â€‹simd\_rows (`Int`):
* â€‹simd\_cols (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(rows: Int, cols: Int) -> Self`

</section>

---

## SubMatmulConfig

<section class='mojo-docs'>

`struct SubMatmulConfig`

Static configuration of sub-matrices in parallel matmul.

## Fields

* â€‹offset (`IndexList[3]`):
* â€‹shape (`IndexList[3]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `is_valid`

`is_valid(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## apply_epilogue

<section class='mojo-docs'>

`apply_epilogue[elementwise_lambda: elementwise_epilogue_type, dst_layout: Layout, dst_element_layout: Layout = Layout(IntTuple(1), IntTuple(1))](src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], offset: Int)`

</section>

---

## calculate_tile_n_k

<section class='mojo-docs'>

`calculate_tile_n_k[a_type: DType, b_type: DType, c_type: DType, kernel_cols: Int](n: Int, k: Int) -> IndexList[2]`

Helper heuristic function to decide on tile size to partition the matmul given the cache size and desired data layout.

**Parameters:**

* â€‹a\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of the A tensor.
* â€‹b\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of the B tensor.
* â€‹c\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of the C tensor.
* â€‹kernel\_cols ([`Int`](/mojo/std/builtin/int/Int)): The umber of columns of the micro kernel.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The calculated tile size to partition the matmul as (TileN, TileK).

`calculate_tile_n_k[a_type: DType, b_type: DType, c_type: DType, kernel_cols: Int](global_tile_shape: GemmShape) -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## dispatch_get_kernel_type

<section class='mojo-docs'>

`dispatch_get_kernel_type[func: fn[x: Bool]() raises capturing -> None](m: Int, n: Int, k: Int)`

`dispatch_get_kernel_type[func: fn[x: Bool]() capturing -> None](m: Int, n: Int, k: Int)`

</section>

---

## get_kernel_config

<section class='mojo-docs'>

`get_kernel_config[a_type: DType, b_type: DType, c_type: DType, *, kernel_type: Bool = False]() -> KernelConfig`

Utility function to extract matmul configuration parameters for exported Functions.     TODO: Add target dependent configuration parameters.

**Returns:**

[`KernelConfig`](/mojo/kernels/linalg/utils/KernelConfig)

</section>

---

## get_kernel_type

<section class='mojo-docs'>

`get_kernel_type(m: Int, n: Int, k: Int) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## get_matmul_arch_factor

<section class='mojo-docs'>

`get_matmul_arch_factor[use_vnni: Bool, use_i8mm: Bool]() -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## get_matmul_kernel_shape

<section class='mojo-docs'>

`get_matmul_kernel_shape[a_type: DType, b_type: DType, c_type: DType, kernel_type: Bool]() -> MicroKernelShape`

**Returns:**

`MicroKernelShape`

</section>

---

## get_matmul_kernel_shape_ARM

<section class='mojo-docs'>

`get_matmul_kernel_shape_ARM[a_type: DType, b_type: DType, c_type: DType, kernel_type: Bool]() -> MicroKernelShape`

**Returns:**

`MicroKernelShape`

</section>

---

## get_matmul_kernel_shape_x86

<section class='mojo-docs'>

`get_matmul_kernel_shape_x86[kernel_type: Bool]() -> MicroKernelShape`

**Returns:**

`MicroKernelShape`

</section>

---

## get_matmul_num_tasks

<section class='mojo-docs'>

`get_matmul_num_tasks[a_type: DType, b_type: DType, c_type: DType, simd_size: Int, kernel_type: Bool](m: Int, n: Int, k: Int, max_num_tasks: Int) -> Int`

Compute the number of tasks for parallel matmul. The max number of tasks is typically the number of threads/cores.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## get_matmul_prefetch_b_distance_k

<section class='mojo-docs'>

`get_matmul_prefetch_b_distance_k() -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## get_min_task_size

<section class='mojo-docs'>

`get_min_task_size() -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## get_packB_unroll_factor

<section class='mojo-docs'>

`get_packB_unroll_factor() -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## get_pack_data_size

<section class='mojo-docs'>

`get_pack_data_size[dtype: DType]() -> Int`

Utility to compute the number of elements to pack in each tile. Returns:     The number of elements to pack.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## get_partitioned_matmul

<section class='mojo-docs'>

`get_partitioned_matmul[a_type: DType, b_type: DType, c_type: DType, kernel_rows: Int, kernel_cols: Int](m: Int, n: Int, k: Int, task_id: Int, num_tasks: Int) -> SubMatmulConfig`

**Returns:**

`SubMatmulConfig`

</section>

---

## get_partitioned_matmul_mojo

<section class='mojo-docs'>

`get_partitioned_matmul_mojo[b_type: DType, kernel_rows: Int, kernel_cols: Int, use_i8mm: Bool = False](m: Int, n: Int, k: Int, task_id: Int, num_tasks: Int) -> SubMatmulConfig`

**Returns:**

`SubMatmulConfig`

</section>

---

## get_partitioned_matmul_mojo_shape

<section class='mojo-docs'>

`get_partitioned_matmul_mojo_shape[b_type: DType, kernel_rows: Int, kernel_cols: Int, use_i8mm: Bool](m: Int, n: Int, k: Int, num_tasks: Int) -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## utils

<section class='mojo-docs'>

## `comptime` values

### `elementwise_compute_lambda_type`

`comptime elementwise_compute_lambda_type = fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> SIMD[dtype, width]`

### `elementwise_epilogue_type`

`comptime elementwise_epilogue_type = fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None`

## Structs

* [â€‹`GemmShape`](./GemmShape): Helper class to unpack gemm dimension and layout.
* [â€‹`InnerKernelID`](./InnerKernelID):
* [â€‹`KernelConfig`](./KernelConfig): Static configuration of the matmul inner kernel.
* [â€‹`MicroKernelShape`](./MicroKernelShape): Record describing the inner kernel shape.
* [â€‹`SubMatmulConfig`](./SubMatmulConfig): Static configuration of sub-matrices in parallel matmul.

## Functions

* [â€‹`apply_epilogue`](./apply_epilogue):
* [â€‹`calculate_tile_n_k`](./calculate_tile_n_k): Helper heuristic function to decide on tile size to partition the matmul given the cache size and desired data layout.
* [â€‹`dispatch_get_kernel_type`](./dispatch_get_kernel_type):
* [â€‹`get_kernel_config`](./get_kernel_config): Utility function to extract matmul configuration parameters for exported Functions.     TODO: Add target dependent configuration parameters.
* [â€‹`get_kernel_type`](./get_kernel_type):
* [â€‹`get_matmul_arch_factor`](./get_matmul_arch_factor):
* [â€‹`get_matmul_kernel_shape`](./get_matmul_kernel_shape):
* [â€‹`get_matmul_kernel_shape_ARM`](./get_matmul_kernel_shape_ARM):
* [â€‹`get_matmul_kernel_shape_x86`](./get_matmul_kernel_shape_x86):
* [â€‹`get_matmul_num_tasks`](./get_matmul_num_tasks): Compute the number of tasks for parallel matmul. The max number of tasks is typically the number of threads/cores.
* [â€‹`get_matmul_prefetch_b_distance_k`](./get_matmul_prefetch_b_distance_k):
* [â€‹`get_min_task_size`](./get_min_task_size):
* [â€‹`get_pack_data_size`](./get_pack_data_size): Utility to compute the number of elements to pack in each tile. Returns:     The number of elements to pack.
* [â€‹`get_packB_unroll_factor`](./get_packB_unroll_factor):
* [â€‹`get_partitioned_matmul`](./get_partitioned_matmul):
* [â€‹`get_partitioned_matmul_mojo`](./get_partitioned_matmul_mojo):
* [â€‹`get_partitioned_matmul_mojo_shape`](./get_partitioned_matmul_mojo_shape):
* [â€‹`packA_i8mm`](./packA_i8mm):
* [â€‹`partition_work`](./partition_work):
* [â€‹`select_inner_kernel`](./select_inner_kernel):
* [â€‹`use_i8mm_fn`](./use_i8mm_fn):
* [â€‹`use_vnni_fn`](./use_vnni_fn):

</section>

---

## packA_i8mm

<section class='mojo-docs'>

`packA_i8mm[a_type: DType](t0: Int, t1: Int, k: Int, a_ptr: LegacyUnsafePointer[Scalar[a_type]], a_packed_ptr: LegacyUnsafePointer[Scalar[a_type]])`

</section>

---

## partition_work

<section class='mojo-docs'>

`partition_work(task_id: Int, num_tasks: Int, work: Int, work_block_size: Int) -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## select_inner_kernel

<section class='mojo-docs'>

`select_inner_kernel[a_type: DType, b_type: DType, c_type: DType]() -> InnerKernelID`

**Returns:**

`InnerKernelID`

</section>

---

## use_i8mm_fn

<section class='mojo-docs'>

`use_i8mm_fn[a_type: DType, b_type: DType, c_type: DType]() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## use_vnni_fn

<section class='mojo-docs'>

`use_vnni_fn[a_type: DType, b_type: DType, c_type: DType]() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## MatmulConfig (Utils_gpu)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MatmulConfig[a_type: DType, b_type: DType, c_type: DType, transpose_b: Bool = False]`

Static configuration of GPU matmul.

## Fields

* â€‹block\_tile\_shape (`IndexList[3]`):
* â€‹warp\_tile\_shape (`IndexList[3]`):
* â€‹mma\_shape (`IndexList[3]`):
* â€‹num\_pipeline\_stages (`UInt`):
* â€‹num\_k\_partitions (`UInt`):
* â€‹k\_group\_size (`UInt`):
* â€‹num\_warp\_k\_partitions (`UInt`):
* â€‹cluster\_shape (`IndexList[3]`):
* â€‹num\_consumer (`UInt`):
* â€‹partitioned\_multicast (`Bool`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ACCUM_PRECISION`

`comptime ACCUM_PRECISION = 1`

### `accum_type`

`comptime accum_type = get_accum_type[a_type]()`

### `OUTPUT_PRECISION`

`comptime OUTPUT_PRECISION = 2`

### `split_k_reduction_scheme`

`comptime split_k_reduction_scheme = env_get_int["SPLITK_REDUCTION_SCHEME", 2]()`

### `split_k_reduction_type`

`comptime split_k_reduction_type = c_type if (eq env_get_int["SPLITK_REDUCTION_SCHEME", 2]()._mlir_value, 2) else MatmulConfig[a_type, b_type, c_type, transpose_b].accum_type`

## Methods

### `__init__`

`__init__(*, block_tile_shape: IndexList[3] = Index(128, 128, 32), warp_tile_shape: IndexList[3] = Index(64, 64, 32), mma_shape: IndexList[3] = get_mma_shape[a_type, MatmulConfig[a_type, b_type, c_type, transpose_b].accum_type](), cluster_shape: IndexList[3] = Index(1, 1, 1), num_pipeline_stages: UInt = 4, num_k_partitions: UInt = 1, k_group_size: UInt = 1, num_warp_k_partitions: UInt = 1, num_consumer: UInt = 1, partitioned_multicast: Bool = False, pdl_level: PDLLevel = PDLLevel()) -> Self`

### `__eq__`

`__eq__(self, rhs: MatmulConfig[a_type, b_type, c_type, transpose_b]) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `copy_field`

`copy_field(mut self, other: MatmulConfig[a_type, b_type, c_type, transpose_b])`

### `swapAB`

`swapAB(self) -> MatmulConfig[b_type, a_type, c_type, transpose_b]`

**Returns:**

[`MatmulConfig`](/mojo/kernels/linalg/utils_gpu/MatmulConfig)

### `num_warps_m`

`num_warps_m(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `num_warps_n`

`num_warps_n(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `num_threads`

`num_threads(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `shared_mem_usage`

`shared_mem_usage(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `grid_dim`

`grid_dim(self, m: UInt, n: UInt) -> IndexList[3]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

### `block_dim`

`block_dim(self) -> IndexList[3]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

### `work_space_size`

`work_space_size(self, M: UInt, N: UInt) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `pdl_level`

`pdl_level(self) -> PDLLevel`

**Returns:**

[`PDLLevel`](/mojo/std/gpu/primitives/grid_controls/PDLLevel)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

### `__repr__`

`__repr__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `__hash__`

`__hash__[H: Hasher](self, mut hasher: H)`

Updates hasher with the underlying bytes.

**Parameters:**

* â€‹H ([`Hasher`](/mojo/std/hashlib/hasher/Hasher)): The hasher type.

**Args:**

* â€‹hasher (`H`): The hasher instance.

</section>

---

## MatmulKernels

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MatmulKernels[a_type: DType, b_type: DType, c_type: DType, transpose_b: Bool = False]`

Supported matmul kernels.

The configurations are named as: <arch>*<BNxBM>*<stages>.
BK, mma shape, and warp tile shape are decided internally.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ampere_128x128_4`

`comptime ampere_128x128_4 = MatmulConfig[a_type, b_type, c_type, transpose_b](Index(128, 128, _bk_base[a_type]()), Index(64, 64, _bk_base[a_type]()), get_mma_shape[a_type, MatmulConfig[a_type, b_type, c_type, transpose_b].accum_type](), Index(1, 1, 1), 4, 1, 1, 1, 1, False, PDLLevel())`

### `ampere_256x128_3`

`comptime ampere_256x128_3 = MatmulConfig[a_type, b_type, c_type, transpose_b](Index(128, 256, (2 * _bk_base[a_type]())), Index(64, 64, (2 * _bk_base[a_type]())), get_mma_shape[a_type, MatmulConfig[a_type, b_type, c_type, transpose_b].accum_type](), Index(1, 1, 1), 3, 1, 1, 1, 1, False, PDLLevel())`

### `ampere_256x64_4`

`comptime ampere_256x64_4 = MatmulConfig[a_type, b_type, c_type, transpose_b](Index(64, 256, _bk_base[a_type]()), Index(64, 64, _bk_base[a_type]()), get_mma_shape[a_type, MatmulConfig[a_type, b_type, c_type, transpose_b].accum_type](), Index(1, 1, 1), 4, 1, 1, 1, 1, False, PDLLevel())`

### `hopper_128x128_4`

`comptime hopper_128x128_4 = MatmulConfig[a_type, b_type, c_type, transpose_b](Index(128, 128, _bk_base[a_type]()), Index(64, 64, _bk_base[a_type]()), get_mma_shape[a_type, MatmulConfig[a_type, b_type, c_type, transpose_b].accum_type](), Index(1, 1, 1), 4, 1, 1, 1, 1, False, PDLLevel())`

### `tuning_config`

`comptime tuning_config = MatmulConfig[a_type, b_type, c_type, transpose_b](Index(env_get_int["TUNE_BM", 128](), env_get_int["TUNE_BN", 128](), env_get_int["TUNE_BK", 32]()), Index(env_get_int["TUNE_WM", 64](), env_get_int["TUNE_WN", 64](), env_get_int["TUNE_BK", 32]()), get_mma_shape[a_type, MatmulConfig[a_type, b_type, c_type, transpose_b].accum_type](), Index(1, 1, 1), UInt(env_get_int["TUNE_NUM_STAGES", 4]()), UInt(env_get_int["TUNE_NUM_K_PARTITIONS", 1]()), 1, UInt(env_get_int["TUNE_NUM_WARP_K_PARTITIONS", 1]()), 1, False, PDLLevel())`

</section>

---

## block_swizzle

<section class='mojo-docs'>

`block_swizzle(block_idx: IndexList[2, element_type=element_type], grid_dim: IndexList[2, element_type=element_type]) -> IndexList[2, element_type=element_type]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## create_hilbert_lut

<section class='mojo-docs'>

`create_hilbert_lut(ctx: DeviceContext, grid_x: Int, grid_y: Int) -> DeviceBuffer[DType.uint32]`

Precompute Hilbert-curve block swizzle lookup-table for a rectangular grid.

The returned device pointer refers to a 1-D UInt32 array of length
grid\_x \* grid\_y.
For linear (row-major) block id `id`, the packed value at `lut[id]`
encodes the swizzled coordinates:  upper 16-bits = y, lower 16-bits = x.

**Returns:**

`DeviceBuffer`

</section>

---

## get_hilbert_lut_with_cache

<section class='mojo-docs'>

`get_hilbert_lut_with_cache(ctx: DeviceContext, grid_x: Int, grid_y: Int) -> DeviceBuffer[DType.uint32]`

Get Hilbert lookup table using global cache (no struct needed).

**Returns:**

`DeviceBuffer`

</section>

---

## utils_gpu

<section class='mojo-docs'>

## Structs

* [â€‹`MatmulConfig`](./MatmulConfig): Static configuration of GPU matmul.
* [â€‹`MatmulKernels`](./MatmulKernels): Supported matmul kernels.

## Functions

* [â€‹`block_swizzle`](./block_swizzle):
* [â€‹`create_hilbert_lut`](./create_hilbert_lut): Precompute Hilbert-curve block swizzle lookup-table for a rectangular grid.
* [â€‹`get_hilbert_lut_with_cache`](./get_hilbert_lut_with_cache): Get Hilbert lookup table using global cache (no struct needed).
* [â€‹`select_config`](./select_config):

</section>

---

## select_config

<section class='mojo-docs'>

`select_config[a_type: DType, b_type: DType, c_type: DType, transpose_b: Bool = False](M: Int, N: Int, K: Int, ctx: DeviceContext) -> MatmulConfig[a_type, b_type, c_type, transpose_b]`

**Returns:**

[`MatmulConfig`](/mojo/kernels/linalg/utils_gpu/MatmulConfig)

</section>

---

## elu

<section class='mojo-docs'>

`elu[dtype: DType, simd_width: Int](x: SIMD[dtype, simd_width]) -> SIMD[dtype, simd_width]`

Compute the Elu Op using the equation $z if z >= 0 else alpha*(e^z -1)$.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType used for the computation.
* â€‹simd\_width ([`Int`](/mojo/std/builtin/int/Int)): SIMD width used for the computation.

**Args:**

* â€‹x ([`SIMD`](/mojo/std/builtin/simd/SIMD)): The value to compute the ELU operation on.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): The result of the ELU operation.

</section>

---

## activations

<section class='mojo-docs'>

The module contains implementations of activation functions.

## Functions

* [â€‹`elu`](./elu): Compute the Elu Op using the equation $z if z >= 0 else alpha*(e^z -1)$.
* [â€‹`leaky_relu`](./leaky_relu): Compute the Leaky ReLU using the equation $max(0, x) + negative_slope * min(0, x)$.
* [â€‹`relu`](./relu): Compute the Relu Op using the equation $max(0, x)$.
* [â€‹`relu_n1`](./relu_n1): Compute the Relu N1 Op using the equation $max(min(x,1),-1)$.
* [â€‹`sign`](./sign): Compute the sign (0, 1) of the input value.

</section>

---

## leaky_relu

<section class='mojo-docs'>

`leaky_relu[dtype: DType, simd_width: Int](x: SIMD[dtype, simd_width], negative_slope: Scalar[dtype]) -> SIMD[dtype, simd_width]`

Compute the Leaky ReLU using the equation $max(0, x) + negative_slope * min(0, x)$.

**Constraints:**

Type must be a floating point Dtype.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType used for the computation.
* â€‹simd\_width ([`Int`](/mojo/std/builtin/int/Int)): SIMD width used for the computation.

**Args:**

* â€‹x ([`SIMD`](/mojo/std/builtin/simd/SIMD)): The value to compute the Leaky ReLU operation on.
* â€‹negative\_slope ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The slope for negative values.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): The result of the Leaky ReLU operation.

</section>

---

## relu

<section class='mojo-docs'>

`relu[dtype: DType, simd_width: Int](x: SIMD[dtype, simd_width]) -> SIMD[dtype, simd_width]`

Compute the Relu Op using the equation $max(0, x)$.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType used for the computation.
* â€‹simd\_width ([`Int`](/mojo/std/builtin/int/Int)): SIMD width used for the computation.

**Args:**

* â€‹x ([`SIMD`](/mojo/std/builtin/simd/SIMD)): The value to compute the RELU operation on.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): The result of the RELU operation.

</section>

---

## relu_n1

<section class='mojo-docs'>

`relu_n1[dtype: DType, simd_width: Int](x: SIMD[dtype, simd_width]) -> SIMD[dtype, simd_width]`

Compute the Relu N1 Op using the equation $max(min(x,1),-1)$.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType used for the computation.
* â€‹simd\_width ([`Int`](/mojo/std/builtin/int/Int)): SIMD width used for the computation.

**Args:**

* â€‹x ([`SIMD`](/mojo/std/builtin/simd/SIMD)): The value to compute the RELU N1 operation on.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): The result of the RELU N1 operation.

</section>

---

## sign

<section class='mojo-docs'>

`sign[dtype: DType, simd_width: Int](x: SIMD[dtype, simd_width]) -> SIMD[dtype, simd_width]`

Compute the sign (0, 1) of the input value.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType used for the computation.
* â€‹simd\_width ([`Int`](/mojo/std/builtin/int/Int)): SIMD width used for the computation.

**Args:**

* â€‹x ([`SIMD`](/mojo/std/builtin/simd/SIMD)): The value to compute the sign operation on.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): The result of the sign operation.

</section>

---

## arange

<section class='mojo-docs'>

`arange[dtype: DType, simd_width: Int](start: Scalar[dtype], stop: Scalar[dtype], step: Scalar[dtype], index: IndexList[1]) -> SIMD[dtype, simd_width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## arange_shape

<section class='mojo-docs'>

`arange_shape[dtype: DType, single_thread_blocking_override: Bool](start: Scalar[dtype], stop: Scalar[dtype], step: Scalar[dtype]) -> IndexList[1]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## arange (Arange)

<section class='mojo-docs'>

## Functions

* [â€‹`arange`](./arange):
* [â€‹`arange_shape`](./arange_shape):

</section>

---

## arg_nonzero

<section class='mojo-docs'>

`arg_nonzero[dtype: DType, output_type: DType](input_buffer: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output_buffer: LayoutTensor[output_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Gather the indices of all non-zero elements in input buffer storing the indices in the output\_buffer.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The element dtype.
* â€‹output\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The integer dtype to store the indices in.

**Args:**

* â€‹input\_buffer ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The tensor to count the non-zeros in.
* â€‹output\_buffer ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The indices of all non-zero elements.

</section>

---

## arg_nonzero_shape

<section class='mojo-docs'>

`arg_nonzero_shape[dtype: DType, single_thread_blocking_override: Bool](input_buffer: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> IndexList[2]`

Return \[NumNonZeros, InputRank] where NumNonZeros are the number of non-zero elements in the input.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The element dtype.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): This op can block.

**Args:**

* â€‹input\_buffer ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The tensor to count the non-zeros in.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): Shape of the arg\_nonzero kernel for this input \[NumNonZeros, InputRank].

</section>

---

## arg_nonzero (Arg_nonzero)

<section class='mojo-docs'>

## Functions

* [â€‹`arg_nonzero`](./arg_nonzero): Gather the indices of all non-zero elements in input buffer storing the indices in the output\_buffer.
* [â€‹`arg_nonzero_shape`](./arg_nonzero_shape): Return \[NumNonZeros, InputRank] where NumNonZeros are the number of non-zero elements in the input.

</section>

---

## argmax

<section class='mojo-docs'>

`argmax(input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], axis: Int, output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Finds the indices of the maximum element along the specified axis.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output tensor.

`argmax(input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], axis_buf: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Finds the indices of the maximum element along the specified axis.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹axis\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The axis tensor.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The axis tensor.

</section>

---

## argmin

<section class='mojo-docs'>

`argmin(input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], axis: Int, output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Finds the indices of the minimum element along the specified axis.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output tensor.

`argmin(input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], axis_buf: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Finds the indices of the minimum element along the specified axis.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹axis\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The axis tensor.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The axis tensor.

</section>

---

## argmaxmin

<section class='mojo-docs'>

## Functions

* [â€‹`argmax`](./argmax): Finds the indices of the maximum element along the specified axis.
* [â€‹`argmin`](./argmin): Finds the indices of the minimum element along the specified axis.

</section>

---

## argmax_gpu

<section class='mojo-docs'>

`argmax_gpu[dtype: DType, output_type: DType](ctx: DeviceContext, input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[output_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## argmaxmin_gpu

<section class='mojo-docs'>

`argmaxmin_gpu[dtype: DType, output_type: DType, largest: Bool](ctx: DeviceContext, input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[output_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Wraps the Top-K GPU kernel with K=1 to perform argmax on the inner-most dimension.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType - The data dtype of the input tensor.
* â€‹output\_type ([`DType`](/mojo/std/builtin/dtype/DType)): DType - The data dtype of the output tensor.
* â€‹largest ([`Bool`](/mojo/std/builtin/bool/Bool)): Bool - Whether to perform argmax or argmin.

</section>

---

## argmin_gpu

<section class='mojo-docs'>

`argmin_gpu[dtype: DType, output_type: DType](ctx: DeviceContext, input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[output_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## argmaxmin_gpu (Argmaxmin_gpu)

<section class='mojo-docs'>

## Functions

* [â€‹`argmax_gpu`](./argmax_gpu):
* [â€‹`argmaxmin_gpu`](./argmaxmin_gpu): Wraps the Top-K GPU kernel with K=1 to perform argmax on the inner-most dimension.
* [â€‹`argmin_gpu`](./argmin_gpu):

</section>

---

## argsort

<section class='mojo-docs'>

`argsort[*, ascending: Bool = True, target: StringSlice[StaticConstantOrigin] = "cpu"](output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

Performs argsort on input buffer, storing indices in output buffer.

**Parameters:**

* â€‹ascending ([`Bool`](/mojo/std/builtin/bool/Bool)): Sort direction (True for ascending, False for descending).
* â€‹target (`StringSlice`): Target device ("cpu" or "gpu").

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Buffer to store sorted indices.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Buffer containing values to sort.
* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): Device context for execution.

`argsort[ascending: Bool = True](output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

CPU-only version of argsort.

**Parameters:**

* â€‹ascending ([`Bool`](/mojo/std/builtin/bool/Bool)): Sort direction (True for ascending, False for descending).

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Buffer to store sorted indices.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Buffer containing values to sort.

</section>

---

## argsort (Argsort)

<section class='mojo-docs'>

## Functions

* [â€‹`argsort`](./argsort): Performs argsort on input buffer, storing indices in output buffer.

</section>

---

## Attention

<section class='mojo-docs'>

`struct Attention[attention_config_t: AttentionConfig, output_type: DType, q_type: DType, k_t: MHAOperand, v_t: MHAOperand, mask_t: MHAMask, dtype: DType, //, config: MHAConfig[dtype], group: Int, token_gen: Bool, sink: Bool, q_depth: Int = Int(config), cache_depth: Int = Int(config), output_depth: Int = Int(config)]`

## Fields

* â€‹out\_reg\_buffer (`Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].OutputRegisterBufferType`):
* â€‹p\_reg\_buffer (`Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].PRegisterBufferType`):
* â€‹gmem\_manager (`Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].GlobalMemoryManagerType`):
* â€‹smem\_manager (`Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].SharedMemoryManagerType`):
* â€‹q\_buffer (`Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].QRegisterBufferType`):
* â€‹output\_ptr (`LegacyUnsafePointer[Scalar[output_type]]`):
* â€‹batch\_idx (`Int`):
* â€‹k (`k_t`):
* â€‹v (`v_t`):
* â€‹mask (`mask_t`):
* â€‹mask\_block\_row (`UInt32`):
* â€‹mask\_warp\_row (`UInt32`):
* â€‹mask\_warp\_col (`UInt32`):
* â€‹kv\_start\_row (`UInt32`):
* â€‹scale (`Scalar[Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].accum_type]`):
* â€‹seq\_len (`Int`):
* â€‹num\_keys (`Int`):
* â€‹start\_pos (`Int`):
* â€‹cache\_start\_pos (`Int`):
* â€‹softmax (`Softmax[Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].accum_type, Layout.row_major(Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].num_m_mmas), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].num_n_mmas)), Layout.row_major(Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].num_warps_m), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].num_warps_n)), Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].warp_layout, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].fragment_layout, True]`):
* â€‹warp\_scratch\_tensor (`LayoutTensor[Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].accum_type, Layout.row_major((2 * Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].num_warps_n)), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BM)), MutAnyOrigin, address_space=AddressSpace.SHARED]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True if True if True if True if True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else True if mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else mask_t.__del__is_trivial if v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial else v_t.__del__is_trivial if k_t.__del__is_trivial else k_t.__del__is_trivial`

### `accum_type`

`comptime accum_type = get_accum_type[q_type]()`

### `BK`

`comptime BK = config.block_k[dtype]()`

### `BM`

`comptime BM = config.block_m[dtype]()`

### `BN`

`comptime BN = config.block_n[dtype]()`

### `depth`

`comptime depth = config.depth`

### `fragment_layout`

`comptime fragment_layout = get_fragment_layout[Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].mma_shape]()`

### `fragment_layout_nested`

`comptime fragment_layout_nested = get_nested_fragment_layout[Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].mma_shape]()`

### `GlobalMemoryManagerType`

`comptime GlobalMemoryManagerType = GlobalMemoryManager[q_type, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BM, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BN, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BK, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].depth, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].num_heads, group, token_gen, q_depth, output_depth]`

### `k_group_size`

`comptime k_group_size = (16 // (num_matrix_reg[Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].mma_shape.__getitem__[3, DType.int64, Int](0), Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].mma_shape.__getitem__[3, DType.int64, Int](2)]() * size_of[q_type]()))`

### `mma_shape`

`comptime mma_shape = attention_config_t.get_mma_shape()()`

### `num_heads`

`comptime num_heads = config.num_heads`

### `num_k_mmas2`

`comptime num_k_mmas2 = ceildiv(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BK, UInt((Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].mma_shape.__getitem__[3, DType.int64, Int](2) * Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].k_group_size)))`

### `num_m_mmas`

`comptime num_m_mmas = ceildiv(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].WM, UInt(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].mma_shape.__getitem__[3, DType.int64, Int](0)))`

### `num_n_mmas`

`comptime num_n_mmas = ceildiv(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].WN, UInt(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].mma_shape.__getitem__[3, DType.int64, Int](1)))`

### `num_n_mmas_output`

`comptime num_n_mmas_output = ceildiv((output_depth // Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].num_warps_n)), Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].mma_shape.__getitem__[3, DType.int64, Int](1))`

### `num_stages`

`comptime num_stages = 2`

### `num_threads`

`comptime num_threads = config.num_threads[dtype]()`

### `num_warps_m`

`comptime num_warps_m = (Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BM // Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].WM)`

### `num_warps_n`

`comptime num_warps_n = (Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BN // Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].WN)`

### `output_frag_size`

`comptime output_frag_size = Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].fragment_layout.size()`

### `OutputRegisterBufferType`

`comptime OutputRegisterBufferType = OutputRegisterBuffer[Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].accum_type, Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].num_m_mmas), Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].num_n_mmas_output, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].output_frag_size]`

### `PRegisterBufferType`

`comptime PRegisterBufferType = PRegisterBuffer[Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].accum_type, q_type, Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BM), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BN), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BK), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].WM), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].WN), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].num_m_mmas), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].num_n_mmas), Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].output_frag_size, (Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BN != Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].WN), Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].mma_shape, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].k_group_size, attention_config_t.double_buffer, 2 if attention_config_t.double_buffer else 1]`

### `QRegisterBufferType`

`comptime QRegisterBufferType = QRegisterBuffer[q_type, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].mma_shape, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].k_group_size, Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].WM), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].WN), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BN), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BK), q_depth, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].warp_layout]`

### `SharedMemoryManagerType`

`comptime SharedMemoryManagerType = SharedMemoryManager[attention_config_t.shared_kv, attention_config_t.full_kv, attention_config_t.depth_padded, attention_config_t.double_buffer, q_type, Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BM), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BN), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BK), Int(Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].depth), token_gen]`

### `swap_a_b`

`comptime swap_a_b = True`

### `use_exp2`

`comptime use_exp2 = True`

### `warp_layout`

`comptime warp_layout = get_warp_layout[Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].mma_shape]()`

### `WM`

`comptime WM = config.warp_m[dtype]()`

### `WN`

`comptime WN = config.warp_n[dtype]()`

## Methods

### `__init__`

`__init__(out self, attention_config: attention_config_t, output_ptr: LegacyUnsafePointer[Scalar[output_type]], q: LegacyUnsafePointer[Scalar[q_type]], k: k_t, v: v_t, mask: mask_t, sink_weights: OptionalReg[LayoutTensor[q_type, Layout.row_major(-1), MutAnyOrigin]], batch_idx: Int, scale: Float32, seq_len: Int, num_keys: Int, start_pos: Int, cache_start_pos: Int = 0)`

### `q_head_idx`

`static q_head_idx() -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `q_tile_idx`

`static q_tile_idx() -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `kv_head_idx`

`static kv_head_idx() -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `zero_p_buffer`

`zero_p_buffer[stage: Int = 0](self)`

### `get_batch_idx`

`get_batch_idx(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `scale_p_reg`

`scale_p_reg[stage: Int = 0](self)`

### `get_tensor_core_mma_qk`

`static get_tensor_core_mma_qk(out result: TiledTensorCore[get_accum_type[q_type](), q_type, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].mma_shape, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].k_group_size, True])`

**Returns:**

[`TiledTensorCore`](/mojo/kernels/layout/tensor_core/TiledTensorCore)

### `get_tensor_core_mma_pv`

`static get_tensor_core_mma_pv(out result: TiledTensorCore[get_accum_type[q_type](), q_type, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].mma_shape, Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].k_group_size])`

**Returns:**

[`TiledTensorCore`](/mojo/kernels/layout/tensor_core/TiledTensorCore)

### `mma_qk`

`mma_qk[k_buffer_type: KVBuffer, //, prefetch_function: OptionalReg[fn() capturing -> None] = None, beg_iter: Int = 0, num_iters: Int = Int((Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].depth // Attention[config, group, token_gen, sink, q_depth, cache_depth, output_depth].BK)), prefetched_b_tile: Bool = False](mut self, mut k_buffer: k_buffer_type)`

### `mma_pv`

`mma_pv[v_buffer_type: KVBuffer, //, prefetch_function: OptionalReg[fn() capturing -> None] = None, prefetched_b_tile: Bool = True](mut self, mut v_buffer: v_buffer_type)`

### `mask_status`

`mask_status(self, kv_tile_start_row: UInt32) -> TileMaskStatus`

**Returns:**

[`TileMaskStatus`](/mojo/kernels/nn/mha_mask/TileMaskStatus)

### `mask_advance`

`mask_advance(mut self)`

### `mask_skip_tile`

`mask_skip_tile(self, status: TileMaskStatus) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `mask_skip_and_advance`

`mask_skip_and_advance(mut self, kv_tile_start_row: UInt32) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `mask_apply`

`mask_apply[stage: Int = 0](mut self, kv_tile_start_row: UInt32, kv_tile_num_rows: UInt32, not_last_iter: Bool)`

### `online_softmax`

`online_softmax[stage: Int = 0](mut self)`

### `store_output`

`store_output(self)`

### `copy_fragment_to_smem`

`copy_fragment_to_smem(self)`

### `store_partition_info`

`store_partition_info(self, num_partitions: Int, exp_sum_ptr: LegacyUnsafePointer[Scalar[get_accum_type[q_type]()]], qk_max_ptr: LegacyUnsafePointer[Scalar[get_accum_type[q_type]()]])`

</section>

---

## AttentionConfig

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__copyinit__` is trivial.

The implementation of `__copyinit__` is considered to be trivial if:

* The struct has a compiler-generated trivial `__copyinit__` and all its fields
  have a trivial `__copyinit__` method.

In practice, it means the value can be copied by copying the bits from
one location to another without side effects.

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__moveinit__` is trivial.

The implementation of `__moveinit__` is considered to be trivial if:

* The struct has a compiler-generated `__moveinit__` and all its fields
  have a trivial `__moveinit__` method.

In practice, it means the value can be moved by moving the bits from
one location to another without side effects.

### `depth_padded`

`comptime depth_padded`

### `double_buffer`

`comptime double_buffer`

### `full_kv`

`comptime full_kv`

### `shared_kv`

`comptime shared_kv`

## Required methods

### `__copyinit__`

`__copyinit__(out self: _Self, existing: _Self, /)`

Create a new instance of the value by copying an existing one.

**Args:**

* â€‹existing (`_Self`): The value to copy.

**Returns:**

`_Self`

### `__moveinit__`

`__moveinit__(out self: _Self, deinit existing: _Self, /)`

Create a new instance of the value by moving the value of another.

**Args:**

* â€‹existing (`_Self`): The value to move.

**Returns:**

`_Self`

### `q_head_idx`

`static q_head_idx() -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `q_tile_idx`

`static q_tile_idx() -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `kv_head_idx`

`static kv_head_idx() -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `get_mma_shape`

`static get_mma_shape() -> IndexList[3]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

### `get_q_offset`

`static get_q_offset[q_depth: UInt]() -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `get_output_offset`

`static get_output_offset[output_depth: UInt]() -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

## Provided methods

### `copy`

`copy(self: _Self) -> _Self`

Explicitly construct a copy of self.

**Returns:**

`_Self`: A copy of this value.

</section>

---

## attention (Attention)

<section class='mojo-docs'>

## Structs

* [â€‹`Attention`](./Attention):

## Traits

* [â€‹`AttentionConfig`](./AttentionConfig):

</section>

---

## KBufferConfig

<section class='mojo-docs'>

`struct KBufferConfig[BN: Int, BK: Int, WN: Int]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`KVBufferConfig`](/mojo/kernels/nn/attention/gpu/amd/buffers/KVBufferConfig)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `btile_dim0`

`comptime btile_dim0 = BN`

### `btile_dim1`

`comptime btile_dim1 = BK`

### `iterator_axis`

`comptime iterator_axis = 1`

### `wsize`

`comptime wsize = KBufferConfig[BN, BK, WN].wtile_dim0`

### `wtile_dim0`

`comptime wtile_dim0 = WN`

### `wtile_dim1`

`comptime wtile_dim1 = BK`

## Methods

### `get_wtile_coord`

`static get_wtile_coord() -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## KVBuffer

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `mma_tile_layout`

`comptime mma_tile_layout`

## Required methods

### `get_dtype`

`static get_dtype() -> DType`

**Returns:**

[`DType`](/mojo/std/builtin/dtype/DType)

### `load_from_dram`

`load_from_dram(mut self: _Self)`

### `get_mma_tile`

`get_mma_tile(self: _Self) -> LayoutTensor[_Self._dtype, _Self.mma_tile_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `copy_to_shared`

`copy_to_shared[tile_id: Int = 0](self: _Self)`

### `load_from_shared`

`load_from_shared[k_mma: Int](self: _Self)`

</section>

---

## KVBufferConfig

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `btile_dim0`

`comptime btile_dim0`

### `btile_dim1`

`comptime btile_dim1`

### `iterator_axis`

`comptime iterator_axis`

### `wsize`

`comptime wsize`

### `wtile_dim0`

`comptime wtile_dim0`

### `wtile_dim1`

`comptime wtile_dim1`

## Required methods

### `get_wtile_coord`

`static get_wtile_coord() -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## KVBufferImpl

<section class='mojo-docs'>

`struct KVBufferImpl[dtype: DType, layout: Layout, address_space: AddressSpace, alignment: Int, mut: Bool, origin: Origin[mut=mut], masked: Bool, layout_int_type: DType, linear_idx_type: DType, out_type: DType, in_type: DType, shape: IndexList[3], group_size: Int, transpose_b: Bool, //, config: KVBufferConfig, tensor_core_mma: TiledTensorCore[out_type, in_type, shape, group_size, transpose_b], swizzle: OptionalReg[Swizzle], BN: Int, WN: Int, BK: Int, depth: Int, num_threads: Int, num_stages: Int = 1, token_gen: Bool = False]`

## Fields

* â€‹load\_tile (`KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].LoadTileType`):
* â€‹mma\_tile (`KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].MMATileType`):
* â€‹smem\_iter (`KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].SharedIterType`):
* â€‹bounds (`Int`):
* â€‹load\_tile\_id (`Int`):
* â€‹global\_iterator (`KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].GlobalTiledIteratorType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`KVBuffer`](/mojo/kernels/nn/attention/gpu/amd/buffers/KVBuffer)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `base_layout`

`comptime base_layout = Layout.row_major(config.btile_dim0, KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].simd_width)`

### `GlobalTensorType`

`comptime GlobalTensorType = LayoutTensor[dtype, layout, origin, address_space=address_space, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

### `GlobalTiledIteratorType`

`comptime GlobalTiledIteratorType = LayoutTensorIter[dtype, LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, Layout(IntTuple(1), IntTuple(1)), layout_int_type, linear_idx_type, masked, alignment, config.btile_dim0, config.btile_dim1]()[0], origin, address_space=address_space, axis=config.iterator_axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked if masked else _tile_is_masked[layout, config.btile_dim0, config.btile_dim1]()]`

### `LoadTileType`

`comptime LoadTileType = LayoutTensor[dtype, Layout.row_major(((num_stages * KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].num_mmas) * KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].num_k_tiles), KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].simd_width), MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `MMA_K`

`comptime MMA_K = shape.__getitem__[3, DType.int64, Int](2)`

### `MMA_N`

`comptime MMA_N = shape.__getitem__[3, DType.int64, Int](1)`

### `mma_tile_layout`

`comptime mma_tile_layout = Layout.row_major(KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].num_mmas, KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].simd_width)`

### `MMATileType`

`comptime MMATileType = LayoutTensor[dtype, KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].mma_tile_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `num_k_tiles`

`comptime num_k_tiles = ceildiv(BK, (KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].MMA_K * group_size))`

### `num_mmas`

`comptime num_mmas = ceildiv(config.wsize, KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].MMA_N)`

### `num_repeats`

`comptime num_repeats = (config.btile_dim1 // KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].simd_width)`

### `num_warps_n`

`comptime num_warps_n = (BN // WN)`

### `SharedIterType`

`comptime SharedIterType = LayoutTensorIter[dtype, KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, circular=True]`

### `SharedTileType`

`comptime SharedTileType = KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].SharedIterType.LayoutTensorType`

### `SharedWarpTileType`

`comptime SharedWarpTileType = LayoutTensor[dtype, LayoutTensor._compute_tile_layout[True, dtype, KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].smem_layout, MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_index_type(AddressSpace.SHARED), _get_index_type(AddressSpace.SHARED), False, align_of[dtype](), KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].wtile_dim0, KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].wtile_dim1]()[0], MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=_get_index_type(AddressSpace.SHARED), linear_idx_type=_get_index_type(AddressSpace.SHARED), masked=_tile_is_masked[KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].smem_layout, KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].wtile_dim0, KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].wtile_dim1]()]`

### `simd_width`

`comptime simd_width = simd_width_of[dtype]()`

### `smem_layout`

`comptime smem_layout = blocked_product(KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].base_layout, KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].tiler_layout, True) if (xor token_gen._mlir_value, True) else Layout.row_major(config.btile_dim0, config.btile_dim1)`

### `thread_layout`

`comptime thread_layout = Layout.row_major(((min(num_threads, ((config.btile_dim0 * config.btile_dim1) // KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].simd_width)) * KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].simd_width) // KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].smem_layout.stride[0].value()), (KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].smem_layout.stride[0].value() // KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].simd_width)) if token_gen else Layout.row_major((num_threads // 4), 4)`

### `tiler_layout`

`comptime tiler_layout = Layout.row_major(1, KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].num_repeats)`

### `wtile_dim0`

`comptime wtile_dim0 = config.wtile_dim0`

### `wtile_dim1`

`comptime wtile_dim1 = config.wtile_dim1`

## Methods

### `__init__`

`__init__(out self, global_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], num_b_rows: OptionalReg[Int], shared_ptr: LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED, mut=mut, origin=origin])`

### `get_dtype`

`static get_dtype() -> DType`

**Returns:**

[`DType`](/mojo/std/builtin/dtype/DType)

### `load_from_dram`

`load_from_dram(mut self)`

### `get_mma_tile`

`get_mma_tile(self) -> KVBufferImpl[config, tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen].MMATileType`

**Returns:**

`KVBufferImpl`

### `copy_to_shared`

`copy_to_shared[tile_id: Int = 0](self)`

### `load_from_shared`

`load_from_shared[k_mma: Int](self)`

</section>

---

## OutputRegisterBuffer

<section class='mojo-docs'>

`struct OutputRegisterBuffer[dtype: DType, num_m_mmas: Int, num_n_mmas: Int, output_frag_size: Int]`

## Fields

* â€‹reg\_tile (`OutputRegisterBuffer[dtype, num_m_mmas, num_n_mmas, output_frag_size].RegisterTileType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`RegisterBuffer`](/mojo/kernels/nn/attention/gpu/amd/buffers/RegisterBuffer)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `reg_dtype`

`comptime reg_dtype = dtype`

### `reg_tile_layout`

`comptime reg_tile_layout = Layout.row_major((num_n_mmas * num_m_mmas), output_frag_size)`

### `RegisterTileType`

`comptime RegisterTileType = LayoutTensor[dtype, OutputRegisterBuffer[dtype, num_m_mmas, num_n_mmas, output_frag_size].reg_tile_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

## Methods

### `__init__`

`__init__(out self)`

### `get_dtype`

`static get_dtype() -> DType`

**Returns:**

[`DType`](/mojo/std/builtin/dtype/DType)

### `vectorize`

`vectorize(self) -> LayoutTensor[dtype, coalesce(LayoutTensor._compute_tile_layout[True, dtype, OutputRegisterBuffer[dtype, num_m_mmas, num_n_mmas, output_frag_size].reg_tile_layout, MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(OutputRegisterBuffer[dtype, num_m_mmas, num_n_mmas, output_frag_size].reg_tile_layout, AddressSpace.LOCAL), _get_index_type(OutputRegisterBuffer[dtype, num_m_mmas, num_n_mmas, output_frag_size].reg_tile_layout, AddressSpace.LOCAL), False, align_of[dtype](), 1, output_frag_size]()[1], True), MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=LayoutTensor._divide_tiles[True, dtype, OutputRegisterBuffer[dtype, num_m_mmas, num_n_mmas, output_frag_size].reg_tile_layout, MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(OutputRegisterBuffer[dtype, num_m_mmas, num_n_mmas, output_frag_size].reg_tile_layout, AddressSpace.LOCAL), _get_index_type(OutputRegisterBuffer[dtype, num_m_mmas, num_n_mmas, output_frag_size].reg_tile_layout, AddressSpace.LOCAL), False, align_of[dtype](), 1, output_frag_size]()[0], layout_int_type=_get_layout_type(OutputRegisterBuffer[dtype, num_m_mmas, num_n_mmas, output_frag_size].reg_tile_layout, AddressSpace.LOCAL), linear_idx_type=_get_index_type(OutputRegisterBuffer[dtype, num_m_mmas, num_n_mmas, output_frag_size].reg_tile_layout, AddressSpace.LOCAL)]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `apply_softmax_denominator`

`apply_softmax_denominator(self, rowsum: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

### `zero`

`zero(self)`

### `get_reg_tile`

`get_reg_tile[stage: Int = 0](self) -> OutputRegisterBuffer[dtype, num_m_mmas, num_n_mmas, output_frag_size].RegisterTileType`

**Returns:**

`OutputRegisterBuffer`

</section>

---

## PRegisterBuffer

<section class='mojo-docs'>

`struct PRegisterBuffer[accum_type_: DType, dtype: DType, BM: Int, BN: Int, BK: Int, WM: Int, WN: Int, num_m_mmas: Int, num_n_mmas: Int, output_frag_size: Int, shared_memory_backed: Bool, mma_shape: IndexList[3], k_group_size: Int, tr_load_enabled: Bool = False, num_stages: Int = 1]`

## Fields

* â€‹reg\_tile (`PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].RegisterTileType_`):
* â€‹shared\_memory\_tile (`PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].SharedMemoryTileType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`RegisterBuffer`](/mojo/kernels/nn/attention/gpu/amd/buffers/RegisterBuffer),
[`RegisterMMABuffer`](/mojo/kernels/nn/attention/gpu/amd/buffers/RegisterMMABuffer)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `mma_dtype`

`comptime mma_dtype = dtype`

### `mma_tile_layout`

`comptime mma_tile_layout = Layout.row_major(num_m_mmas, simd_width_of[dtype]())`

### `MMATileType`

`comptime MMATileType = LayoutTensor[PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].mma_dtype, PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].mma_tile_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `reg_dtype`

`comptime reg_dtype = accum_type_`

### `reg_tile_layout`

`comptime reg_tile_layout = Layout.row_major((num_n_mmas * num_m_mmas), output_frag_size)`

### `reg_tile_layout_`

`comptime reg_tile_layout_ = Layout.row_major(((num_stages * num_n_mmas) * num_m_mmas), output_frag_size)`

### `RegisterTileType`

`comptime RegisterTileType = LayoutTensor[accum_type_, PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].reg_tile_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `RegisterTileType_`

`comptime RegisterTileType_ = LayoutTensor[accum_type_, PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].reg_tile_layout_, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `shared_memory_layout`

`comptime shared_memory_layout = blocked_product(Layout.row_major(BM, BK), Layout.row_major(1, (BN // BK)), False)`

### `SharedMemoryTileType`

`comptime SharedMemoryTileType = LayoutTensor[dtype, PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].shared_memory_layout, MutAnyOrigin, address_space=AddressSpace.SHARED]`

## Methods

### `__init__`

`__init__(out self, shared_ptr: LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED, mut=mut, origin=origin])`

### `get_mma_tile_reg`

`get_mma_tile_reg[tile_idx: Int, k_idx: Int, stage: Int = 0](self) -> PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].MMATileType`

**Returns:**

`PRegisterBuffer`

### `get_mma_tile_shared`

`get_mma_tile_shared[tile_idx: Int, k_idx: Int](self) -> PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].MMATileType`

**Returns:**

`PRegisterBuffer`

### `get_mma_tile`

`get_mma_tile[tile_idx: Int, k_idx: Int, stage: Int = 0](self) -> PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].MMATileType`

**Returns:**

`PRegisterBuffer`

### `get_dtype`

`static get_dtype() -> DType`

**Returns:**

[`DType`](/mojo/std/builtin/dtype/DType)

### `vectorize`

`vectorize[stage: Int = 0](self) -> LayoutTensor[accum_type_, coalesce(LayoutTensor._compute_tile_layout[True, accum_type_, PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].reg_tile_layout, MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].reg_tile_layout, AddressSpace.LOCAL), _get_index_type(PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].reg_tile_layout, AddressSpace.LOCAL), False, align_of[accum_type_](), 1, output_frag_size]()[1], True), MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=LayoutTensor._divide_tiles[True, accum_type_, PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].reg_tile_layout, MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].reg_tile_layout, AddressSpace.LOCAL), _get_index_type(PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].reg_tile_layout, AddressSpace.LOCAL), False, align_of[accum_type_](), 1, output_frag_size]()[0], layout_int_type=_get_layout_type(PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].reg_tile_layout, AddressSpace.LOCAL), linear_idx_type=_get_index_type(PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].reg_tile_layout, AddressSpace.LOCAL)]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `zero`

`zero[stage: Int = 0](self)`

### `get_reg_tile`

`get_reg_tile[stage: Int = 0](self) -> PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].RegisterTileType`

**Returns:**

`PRegisterBuffer`

### `get_shared_memory_tile`

`get_shared_memory_tile(self, tile_idx: Int) -> LayoutTensor[dtype, LayoutTensor._compute_tile_layout[True, dtype, PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].shared_memory_layout, MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].shared_memory_layout, AddressSpace.SHARED), _get_index_type(PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].shared_memory_layout, AddressSpace.SHARED), False, align_of[dtype](), BM, BK]()[0], MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=_get_layout_type(PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].shared_memory_layout, AddressSpace.SHARED), linear_idx_type=_get_index_type(PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].shared_memory_layout, AddressSpace.SHARED), masked=_tile_is_masked[PRegisterBuffer[accum_type_, dtype, BM, BN, BK, WM, WN, num_m_mmas, num_n_mmas, output_frag_size, shared_memory_backed, mma_shape, k_group_size, tr_load_enabled, num_stages].shared_memory_layout, BM, BK]()]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `copy_to_shared`

`copy_to_shared(self)`

</section>

---

## QRegisterBuffer

<section class='mojo-docs'>

`struct QRegisterBuffer[dtype: DType, mma_shape: IndexList[3], k_group_size: Int, WM: Int, WN: Int, BN: Int, BK: Int, depth: Int, thread_layout: Layout]`

## Fields

* â€‹reg\_tile (`QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].RegisterTileType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`RegisterBuffer`](/mojo/kernels/nn/attention/gpu/amd/buffers/RegisterBuffer),
[`RegisterMMABuffer`](/mojo/kernels/nn/attention/gpu/amd/buffers/RegisterMMABuffer)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `mma_dtype`

`comptime mma_dtype = dtype`

### `MMA_K`

`comptime MMA_K = mma_shape.__getitem__[3, DType.int64, Int](2)`

### `MMA_M`

`comptime MMA_M = mma_shape.__getitem__[3, DType.int64, Int](0)`

### `mma_tile_layout`

`comptime mma_tile_layout = LayoutTensor._compute_tile_layout[True, dtype, LayoutTensor._compute_tile_layout[True, dtype, QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), _get_index_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), False, align_of[dtype](), (QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout.shape[0].value() // QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_tiles), 0]()[0], MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(LayoutTensor._compute_tile_layout[True, dtype, QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), _get_index_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), False, align_of[dtype](), (QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout.shape[0].value() // QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_tiles), 0]()[0], AddressSpace.LOCAL), _get_index_type(LayoutTensor._compute_tile_layout[True, dtype, QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), _get_index_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), False, align_of[dtype](), (QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout.shape[0].value() // QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_tiles), 0]()[0], AddressSpace.LOCAL), False, align_of[dtype](), (LayoutTensor._compute_tile_layout[True, dtype, QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), _get_index_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), False, align_of[dtype](), (QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout.shape[0].value() // QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_tiles), 0]()[0].shape[0].value() // QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_k_tiles), 0]()[0]`

### `MMATileType`

`comptime MMATileType = LayoutTensor[dtype, LayoutTensor._compute_tile_layout[True, dtype, LayoutTensor._compute_tile_layout[True, dtype, QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), _get_index_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), False, align_of[dtype](), (QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout.shape[0].value() // QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_tiles), 0]()[0], MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(LayoutTensor._compute_tile_layout[True, dtype, QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), _get_index_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), False, align_of[dtype](), (QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout.shape[0].value() // QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_tiles), 0]()[0], AddressSpace.LOCAL), _get_index_type(LayoutTensor._compute_tile_layout[True, dtype, QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), _get_index_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), False, align_of[dtype](), (QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout.shape[0].value() // QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_tiles), 0]()[0], AddressSpace.LOCAL), False, align_of[dtype](), (LayoutTensor._compute_tile_layout[True, dtype, QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), _get_index_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), False, align_of[dtype](), (QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout.shape[0].value() // QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_tiles), 0]()[0].shape[0].value() // QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_k_tiles), 0]()[0], MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `num_k_tiles`

`comptime num_k_tiles = ceildiv(BK, (QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].MMA_K * k_group_size))`

### `num_mmas`

`comptime num_mmas = ceildiv(WM, QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].MMA_M)`

### `num_tiles`

`comptime num_tiles = (depth // BK)`

### `reg_dtype`

`comptime reg_dtype = dtype`

### `reg_tile_layout`

`comptime reg_tile_layout = Layout.row_major(((QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_mmas * QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_k_tiles) * QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_tiles), QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].simd_width)`

### `RegisterTileType`

`comptime RegisterTileType = LayoutTensor[dtype, QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `simd_width`

`comptime simd_width = simd_width_of[dtype]()`

### `TiledIteratorType`

`comptime TiledIteratorType = LayoutTensorIter[dtype, LayoutTensor._compute_tile_layout[True, dtype, QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), _get_index_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), False, align_of[dtype](), (QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_mmas * QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_k_tiles), QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].simd_width]()[0], MutAnyOrigin, address_space=AddressSpace.LOCAL, axis=0, layout_int_type=_get_layout_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), linear_idx_type=_get_index_type(QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, AddressSpace.LOCAL), masked=_tile_is_masked[QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].reg_tile_layout, (QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_mmas * QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].num_k_tiles), QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].simd_width]()]`

## Methods

### `__init__`

`__init__(out self, tensor: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

### `get_dtype`

`static get_dtype() -> DType`

**Returns:**

[`DType`](/mojo/std/builtin/dtype/DType)

### `get_iter`

`get_iter(self) -> QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].TiledIteratorType`

**Returns:**

`QRegisterBuffer`

### `get_mma_tile`

`get_mma_tile[tile_idx: Int, k_idx: Int](self) -> QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].MMATileType`

**Returns:**

`QRegisterBuffer`

### `get_reg_tile`

`get_reg_tile[stage: Int = 0](self) -> QRegisterBuffer[dtype, mma_shape, k_group_size, WM, WN, BN, BK, depth, thread_layout].RegisterTileType`

**Returns:**

`QRegisterBuffer`

### `zero`

`zero(self)`

</section>

---

## RegisterBuffer

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `reg_dtype`

`comptime reg_dtype`

### `reg_tile_layout`

`comptime reg_tile_layout`

## Required methods

### `get_dtype`

`static get_dtype() -> DType`

**Returns:**

[`DType`](/mojo/std/builtin/dtype/DType)

### `zero`

`zero(self: _Self)`

### `get_reg_tile`

`get_reg_tile[stage: Int = 0](self: _Self) -> LayoutTensor[_Self.reg_dtype, _Self.reg_tile_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

</section>

---

## RegisterMMABuffer

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`RegisterBuffer`](/mojo/kernels/nn/attention/gpu/amd/buffers/RegisterBuffer)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `mma_dtype`

`comptime mma_dtype`

### `mma_tile_layout`

`comptime mma_tile_layout`

### `reg_dtype`

`comptime reg_dtype`

### `reg_tile_layout`

`comptime reg_tile_layout`

## Required methods

### `get_mma_tile`

`get_mma_tile[tile_idx: Int, k_idx: Int](self: _Self) -> LayoutTensor[_Self.mma_dtype, _Self.mma_tile_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `get_dtype`

`static get_dtype() -> DType`

**Returns:**

[`DType`](/mojo/std/builtin/dtype/DType)

### `zero`

`zero(self: _Self)`

### `get_reg_tile`

`get_reg_tile[stage: Int = 0](self: _Self) -> LayoutTensor[_Self.reg_dtype, _Self.reg_tile_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

</section>

---

## VBufferConfig

<section class='mojo-docs'>

`struct VBufferConfig[BN: Int, BK: Int, WN: Int, depth: Int]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`KVBufferConfig`](/mojo/kernels/nn/attention/gpu/amd/buffers/KVBufferConfig)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `btile_dim0`

`comptime btile_dim0 = BK`

### `btile_dim1`

`comptime btile_dim1 = depth`

### `iterator_axis`

`comptime iterator_axis = 0`

### `wsize`

`comptime wsize = VBufferConfig[BN, BK, WN, depth].wtile_dim1`

### `wtile_dim0`

`comptime wtile_dim0 = BK`

### `wtile_dim1`

`comptime wtile_dim1 = (depth // (BN // WN))`

## Methods

### `get_wtile_coord`

`static get_wtile_coord() -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## VBufferTransposeLoads

<section class='mojo-docs'>

`struct VBufferTransposeLoads[dtype: DType, layout: Layout, address_space: AddressSpace, alignment: Int, mut: Bool, origin: Origin[mut=mut], masked: Bool, layout_int_type: DType, linear_idx_type: DType, out_type: DType, in_type: DType, shape: IndexList[3], group_size: Int, transpose_b: Bool, //, tensor_core_mma: TiledTensorCore[out_type, in_type, shape, group_size, transpose_b], BN: Int, BK: Int, depth: Int, num_threads: Int, num_stages: Int = 1]`

## Fields

* â€‹load\_tile (`VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].LoadTileType`):
* â€‹mma\_tile (`VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].MMATileType`):
* â€‹smem\_iter (`VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].SharedIterType`):
* â€‹global\_iterator (`VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].GlobalTiledIteratorType`):
* â€‹global\_base\_tile (`VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].GlobalTensorType`):
* â€‹current\_stage (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`KVBuffer`](/mojo/kernels/nn/attention/gpu/amd/buffers/KVBuffer)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `base_layout`

`comptime base_layout = Layout.row_major(VBufferTransposeLoads.pad[dtype, layout, address_space, alignment, mut, origin, masked, layout_int_type, linear_idx_type, out_type, in_type, shape, group_size, transpose_b, tensor_core_mma, BN, BK, depth, num_threads, num_stages, depth](), VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].simd_width)`

### `depth_tile_size`

`comptime depth_tile_size = min(depth, 128)`

### `GlobalTensorType`

`comptime GlobalTensorType = LayoutTensor[dtype, layout, origin, address_space=address_space, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]`

### `GlobalTiledIteratorType`

`comptime GlobalTiledIteratorType = LayoutTensorIter[dtype, LayoutTensor._compute_tile_layout[mut, dtype, layout, origin, address_space, Layout(IntTuple(1), IntTuple(1)), layout_int_type, linear_idx_type, masked, alignment, BK, depth]()[0], origin, address_space=address_space, axis=0, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked if masked else _tile_is_masked[layout, BK, depth]()]`

### `load_width`

`comptime load_width = 4 if (eq depth._mlir_value, 64) else VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].simd_width`

### `loads_per_thread_per_depth_tile`

`comptime loads_per_thread_per_depth_tile = ((VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].depth_tile_size * BK) // (VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].load_width * num_threads))`

### `LoadTileType`

`comptime LoadTileType = LayoutTensor[dtype, Layout.row_major(((VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].loads_per_thread_per_depth_tile * (depth // VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].depth_tile_size)) * num_stages), VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].load_width), MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `MMA_K`

`comptime MMA_K = shape.__getitem__[3, DType.int64, Int](2)`

### `MMA_M`

`comptime MMA_M = shape.__getitem__[3, DType.int64, Int](0)`

### `mma_tile_layout`

`comptime mma_tile_layout = Layout.row_major((depth // VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].MMA_M), VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].simd_width)`

### `MMATileType`

`comptime MMATileType = LayoutTensor[dtype, VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].mma_tile_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `num_depth_tiles`

`comptime num_depth_tiles = (depth // VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].MMA_M)`

### `num_k_tiles`

`comptime num_k_tiles = ceildiv(BK, (VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].MMA_K * group_size))`

### `num_repeats`

`comptime num_repeats = (BK // VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].simd_width)`

### `SharedIterType`

`comptime SharedIterType = LayoutTensorIter[dtype, VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, circular=True]`

### `SharedTileType`

`comptime SharedTileType = VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].SharedIterType.LayoutTensorType`

### `simd_width`

`comptime simd_width = simd_width_of[dtype]()`

### `smem_layout`

`comptime smem_layout = blocked_product(VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].base_layout, VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].tiler_layout, True)`

### `tiler_layout`

`comptime tiler_layout = Layout.row_major(1, VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].num_repeats)`

## Methods

### `__init__`

`__init__(out self, global_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], shared_ptr: LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED, mut=mut, origin=origin])`

### `get_dtype`

`static get_dtype() -> DType`

**Returns:**

[`DType`](/mojo/std/builtin/dtype/DType)

### `pad`

`static pad[dim: Int]() -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `load_from_dram`

`load_from_dram(mut self)`

### `get_mma_tile`

`get_mma_tile(self) -> VBufferTransposeLoads[tensor_core_mma, BN, BK, depth, num_threads, num_stages].MMATileType`

**Returns:**

`VBufferTransposeLoads`

### `copy_to_shared`

`copy_to_shared[tile_id: Int = 0](self)`

### `load_from_shared`

`load_from_shared[k_mma: Int](self)`

</section>

---

## buffers

<section class='mojo-docs'>

## `comptime` values

### `KBuffer`

`comptime KBuffer[out_type: DType, in_type: DType, shape: IndexList[3], group_size: Int, transpose_b: Bool, //, tensor_core_mma: TiledTensorCore[out_type, in_type, shape, group_size, transpose_b], swizzle: OptionalReg[Swizzle], BN: Int, WN: Int, BK: Int, depth: Int, num_threads: Int, num_stages: Int = 1, token_gen: Bool = False] = KVBufferImpl[KBufferConfig[BN, BK, WN], tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen]`

#### Parameters

* â€‹out\_type ([`DType`](/std/builtin/dtype/DType)):
* â€‹in\_type ([`DType`](/std/builtin/dtype/DType)):
* â€‹shape ([`IndexList`](/std/utils/index_/IndexList)):
* â€‹group\_size ([`Int`](/std/builtin/int/Int)):
* â€‹transpose\_b ([`Bool`](/std/builtin/bool/Bool)):
* â€‹tensor\_core\_mma ([`TiledTensorCore`](/kernels/layout/tensor_core/TiledTensorCore)):
* â€‹swizzle ([`OptionalReg`](/std/collections/optional/OptionalReg)):
* â€‹BN ([`Int`](/std/builtin/int/Int)):
* â€‹WN ([`Int`](/std/builtin/int/Int)):
* â€‹BK ([`Int`](/std/builtin/int/Int)):
* â€‹depth ([`Int`](/std/builtin/int/Int)):
* â€‹num\_threads ([`Int`](/std/builtin/int/Int)):
* â€‹num\_stages ([`Int`](/std/builtin/int/Int)):
* â€‹token\_gen ([`Bool`](/std/builtin/bool/Bool)):

### `VBuffer`

`comptime VBuffer[out_type: DType, in_type: DType, shape: IndexList[3], group_size: Int, transpose_b: Bool, //, tensor_core_mma: TiledTensorCore[out_type, in_type, shape, group_size, transpose_b], swizzle: OptionalReg[Swizzle], BN: Int, WN: Int, BK: Int, depth: Int, num_threads: Int, num_stages: Int = 1, token_gen: Bool = False] = KVBufferImpl[VBufferConfig[BN, BK, WN, depth], tensor_core_mma, swizzle, BN, WN, BK, depth, num_threads, num_stages, token_gen]`

#### Parameters

* â€‹out\_type ([`DType`](/std/builtin/dtype/DType)):
* â€‹in\_type ([`DType`](/std/builtin/dtype/DType)):
* â€‹shape ([`IndexList`](/std/utils/index_/IndexList)):
* â€‹group\_size ([`Int`](/std/builtin/int/Int)):
* â€‹transpose\_b ([`Bool`](/std/builtin/bool/Bool)):
* â€‹tensor\_core\_mma ([`TiledTensorCore`](/kernels/layout/tensor_core/TiledTensorCore)):
* â€‹swizzle ([`OptionalReg`](/std/collections/optional/OptionalReg)):
* â€‹BN ([`Int`](/std/builtin/int/Int)):
* â€‹WN ([`Int`](/std/builtin/int/Int)):
* â€‹BK ([`Int`](/std/builtin/int/Int)):
* â€‹depth ([`Int`](/std/builtin/int/Int)):
* â€‹num\_threads ([`Int`](/std/builtin/int/Int)):
* â€‹num\_stages ([`Int`](/std/builtin/int/Int)):
* â€‹token\_gen ([`Bool`](/std/builtin/bool/Bool)):

## Structs

* [â€‹`KBufferConfig`](./KBufferConfig):
* [â€‹`KVBufferImpl`](./KVBufferImpl):
* [â€‹`OutputRegisterBuffer`](./OutputRegisterBuffer):
* [â€‹`PRegisterBuffer`](./PRegisterBuffer):
* [â€‹`QRegisterBuffer`](./QRegisterBuffer):
* [â€‹`VBufferConfig`](./VBufferConfig):
* [â€‹`VBufferTransposeLoads`](./VBufferTransposeLoads):

## Traits

* [â€‹`KVBuffer`](./KVBuffer):
* [â€‹`KVBufferConfig`](./KVBufferConfig):
* [â€‹`RegisterBuffer`](./RegisterBuffer):
* [â€‹`RegisterMMABuffer`](./RegisterMMABuffer):

</section>

---

## amd (Amd)

<section class='mojo-docs'>

AMD GPU attention operations.

## Modules

* [â€‹`attention`](./attention/):
* [â€‹`buffers`](./buffers/):
* [â€‹`mha_gfx942`](./mha_gfx942/):
* [â€‹`mha_gfx950`](./mha_gfx950/):
* [â€‹`mla`](./mla/):
* [â€‹`mma`](./mma/):
* [â€‹`softmax`](./softmax/):
* [â€‹`utils`](./utils/):

</section>

---

## MHAAttentionConfig

<section class='mojo-docs'>

`struct MHAAttentionConfig[dtype: DType, //, token_gen: Bool, config: MHAConfig[dtype], group: Int]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`AttentionConfig`](/mojo/kernels/nn/attention/gpu/amd/attention/AttentionConfig),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `depth_padded`

`comptime depth_padded = False if (xor token_gen._mlir_value, True) if env_get_bool["USE_EXPERIMENTAL_CDNA4_MHA_KERNEL", False]() if _cdna_4_or_newer() else _cdna_4_or_newer() else env_get_bool["USE_EXPERIMENTAL_CDNA4_MHA_KERNEL", False]() if _cdna_4_or_newer() else _cdna_4_or_newer() else True`

### `double_buffer`

`comptime double_buffer = True if (xor token_gen._mlir_value, True) if env_get_bool["USE_EXPERIMENTAL_CDNA4_MHA_KERNEL", False]() if _cdna_4_or_newer() else _cdna_4_or_newer() else env_get_bool["USE_EXPERIMENTAL_CDNA4_MHA_KERNEL", False]() if _cdna_4_or_newer() else _cdna_4_or_newer() else False`

### `full_kv`

`comptime full_kv = True if (xor token_gen._mlir_value, True) if env_get_bool["USE_EXPERIMENTAL_CDNA4_MHA_KERNEL", False]() if _cdna_4_or_newer() else _cdna_4_or_newer() else env_get_bool["USE_EXPERIMENTAL_CDNA4_MHA_KERNEL", False]() if _cdna_4_or_newer() else _cdna_4_or_newer() else False`

### `shared_kv`

`comptime shared_kv = False if (xor token_gen._mlir_value, True) if env_get_bool["USE_EXPERIMENTAL_CDNA4_MHA_KERNEL", False]() if _cdna_4_or_newer() else _cdna_4_or_newer() else env_get_bool["USE_EXPERIMENTAL_CDNA4_MHA_KERNEL", False]() if _cdna_4_or_newer() else _cdna_4_or_newer() else True`

### `USE_EXPERIMENTAL_CDNA4_MHA_KERNEL`

`comptime USE_EXPERIMENTAL_CDNA4_MHA_KERNEL = token_gen.__invert__() if env_get_bool["USE_EXPERIMENTAL_CDNA4_MHA_KERNEL", False]() if _cdna_4_or_newer() else _cdna_4_or_newer() else env_get_bool["USE_EXPERIMENTAL_CDNA4_MHA_KERNEL", False]() if _cdna_4_or_newer() else _cdna_4_or_newer()`

## Methods

### `q_head_idx`

`static q_head_idx() -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `q_tile_idx`

`static q_tile_idx() -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `kv_head_idx`

`static kv_head_idx() -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `get_mma_shape`

`static get_mma_shape() -> IndexList[3]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

### `get_q_offset`

`static get_q_offset[q_depth: UInt]() -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `get_output_offset`

`static get_output_offset[output_depth: UInt]() -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

</section>

---

## mha_gfx942

<section class='mojo-docs'>

## Structs

* [â€‹`MHAAttentionConfig`](./MHAAttentionConfig):

</section>

---

## KVBuffer (Mha_gfx950)

<section class='mojo-docs'>

`struct KVBuffer[kv_t: MHAOperand, //, mma_shape: IndexList[3], k_group_size: Int, swizzle: OptionalReg[Swizzle], BN: Int, WN: Int, BK: Int, num_threads: Int, depth: Int, kv_num_heads: Int, transpose: Bool]`

## Fields

* â€‹mma\_tile (`KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].MMATileType`):
* â€‹smem\_iter (`KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].SharedIterType`):
* â€‹kv\_cache\_iter (`KVCacheIterator[kv_t, BN, kv_num_heads, depth]`):
* â€‹lds\_base\_ptrs (`InlineArray[UInt32, 2]`):
* â€‹warp\_id (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True if True if True if True if True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if True if True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if True if kv_t.__del__is_trivial else kv_t.__del__is_trivial else True if kv_t.__del__is_trivial else kv_t.__del__is_trivial`

### `base_layout`

`comptime base_layout = Layout.row_major(BN, BK)`

### `MMA_K`

`comptime MMA_K = mma_shape.__getitem__[3, DType.int64, Int](2)`

### `MMA_N`

`comptime MMA_N = mma_shape.__getitem__[3, DType.int64, Int](1)`

### `MMATileType`

`comptime MMATileType = LayoutTensor[kv_t.dtype, Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width), MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `num_k_mmas2`

`comptime num_k_mmas2 = ceildiv(BK, Int.__init__[Int]((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].MMA_K * k_group_size)))`

### `num_k_tiles`

`comptime num_k_tiles = ceildiv(depth if transpose else WN, BK)`

### `num_mmas`

`comptime num_mmas = ceildiv(WN if transpose else depth, KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].MMA_N)`

### `num_repeats`

`comptime num_repeats = (depth // BK)`

### `SharedIterType`

`comptime SharedIterType = LayoutTensorIter[kv_t.dtype, KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].smem_layout, MutAnyOrigin, address_space=AddressSpace.SHARED, circular=True]`

### `SharedTileType`

`comptime SharedTileType = KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].SharedIterType.LayoutTensorType`

### `SharedWarpTileType`

`comptime SharedWarpTileType = LayoutTensor[kv_t.dtype, LayoutTensor._compute_tile_layout[True, kv_t.dtype, KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].smem_layout, MutAnyOrigin, AddressSpace.SHARED, Layout(IntTuple(1), IntTuple(1)), _get_index_type(AddressSpace.SHARED), _get_index_type(AddressSpace.SHARED), False, align_of[kv_t.dtype](), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].wtile_dim0, KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].wtile_dim1]()[0], MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=_get_index_type(AddressSpace.SHARED), linear_idx_type=_get_index_type(AddressSpace.SHARED), masked=_tile_is_masked[KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].smem_layout, KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].wtile_dim0, KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].wtile_dim1]()]`

### `simd_width`

`comptime simd_width = simd_width_of[kv_t.dtype]()`

### `smem_layout`

`comptime smem_layout = blocked_product(KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].base_layout, KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].tiler_layout, False)`

### `tiler_layout`

`comptime tiler_layout = Layout.row_major(1, KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_repeats)`

### `wtile_dim0`

`comptime wtile_dim0 = WN`

### `wtile_dim1`

`comptime wtile_dim1 = BK`

## Methods

### `__init__`

`__init__(out self, k_cache: kv_t, batch_idx: UInt, head_idx: UInt, shared_ptr: UnsafePointer[Scalar[kv_t.dtype], origin, address_space=AddressSpace.SHARED], end: UInt, warp_id: UInt32)`

### `load_from_dram`

`load_from_dram[buffer_idx: Int](mut self)`

### `get_mma_tile`

`get_mma_tile[k_mma_tile_idx: Int, bk_tile_idx: Int](self) -> LayoutTensor[kv_t.dtype, LayoutTensor._compute_tile_layout[True, kv_t.dtype, LayoutTensor._compute_tile_layout[True, kv_t.dtype, Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width), MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width), AddressSpace.LOCAL), _get_index_type(Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width), AddressSpace.LOCAL), False, align_of[kv_t.dtype](), (Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width).shape[0].value() // KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), 0]()[0], MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(LayoutTensor._compute_tile_layout[True, kv_t.dtype, Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width), MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width), AddressSpace.LOCAL), _get_index_type(Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width), AddressSpace.LOCAL), False, align_of[kv_t.dtype](), (Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width).shape[0].value() // KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), 0]()[0], AddressSpace.LOCAL), _get_index_type(LayoutTensor._compute_tile_layout[True, kv_t.dtype, Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width), MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width), AddressSpace.LOCAL), _get_index_type(Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width), AddressSpace.LOCAL), False, align_of[kv_t.dtype](), (Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width).shape[0].value() // KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), 0]()[0], AddressSpace.LOCAL), False, align_of[kv_t.dtype](), (LayoutTensor._compute_tile_layout[True, kv_t.dtype, Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width), MutAnyOrigin, AddressSpace.LOCAL, Layout(IntTuple(1), IntTuple(1)), _get_layout_type(Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width), AddressSpace.LOCAL), _get_index_type(Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width), AddressSpace.LOCAL), False, align_of[kv_t.dtype](), (Layout.row_major(((KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_mmas * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2) * KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].simd_width).shape[0].value() // KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_tiles), 0]()[0].shape[0].value() // KVBuffer[mma_shape, k_group_size, swizzle, BN, WN, BK, num_threads, depth, kv_num_heads, transpose].num_k_mmas2), 0]()[0], MutAnyOrigin, address_space=AddressSpace.LOCAL]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `copy_to_shared`

`copy_to_shared(self)`

### `load_from_shared`

`load_from_shared(self, buffer: UInt)`

`load_from_shared[bk_tile: Int](self, buffer: UInt)`

</section>

---

## KVCacheIterator

<section class='mojo-docs'>

`struct KVCacheIterator[cache_t: MHAOperand, tile_size: Int, kv_num_heads: Int, depth: Int]`

## Fields

* â€‹cache (`cache_t`):
* â€‹end (`Int`):
* â€‹tile\_start\_row (`Int`):
* â€‹batch\_idx (`Int`):
* â€‹kv\_head\_idx (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True if True if True if True if cache_t.__del__is_trivial else cache_t.__del__is_trivial else True if cache_t.__del__is_trivial else cache_t.__del__is_trivial else True if True if cache_t.__del__is_trivial else cache_t.__del__is_trivial else True if cache_t.__del__is_trivial else cache_t.__del__is_trivial else True if True if True if cache_t.__del__is_trivial else cache_t.__del__is_trivial else True if cache_t.__del__is_trivial else cache_t.__del__is_trivial else True if True if cache_t.__del__is_trivial else cache_t.__del__is_trivial else True if cache_t.__del__is_trivial else cache_t.__del__is_trivial`

### `kv_gmem_layout`

`comptime kv_gmem_layout = Layout(IntTuple(Int.__init__[Int](tile_size), Int.__init__[Int](depth)), IntTuple(Int.__init__[Int]((kv_num_heads * depth)), 1))`

## Methods

### `__init__`

`__init__(out self, cache: cache_t, batch_idx: Int, kv_head_idx: Int, end: Int)`

### `next_unsafe`

`next_unsafe(mut self) -> LayoutTensor[cache_t.dtype, KVCacheIterator[cache_t, tile_size, kv_num_heads, depth].kv_gmem_layout, MutAnyOrigin, masked=True]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `increment`

`increment(mut self)`

</section>

---

## barrier

<section class='mojo-docs'>

`barrier[*, schedule_barrier_before: Bool = True, schedule_barrier_after: Bool = True]()`

</section>

---

## block_sync_lds

<section class='mojo-docs'>

`block_sync_lds[*, lgkmcnt: UInt32 = 0]()`

Synchronize LDS (local data share) with waitcnt barrier.

</section>

---

## block_sync_lds_direct_load

<section class='mojo-docs'>

`block_sync_lds_direct_load[*, vmcnt: UInt32 = 0]()`

Synchronize LDS for direct load with waitcnt barrier.

</section>

---

## mha_gfx950

<section class='mojo-docs'>

## Structs

* [â€‹`KVBuffer`](./KVBuffer):
* [â€‹`KVCacheIterator`](./KVCacheIterator):

## Functions

* [â€‹`barrier`](./barrier):
* [â€‹`block_sync_lds`](./block_sync_lds): Synchronize LDS (local data share) with waitcnt barrier.
* [â€‹`block_sync_lds_direct_load`](./block_sync_lds_direct_load): Synchronize LDS for direct load with waitcnt barrier.
* [â€‹`scheduling_hints_pv`](./scheduling_hints_pv):
* [â€‹`scheduling_hints_qk`](./scheduling_hints_qk):
* [â€‹`set_priority`](./set_priority):

</section>

---

## scheduling_hints_pv

<section class='mojo-docs'>

`scheduling_hints_pv[group: Int]()`

</section>

---

## scheduling_hints_qk

<section class='mojo-docs'>

`scheduling_hints_qk[group: Int]()`

</section>

---

## set_priority

<section class='mojo-docs'>

`set_priority[priority: Int]()`

</section>

---

## MLAAttentionConfig

<section class='mojo-docs'>

`struct MLAAttentionConfig[dtype: DType, //, token_gen: Bool, config: MHAConfig[dtype]]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`AttentionConfig`](/mojo/kernels/nn/attention/gpu/amd/attention/AttentionConfig),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `depth_padded`

`comptime depth_padded = True`

### `double_buffer`

`comptime double_buffer = False`

### `full_kv`

`comptime full_kv = False`

### `shared_kv`

`comptime shared_kv = True`

## Methods

### `q_head_idx`

`static q_head_idx() -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `q_tile_idx`

`static q_tile_idx() -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `kv_head_idx`

`static kv_head_idx() -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `get_mma_shape`

`static get_mma_shape() -> IndexList[3]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

### `get_q_offset`

`static get_q_offset[q_depth: UInt]() -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `get_output_offset`

`static get_output_offset[output_depth: UInt]() -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

</section>

---

## mla

<section class='mojo-docs'>

## Structs

* [â€‹`MLAAttentionConfig`](./MLAAttentionConfig):

</section>

---

## mma (Mma)

<section class='mojo-docs'>

## Functions

* [â€‹`mma`](./mma):

</section>

---

## mma (3)

<section class='mojo-docs'>

`mma[c_register_buffer_type: RegisterBuffer, a_register_buffer_type: RegisterMMABuffer, b_buffer_type: KVBuffer, //, tensor_core_mma: TiledTensorCore[out_type, in_type, shape, group_size, transpose_b], BK: Int, prefetch_function: OptionalReg[fn() capturing -> None], swap_a_b: Bool = False, beg_iter: Int = 0, num_iters: Int = 1, prefetched_b_tile: Bool = False](c: c_register_buffer_type, mut a_tile: a_register_buffer_type, mut b_tile: b_buffer_type)`

</section>

---

## Softmax

<section class='mojo-docs'>

`struct Softmax[dtype: DType, score_layout_by_mma_unit: Layout, block_layout_by_warp: Layout, warp_layout: Layout, fragment_layout: Layout, use_exp2: Bool = False]`

## Fields

* â€‹rowmax\_tensor (`Softmax[dtype, score_layout_by_mma_unit, block_layout_by_warp, warp_layout, fragment_layout, use_exp2].RowMaxTensorType`):
* â€‹rowsum\_tensor (`Softmax[dtype, score_layout_by_mma_unit, block_layout_by_warp, warp_layout, fragment_layout, use_exp2].RowSumTensorType`):
* â€‹score\_frag\_rowmax (`LayoutTensor[dtype, Layout.row_major(Softmax[dtype, score_layout_by_mma_unit, block_layout_by_warp, warp_layout, fragment_layout, use_exp2].num_colwise_tiles, Softmax[dtype, score_layout_by_mma_unit, block_layout_by_warp, warp_layout, fragment_layout, use_exp2].frag_num_rows), MutAnyOrigin, address_space=AddressSpace.LOCAL]`):
* â€‹score\_frag\_rowsum (`LayoutTensor[dtype, Layout.row_major(Softmax[dtype, score_layout_by_mma_unit, block_layout_by_warp, warp_layout, fragment_layout, use_exp2].num_colwise_tiles, Softmax[dtype, score_layout_by_mma_unit, block_layout_by_warp, warp_layout, fragment_layout, use_exp2].frag_num_rows), MutAnyOrigin, address_space=AddressSpace.LOCAL]`):
* â€‹correction (`LayoutTensor[dtype, Layout.row_major(Softmax[dtype, score_layout_by_mma_unit, block_layout_by_warp, warp_layout, fragment_layout, use_exp2].num_colwise_tiles, Softmax[dtype, score_layout_by_mma_unit, block_layout_by_warp, warp_layout, fragment_layout, use_exp2].frag_num_rows), MutAnyOrigin, address_space=AddressSpace.LOCAL]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `exp_function`

`comptime exp_function = _exp2_concrete if use_exp2 else _exp_concrete`

### `frag_is_row_vector`

`comptime frag_is_row_vector = (Softmax[dtype, score_layout_by_mma_unit, block_layout_by_warp, warp_layout, fragment_layout, use_exp2].frag_num_rows == 1)`

### `frag_num_cols`

`comptime frag_num_cols = fragment_layout.shape[1].value()`

### `frag_num_rows`

`comptime frag_num_rows = fragment_layout.shape[0].value()`

### `num_colwise_lanes`

`comptime num_colwise_lanes = SIMD[DType.uint32, 1](warp_layout.shape[0].value())`

### `num_colwise_tiles`

`comptime num_colwise_tiles = score_layout_by_mma_unit.shape[0].value()`

### `num_colwise_warps`

`comptime num_colwise_warps = block_layout_by_warp.shape[0].value()`

### `num_m_mmas`

`comptime num_m_mmas = score_layout_by_mma_unit.shape[0].value()`

### `num_rows_per_thread`

`comptime num_rows_per_thread = (Softmax[dtype, score_layout_by_mma_unit, block_layout_by_warp, warp_layout, fragment_layout, use_exp2].num_colwise_tiles * Softmax[dtype, score_layout_by_mma_unit, block_layout_by_warp, warp_layout, fragment_layout, use_exp2].frag_num_rows)`

### `num_rowwise_lanes`

`comptime num_rowwise_lanes = SIMD[DType.uint32, 1](warp_layout.shape[1].value())`

### `num_rowwise_tiles`

`comptime num_rowwise_tiles = score_layout_by_mma_unit.shape[1].value()`

### `num_rowwise_warps`

`comptime num_rowwise_warps = block_layout_by_warp.shape[1].value()`

### `num_shuffles_per_row`

`comptime num_shuffles_per_row = log2_floor(warp_layout.shape[1].value())`

### `row_layout`

`comptime row_layout = Layout.row_major(Int.__init__[Int](Softmax[dtype, score_layout_by_mma_unit, block_layout_by_warp, warp_layout, fragment_layout, use_exp2].num_m_mmas), fragment_layout.shape[0].value())`

### `RowMaxTensorType`

`comptime RowMaxTensorType = LayoutTensor[dtype, Softmax[dtype, score_layout_by_mma_unit, block_layout_by_warp, warp_layout, fragment_layout, use_exp2].row_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

### `RowSumTensorType`

`comptime RowSumTensorType = Softmax[dtype, score_layout_by_mma_unit, block_layout_by_warp, warp_layout, fragment_layout, use_exp2].RowMaxTensorType`

### `rowwise_lanes_stride`

`comptime rowwise_lanes_stride = SIMD[DType.uint32, 1](warp_layout.stride[1].value())`

## Methods

### `__init__`

`__init__(out self)`

### `calculate_qk_max`

`calculate_qk_max(self, score_reg_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], warp_scratch: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

### `calculate_qk_sum`

`calculate_qk_sum(self, score_reg_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], warp_scratch: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

### `exp`

`exp[start: Int = 0, stride: Int = 1](self, score_reg_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

### `calculate_correction`

`calculate_correction(self)`

### `update_output`

`update_output(self, output_reg_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

### `update_sum`

`update_sum(self)`

### `update_max`

`update_max(self)`

### `full`

`full(self, output_reg_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], score_reg_tile: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], warp_scratch: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## softmax (Softmax)

<section class='mojo-docs'>

## Structs

* [â€‹`Softmax`](./Softmax):

</section>

---

## GlobalMemoryManager

<section class='mojo-docs'>

`struct GlobalMemoryManager[dtype: DType, BM: UInt32, BN: UInt32, BK: UInt32, depth: UInt32, num_heads: UInt32, group: UInt32, token_gen: Bool, q_depth: UInt32 = depth, output_depth: UInt32 = depth]`

## Fields

* â€‹q\_offset (`UInt32`):
* â€‹q\_runtime\_layout (`RuntimeLayout[GlobalMemoryManager[dtype, BM, BN, BK, depth, num_heads, group, token_gen, q_depth, output_depth].q_gmem_layout, element_type=DType.int32, linear_idx_type=DType.int32]`):
* â€‹output\_offset (`UInt32`):
* â€‹output\_runtime\_layout (`RuntimeLayout[GlobalMemoryManager[dtype, BM, BN, BK, depth, num_heads, group, token_gen, q_depth, output_depth].output_gmem_layout, element_type=DType.int32, linear_idx_type=DType.int32]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `kv_gmem_layout`

`comptime kv_gmem_layout = Layout(IntTuple(Int.__init__[UInt32](BN), Int.__init__[UInt32](depth)), IntTuple(Int.__init__[UInt32]((GlobalMemoryManager[dtype, BM, BN, BK, depth, num_heads, group, token_gen, q_depth, output_depth].kv_num_heads * depth)), 1))`

### `kv_num_heads`

`comptime kv_num_heads = (num_heads // group)`

### `output_gmem_layout`

`comptime output_gmem_layout = Layout(IntTuple(Int.__init__[UInt32](BM), Int.__init__[UInt32](output_depth)), IntTuple(Int.__init__[UInt32]((num_heads * output_depth)), 1)) if (xor token_gen._mlir_value, True) else Layout.row_major(Int.__init__[UInt32](BM), Int.__init__[UInt32](output_depth))`

### `q_gmem_layout`

`comptime q_gmem_layout = Layout(IntTuple(Int.__init__[UInt32](BM), Int.__init__[UInt32](q_depth)), IntTuple(Int.__init__[UInt32]((num_heads * q_depth)), 1)) if (xor token_gen._mlir_value, True) else Layout.row_major(Int.__init__[UInt32](BM), Int.__init__[UInt32](q_depth))`

## Methods

### `__init__`

`__init__(out self, q_tile_idx: UInt32, kv_head_idx: UInt32, seq_len: Int, q_offset: UInt32, output_offset: UInt32)`

### `get_q_tensor`

`get_q_tensor[qtype: DType](self, ptr: LegacyUnsafePointer[Scalar[qtype]]) -> LayoutTensor[qtype, GlobalMemoryManager[dtype, BM, BN, BK, depth, num_heads, group, token_gen, q_depth, output_depth].q_gmem_layout, MutAnyOrigin, layout_int_type=DType.int32, linear_idx_type=DType.int32, masked=True]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `get_output_tensor`

`get_output_tensor[out_type: DType](self, ptr: LegacyUnsafePointer[Scalar[out_type]]) -> LayoutTensor[out_type, GlobalMemoryManager[dtype, BM, BN, BK, depth, num_heads, group, token_gen, q_depth, output_depth].output_gmem_layout, MutAnyOrigin, layout_int_type=DType.int32, linear_idx_type=DType.int32, masked=True]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `get_kv_tensor`

`get_kv_tensor[kvtype: DType, //](self, ptr: LegacyUnsafePointer[Scalar[kvtype], address_space=address_space, mut=mut, origin=origin], kv_tile_num_rows: UInt32) -> LayoutTensor[kvtype, GlobalMemoryManager[dtype, BM, BN, BK, depth, num_heads, group, token_gen, q_depth, output_depth].kv_gmem_layout, origin, address_space=address_space, masked=True]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

</section>

---

## SharedMemoryManager (Utils)

<section class='mojo-docs'>

`struct SharedMemoryManager[shared_kv: Bool, full_kv: Bool, depth_padded: Bool, double_buffer: Bool, dtype: DType, BM: Int, BN: Int, BK: Int, depth: Int, token_gen: Bool]`

## Fields

* â€‹p\_smem (`LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED]`):
* â€‹k\_smem (`LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED]`):
* â€‹v\_smem (`LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `accum_type`

`comptime accum_type = get_accum_type[dtype]()`

### `alignment`

`comptime alignment = align_of[SIMD[dtype, simd_width_of[dtype]()]]()`

### `k_smem_size`

`comptime k_smem_size = ((BN * depth if full_kv else BK) * 2 if double_buffer else 1)`

### `p_smem_size`

`comptime p_smem_size = (BM * BN) if token_gen else 0`

### `simd_width`

`comptime simd_width = simd_width_of[dtype]()`

### `v_smem_size`

`comptime v_smem_size = ((BN if full_kv else BK * pad[dtype, depth, depth]() if depth_padded else depth) * 2 if double_buffer else 1)`

## Methods

### `__init__`

`__init__(out self)`

### `get_k_ptr`

`get_k_ptr[_dtype: DType](self) -> LegacyUnsafePointer[Scalar[_dtype], address_space=AddressSpace.SHARED]`

**Returns:**

[`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)

### `get_v_ptr`

`get_v_ptr[_dtype: DType](self) -> LegacyUnsafePointer[Scalar[_dtype], address_space=AddressSpace.SHARED]`

**Returns:**

[`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)

### `get_p_ptr`

`get_p_ptr[_dtype: DType](self) -> LegacyUnsafePointer[Scalar[_dtype], address_space=AddressSpace.SHARED]`

**Returns:**

[`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)

### `get_warp_scratch_ptr`

`get_warp_scratch_ptr[_dtype: DType](self) -> LegacyUnsafePointer[Scalar[_dtype], address_space=AddressSpace.SHARED]`

**Returns:**

[`LegacyUnsafePointer`](/mojo/std/memory/legacy_unsafe_pointer/LegacyUnsafePointer)

</section>

---

## copy_dram_to_sram_lds

<section class='mojo-docs'>

`copy_dram_to_sram_lds[swizzle: OptionalReg[Swizzle] = OptionalReg[Swizzle]()](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], lds_base_ptr: UInt32 = 0)`

</section>

---

## copy_local_to_dram2

<section class='mojo-docs'>

`copy_local_to_dram2[dst_thread_layout: Layout, thread_scope: ThreadScope = ThreadScope.BLOCK](dst: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dst_base: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## get_fragment_layout

<section class='mojo-docs'>

`get_fragment_layout[mma_shape: IndexList[3]]() -> Layout`

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout)

</section>

---

## get_nested_fragment_layout

<section class='mojo-docs'>

`get_nested_fragment_layout[mma_shape: IndexList[3]]() -> Layout`

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout)

</section>

---

## get_warp_coords

<section class='mojo-docs'>

`get_warp_coords[BN: Int, WN: Int]() -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## get_warp_layout

<section class='mojo-docs'>

`get_warp_layout[mma_shape: IndexList[3]]() -> Layout`

**Returns:**

[`Layout`](/mojo/kernels/layout/layout/Layout)

</section>

---

## utils (Utils)

<section class='mojo-docs'>

## `comptime` values

### `LocalLayoutTensor`

`comptime LocalLayoutTensor[dtype: DType, layout: Layout] = LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

#### Parameters

* â€‹dtype ([`DType`](/std/builtin/dtype/DType)):
* â€‹layout ([`Layout`](/kernels/layout/layout/Layout)):

### `SharedLayoutTensor`

`comptime SharedLayoutTensor[dtype: DType, layout: Layout] = LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED]`

#### Parameters

* â€‹dtype ([`DType`](/std/builtin/dtype/DType)):
* â€‹layout ([`Layout`](/kernels/layout/layout/Layout)):

## Structs

* [â€‹`GlobalMemoryManager`](./GlobalMemoryManager):
* [â€‹`SharedMemoryManager`](./SharedMemoryManager):

## Functions

* [â€‹`copy_dram_to_sram_lds`](./copy_dram_to_sram_lds):
* [â€‹`copy_local_to_dram2`](./copy_local_to_dram2):
* [â€‹`get_fragment_layout`](./get_fragment_layout):
* [â€‹`get_nested_fragment_layout`](./get_nested_fragment_layout):
* [â€‹`get_warp_coords`](./get_warp_coords):
* [â€‹`get_warp_layout`](./get_warp_layout):
* [â€‹`load_b`](./load_b):
* [â€‹`load_b_`](./load_b_):
* [â€‹`load_b_tr`](./load_b_tr): Loads the b operand tile for AMD tensor core MFMA instructions using transposed memory access.
* [â€‹`pad`](./pad):

</section>

---

## load_b

<section class='mojo-docs'>

`load_b[mma_shape: IndexList[3], swizzle: OptionalReg[Swizzle]](src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> LayoutTensor[dtype, Layout.row_major((layout.size() // (WARP_SIZE * 8)), 8), MutAnyOrigin, address_space=AddressSpace.LOCAL]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

</section>

---

## load_b_

<section class='mojo-docs'>

`load_b_[mma_shape: IndexList[3], swizzle: OptionalReg[Swizzle], k_tile_idx: Int](src: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> SIMD[dtype, simd_width_of[dtype]()]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## load_b_tr (Utils)

<section class='mojo-docs'>

`load_b_tr[mma_shape: IndexList[3]](tile: LayoutTensor[dtype, layout, origin, address_space=AddressSpace.SHARED, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> SIMD[dtype, 8]`

Loads the b operand tile for AMD tensor core MFMA instructions using transposed memory access.

This function supports double-rate MFMA shapes (32x32x16, 16x16x32) with bfloat16 input.
The input tile (shape = (mma\_shape\[2], mma\_shape\[1])) is split along the K dimension into
two halves of shape (MMA\_K//2, MMA\_N). Each half is loaded using `_load_tr16_b64_warp`, which
performs a transposed (column-major) load from shared memory. The resulting two 4-element SIMD
vectors are concatenated into a single `SIMD[tile.dtype, 8]` vector.

**Parameters:**

* â€‹mma\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The MMA instruction tile shape (only 32x32x16 or 16x16x32 supported).

**Args:**

* â€‹tile ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): A `LayoutTensor`, residing in shared memory, with shape (mma\_shape\[2], mma\_shape\[1])
  and dtype `DType.bfloat16`.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): SIMD\[tile.dtype, 8]: Concatenated transposed SIMD loads from both halves of the tile.

</section>

---

## pad

<section class='mojo-docs'>

`pad[dtype: DType, depth: Int, size: Int]() -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## gpu (Gpu)

<section class='mojo-docs'>

GPU attention operations.

## Packages

* [â€‹`amd`](./amd/): AMD GPU attention operations.

</section>

---

## attention (3)

<section class='mojo-docs'>

Attention operations.

## Packages

* [â€‹`gpu`](./gpu/): GPU attention operations.

</section>

---

## cpu_bicubic_kernel

<section class='mojo-docs'>

`cpu_bicubic_kernel(output_host: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_host: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Perform bicubic interpolation on a LayoutTensor of form NCHW.

**Args:**

* â€‹output\_host ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output tensor with desired dimensions.
* â€‹input\_host ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input tensor of shape \[B, C, H, W].

</section>

---

## cubic_kernel

<section class='mojo-docs'>

`cubic_kernel(x: Float32) -> Float32`

Cubic interpolation kernel matching PyTorch/torchvision's BICUBIC filter.

This uses the Catmull-Rom variant (Robidoux cubic) with a = -0.75,
which is what PyTorch uses in get\_cubic\_upsample\_coefficients.
([Source](https://github.com/pytorch/pytorch/blob/59eb61b2d1e4b64debbefa036acd0d8c7d55f0a3/aten/src/ATen/native/UpSample.h#L410-L423)).
This also matches OpenCV's [interpolateCubic](https://github.com/opencv/opencv/blob/cf2a3c8e7430cc92569dd7f114609f9377b12d9e/modules/imgproc/src/resize.cpp#L907-L915).

**Args:**

* â€‹x ([`Float32`](/mojo/std/builtin/simd/#float32)): Distance from the center point.

**Returns:**

[`Float32`](/mojo/std/builtin/simd/#float32): Weight contribution based on the distance.

`cubic_kernel(x: SIMD[dtype, size]) -> SIMD[dtype, size]`

Cubic interpolation kernel matching PyTorch/torchvision's BICUBIC filter.

This uses the Catmull-Rom variant (Robidoux cubic) with a = -0.75,
which is what PyTorch uses in get\_cubic\_upsample\_coefficients.
([Source](https://github.com/pytorch/pytorch/blob/59eb61b2d1e4b64debbefa036acd0d8c7d55f0a3/aten/src/ATen/native/UpSample.h#L410-L423)).
This also matches OpenCV's [interpolateCubic](https://github.com/opencv/opencv/blob/cf2a3c8e7430cc92569dd7f114609f9377b12d9e/modules/imgproc/src/resize.cpp#L907-L915).

**Args:**

* â€‹x ([`SIMD`](/mojo/std/builtin/simd/SIMD)): Distance from the center point.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): Weight contribution based on the distance.

</section>

---

## gpu_bicubic_kernel

<section class='mojo-docs'>

`gpu_bicubic_kernel[dtype: DType, input_layout: Layout, output_layout: Layout, address_space: AddressSpace = AddressSpace.GENERIC](output: LayoutTensor[dtype, output_layout, MutAnyOrigin, address_space=address_space], input: LayoutTensor[dtype, input_layout, MutAnyOrigin, address_space=address_space])`

Perform bicubic interpolation using GPU.

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output tensor with desired dimensions on the device.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input tensor of shape \[B, C, H, W] on the device.

</section>

---

## bicubic

<section class='mojo-docs'>

This module provides CPU and GPU implementations for bicubic interpolation.

Bicubic interpolation is a 2D extension of cubic interpolation for resampling
digital images. It uses the weighted average of the 4x4 neighborhood of pixels
around the target location to compute the interpolated value.

## Functions

* [â€‹`cpu_bicubic_kernel`](./cpu_bicubic_kernel): Perform bicubic interpolation on a LayoutTensor of form NCHW.
* [â€‹`cubic_kernel`](./cubic_kernel): Cubic interpolation kernel matching PyTorch/torchvision's BICUBIC filter.
* [â€‹`gpu_bicubic_kernel`](./gpu_bicubic_kernel): Perform bicubic interpolation using GPU.
* [â€‹`map_output_to_input_coord`](./map_output_to_input_coord): Map output pixel coordinate to input coordinate using center alignment. This implements the standard coordinate mapping for image resizing: input\_coord = (output\_coord + 0.5) \* scale - 0.5 The +0.5 and -0.5 terms ensure pixel centers are aligned properly. Args:     output\_coord: Output pixel coordinate.     scale: Scale factor (input\_size / output\_size). Returns:     Corresponding input coordinate as a float.
* [â€‹`resize_bicubic`](./resize_bicubic): Perform bicubic interpolation.

</section>

---

## map_output_to_input_coord

<section class='mojo-docs'>

`map_output_to_input_coord(output_coord: Int, scale: Float32) -> Float32`

Map output pixel coordinate to input coordinate using center alignment. This implements the standard coordinate mapping for image resizing: input\_coord = (output\_coord + 0.5) \* scale - 0.5 The +0.5 and -0.5 terms ensure pixel centers are aligned properly. Args:     output\_coord: Output pixel coordinate.     scale: Scale factor (input\_size / output\_size). Returns:     Corresponding input coordinate as a float.

**Returns:**

[`Float32`](/mojo/std/builtin/simd/#float32)

</section>

---

## resize_bicubic

<section class='mojo-docs'>

`resize_bicubic[dtype: DType, //, target: StringSlice[StaticConstantOrigin]](output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr)`

Perform bicubic interpolation.

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output tensor with desired dimensions on host or device.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input tensor of shape \[B, C, H, W] on host or device.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): Device context to enqueue GPU kernels on.

</section>

---

## broadcast

<section class='mojo-docs'>

`broadcast[dtype: DType](output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

For each axis of `input`, if the dimension is 1, duplicate the data at each index of the corresponding axis in `output`, otherwise copy over the entire axis to the corresponding axis in `output`.

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output buffer.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input buffer.

</section>

---

## broadcast_impl

<section class='mojo-docs'>

`broadcast_impl[dtype: DType](axis: Int, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_prev_axis_stride: Int, output_prev_axis_stride: Int, input_offset: Int, output_offset: Int, rightmost_broadcast_axis: Int)`

For each axis of `input` âˆˆ \[axis, rank), if the dimension is 1, duplicate the data at each index of the corresponding axis in `output`, otherwise copy over the entire axis to the corresponding axis in `output`.

**Args:**

* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis value.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output buffer.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input buffer.
* â€‹input\_prev\_axis\_stride ([`Int`](/mojo/std/builtin/int/Int)): The stride at axis `axis - 1` for input.
* â€‹output\_prev\_axis\_stride ([`Int`](/mojo/std/builtin/int/Int)): The stride at axis `axis - 1` for output.
* â€‹input\_offset ([`Int`](/mojo/std/builtin/int/Int)): The offset at which we start copying data from.
* â€‹output\_offset ([`Int`](/mojo/std/builtin/int/Int)): The offset at which we start copying data to.
* â€‹rightmost\_broadcast\_axis ([`Int`](/mojo/std/builtin/int/Int)): The largest axis at which we need to duplicate `input` data.

</section>

---

## broadcast (Broadcast)

<section class='mojo-docs'>

## Functions

* [â€‹`broadcast`](./broadcast): For each axis of `input`, if the dimension is 1, duplicate the data at each index of the corresponding axis in `output`, otherwise copy over the entire axis to the corresponding axis in `output`.
* [â€‹`broadcast_impl`](./broadcast_impl): For each axis of `input` âˆˆ \[axis, rank), if the dimension is 1, duplicate the data at each index of the corresponding axis in `output`, otherwise copy over the entire axis to the corresponding axis in `output`.

</section>

---

## concat (Concat)

<section class='mojo-docs'>

`concat[output_layout: Layout, inputs_layout: Layout, //, dtype: DType, single_thread_blocking_override: Bool, target: StringSlice[StaticConstantOrigin] = "cpu", epilogue_fn: OptionalReg[fn[c_type: DType, rank: Int, width: Int = 1, *, alignment: Int = 1](IndexList[rank], SIMD[c_type, width]) capturing -> None] = None](output: LayoutTensor[dtype, output_layout, origin], axis: Int, inputs: StaticTuple[LayoutTensor[dtype, inputs_layout, MutAnyOrigin], size], context: DeviceContextPtr = DeviceContextPtr())`

</section>

---

## concat_shape

<section class='mojo-docs'>

`concat_shape[inputs_layout: Layout, //, input_type: DType, single_thread_blocking_override: Bool](input_bufs: List[LayoutTensor[input_type, inputs_layout, MutAnyOrigin]], axis: Int) -> IndexList[inputs_layout.rank()]`

Compute the output shape of a `pad` operation, and assert the inputs are compatible.

**Parameters:**

* â€‹inputs\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Input layout of the input tensor.
* â€‹input\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input tensor.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.

**Args:**

* â€‹input\_bufs ([`List`](/mojo/std/collections/list/List)): The input tensors list.
* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## fused_concat

<section class='mojo-docs'>

`fused_concat[output_layout: Layout, //, dtype: DType, rank: Int, single_thread_blocking_override: Bool, input_fn: fn[input_index: Int, width: Int, _rank: Int](IndexList[_rank]) capturing -> SIMD[dtype, width], output_0_fn: elementwise_epilogue_type, target: StringSlice[StaticConstantOrigin] = "cpu"](axis: Int, input_shapes: StaticTuple[IndexList[rank], size], output: LayoutTensor[dtype, output_layout, origin], ctx: DeviceContextPtr)`

</section>

---

## concat (3)

<section class='mojo-docs'>

## `comptime` values

### `elementwise_epilogue_type`

`comptime elementwise_epilogue_type = fn[c_type: DType, rank: Int, width: Int = 1, *, alignment: Int = 1](IndexList[rank], SIMD[c_type, width]) capturing -> None`

## Functions

* [â€‹`concat`](./concat):
* [â€‹`concat_shape`](./concat_shape): Compute the output shape of a `pad` operation, and assert the inputs are compatible.
* [â€‹`fused_concat`](./fused_concat):
* [â€‹`memcpy_or_fuse`](./memcpy_or_fuse):

</section>

---

## memcpy_or_fuse

<section class='mojo-docs'>

`memcpy_or_fuse[rank: Int, dtype: DType, epilogue_fn: OptionalReg[fn[c_type: DType, rank: Int, width: Int = 1, *, alignment: Int = 1](IndexList[rank], SIMD[c_type, width]) capturing -> None]](dest_data: LegacyUnsafePointer[Int8], out_byte_offset: Int, src_data: LegacyUnsafePointer[Int8], n: Int, out_shape: IndexList[rank, element_type=element_type])`

</section>

---

## ConvDirectNHWC

<section class='mojo-docs'>

`struct ConvDirectNHWC[input_mut: Bool, filter_mut: Bool, conv_attr_rank: Int, //, input_layout: Layout, filter_layout: Layout, output_layout: Layout, input_origin: Origin[mut=input_mut], filter_origin: Origin[mut=filter_mut], output_origin: MutOrigin, input_type: DType, filter_type: DType, output_type: DType, filter_packed: Bool, conv_attr: ConvInfoStatic[conv_attr_rank], elementwise_epilogue: OptionalReg[fn[rank: Int](coords: IndexList[rank], f_size: Int) capturing -> None] = None]`

Implement the outer loops for direct convolution. Collapse N, HO, WO into one dimension n\_ho\_wo. Tile n\_ho\_wo, C, and F. The tile factor for C and F are chosen by a heuristic prioritizing C. n\_ho\_wo is tiled by micro kernel's height.

If n\_ho\_wo is large enough to spill LLC, we may need to tile n\_ho\_wo as the
outer most loop with a factor fit in LLC.

Assume F is divisible at least by simd\_size.

## Fields

* â€‹output (`LayoutTensor[output_type, output_layout, output_origin]`):
* â€‹input (`LayoutTensor[input_type, input_layout, input_origin]`):
* â€‹filter (`LayoutTensor[filter_type, filter_layout, filter_origin]`):
* â€‹conv\_shape (`ConvShape[conv_attr_rank]`):
* â€‹partition (`ConvPartition`):
* â€‹cf\_tile\_size (`IndexList[2]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `packed_and_fully_static`

`comptime packed_and_fully_static = filter_packed if filter_layout.shape.all_known() if output_layout.shape.all_known[1, output_layout.rank()]() if input_layout.shape.all_known[1, input_layout.rank()]() if conv_attr.all_known[conv_attr_rank]() else conv_attr.all_known[conv_attr_rank]() else input_layout.shape.all_known[1, input_layout.rank()]() if conv_attr.all_known[conv_attr_rank]() else conv_attr.all_known[conv_attr_rank]() else output_layout.shape.all_known[1, output_layout.rank()]() if input_layout.shape.all_known[1, input_layout.rank()]() if conv_attr.all_known[conv_attr_rank]() else conv_attr.all_known[conv_attr_rank]() else input_layout.shape.all_known[1, input_layout.rank()]() if conv_attr.all_known[conv_attr_rank]() else conv_attr.all_known[conv_attr_rank]() else filter_layout.shape.all_known() if output_layout.shape.all_known[1, output_layout.rank()]() if input_layout.shape.all_known[1, input_layout.rank()]() if conv_attr.all_known[conv_attr_rank]() else conv_attr.all_known[conv_attr_rank]() else input_layout.shape.all_known[1, input_layout.rank()]() if conv_attr.all_known[conv_attr_rank]() else conv_attr.all_known[conv_attr_rank]() else output_layout.shape.all_known[1, output_layout.rank()]() if input_layout.shape.all_known[1, input_layout.rank()]() if conv_attr.all_known[conv_attr_rank]() else conv_attr.all_known[conv_attr_rank]() else input_layout.shape.all_known[1, input_layout.rank()]() if conv_attr.all_known[conv_attr_rank]() else conv_attr.all_known[conv_attr_rank]()`

## Methods

### `run`

`static run(output: LayoutTensor[output_type, output_layout, output_origin], input: LayoutTensor[input_type, input_layout, input_origin], filter: LayoutTensor[filter_type, filter_layout, filter_origin], conv_shape: ConvShape[conv_attr_rank])`

### `is_new_c_accum`

`is_new_c_accum(self, c_idx: Int) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `update_output_tile_no_padding`

`update_output_tile_no_padding[micro_kernel_height: Int, micro_kernel_width: Int, c_fully_cached: Bool, has_residual: Bool, last_c_tile: Bool](self, n: Int, f_tile_offset: Int, f_tile_size: Int, c_tile_offset: Int, c_tile_size: Int, output_flat_coord: Int)`

### `output_space_flat_loop`

`output_space_flat_loop[micro_kernel_f_size: Int, has_residual: Bool, last_c_tile: Bool](self, n: Int, f_tile_offset: Int, f_tile_size: Int, c_tile_offset: Int, c_tile_size: Int)`

### `output_space_loop`

`output_space_loop[micro_kernel_height: Int, micro_kernel_width: Int, has_residual: Bool, last_c_tile: Bool](self, n: Int, f_tile_offset: Int, f_tile_size: Int, c_tile_offset: Int, c_tile_size: Int)`

### `output_space_loop_1d`

`output_space_loop_1d[micro_kernel_height: Int, micro_kernel_width: Int, has_residual: Bool, last_c_tile: Bool, output_dt: DType, input_dt: DType, filter_dt: DType](self, output: LegacyUnsafePointer[Scalar[output_dt]], input: LegacyUnsafePointer[Scalar[input_dt]], filter: LegacyUnsafePointer[Scalar[filter_dt]], n: Int, first_c_tile_in_group: Bool, c_tile_size: Int, f_tile_offset: Int, f_tile_size: Int, left_pad_impact_end: Int, right_pad_impact_start: Int)`

### `output_space_loop_2d`

`output_space_loop_2d[micro_kernel_height: Int, micro_kernel_width: Int, has_residual: Bool, last_c_tile: Bool, output_dt: DType, input_dt: DType, filter_dt: DType](self, output: LegacyUnsafePointer[Scalar[output_dt]], input: LegacyUnsafePointer[Scalar[input_dt]], filter: LegacyUnsafePointer[Scalar[filter_dt]], n: Int, first_c_tile_in_group: Bool, c_tile_size: Int, f_tile_offset: Int, f_tile_size: Int, left_pad_impact_end: Int, right_pad_impact_start: Int)`

### `output_space_loop_3d`

`output_space_loop_3d[micro_kernel_height: Int, micro_kernel_width: Int, has_residual: Bool, last_c_tile: Bool, output_dt: DType, input_dt: DType, filter_dt: DType](self, output: LegacyUnsafePointer[Scalar[output_dt]], input: LegacyUnsafePointer[Scalar[input_dt]], filter: LegacyUnsafePointer[Scalar[filter_dt]], n: Int, first_c_tile_in_group: Bool, c_tile_size: Int, f_tile_offset: Int, f_tile_size: Int, left_pad_impact_end: Int, right_pad_impact_start: Int)`

</section>

---

## CuDNNConvMeta

<section class='mojo-docs'>

`@register_passable`
`struct CuDNNConvMeta`

## Fields

* â€‹ptr\_handle (`LegacyUnsafePointer[cudnnContext]`):
* â€‹ptr\_input\_desc (`LegacyUnsafePointer[cudnnTensorStruct]`):
* â€‹ptr\_filter\_desc (`LegacyUnsafePointer[cudnnFilterStruct]`):
* â€‹ptr\_conv\_desc (`LegacyUnsafePointer[cudnnConvolutionStruct]`):
* â€‹ptr\_output\_desc (`LegacyUnsafePointer[cudnnTensorStruct]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = False`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(out self)`

### `__del__`

`__del__(deinit self)`

</section>

---

## Naive2dConvolution

<section class='mojo-docs'>

`struct Naive2dConvolution[output_type: DType, input_type: DType, filter_type: DType]`

Struct wrapper for naive 2d convolution implementation.

## Fields

* â€‹output (`LegacyUnsafePointer[Scalar[output_type]]`):
* â€‹input (`LegacyUnsafePointer[Scalar[input_type]]`):
* â€‹filter (`LegacyUnsafePointer[Scalar[filter_type]]`):
* â€‹pad\_d (`IndexList[2]`):
* â€‹pad\_h (`IndexList[2]`):
* â€‹pad\_w (`IndexList[2]`):
* â€‹stride (`IndexList[3]`):
* â€‹dilation (`IndexList[3]`):
* â€‹num\_groups (`Int`):
* â€‹output\_shape (`IndexList[5]`):
* â€‹input\_shape (`IndexList[5]`):
* â€‹filter\_shape (`IndexList[5]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(out self, output: LegacyUnsafePointer[Scalar[output_type]], input: LegacyUnsafePointer[Scalar[input_type]], filter: LegacyUnsafePointer[Scalar[filter_type]], output_shape: IndexList[5], input_shape: IndexList[5], filter_shape: IndexList[5], pad_d: IndexList[2], pad_h: IndexList[2], pad_w: IndexList[2], stride: IndexList[3], dilation: IndexList[3], num_groups: Int)`

### `run`

`static run(output: LegacyUnsafePointer[Scalar[output_type]], input: LegacyUnsafePointer[Scalar[input_type]], filter: LegacyUnsafePointer[Scalar[filter_type]], output_shape: IndexList[5], input_shape: IndexList[5], filter_shape: IndexList[5], pad_d: IndexList[2], pad_h: IndexList[2], pad_w: IndexList[2], stride: IndexList[3], dilation: IndexList[3], num_groups: Int)`

</section>

---

## accumulate_wo_tile_1d

<section class='mojo-docs'>

`accumulate_wo_tile_1d[micro_kernel_height: Int, micro_kernel_width: Int, simd_size: Int, partial_load_filter: Bool, effected_by_padding: Bool, input_dt: DType, filter_dt: DType](c_tile_size: Int, S: Int, mut acc: _Accumulator[dtype, num_rows, num_cols, simd_width, row_start, row_stop], input: LegacyUnsafePointer[Scalar[input_dt]], input_stride: Int, input_stride_to_nbr: Int, filter: LegacyUnsafePointer[Scalar[filter_dt]], filter_stride: Int, filter_stride_to_nbr: Int, partial_load_filter_size: Int, w: Int, W: Int, dilation: Int)`

Update one row in the output for a given (c, f) tile.

**Parameters:**

* â€‹micro\_kernel\_height ([`Int`](/mojo/std/builtin/int/Int)): Number of input points in register tiling.
* â€‹micro\_kernel\_width ([`Int`](/mojo/std/builtin/int/Int)): Number of SIMD resgiters assigned to F.
* â€‹simd\_size ([`Int`](/mojo/std/builtin/int/Int)): Number of elements in a SIMD register.
* â€‹partial\_load\_filter ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether using partial load for filter.
* â€‹effected\_by\_padding ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether the tile is effected by padding.
* â€‹input\_dt ([`DType`](/mojo/std/builtin/dtype/DType)): DType of input.
* â€‹filter\_dt ([`DType`](/mojo/std/builtin/dtype/DType)): DType of filter.

**Args:**

* â€‹c\_tile\_size ([`Int`](/mojo/std/builtin/int/Int)): Tile size in input channel.
* â€‹S ([`Int`](/mojo/std/builtin/int/Int)): Filter window width.
* â€‹acc ([`_Accumulator`](/mojo/kernels/linalg/accumulate/_Accumulator)): Pointer to register tile accumulator.
* â€‹input (`LegacyUnsafePointer`): Pointer to the first input point in WO tile.
* â€‹input\_stride ([`Int`](/mojo/std/builtin/int/Int)): Stride between two input points, i.e., C w/ NHWC layout.
* â€‹input\_stride\_to\_nbr ([`Int`](/mojo/std/builtin/int/Int)): Stride between an input point and its neighbor.
* â€‹filter (`LegacyUnsafePointer`): Pointer to the first coef in the filter window.
* â€‹filter\_stride ([`Int`](/mojo/std/builtin/int/Int)): Stride between two segments of size `micro_kernel_width * simd_size`.
* â€‹filter\_stride\_to\_nbr ([`Int`](/mojo/std/builtin/int/Int)): Stride between between two neighbor coefs, i.e.,
  CF w/ RSCF layout.
* â€‹partial\_load\_filter\_size ([`Int`](/mojo/std/builtin/int/Int)): Size of partial load for filter.
* â€‹w ([`Int`](/mojo/std/builtin/int/Int)): Coordinate in an input row.
* â€‹W ([`Int`](/mojo/std/builtin/int/Int)): Input width.
* â€‹dilation ([`Int`](/mojo/std/builtin/int/Int)): Convolution dilation.

</section>

---

## accumulate_wo_tile_2d

<section class='mojo-docs'>

`accumulate_wo_tile_2d[micro_kernel_height: Int, micro_kernel_width: Int, simd_size: Int, partial_load_filter: Bool, effected_by_padding: Bool, input_dt: DType, filter_dt: DType](c_tile_size: Int, RS: IndexList[2], mut acc: _Accumulator[dtype, num_rows, num_cols, simd_width, row_start, row_stop], input: LegacyUnsafePointer[Scalar[input_dt]], input_stride: Int, input_stride_to_nbr: IndexList[2], filter: LegacyUnsafePointer[Scalar[filter_dt]], filter_stride: Int, filter_stride_to_nbr: IndexList[2], partial_load_filter_size: Int, hw: IndexList[2], HW: IndexList[2], dilation: IndexList[2])`

</section>

---

## accumulate_wo_tile_3d

<section class='mojo-docs'>

`accumulate_wo_tile_3d[micro_kernel_height: Int, micro_kernel_width: Int, simd_size: Int, partial_load_filter: Bool, effected_by_padding: Bool, input_dt: DType, filter_dt: DType](c_tile_size: Int, QRS: IndexList[3], mut acc: _Accumulator[dtype, num_rows, num_cols, simd_width, row_start, row_stop], input: LegacyUnsafePointer[Scalar[input_dt]], input_stride: Int, input_stride_to_nbr: IndexList[3], filter: LegacyUnsafePointer[Scalar[filter_dt]], filter_stride: Int, filter_stride_to_nbr: IndexList[3], partial_load_filter_size: Int, dhw: IndexList[3], DHW: IndexList[3], dilation: IndexList[3])`

</section>

---

## check_cudnn_error

<section class='mojo-docs'>

`check_cudnn_error(stat: cudnnStatus_t)`

</section>

---

## conv1d_update_wo_tile

<section class='mojo-docs'>

`conv1d_update_wo_tile[micro_kernel_height: Int, micro_kernel_width: Int, simd_size: Int, filter_packed: Bool, effected_by_padding: Bool, has_residual: Bool, last_c_tile: Bool, output_dt: DType, input_dt: DType, filter_dt: DType, elementwise_epilogue: OptionalReg[fn[rank: Int](coords: IndexList[rank], f_size: Int) capturing -> None] = None](output: LegacyUnsafePointer[Scalar[output_dt]], input: LegacyUnsafePointer[Scalar[input_dt]], filter: LegacyUnsafePointer[Scalar[filter_dt]], first_c_tile: Bool, c_tile_size: Int, f_tile_offset: Int, f_tile_size: Int, conv_shape: ConvShape[rank], n: Int, wo: Int)`

</section>

---

## conv2d_gpu_naive_nhwc_rscf

<section class='mojo-docs'>

`conv2d_gpu_naive_nhwc_rscf[input_layout: Layout, filter_layout: Layout, output_layout: Layout, input_type: DType, filter_type: DType, output_type: DType, block_size: Int, maybe_epilogue_func: OptionalReg[fn[dtype: DType, rank: Int, width: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None]](input: LayoutTensor[input_type, input_layout, MutAnyOrigin], filter: LayoutTensor[filter_type, filter_layout, MutAnyOrigin], output: LayoutTensor[output_type, output_layout, MutAnyOrigin], stride: IndexList[2], dilation: IndexList[2], padding: IndexList[2])`

</section>

---

## conv2d_update_wo_tile

<section class='mojo-docs'>

`conv2d_update_wo_tile[micro_kernel_height: Int, micro_kernel_width: Int, simd_size: Int, filter_packed: Bool, effected_by_padding: Bool, has_residual: Bool, last_c_tile: Bool, output_dt: DType, input_dt: DType, filter_dt: DType, elementwise_epilogue: OptionalReg[fn[rank: Int](coords: IndexList[rank], f_size: Int) capturing -> None] = None](output: LegacyUnsafePointer[Scalar[output_dt]], input: LegacyUnsafePointer[Scalar[input_dt]], filter: LegacyUnsafePointer[Scalar[filter_dt]], first_c_tile: Bool, c_tile_size: Int, f_tile_offset: Int, f_tile_size: Int, conv_shape: ConvShape[2], n: Int, howo: IndexList[2])`

</section>

---

## conv3d_gpu_naive_ndhwc_qrscf

<section class='mojo-docs'>

`conv3d_gpu_naive_ndhwc_qrscf[input_layout: Layout, filter_layout: Layout, output_layout: Layout, input_type: DType, filter_type: DType, output_type: DType, block_size: Int, maybe_epilogue_func: OptionalReg[fn[dtype: DType, rank: Int, width: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None]](input: LayoutTensor[input_type, input_layout, MutAnyOrigin], filter: LayoutTensor[filter_type, filter_layout, MutAnyOrigin], output: LayoutTensor[output_type, output_layout, MutAnyOrigin], stride: IndexList[3], dilation: IndexList[3], padding: IndexList[3])`

</section>

---

## conv3d_update_wo_tile

<section class='mojo-docs'>

`conv3d_update_wo_tile[micro_kernel_height: Int, micro_kernel_width: Int, simd_size: Int, filter_packed: Bool, effected_by_padding: Bool, has_residual: Bool, last_c_tile: Bool, output_dt: DType, input_dt: DType, filter_dt: DType, elementwise_epilogue: OptionalReg[fn[rank: Int](coords: IndexList[rank], f_size: Int) capturing -> None] = None](output: LegacyUnsafePointer[Scalar[output_dt]], input: LegacyUnsafePointer[Scalar[input_dt]], filter: LegacyUnsafePointer[Scalar[filter_dt]], first_c_tile: Bool, c_tile_size: Int, f_tile_offset: Int, f_tile_size: Int, conv_shape: ConvShape[3], n: Int, dohowo: IndexList[3])`

</section>

---

## conv_cudnn

<section class='mojo-docs'>

`conv_cudnn[input_type: DType, filter_type: DType, output_type: DType](input: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter: LayoutTensor[filter_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[output_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], stride: IndexList[2], dilation: IndexList[2], padding: IndexList[2], num_groups: Int, ctx: DeviceContext)`

</section>

---

## conv_gpu

<section class='mojo-docs'>

`conv_gpu[conv_rank: Int, //, input_layout: Layout, filter_layout: Layout, output_layout: Layout, input_type: DType, filter_type: DType, output_type: DType, maybe_epilogue_func: OptionalReg[fn[dtype: DType, rank: Int, width: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None] = None, filter_is_fcrs: Bool = False](input: LayoutTensor[input_type, input_layout, MutAnyOrigin], filter: LayoutTensor[filter_type, filter_layout, MutAnyOrigin], output: LayoutTensor[output_type, output_layout, MutAnyOrigin], stride: IndexList[conv_rank], dilation: IndexList[conv_rank], padding: IndexList[(2 * conv_rank)], num_groups: Int, ctx: DeviceContext)`

</section>

---

## conv_nhwc_direct

<section class='mojo-docs'>

`conv_nhwc_direct[conv_info_rank: Int, //, input_layout: Layout, filter_layout: Layout, output_layout: Layout, input_type: DType, filter_type: DType, output_type: DType, filter_packed: Bool, conv_info_static: ConvInfoStatic[conv_info_rank], lambdas_have_fusion: Bool, elementwise_lambda: elementwise_simd_epilogue_type](input: LayoutTensor[input_type, input_layout, origin], filter: LayoutTensor[filter_type, filter_layout, origin], output: LayoutTensor[output_type, output_layout, origin], stride: IndexList[conv_info_rank], dilation: IndexList[conv_info_rank], pad_d: IndexList[2], pad_h: IndexList[2], pad_w: IndexList[2], num_groups: Int)`

</section>

---

## conv_shape

<section class='mojo-docs'>

`conv_shape[input_type: DType, filter_type: DType, strides_type: DType, dilations_type: DType, paddings_type: DType, single_thread_blocking_override: Bool](input_buf: LayoutTensor[input_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter_buf: LayoutTensor[filter_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], strides_buf: LayoutTensor[strides_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dilations_buf: LayoutTensor[dilations_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], paddings_buf: LayoutTensor[paddings_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], num_groups_scalar: Scalar[dtype]) -> IndexList[LayoutTensor[input_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

Compute the output shape of a `conv` operation, and assert the inputs are compatible.

**Parameters:**

* â€‹input\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input tensor.
* â€‹filter\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the filter tensor.
* â€‹strides\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the strides tensor.
* â€‹dilations\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the dilations tensor.
* â€‹paddings\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the paddings tensor.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  ssynchronouslysing a single thread.

**Args:**

* â€‹input\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹filter\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The filter tensor.
* â€‹strides\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The strides tensor.
* â€‹dilations\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The dilations tensor.
* â€‹paddings\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The paddings tensor.
* â€‹num\_groups\_scalar ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The num\_groups scalar.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## get_cudnn_dtype

<section class='mojo-docs'>

`get_cudnn_dtype[dtype: DType]() -> cudnnDataType_t`

Map Mojo DType to cuDNN data type.

Support only floating point dtypes for now.

**Returns:**

`cudnnDataType_t`

</section>

---

## conv

<section class='mojo-docs'>

## Structs

* [â€‹`ConvDirectNHWC`](./ConvDirectNHWC): Implement the outer loops for direct convolution. Collapse N, HO, WO into one dimension n\_ho\_wo. Tile n\_ho\_wo, C, and F. The tile factor for C and F are chosen by a heuristic prioritizing C. n\_ho\_wo is tiled by micro kernel's height.
* [â€‹`CuDNNConvMeta`](./CuDNNConvMeta):
* [â€‹`Naive2dConvolution`](./Naive2dConvolution): Struct wrapper for naive 2d convolution implementation.

## Functions

* [â€‹`accumulate_wo_tile_1d`](./accumulate_wo_tile_1d): Update one row in the output for a given (c, f) tile.
* [â€‹`accumulate_wo_tile_2d`](./accumulate_wo_tile_2d):
* [â€‹`accumulate_wo_tile_3d`](./accumulate_wo_tile_3d):
* [â€‹`check_cudnn_error`](./check_cudnn_error):
* [â€‹`conv1d_update_wo_tile`](./conv1d_update_wo_tile):
* [â€‹`conv2d_gpu_naive_nhwc_rscf`](./conv2d_gpu_naive_nhwc_rscf):
* [â€‹`conv2d_update_wo_tile`](./conv2d_update_wo_tile):
* [â€‹`conv3d_gpu_naive_ndhwc_qrscf`](./conv3d_gpu_naive_ndhwc_qrscf):
* [â€‹`conv3d_update_wo_tile`](./conv3d_update_wo_tile):
* [â€‹`conv_cudnn`](./conv_cudnn):
* [â€‹`conv_gpu`](./conv_gpu):
* [â€‹`conv_nhwc_direct`](./conv_nhwc_direct):
* [â€‹`conv_shape`](./conv_shape): Compute the output shape of a `conv` operation, and assert the inputs are compatible.
* [â€‹`get_cudnn_dtype`](./get_cudnn_dtype): Map Mojo DType to cuDNN data type.
* [â€‹`pack_conv_filter_shape`](./pack_conv_filter_shape): Compute the output shape of convolution filter packing.
* [â€‹`pack_filter`](./pack_filter): This packs the filter form RSCF to FRSCf. Use the default micro kernel size for dynamic shapes.
* [â€‹`pack_filter_shape`](./pack_filter_shape): Compute the shape of packed filter. The packed layout is FRSCf. shape\_ref should be allocated with size 5 outside this kernel.
* [â€‹`pack_filter_shape_impl`](./pack_filter_shape_impl): Compute the shape of packed filter. The packed layout is FRSCf. shape\_ref should be allocated with size 5 outside this kernel.

</section>

---

## pack_conv_filter_shape

<section class='mojo-docs'>

`pack_conv_filter_shape[single_thread_blocking_override: Bool](filter: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], num_groups: Int) -> IndexList[(LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank + 1)]`

Compute the output shape of convolution filter packing.

**Parameters:**

* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.

**Args:**

* â€‹filter ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The filter to be packed.
* â€‹num\_groups ([`Int`](/mojo/std/builtin/int/Int)): The number of groups in the convolution.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## pack_filter

<section class='mojo-docs'>

`pack_filter(filter: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], packed_filter: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], num_groups: Int)`

This packs the filter form RSCF to FRSCf. Use the default micro kernel size for dynamic shapes.

`pack_filter[simd_size: Int, micro_kernel_f_size: Int](filter: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], packed_filter: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], num_groups: Int)`

This packs the filter form RSCF to FRSCf.

F is first broken down to segments of size micro\_kernel\_f\_size, then the
remainder is further divided by simd\_size. The last residual elements if
any is padded with zero to fill simd\_size.

**Parameters:**

* â€‹simd\_size ([`Int`](/mojo/std/builtin/int/Int)): Can differ from the simd size of the input type.
* â€‹micro\_kernel\_f\_size ([`Int`](/mojo/std/builtin/int/Int)): The size of the last dimension in FRSCf, which is
  equals the size of the micro kernel's F dimension.

**Args:**

* â€‹filter ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Filter in RSCF layout (if 2D).
* â€‹packed\_filter ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Packed filter in FRSCf layout (if 2D).
  F       - the index of continuous segments in micro kernel.
  R, S, C - original R, S, C.
  f       - the index within a continuous segments.
* â€‹num\_groups ([`Int`](/mojo/std/builtin/int/Int)): The number of groups in the convolution.

</section>

---

## pack_filter_shape

<section class='mojo-docs'>

`pack_filter_shape[filter_type: DType, input_shape: DimList, filter_shape: DimList, output_shape: DimList, strides: DimList, dilations: DimList, paddings: DimList, num_groups: Int, single_thread_blocking_override: Bool](filter: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> IndexList[(LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank + 1)]`

Compute the shape of packed filter. The packed layout is FRSCf. shape\_ref should be allocated with size 5 outside this kernel.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## pack_filter_shape_impl

<section class='mojo-docs'>

`pack_filter_shape_impl[filter_type: DType](Q: Int, R: Int, S: Int, C: Int, F: Int, num_groups: Int) -> IndexList[6]`

Compute the shape of packed filter. The packed layout is FRSCf. shape\_ref should be allocated with size 5 outside this kernel.

**Args:**

* â€‹Q ([`Int`](/mojo/std/builtin/int/Int)): Original Q filter dimension.
* â€‹R ([`Int`](/mojo/std/builtin/int/Int)): Original R filter dimension.
* â€‹S ([`Int`](/mojo/std/builtin/int/Int)): Original S filter dimension.
* â€‹C ([`Int`](/mojo/std/builtin/int/Int)): Original C filter dimension.
* â€‹F ([`Int`](/mojo/std/builtin/int/Int)): Original F filter dimension.
* â€‹num\_groups ([`Int`](/mojo/std/builtin/int/Int)): Number of groups in the convolution.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## ConvTransposedPacked

<section class='mojo-docs'>

`struct ConvTransposedPacked[input_mut: Bool, input_element_layout: Layout, input_layout_int_type: DType, input_linear_idx_type: DType, input_masked: Bool, input_alignment: Int, filter_mut: Bool, filter_element_layout: Layout, filter_layout_int_type: DType, filter_linear_idx_type: DType, filter_masked: Bool, filter_alignment: Int, output_element_layout: Layout, output_layout_int_type: DType, output_linear_idx_type: DType, output_masked: Bool, output_alignment: Int, //, input_origin: Origin[mut=input_mut], filter_origin: Origin[mut=filter_mut], output_origin: MutOrigin, input_layout: Layout, filter_layout: Layout, output_layout: Layout, input_type: DType, filter_type: DType, output_type: DType, conv_attr: ConvInfoStatic[(input_layout.rank() - 2)], elementwise_epilogue: OptionalReg[fn[rank: Int](coords: IndexList[rank], f_size: Int) capturing -> None] = None]`

## Fields

* â€‹output (`LayoutTensor[output_type, output_layout, output_origin, element_layout=output_element_layout, layout_int_type=output_layout_int_type, linear_idx_type=output_linear_idx_type, masked=output_masked, alignment=output_alignment]`):
* â€‹input (`LayoutTensor[input_type, input_layout, input_origin, element_layout=input_element_layout, layout_int_type=input_layout_int_type, linear_idx_type=input_linear_idx_type, masked=input_masked, alignment=input_alignment]`):
* â€‹filter (`LayoutTensor[filter_type, filter_layout, filter_origin, element_layout=filter_element_layout, layout_int_type=filter_layout_int_type, linear_idx_type=filter_linear_idx_type, masked=filter_masked, alignment=filter_alignment]`):
* â€‹conv\_shape (`ConvShape[(input_layout.rank() - 2)]`):
* â€‹partition (`ConvPartition`):
* â€‹cf\_tile\_size (`IndexList[2]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `run`

`static run(output: LayoutTensor[output_type, output_layout, output_origin, element_layout=output_element_layout, layout_int_type=output_layout_int_type, linear_idx_type=output_linear_idx_type, masked=output_masked, alignment=output_alignment], input: LayoutTensor[input_type, input_layout, input_origin, element_layout=input_element_layout, layout_int_type=input_layout_int_type, linear_idx_type=input_linear_idx_type, masked=input_masked, alignment=input_alignment], filter: LayoutTensor[filter_type, filter_layout, filter_origin, element_layout=filter_element_layout, layout_int_type=filter_layout_int_type, linear_idx_type=filter_linear_idx_type, masked=filter_masked, alignment=filter_alignment], conv_shape: ConvShape[(input_layout.rank() - 2)])`

### `input_space_loop`

`input_space_loop[micro_kernel_height: Int, micro_kernel_width: Int, has_residual: Bool, last_c_tile: Bool](self, n: Int, f_tile_offset: Int, f_tile_size: Int, c_tile_offset: Int, c_tile_size: Int)`

### `input_space_loop_2d`

`input_space_loop_2d[micro_kernel_height: Int, micro_kernel_width: Int, has_residual: Bool, last_c_tile: Bool, output_dt: DType, input_dt: DType, filter_dt: DType](self, output: LegacyUnsafePointer[Scalar[output_dt]], input: LegacyUnsafePointer[Scalar[input_dt]], filter: LegacyUnsafePointer[Scalar[filter_dt]], n: Int, first_c_tile_in_group: Bool, c_tile_size: Int, f_tile_offset: Int, f_tile_size: Int, left_pad_impact_end: Int, right_pad_impact_start: Int)`

### `input_space_loop_3d`

`input_space_loop_3d[micro_kernel_height: Int, micro_kernel_width: Int, has_residual: Bool, last_c_tile: Bool, output_dt: DType, input_dt: DType, filter_dt: DType](self, output: LegacyUnsafePointer[Scalar[output_dt]], input: LegacyUnsafePointer[Scalar[input_dt]], filter: LegacyUnsafePointer[Scalar[filter_dt]], n: Int, first_c_tile_in_group: Bool, c_tile_size: Int, f_tile_offset: Int, f_tile_size: Int, left_pad_impact_end: Int, right_pad_impact_start: Int)`

### `apply_epilogue`

`apply_epilogue(self, n: Int, g: Int)`

</section>

---

## accumulate_wo_tile

<section class='mojo-docs'>

`accumulate_wo_tile[micro_kernel_height: Int, micro_kernel_width: Int, simd_size: Int, partial_load: Bool, output_dt: DType, input_dt: DType, filter_dt: DType](c_tile_size: Int, output: LegacyUnsafePointer[Scalar[output_dt]], output_stride: Int, input: LegacyUnsafePointer[Scalar[input_dt]], input_stride: Int, filter: LegacyUnsafePointer[Scalar[filter_dt]], filter_stride: Int, partial_load_size: Int)`

</section>

---

## conv_transpose_naive

<section class='mojo-docs'>

`conv_transpose_naive[dtype: DType](output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], stride: IndexList[3], dilation: IndexList[3], pad_d: IndexList[2], pad_h: IndexList[2], pad_w: IndexList[2])`

Implements the ConvTranspose operator from the MO spec.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input, output, and kernel tensors.

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output data tensor that contains the result of the convolution.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input data tensor from previous layer, with size of (N x H x W x C),
  where N is the batch size, C is the number of channels, and H and
  W are the height and width.
* â€‹filter ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The weight (kernel) tensor, with size of (kH x kW x M/groups x C),
  where C is the number of channels, kH and kW are the height and
  width of the kernel, and M is the number of feature maps.
* â€‹stride ([`IndexList`](/mojo/std/utils/index_/IndexList)): Stride along each spatial axis.
* â€‹dilation ([`IndexList`](/mojo/std/utils/index_/IndexList)): Dilation value along each spatial axis of the filter.
* â€‹pad\_d ([`IndexList`](/mojo/std/utils/index_/IndexList)): Padding in depth dimension.
* â€‹pad\_h ([`IndexList`](/mojo/std/utils/index_/IndexList)): Padding in height dimension.
* â€‹pad\_w ([`IndexList`](/mojo/std/utils/index_/IndexList)): Padding in width dimension.

</section>

---

## conv_transpose_shape

<section class='mojo-docs'>

`conv_transpose_shape[dtype: DType, strides_type: DType, dilations_type: DType, pads_type: DType, output_pads_type: DType, single_thread_blocking_override: Bool](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kernel: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], strides: LayoutTensor[strides_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dilations: LayoutTensor[dilations_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], pads: LayoutTensor[pads_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output_pads: LayoutTensor[output_pads_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> IndexList[LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

Compute the output shape of a `conv-transpose` operation, and assert the inputs are compatible.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Element type of the input and kernel tensor.
* â€‹strides\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Element type of the strides tensor.
* â€‹dilations\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Element type of the dilations tensor.
* â€‹pads\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Element type of the pads tensor.
* â€‹output\_pads\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Element type of the output\_pads tensor.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹kernel ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The kernel tensor.
* â€‹strides ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The strides tensor.
* â€‹dilations ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The dilations tensor.
* â€‹pads ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The paddings tensor.
* â€‹output\_pads ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output paddings tensor.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## conv_transposed_cpu

<section class='mojo-docs'>

`conv_transposed_cpu[input_layout: Layout, filter_layout: Layout, output_layout: Layout, input_type: DType, filter_type: DType, output_type: DType, filter_packed: Bool, filter_is_cfrs: Bool, lambdas_have_fusion: Bool, elementwise_lambda: fn[dtype: DType, rank: Int, width: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None](output: LayoutTensor[output_type, output_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[input_type, input_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter: LayoutTensor[filter_type, filter_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], stride: IndexList[(LayoutTensor[input_type, input_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank - 2)], dilation: IndexList[(LayoutTensor[input_type, input_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank - 2)], pad_d: IndexList[2], pad_h: IndexList[2], pad_w: IndexList[2])`

</section>

---

## conv_transposed_cudnn

<section class='mojo-docs'>

`conv_transposed_cudnn[input_type: DType, filter_type: DType, output_type: DType](input: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter: LayoutTensor[filter_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[output_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], stride: IndexList[2], dilation: IndexList[2], padding: IndexList[2], ctx: DeviceContext)`

</section>

---

## conv_transposed_gpu

<section class='mojo-docs'>

`conv_transposed_gpu[input_layout: Layout, filter_layout: Layout, output_layout: Layout, input_type: DType, filter_type: DType, output_type: DType, elementwise_epilogue: OptionalReg[fn[dtype: DType, rank: Int, width: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None] = None](output: LayoutTensor[output_type, output_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[input_type, input_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter: LayoutTensor[filter_type, filter_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], stride: IndexList[(LayoutTensor[input_type, input_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank - 2)], dilation: IndexList[(LayoutTensor[input_type, input_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank - 2)], padding: IndexList[(LayoutTensor[input_type, input_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank - 2)], ctx: DeviceContext)`

</section>

---

## get_num_partitions

<section class='mojo-docs'>

`get_num_partitions[micro_kernel_height: Int, micro_kernel_f_size: Int](num_threads: Int, conv_shape: ConvShape[rank]) -> IndexList[4]`

Partition the workload in (batch\&group, C, F, H) dimensions. HOWO is the combination of HO and WO dimensions. The actual number of tasks are the product of return num\_partitions.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## get_partition

<section class='mojo-docs'>

`get_partition(task_id: Int, num_partitions: IndexList[4], conv_shape: ConvShape[rank], micro_kernel_height: Int, micro_kernel_f_size: Int) -> ConvPartition`

**Returns:**

`ConvPartition`

</section>

---

## conv_transpose

<section class='mojo-docs'>

## Structs

* [â€‹`ConvTransposedPacked`](./ConvTransposedPacked):

## Functions

* [â€‹`accumulate_wo_tile`](./accumulate_wo_tile):
* [â€‹`conv_transpose_naive`](./conv_transpose_naive): Implements the ConvTranspose operator from the MO spec.
* [â€‹`conv_transpose_shape`](./conv_transpose_shape): Compute the output shape of a `conv-transpose` operation, and assert the inputs are compatible.
* [â€‹`conv_transposed_cpu`](./conv_transposed_cpu):
* [â€‹`conv_transposed_cudnn`](./conv_transposed_cudnn):
* [â€‹`conv_transposed_gpu`](./conv_transposed_gpu):
* [â€‹`get_num_partitions`](./get_num_partitions): Partition the workload in (batch\&group, C, F, H) dimensions. HOWO is the combination of HO and WO dimensions. The actual number of tasks are the product of return num\_partitions.
* [â€‹`get_partition`](./get_partition):
* [â€‹`pack_filter`](./pack_filter): This packs the filter form RSFC to FRSCf.
* [â€‹`pack_filter_shape`](./pack_filter_shape): Compute the output shape of transposed convolution filter packing.
* [â€‹`update_w_tile_2d`](./update_w_tile_2d):
* [â€‹`update_w_tile_3d`](./update_w_tile_3d):

</section>

---

## pack_filter (Conv_transpose)

<section class='mojo-docs'>

`pack_filter(filter: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], packed_filter: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], num_groups: Int)`

This packs the filter form RSFC to FRSCf.

</section>

---

## pack_filter_shape (Conv_transpose)

<section class='mojo-docs'>

`pack_filter_shape(filter: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], num_groups: Int) -> IndexList[(LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank + 1)]`

Compute the output shape of transposed convolution filter packing.

**Args:**

* â€‹filter ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The filter to be packed.
* â€‹num\_groups ([`Int`](/mojo/std/builtin/int/Int)): The number of groups in the convolution.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## update_w_tile_2d

<section class='mojo-docs'>

`update_w_tile_2d[micro_kernel_height: Int, micro_kernel_width: Int, simd_size: Int, effected_by_padding: Bool, has_residual: Bool, last_c_tile: Bool, output_dt: DType, input_dt: DType, filter_dt: DType](output: LegacyUnsafePointer[Scalar[output_dt]], input: LegacyUnsafePointer[Scalar[input_dt]], filter: LegacyUnsafePointer[Scalar[filter_dt]], _init_output: Bool, c_tile_size: Int, f_tile_offset: Int, f_tile_size: Int, conv_shape: ConvShape[2], n: Int, hw: IndexList[2])`

</section>

---

## update_w_tile_3d

<section class='mojo-docs'>

`update_w_tile_3d[micro_kernel_height: Int, micro_kernel_width: Int, simd_size: Int, effected_by_padding: Bool, has_residual: Bool, last_c_tile: Bool, output_dt: DType, input_dt: DType, filter_dt: DType](output: LegacyUnsafePointer[Scalar[output_dt]], input: LegacyUnsafePointer[Scalar[input_dt]], filter: LegacyUnsafePointer[Scalar[filter_dt]], _init_output: Bool, c_tile_size: Int, f_tile_offset: Int, f_tile_size: Int, conv_shape: ConvShape[3], n: Int, hw: IndexList[3])`

</section>

---

## ConvAlgorithm

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ConvAlgorithm`

## Fields

* â€‹value (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `Default`

`comptime Default = ConvAlgorithm(0)`

### `Direct`

`comptime Direct = ConvAlgorithm(2)`

### `Im2Col`

`comptime Im2Col = ConvAlgorithm(1)`

## Methods

### `__eq__`

`__eq__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## ConvInfoStatic

<section class='mojo-docs'>

`struct ConvInfoStatic[rank: Int]`

## Fields

* â€‹pad (`IntTuple`):
* â€‹stride (`IntTuple`):
* â€‹dilation (`IntTuple`):
* â€‹num\_groups (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = False`

## Methods

### `__init__`

`__init__(out self, pad: IntTuple, stride: IntTuple, dilation: IntTuple, num_groups: Int)`

`__init__(out self)`

`__init__(out self, pad: IntTuple, stride: IntTuple, dilation: IntTuple, input_c: Int, filter_c: Int)`

### `all_known`

`all_known(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `pad_left`

`pad_left(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `pad_bottom`

`pad_bottom(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `strides`

`strides(self) -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

### `dilations`

`dilations(self) -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## ConvPartition

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ConvPartition`

Work range for a partition.

## Fields

* â€‹ng\_offset (`Int`):
* â€‹ng\_size (`Int`):
* â€‹f\_offset (`Int`):
* â€‹f\_size (`Int`):
* â€‹ho\_or\_howo\_offset (`Int`):
* â€‹ho\_or\_howo\_size (`Int`):
* â€‹c\_offset (`Int`):
* â€‹c\_size (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `empty`

`empty(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## ConvShape

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ConvShape[rank: Int]`

A shape struct describing the convolution dimensions.

## Fields

* â€‹n (`Int`):
* â€‹input\_dims (`IndexList[rank]`):
* â€‹output\_dims (`IndexList[rank]`):
* â€‹filter\_dims (`IndexList[rank]`):
* â€‹c (`Int`):
* â€‹f (`Int`):
* â€‹stride (`IndexList[rank]`):
* â€‹dilation (`IndexList[rank]`):
* â€‹pad\_d (`IndexList[2]`):
* â€‹pad\_h (`IndexList[2]`):
* â€‹pad\_w (`IndexList[2]`):
* â€‹num\_groups (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `d`

`d(self) -> Int`

Input depth.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `h`

`h(self) -> Int`

Input height.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `w`

`w(self) -> Int`

Input width.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `do`

`do(self) -> Int`

Output depth.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `ho`

`ho(self) -> Int`

Output height.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `wo`

`wo(self) -> Int`

Output width.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `q`

`q(self) -> Int`

Filter window depth.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `r`

`r(self) -> Int`

Filter window height.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `s`

`s(self) -> Int`

Filter windown width.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `filter_window_flat_size`

`filter_window_flat_size(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `input_image_flat_size`

`input_image_flat_size(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `output_image_flat_size`

`output_image_flat_size(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `output_space_dims`

`output_space_dims(self) -> IndexList[rank]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

### `output_flat_coord_to_input_offset`

`output_flat_coord_to_input_offset(self, n: Int, output_flat_coord: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `matmul_M`

`matmul_M(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `matmul_N`

`matmul_N(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `matmul_K`

`matmul_K(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `padded`

`padded(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `c_per_group`

`c_per_group(self) -> Int`

Returns the number of channels per group. Channel count must be divisible by group size.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `f_per_group`

`f_per_group(self) -> Int`

Returns the number of filters per group. Filter count must be divisible by group size.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `f_to_group`

`f_to_group(self, f_idx: Int) -> Int`

Given a global filter idx, returns the group idx of the group the filter belongs to.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `c_to_group`

`c_to_group(self, c_idx: Int) -> Int`

Given a global channel idx, returns the group idx of the group the channel belongs to.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `f_in_group`

`f_in_group(self, f_idx: Int) -> Int`

Given a global filter idx, returns the offset of the filter in its group.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `c_in_group`

`c_in_group(self, c_idx: Int) -> Int`

Given a global channel idx, returns the offset of the channel in its group.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## align_down_residual

<section class='mojo-docs'>

`align_down_residual(value: Int, alignment: Int) -> Int`

Returns the remainder after aligning down value to alignment.

**Args:**

* â€‹value ([`Int`](/mojo/std/builtin/int/Int)): The value to align.
* â€‹alignment ([`Int`](/mojo/std/builtin/int/Int)): Value to align to.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The remainder after aligning down value to the closest multiple of
alignment. In other words, value - align\_down(value, alignment).

</section>

---

## append_shape

<section class='mojo-docs'>

`append_shape[rank: Int](in_shape: IndexList[rank], last2nd: Int, last: Int) -> IndexList[(rank + 2)]`

Append input shape by inserting `last2nd` and `last` at the end.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## extend_shape

<section class='mojo-docs'>

`extend_shape[rank: Int](in_shape: IndexList[rank], first: Int, last: Int) -> IndexList[(rank + 2)]`

Extend input shape by inserting `first` and `last` at both ends.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## get_conv2d_shape

<section class='mojo-docs'>

`get_conv2d_shape[output_layout: Layout, input_layout: Layout, filter_layout_param: Layout, dtype: DType, data_layout: Image2DLayout, filter_layout: Image2DLayout](output: LayoutTensor[dtype, output_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, input_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter: LayoutTensor[dtype, filter_layout_param, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], pad_h: IndexList[2], pad_w: IndexList[2], stride: IndexList[2], dilation: IndexList[2], num_groups: Int) -> ConvShape[2]`

**Returns:**

[`ConvShape`](/mojo/kernels/nn/conv_utils/ConvShape)

</section>

---

## get_conv_num_partitions

<section class='mojo-docs'>

`get_conv_num_partitions[micro_kernel_w: Int, micro_kernel_f: Int](num_threads: Int, conv_shape: ConvShape[rank]) -> IndexList[4]`

Partition the workload in (batch, C, F, HOWO) dimensions. HOWO is the combination of HO and WO dimensions. The actual number of tasks are the product of return num\_partitions.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## get_conv_num_tasks

<section class='mojo-docs'>

`get_conv_num_tasks(num_threads: Int, conv_shape: ConvShape[rank]) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## get_conv_shape

<section class='mojo-docs'>

`get_conv_shape[rank: Int, filter_packed: Bool](output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], stride: IndexList[rank], dilation: IndexList[rank], pad_d: IndexList[2], pad_h: IndexList[2], pad_w: IndexList[2], num_groups: Int) -> ConvShape[rank]`

**Returns:**

[`ConvShape`](/mojo/kernels/nn/conv_utils/ConvShape)

</section>

---

## get_conv_tile_shape

<section class='mojo-docs'>

`get_conv_tile_shape[dtype: DType](c: Int, filter_window_size: Int, micro_kernel_width: Int) -> IndexList[2]`

Compute the (c, f) tile shape in L2. Assume NHWC layout, the tile shape is (R, S, c\_tile, f\_tile). R and S are by default fully covered. The heuristic tried to block in C as much as possible. If C is small, it would start to block F.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## get_conv_tile_size

<section class='mojo-docs'>

`get_conv_tile_size[dtype: DType]() -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## get_direct_conv_micro_kernel_height

<section class='mojo-docs'>

`get_direct_conv_micro_kernel_height() -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## get_direct_conv_micro_kernel_width

<section class='mojo-docs'>

`get_direct_conv_micro_kernel_width() -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## get_micro_kernel_shape

<section class='mojo-docs'>

`get_micro_kernel_shape[rank: Int, WO: Int, F: Int, conv_attr: ConvInfoStatic[rank], simd_size: Int]() -> IndexList[2]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## get_partition (Conv_utils)

<section class='mojo-docs'>

`get_partition(task_id: Int, num_partitions: IndexList[4], conv_shape: ConvShape[rank], micro_kernel_height: Int, micro_kernel_f_size: Int) -> ConvPartition`

**Returns:**

`ConvPartition`

</section>

---

## conv_utils

<section class='mojo-docs'>

## `comptime` values

### `elementwise_epilogue_type`

`comptime elementwise_epilogue_type = fn[rank: Int](coords: IndexList[rank], f_size: Int) capturing -> None`

### `elementwise_simd_epilogue_type`

`comptime elementwise_simd_epilogue_type = fn[dtype: DType, rank: Int, width: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None`

## Structs

* [â€‹`ConvAlgorithm`](./ConvAlgorithm):
* [â€‹`ConvInfoStatic`](./ConvInfoStatic):
* [â€‹`ConvPartition`](./ConvPartition): Work range for a partition.
* [â€‹`ConvShape`](./ConvShape): A shape struct describing the convolution dimensions.

## Functions

* [â€‹`align_down_residual`](./align_down_residual): Returns the remainder after aligning down value to alignment.
* [â€‹`append_shape`](./append_shape): Append input shape by inserting `last2nd` and `last` at the end.
* [â€‹`extend_shape`](./extend_shape): Extend input shape by inserting `first` and `last` at both ends.
* [â€‹`get_conv2d_shape`](./get_conv2d_shape):
* [â€‹`get_conv_num_partitions`](./get_conv_num_partitions): Partition the workload in (batch, C, F, HOWO) dimensions. HOWO is the combination of HO and WO dimensions. The actual number of tasks are the product of return num\_partitions.
* [â€‹`get_conv_num_tasks`](./get_conv_num_tasks):
* [â€‹`get_conv_shape`](./get_conv_shape):
* [â€‹`get_conv_tile_shape`](./get_conv_tile_shape): Compute the (c, f) tile shape in L2. Assume NHWC layout, the tile shape is (R, S, c\_tile, f\_tile). R and S are by default fully covered. The heuristic tried to block in C as much as possible. If C is small, it would start to block F.
* [â€‹`get_conv_tile_size`](./get_conv_tile_size):
* [â€‹`get_direct_conv_micro_kernel_height`](./get_direct_conv_micro_kernel_height):
* [â€‹`get_direct_conv_micro_kernel_width`](./get_direct_conv_micro_kernel_width):
* [â€‹`get_micro_kernel_shape`](./get_micro_kernel_shape):
* [â€‹`get_partition`](./get_partition):
* [â€‹`reorder_padding`](./reorder_padding):

</section>

---

## reorder_padding

<section class='mojo-docs'>

`reorder_padding[rank: Int](pad: IntTuple) -> IntTuple`

**Returns:**

[`IntTuple`](/mojo/kernels/layout/int_tuple/IntTuple)

</section>

---

## cumsum

<section class='mojo-docs'>

`cumsum[dtype: DType, exclusive: Bool, reverse: Bool](output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], axis: Int)`

Implements the CumSum operator from the ONNX spec: <https://github.com/onnx/onnx/blob/main/docs/Operators.md#CumSum> Computes cumulative sum of the input elements along the given axis. Cumulative sum can be inclusive or exclusive of the top element, and normal or reverse (direction along a given axis).

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input and output tensors.
* â€‹exclusive ([`Bool`](/mojo/std/builtin/bool/Bool)): If set to True, return exclusive sum (top element not included).
* â€‹reverse ([`Bool`](/mojo/std/builtin/bool/Bool)): If set to True, perform cumsum operation in reverse direction.

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output tensor.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis on which to perform the cumsum operation.

</section>

---

## cumsum (Cumsum)

<section class='mojo-docs'>

## Functions

* [â€‹`cumsum`](./cumsum): Implements the CumSum operator from the ONNX spec: <https://github.com/onnx/onnx/blob/main/docs/Operators.md#CumSum> Computes cumulative sum of the input elements along the given axis. Cumulative sum can be inclusive or exclusive of the top element, and normal or reverse (direction along a given axis).

</section>

---

## flash_attention

<section class='mojo-docs'>

`flash_attention[dtype: DType, rank: Int, mask_rank: Int, //, input_k_fn: fn[simd_width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, simd_width], input_v_fn: fn[simd_width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, simd_width], input_mask_fn: fn[simd_width: Int, mask_rank: Int](IndexList[mask_rank]) capturing -> SIMD[dtype, simd_width]](q: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k_shape: IndexList[rank], v_shape: IndexList[rank], mask_shape: IndexList[mask_rank], output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scale: Float32, sink_weights: OptionalReg[LayoutTensor[dtype, Layout.row_major(-1), MutAnyOrigin]] = None)`

</section>

---

## flash_attention_kv_cache

<section class='mojo-docs'>

`flash_attention_kv_cache[dtype: DType, cache_t: KVCacheT, //](q: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: cache_t, v: cache_t, mask: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scale: Float32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], sink_weights: OptionalReg[LayoutTensor[dtype, Layout.row_major(-1), MutAnyOrigin]] = None)`

`flash_attention_kv_cache[dtype: DType, cache_t: KVCacheT, mask_t: MHAMask, //](q: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: cache_t, v: cache_t, mask: mask_t, scale: Float32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], sink_weights: OptionalReg[LayoutTensor[dtype, Layout.row_major(-1), MutAnyOrigin]] = None)`

`flash_attention_kv_cache[dtype: DType, cache_t: KVCacheT, mask_t: MHAMask, //](q: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q_input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: cache_t, v: cache_t, mask: mask_t, scale: Float32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], sink_weights: OptionalReg[LayoutTensor[dtype, Layout.row_major(-1), MutAnyOrigin]] = None)`

Entrypoint for ragged tensors.

</section>

---

## flash_attention_split_kv

<section class='mojo-docs'>

`flash_attention_split_kv[dtype: DType, rank: Int, mask_rank: Int, //, input_k_fn: fn[simd_width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, simd_width], input_v_fn: fn[simd_width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, simd_width], input_k_cache_fn: fn[simd_width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, simd_width], input_v_cache_fn: fn[simd_width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, simd_width], input_mask_fn: fn[simd_width: Int, mask_rank: Int](IndexList[mask_rank]) capturing -> SIMD[dtype, simd_width]](q: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k_shape: IndexList[rank], v_shape: IndexList[rank], k_cache_shape: IndexList[(rank + 1)], v_cache_shape: IndexList[(rank + 1)], mask_shape: IndexList[mask_rank], output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scale: Float32)`

Variant of flash attention that takes the previous KV cache `input_{k,v}_cache_fn` and the current KV tensors `input_k_fn` and `input_v_fn` as separate arguments.

This works around the fact that fusion can't currently look through concat.
So this kernel does an in-place concat fusion by changing the input lambdas
`input_{k,v}_cache_fn_wrapper` to take previous sequence KV elements from
the KV cache, and current KV elements from tensors `k` and `v`.

</section>

---

## flash_attention (Flash_attention)

<section class='mojo-docs'>

## Functions

* [â€‹`flash_attention`](./flash_attention):
* [â€‹`flash_attention_kv_cache`](./flash_attention_kv_cache):
* [â€‹`flash_attention_split_kv`](./flash_attention_split_kv): Variant of flash attention that takes the previous KV cache `input_{k,v}_cache_fn` and the current KV tensors `input_k_fn` and `input_v_fn` as separate arguments.

</section>

---

## fold

<section class='mojo-docs'>

`fold[dtype: DType, stride: Tuple[Int, Int], dilation: Tuple[Int, Int], padding: Tuple[Int, Int], target: StringSlice[StaticConstantOrigin]](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output_size: IndexList[2], kernel_size: IndexList[2], ctx: DeviceContextPtr)`

Folds array of sliding local blocks into a single output tensor.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type for the input and output.
* â€‹stride ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Stride of the sliding blocks.
* â€‹dilation ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): Dilation of the sliding blocks.
* â€‹padding ([`Tuple`](/mojo/std/builtin/tuple/Tuple)): 0-paddings to be added on both sides of the inputs.
* â€‹target (`StringSlice`): The target architecture to compile for.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input tensor to fold, shape \[N, C x kernel size, num\_blocks].
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output tensor to write to, shape \[N, C, H, W].
* â€‹output\_size ([`IndexList`](/mojo/std/utils/index_/IndexList)): Spatial shape of the output tensor (H, W).
* â€‹kernel\_size ([`IndexList`](/mojo/std/utils/index_/IndexList)): Size of the sliding blocks.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): DeviceContextPtr.

</section>

---

## fold_shape

<section class='mojo-docs'>

`fold_shape[dtype: DType](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output_size: IndexList[2], kernel_size: IndexList[2]) -> IndexList[4]`

Returns the shape of the output tensor of the fold operation.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## fold (Fold)

<section class='mojo-docs'>

Implements the fold operation.

## Functions

* [â€‹`fold`](./fold): Folds array of sliding local blocks into a single output tensor.
* [â€‹`fold_shape`](./fold_shape): Returns the shape of the output tensor of the fold operation.

</section>

---

## fused_qk_rope

<section class='mojo-docs'>

`fused_qk_rope[dtype: DType, collection_t: KVCollectionT, //, cache_t: KVCacheT, *, interleaved: Bool, target: StringSlice[StaticConstantOrigin]](q_proj: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: collection_t, freqs_cis: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], layer_idx: UInt32, valid_lengths: LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: Optional[DeviceContext])`

Applies RoPE to query and key tensors.

**Args:**

* â€‹q\_proj ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Query projection tensor of shape \[batch, seq\_len, n\_heads, head\_dim].
* â€‹kv\_collection (`collection_t`): The KV cache collection containing the key cache.
* â€‹freqs\_cis ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Frequency tensor for RoPE of shape \[max\_seq\_len, head\_dim].
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The layer index for accessing the correct cache.
* â€‹valid\_lengths ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor of shape \[batch] containing the valid length for each
  sequence. RoPE is only applied to positions within these lengths.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output tensor for Q with RoPE applied, same shape as q\_proj.
* â€‹context ([`Optional`](/mojo/std/collections/optional/Optional)): Optional device context for GPU execution.

</section>

---

## fused_qk_rope_ragged

<section class='mojo-docs'>

`fused_qk_rope_ragged[dtype: DType, freq_dtype: DType, collection_t: KVCollectionT, //, cache_t: KVCacheT, *, interleaved: Bool, target: StringSlice[StaticConstantOrigin], mrope_section: Optional[IntTuple] = None](q_proj: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: collection_t, freqs_cis: LayoutTensor[freq_dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], position_ids: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major[2](), MutAnyOrigin]], layer_idx: UInt32, output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: Optional[DeviceContext])`

Applies RoPE (Rotary Position Embedding) to query and key tensors.

This function can applies RoPE only to the last `rope_dim` elements of each
head, leaving the first `unroped_dim` elements unchanged. This is required
for DeepSeek models where only part of each head undergoes rotary
transformation.

</section>

---

## get_identity_rope_coeff

<section class='mojo-docs'>

`get_identity_rope_coeff[width: Int, dtype: DType]() -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## get_safetensors_idx

<section class='mojo-docs'>

`get_safetensors_idx(head_dim_idx: Int, head_size: Int) -> Tuple[Int, Int]`

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

</section>

---

## fused_qk_rope (Fused_qk_rope)

<section class='mojo-docs'>

## Functions

* [â€‹`fused_qk_rope`](./fused_qk_rope): Applies RoPE to query and key tensors.
* [â€‹`fused_qk_rope_ragged`](./fused_qk_rope_ragged): Applies RoPE (Rotary Position Embedding) to query and key tensors.
* [â€‹`get_identity_rope_coeff`](./get_identity_rope_coeff):
* [â€‹`get_safetensors_idx`](./get_safetensors_idx):
* [â€‹`rope_k_cache`](./rope_k_cache):
* [â€‹`rope_q_proj`](./rope_q_proj):

</section>

---

## rope_k_cache

<section class='mojo-docs'>

`rope_k_cache[freq_dtype: DType, cache_t: KVCacheT, width: Int, //, *, interleaved: Bool](k_cache: cache_t, b_idx: Int, h_idx: Int, s_idx: Int, d_idx: Int, freq_val: SIMD[freq_dtype, width], head_size: Int)`

</section>

---

## rope_q_proj

<section class='mojo-docs'>

`rope_q_proj[dtype: DType, freq_dtype: DType, rank: Int, width: Int, //, *, interleaved: Bool](q_proj: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], idx: IndexList[rank], freq_val: SIMD[freq_dtype, width], head_size: Int)`

</section>

---

## Axis

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct Axis`

## Fields

* â€‹axis (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Indexer`](/mojo/std/builtin/int/Indexer),
[`Intable`](/mojo/std/builtin/int/Intable),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(axis: Int) -> Self`

`__init__(out self, axis: Int, rank: Int)`

### `__int__`

`__int__(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `__mlir_index__`

`__mlir_index__(self) -> __mlir_type.index`

Convert to index.

**Returns:**

`__mlir_type.index`: The corresponding \_\_mlir\_type.index value.

</section>

---

## gather

<section class='mojo-docs'>

`gather[dtype: DType, indices_type: DType, //, *, axis: Int, target: StringSlice[StaticConstantOrigin] = "cpu"](output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], indices: LayoutTensor[indices_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], *, context: DeviceContext)`

Gather operation as defined in <https://github.com/onnx/onnx/blob/main/docs/Operators.md#Gather>.

Note that this is NOT the same as the default PyTorch gather (which is equivalent to
<https://github.com/onnx/onnx/blob/main/docs/Operators.md#gatherelements>).

`gather[dtype: DType, indices_type: DType, //, *, axis: Int, target: StringSlice[StaticConstantOrigin] = "cpu"](output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], indices: LayoutTensor[indices_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], *, context: DeviceContextPtr = DeviceContextPtr())`

Gather operation as defined in <https://github.com/onnx/onnx/blob/main/docs/Operators.md#Gather>.

Note that this is NOT the same as the default PyTorch gather (which is equivalent to
<https://github.com/onnx/onnx/blob/main/docs/Operators.md#gatherelements>).

`gather[*, dtype: DType, indices_type: DType, input_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], indices_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[indices_type, width], output_fn: fn[width: Int, rank: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None, prefetch_fn: OptionalReg[fn[input_rank: Int, indices_rank: Int](IndexList[input_rank], IndexList[indices_rank]) capturing -> None] = None, target: StringSlice[StaticConstantOrigin] = "cpu", single_thread_blocking_override: Bool = False](axis: Axis, input_shape: IndexList[size, element_type=element_type], indices_shape: IndexList[size, element_type=element_type], output_shape: IndexList[size, element_type=element_type], *, context: DeviceContext)`

Gather operation as defined in <https://github.com/onnx/onnx/blob/main/docs/Operators.md#Gather>.

Note that this is NOT the same as the default PyTorch gather (which is equivalent to
<https://github.com/onnx/onnx/blob/main/docs/Operators.md#gatherelements>).

`gather[*, dtype: DType, indices_type: DType, input_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], indices_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[indices_type, width], output_fn: fn[width: Int, rank: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None, prefetch_fn: OptionalReg[fn[input_rank: Int, indices_rank: Int](IndexList[input_rank], IndexList[indices_rank]) capturing -> None] = None, target: StringSlice[StaticConstantOrigin] = "cpu", single_thread_blocking_override: Bool = False](axis: Axis, input_shape: IndexList[size, element_type=element_type], indices_shape: IndexList[size, element_type=element_type], output_shape: IndexList[size, element_type=element_type], *, context: DeviceContextPtr = DeviceContextPtr())`

Gather operation as defined in <https://github.com/onnx/onnx/blob/main/docs/Operators.md#Gather>.

Note that this is NOT the same as the default PyTorch gather (which is equivalent to
<https://github.com/onnx/onnx/blob/main/docs/Operators.md#gatherelements>).

</section>

---

## gather_elements

<section class='mojo-docs'>

`gather_elements[input_type: DType, indices_type: DType](input: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], indices: LayoutTensor[indices_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], _axis: Int, output: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Implements ONNX GatherElements op which is equivalent to Pytorch gather.

</section>

---

## gather_elementwise_fn_wrapper

<section class='mojo-docs'>

`gather_elementwise_fn_wrapper[*, dtype: DType, indices_type: DType, input_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], indices_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[indices_type, width], output_fn: fn[width: Int, rank: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None, simd_width: Int, prefetch_fn: OptionalReg[fn[input_rank: Int, indices_rank: Int](IndexList[input_rank], IndexList[indices_rank]) capturing -> None] = None, error_index_fn: OptionalReg[fn(Int) capturing -> None] = None](axis: Axis, input_shape: IndexList[size, element_type=element_type], indices_shape: IndexList[size, element_type=element_type], output_shape: IndexList[size, element_type=element_type], coords: IndexList[size, element_type=element_type])`

</section>

---

## gather_guards

<section class='mojo-docs'>

`gather_guards(axis: Axis, input_shape: IndexList[size, element_type=element_type], indices_shape: IndexList[size, element_type=element_type], output_shape: IndexList[size, element_type=element_type])`

</section>

---

## gather_nd

<section class='mojo-docs'>

`gather_nd[dtype: DType, indices_type: DType, batch_dims: Int, target: StringSlice[StaticConstantOrigin] = "cpu", single_thread_blocking_override: Bool = False](data: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], indices: LayoutTensor[indices_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr)`

GatherND operation as defined in <https://github.com/onnx/onnx/blob/main/docs/Operators.md#GatherND>. Based on reference implementation: <https://github.com/onnx/onnx/blob/main/onnx/backend/test/case/node/gathernd.py>.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Type of data tensor.
* â€‹indices\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of indices tensor.
* â€‹batch\_dims ([`Int`](/mojo/std/builtin/int/Int)): Number of batch dimensions. The gather of indexing
  starts from dimension of data\[batch\_dims:].
* â€‹target (`StringSlice`): The target architecture to execute on.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.

**Args:**

* â€‹data ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor of rank data\_rank >= 1.
* â€‹indices ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor of rank indices\_rank >= 1. All index values are expected
  to be within bounds \[-s, s-1] along axis of size s. It is an
  error if any of the index values are out of bounds.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor of rank data\_rank + indices\_rank - indices\_shape\[-1] - 1 - b.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The DeviceContextPtr as prepared by the graph compiler.

</section>

---

## gather_nd_shape

<section class='mojo-docs'>

`gather_nd_shape[output_rank: Int, input_type: DType, indices_type: DType, batch_dims: Int, single_thread_blocking_override: Bool = True](input_buf: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], indices_buf: LayoutTensor[indices_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> IndexList[output_rank]`

Compute the output shape of a `gather` operation, and assert the inputs are compatible.

**Parameters:**

* â€‹output\_rank ([`Int`](/mojo/std/builtin/int/Int)): Rank of the output tensor.
* â€‹input\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input tensor.
* â€‹indices\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the indices tensor.
* â€‹batch\_dims ([`Int`](/mojo/std/builtin/int/Int)): Batch dimensions.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then reduction is run
  synchronously using a single thread.

**Args:**

* â€‹input\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹indices\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The indices tensor.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## gather_reduce

<section class='mojo-docs'>

`gather_reduce[dtype: DType, gather_axis: Int, reduce_axis: Int, simd_width: Int, reduce_fn: fn[dtype: DType, width: Int](SIMD[dtype, width], SIMD[dtype, width]) -> SIMD[dtype, width]](output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], indices: LayoutTensor[DType.int32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], reduce_init: Scalar[dtype])`

Computes output\[i, j, k] = input\[indices\[i, j], k] and simultaneously reduces the output across axis 1 to produce output\[i, k].

The motivating use-case for this is multi-hot embeddings in recommender models.
This provides similar functionality to Torch's EmbeddingBag layer. In that
context, i is the batch dimension, j is the multi-hot dimension, and k is
the embedding dimension.

</section>

---

## gather_shape

<section class='mojo-docs'>

`gather_shape[output_rank: Int, input_type: DType, indices_type: DType, single_thread_blocking_override: Bool = False](input_buf: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], indices_buf: LayoutTensor[indices_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], axis: Int) -> IndexList[output_rank]`

Compute the output shape of a `gather` operation, and assert the inputs are compatible.

**Parameters:**

* â€‹output\_rank ([`Int`](/mojo/std/builtin/int/Int)): Rank of the output tensor.
* â€‹input\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input tensor.
* â€‹indices\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the indices tensor.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.

**Args:**

* â€‹input\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹indices\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The indices tensor.
* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## gather_scatter

<section class='mojo-docs'>

## `comptime` values

### `error_index_fn_type`

`comptime error_index_fn_type = fn(Int) capturing -> None`

## Structs

* [â€‹`Axis`](./Axis):

## Functions

* [â€‹`gather`](./gather): Gather operation as defined in <https://github.com/onnx/onnx/blob/main/docs/Operators.md#Gather>.
* [â€‹`gather_elements`](./gather_elements): Implements ONNX GatherElements op which is equivalent to Pytorch gather.
* [â€‹`gather_elementwise_fn_wrapper`](./gather_elementwise_fn_wrapper):
* [â€‹`gather_guards`](./gather_guards):
* [â€‹`gather_nd`](./gather_nd): GatherND operation as defined in <https://github.com/onnx/onnx/blob/main/docs/Operators.md#GatherND>. Based on reference implementation: <https://github.com/onnx/onnx/blob/main/onnx/backend/test/case/node/gathernd.py>.
* [â€‹`gather_nd_shape`](./gather_nd_shape): Compute the output shape of a `gather` operation, and assert the inputs are compatible.
* [â€‹`gather_reduce`](./gather_reduce): Computes output\[i, j, k] = input\[indices\[i, j], k] and simultaneously reduces the output across axis 1 to produce output\[i, k].
* [â€‹`gather_shape`](./gather_shape): Compute the output shape of a `gather` operation, and assert the inputs are compatible.
* [â€‹`normalize_neg_index`](./normalize_neg_index): Indices passed to gather and scatter ops may be negative. This performs a normalization so that they can be used to index into a buffer.
* [â€‹`scatter_elements`](./scatter_elements): Implements ONNX ScatterElements op which is equivalent to Pytorch scatter.
* [â€‹`scatter_elements_shape`](./scatter_elements_shape): Compute the output shape of a `scatter_elements` operation, and assert the inputs are compatible.
* [â€‹`scatter_nd`](./scatter_nd): Scatter\_nd operation without any reduction.
* [â€‹`scatter_nd_generator`](./scatter_nd_generator): Implements ONNX ScatterND operation as defined in <https://github.com/onnx/onnx/blob/main/docs/Operators.md#ScatterND>.
* [â€‹`scatter_nd_shape`](./scatter_nd_shape): Compute the output shape of a `scatter_nd` operation, and assert the inputs are compatible.
* [â€‹`scatter_set_constant`](./scatter_set_constant): Scatter the fill\_value into the data at the specified indices.

</section>

---

## normalize_neg_index

<section class='mojo-docs'>

`normalize_neg_index(idx: Int, dim_size: Int) -> Int`

Indices passed to gather and scatter ops may be negative. This performs a normalization so that they can be used to index into a buffer.

Returns val + dim if val < 0 else val

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

`normalize_neg_index[dtype: DType, width: Int, out_type: DType = DType.index](idx: SIMD[dtype, width], dim_size: Int) -> SIMD[out_type, width]`

Indices passed to gather and scatter ops may be negative. This performs a normalization so that they can be used to index into a buffer.

Returns val + dim if val < 0 else val

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## scatter_elements

<section class='mojo-docs'>

`scatter_elements[reduce_fn: fn[dtype: DType, width: Int](SIMD[dtype, width], SIMD[dtype, width]) capturing -> SIMD[dtype, width], rank: Int, input_type: DType, indices_type: DType](input: ManagedTensorSlice[io_spec, static_spec=static_spec], indices: ManagedTensorSlice[io_spec, static_spec=static_spec], updates: ManagedTensorSlice[io_spec, static_spec=static_spec], _axis: Int, output: ManagedTensorSlice[io_spec, static_spec=static_spec])`

Implements ONNX ScatterElements op which is equivalent to Pytorch scatter.

</section>

---

## scatter_elements_shape

<section class='mojo-docs'>

`scatter_elements_shape[input_type: DType, indices_type: DType, //, *, single_thread_blocking_override: Bool](input: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], updates: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], indices: LayoutTensor[indices_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], axis: Int) -> IndexList[LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

Compute the output shape of a `scatter_elements` operation, and assert the inputs are compatible.

**Parameters:**

* â€‹input\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input tensor.
* â€‹indices\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the indices tensor.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹updates ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹indices ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The indices tensor.
* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## scatter_nd

<section class='mojo-docs'>

`scatter_nd[output_type: DType, indices_type: DType, single_thread_blocking_override: Bool, target: StringSlice[StaticConstantOrigin] = "cpu"](data: LayoutTensor[output_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], indices: LayoutTensor[indices_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], updates: LayoutTensor[output_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[output_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr = DeviceContextPtr())`

Scatter\_nd operation without any reduction.

</section>

---

## scatter_nd_generator

<section class='mojo-docs'>

`scatter_nd_generator[output_type: DType, indices_type: DType, single_thread_blocking_override: Bool, target: StringSlice[StaticConstantOrigin] = "cpu", /, reduce_fn: OptionalReg[fn[dtype: DType, width: Int](SIMD[dtype, width], SIMD[dtype, width]) capturing -> SIMD[dtype, width]] = None, *, _trace_description: StringSlice[StaticConstantOrigin] = "scatter_nd"](data: LayoutTensor[output_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], indices: LayoutTensor[indices_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], updates: LayoutTensor[output_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[output_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr = DeviceContextPtr())`

Implements ONNX ScatterND operation as defined in <https://github.com/onnx/onnx/blob/main/docs/Operators.md#ScatterND>.

**Parameters:**

* â€‹output\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of data, updates, and output tensors.
* â€‹indices\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the indices tensor.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.
* â€‹target (`StringSlice`): Target cpu or cuda.
* â€‹reduce\_fn ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Reduction function to apply: none (default), add, mul, max,
  min.
* â€‹\_trace\_description (`StringSlice`): A description of the function, used for profiling and tracing.

**Args:**

* â€‹data ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor of rank data\_rank >= 1.
* â€‹indices ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor of rank indices\_rank containing indices for the scatter
  operation.
* â€‹updates ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor containing values to update output tensor based on
  indices tensor.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor of rank data\_rank, shaped the same as data tensor.
* â€‹context ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): Pointer to DeviceContext.

</section>

---

## scatter_nd_shape

<section class='mojo-docs'>

`scatter_nd_shape[input_type: DType, indices_type: DType, single_thread_blocking_override: Bool](input: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], updates: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], indices: LayoutTensor[indices_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> IndexList[LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

Compute the output shape of a `scatter_nd` operation, and assert the inputs are compatible.

**Parameters:**

* â€‹input\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input tensor.
* â€‹indices\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the indices tensor.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹updates ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹indices ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The indices tensor.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## scatter_set_constant

<section class='mojo-docs'>

`scatter_set_constant[data_type: DType, index_type: DType, //, target: StringSlice[StaticConstantOrigin], single_thread_blocking_override: Bool = False](data: LayoutTensor[data_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], indices: LayoutTensor[index_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], fill_value: Scalar[data_type], ctx: DeviceContextPtr)`

Scatter the fill\_value into the data at the specified indices.

Example:
Suppose we have a 3x3 matrix `data` initialized to zeros:

data = [\[0, 0, 0],
\[0, 0, 0],
\[0, 0, 0]]

And `indices` is a 2D tensor with shape \[2, 2]:

indices = [\[0, 1],
\[2, 0]]

If `fill_value` is 5, after calling `scatter_set_constant`, `data` will be:

data = [\[0, 5, 0],
\[0, 0, 0],
\[5, 0, 0]]

Arguments:
data: The data to scatter the updates into.
indices: The indices to scatter the updates into.
fill\_value: The value to fill the data with.
ctx: The device context.

</section>

---

## Image2DLayout

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct Image2DLayout`

## Fields

* â€‹value (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `FRSCf`

`comptime FRSCf = Image2DLayout(3)`

### `NCHW`

`comptime NCHW = Image2DLayout(1)`

### `NHWC`

`comptime NHWC = Image2DLayout(0)`

### `RSCF`

`comptime RSCF = Image2DLayout(2)`

### `UNKNOWN`

`comptime UNKNOWN = Image2DLayout(-1)`

## Methods

### `__eq__`

`__eq__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## ImageData

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ImageData[layout: Layout, dtype: DType, static_image_layout: Image2DLayout, origin: MutOrigin]`

Utility class that generalizes conv2d data and filter tensor with a given data layout.

## Fields

* â€‹data (`LayoutTensor[dtype, layout, origin]`):
* â€‹dynamic\_image\_layout (`Image2DLayout`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(data: LayoutTensor[dtype, layout, origin], _layout: Image2DLayout) -> Self`

Construct of an image data instance with dynamic layout param.

**Args:**

* â€‹data ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): A 4d buffer containing the actual data.
* â€‹\_layout ([`Image2DLayout`](/mojo/kernels/nn/image/Image2DLayout)): Data layout tag.

`__init__(data: LayoutTensor[dtype, layout, origin]) -> Self`

### `__getitem__`

`__getitem__(self, n: Int, c: Int, h: Int, w: Int) -> Scalar[dtype]`

Reads the underlying data buffer based on the tensor index and under- lying data layout.

**Args:**

* â€‹n ([`Int`](/mojo/std/builtin/int/Int)): Index on the batch dimension.
* â€‹c ([`Int`](/mojo/std/builtin/int/Int)): Index on the channel dimension.
* â€‹h ([`Int`](/mojo/std/builtin/int/Int)): Index on the height dimension.
* â€‹w ([`Int`](/mojo/std/builtin/int/Int)): Index on the width dimension.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The value stored at the given index position.

### `__setitem__`

`__setitem__(self, n: Int, c: Int, h: Int, w: Int, value: Scalar[dtype])`

Writes the underlying data buffer based on the tensor index and under- lying data layout.

**Args:**

* â€‹n ([`Int`](/mojo/std/builtin/int/Int)): Index on the batch dimension.
* â€‹c ([`Int`](/mojo/std/builtin/int/Int)): Index on the channel dimension.
* â€‹h ([`Int`](/mojo/std/builtin/int/Int)): Index on the height dimension.
* â€‹w ([`Int`](/mojo/std/builtin/int/Int)): Index on the width dimension.
* â€‹value ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The value to store at the given index position.

### `to_static_layout`

`to_static_layout[new_static_image_layout: Image2DLayout](self) -> ImageData[layout, dtype, new_static_image_layout, origin]`

Conversion utility from a fully dynamic data structure, e.g. from c shim to one with compile-time known data layout.

**Returns:**

`ImageData`: The image data with static data layout.

### `get_image_layout`

`get_image_layout(self) -> Image2DLayout`

The getter function of the underlying data layout, resolving from either statically or dynamically provided information.

**Returns:**

[`Image2DLayout`](/mojo/kernels/nn/image/Image2DLayout): The resolved data layout tag for this image instance.

### `get_flat_index`

`get_flat_index(self, n: Int, c: Int, h: Int, w: Int) -> Int`

Converts the dimension index to the flat index of the underlying data based on the tensor layout.

**Args:**

* â€‹n ([`Int`](/mojo/std/builtin/int/Int)): Index on the batch dimension.
* â€‹c ([`Int`](/mojo/std/builtin/int/Int)): Index on the channel dimension.
* â€‹h ([`Int`](/mojo/std/builtin/int/Int)): Index on the height dimension.
* â€‹w ([`Int`](/mojo/std/builtin/int/Int)): Index on the width dimension.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): An integer containing the index based on the underlying
data layout.

### `get_tuple_index`

`get_tuple_index(self, idx: Int) -> IndexList[4]`

Converts the flat index to the dimension index of the underlying data based on the tensor layout.

**Args:**

* â€‹idx ([`Int`](/mojo/std/builtin/int/Int)): Flat index.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): A IndexList containing the index in NCHW order.

### `num_elements`

`num_elements(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## ImageShape

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ImageShape`

A data-layout agnostic representation of tensor shapes used in conv2d.

## Fields

* â€‹N (`Int`):
* â€‹C (`Int`):
* â€‹H (`Int`):
* â€‹W (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__[layout: Layout, dtype: DType, image_layout: Image2DLayout](image_data: ImageData[layout, dtype, image_layout, origin]) -> Self`

Constructor of an ImageShape instance from an ImageData.

**Args:**

* â€‹image\_data ([`ImageData`](/mojo/kernels/nn/image/ImageData)): The image data instance to extract shape
  info from.

</section>

---

## PadHandling

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct PadHandling`

## Fields

* â€‹value (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `EXCLUDE_PAD`

`comptime EXCLUDE_PAD = PadHandling(0)`

### `INCLUDE_PAD`

`comptime INCLUDE_PAD = PadHandling(2)`

## Methods

### `__eq__`

`__eq__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## image

<section class='mojo-docs'>

## Structs

* [â€‹`Image2DLayout`](./Image2DLayout):
* [â€‹`ImageData`](./ImageData): Utility class that generalizes conv2d data and filter tensor with a given data layout.
* [â€‹`ImageShape`](./ImageShape): A data-layout agnostic representation of tensor shapes used in conv2d.
* [â€‹`PadHandling`](./PadHandling):

</section>

---

## nn

<section class='mojo-docs'>

Provides neural network operators for deep learning models.

## Packages

* [â€‹`attention`](./attention/): Attention operations.

## Modules

* [â€‹`activations`](./activations/): The module contains implementations of activation functions.
* [â€‹`arange`](./arange/):
* [â€‹`arg_nonzero`](./arg_nonzero/):
* [â€‹`argmaxmin`](./argmaxmin/):
* [â€‹`argmaxmin_gpu`](./argmaxmin_gpu/):
* [â€‹`argsort`](./argsort/):
* [â€‹`bicubic`](./bicubic/): This module provides CPU and GPU implementations for bicubic interpolation.
* [â€‹`broadcast`](./broadcast/):
* [â€‹`concat`](./concat/):
* [â€‹`conv`](./conv/):
* [â€‹`conv_transpose`](./conv_transpose/):
* [â€‹`conv_utils`](./conv_utils/):
* [â€‹`cumsum`](./cumsum/):
* [â€‹`flash_attention`](./flash_attention/):
* [â€‹`fold`](./fold/): Implements the fold operation.
* [â€‹`fused_qk_rope`](./fused_qk_rope/):
* [â€‹`gather_scatter`](./gather_scatter/):
* [â€‹`image`](./image/):
* [â€‹`index_tensor`](./index_tensor/):
* [â€‹`irfft`](./irfft/): Inverse real FFT kernel using cuFFT.
* [â€‹`kv_cache`](./kv_cache/):
* [â€‹`kv_cache_ragged`](./kv_cache_ragged/):
* [â€‹`mha`](./mha/):
* [â€‹`mha_cross`](./mha_cross/):
* [â€‹`mha_fa3_utils`](./mha_fa3_utils/):
* [â€‹`mha_mask`](./mha_mask/):
* [â€‹`mha_operand`](./mha_operand/):
* [â€‹`mha_score_mod`](./mha_score_mod/):
* [â€‹`mha_sm100_1q`](./mha_sm100_1q/):
* [â€‹`mha_sm100_2q`](./mha_sm100_2q/):
* [â€‹`mha_sm90`](./mha_sm90/):
* [â€‹`mha_tile_scheduler`](./mha_tile_scheduler/):
* [â€‹`mha_utils`](./mha_utils/):
* [â€‹`mla`](./mla/):
* [â€‹`mla_decode_sm100`](./mla_decode_sm100/):
* [â€‹`mla_graph`](./mla_graph/):
* [â€‹`mla_prefill_sm100`](./mla_prefill_sm100/):
* [â€‹`moe`](./moe/):
* [â€‹`nms`](./nms/):
* [â€‹`normalization`](./normalization/):
* [â€‹`pad`](./pad/):
* [â€‹`pad_gpu`](./pad_gpu/):
* [â€‹`pool`](./pool/):
* [â€‹`rand_normal`](./rand_normal/):
* [â€‹`rand_uniform`](./rand_uniform/):
* [â€‹`randn`](./randn/):
* [â€‹`repeat_interleave`](./repeat_interleave/):
* [â€‹`reshape`](./reshape/):
* [â€‹`resize`](./resize/):
* [â€‹`roi_align`](./roi_align/):
* [â€‹`rope`](./rope/):
* [â€‹`sampling`](./sampling/):
* [â€‹`shapes`](./shapes/):
* [â€‹`slice`](./slice/):
* [â€‹`softmax`](./softmax/):
* [â€‹`spatial_merge`](./spatial_merge/):
* [â€‹`split`](./split/):
* [â€‹`tile`](./tile/):
* [â€‹`topk`](./topk/):
* [â€‹`topk_fi`](./topk_fi/):
* [â€‹`toppminp`](./toppminp/):
* [â€‹`toppminp_gpu`](./toppminp_gpu/):

</section>

---

## advanced_indexing_getitem

<section class='mojo-docs'>

`advanced_indexing_getitem[input_rank: Int, index_rank: Int, input_type: DType, index_type: DType, //, start_axis: Int, num_index_tensors: Int, target: StringSlice[StaticConstantOrigin], single_thread_blocking_override: Bool, trace_description: StringSlice[StaticConstantOrigin], input_tensor_fn: fn[width: Int](IndexList[input_rank]) capturing -> SIMD[input_type, width], indices_fn: fn[indices_index: Int](IndexList[index_rank]) capturing -> Scalar[index_type]](out_tensor: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], in_tensor_strides: IndexList[input_rank], ctx: DeviceContextPtr)`

Implement basic numpy-style advanced indexing.

This is designed to be fused with other view-producing operations to
implement full numpy-indexing semantics.

This assumes the dimensions in `input_tensor` not indexed by index tensors
are ":", ie selecting all indices along the slice. For example in numpy:

```
# rank(indices1) == 3
# rank(indices2) == 3
out_tensor = input_tensor[:, :, :, indices1, indices2, :, :]
```

We calculate the following for all valid valued indexing variables:

```
out_tensor[a, b, c, i, j, k, d, e] = input_tensor[
    a, b, c,
    indices1[i, j, k],
    indices2[i, j, k],
    d, e
]
```

In this example `start_axis = 3` and `num_index_tensors = 2`.

TODO(GEX-1951): Support boolean tensor mask support
TODO(GEX-1952): Support non-contiguous indexing tensor case
TODO(GEX-1953): Support fusion (especially view-fusion)

**Parameters:**

* â€‹input\_rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the input tensor.
* â€‹index\_rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the indexing tensors.
* â€‹input\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of the input tensor.
* â€‹index\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of the indexing tensors.
* â€‹start\_axis ([`Int`](/mojo/std/builtin/int/Int)): The first dimension in input where the indexing tensors
  are applied. It is assumed the indexing tensors are applied in
  consecutive dimensions.
* â€‹num\_index\_tensors ([`Int`](/mojo/std/builtin/int/Int)): The number of indexing tensors.
* â€‹target (`StringSlice`): The target architecture to operation on.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.
* â€‹trace\_description (`StringSlice`): For profiling, the trace name the operation will
  appear under.
* â€‹input\_tensor\_fn (`fn[width: Int](IndexList[input_rank]) capturing -> SIMD[input_type, width]`): Fusion lambda for the input tensor.
* â€‹indices\_fn (`fn[indices_index: Int](IndexList[index_rank]) capturing -> Scalar[index_type]`): Fusion lambda for the indices tensors.

**Args:**

* â€‹out\_tensor ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output tensor to write to.
* â€‹in\_tensor\_strides ([`IndexList`](/mojo/std/utils/index_/IndexList)): The strides of the input tensor.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The DeviceContextPtr as prepared by the graph compiler.

</section>

---

## advanced_indexing_getitem_shape

<section class='mojo-docs'>

`advanced_indexing_getitem_shape[input_rank: Int, index_rank: Int, //, start_axis: Int, num_index_tensors: Int](input_shape: IndexList[input_rank], index_shape: IndexList[index_rank]) -> IndexList[((input_rank + index_rank) - num_index_tensors)]`

Calculate the output shape from advanced indexing.

**Parameters:**

* â€‹input\_rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the input tensor.
* â€‹index\_rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the indexing tensors.
* â€‹start\_axis ([`Int`](/mojo/std/builtin/int/Int)): The first dimension in input where the indexing tensors
  are applied. It is assumed the indexing tensors are applied in
  consecutive dimensions.
* â€‹num\_index\_tensors ([`Int`](/mojo/std/builtin/int/Int)): The number of indexing tensors.

**Args:**

* â€‹input\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The shape of the input tensor in the operation.
* â€‹index\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The shape of the indexing tensors in the operation.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## advanced_indexing_setitem_inplace

<section class='mojo-docs'>

`advanced_indexing_setitem_inplace[index_rank: Int, updates_rank: Int, input_type: DType, index_type: DType, //, start_axis: Int, num_index_tensors: Int, target: StringSlice[StaticConstantOrigin], single_thread_blocking_override: Bool, trace_description: StringSlice[StaticConstantOrigin], updates_tensor_fn: fn[width: Int](IndexList[updates_rank]) capturing -> SIMD[input_type, width], indices_fn: fn[indices_index: Int](IndexList[index_rank]) capturing -> Scalar[index_type]](input_tensor: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], index_tensor_shape: IndexList[index_rank], updates_tensor_strides: IndexList[updates_rank], ctx: DeviceContextPtr)`

Implement basic numpy-style advanced indexing with assignment.

This is designed to be fused with other view-producing operations to
implement full numpy-indexing semantics.

This assumes the dimensions in `input_tensor` not indexed by index tensors
are ":", ie selecting all indices along the slice. For example in numpy:

```
# rank(indices1) == 2
# rank(indices2) == 2
# rank(updates) == 2
input_tensor[:, :, :, indices1, indices2, :, :] = updates
```

We calculate the following for all valid valued indexing variables:

```
input_tensor[
    a, b, c,
    indices1[i, j],
    indices2[i, j],
    d, e
] = updates[i, j]
```

In this example `start_axis = 3` and `num_index_tensors = 2`.

In terms of implementation details, our strategy is to iterate over
all indices over a common iteration range. The idea is we can map
indices in this range to the write location in `input_tensor` as well
as the data location in `updates`. An update can illustrate how this is
possible best:

Imagine the `input_tensor` shape is \[A, B, C, D] and we have indexing
tensors I1 and I2 with shape \[M, N, K]. Assume I1 and I2 are applied
to dimensions 1 and 2.

I claim an appropriate common iteration range is then (A, M, N, K, D).
Note we expect `updates` to be the shape \[A, M, N, K, D]. We will show
this by providing the mappings into `updates` and `input_tensor`:

Consider an arbitrary set of indices in this range (a, m, n, k, d):
\- The index into `updates` is (a, m, n, k, d).
\- The index into `input_tensor` is (a, I1\[m, n, k], I2\[m, n, k], d).

TODO(GEX-1951): Support boolean tensor mask support
TODO(GEX-1952): Support non-contiguous indexing tensor case
TODO(GEX-1953): Support fusion (especially view-fusion)
TODO(GEX-1954): Unify getitem and setitem using generic views.
(Requires non-strided view functions).

**Parameters:**

* â€‹index\_rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the indexing tensors.
* â€‹updates\_rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the updates tensor.
* â€‹input\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of the input tensor.
* â€‹index\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of the indexing tensors.
* â€‹start\_axis ([`Int`](/mojo/std/builtin/int/Int)): The first dimension in input where the indexing tensors
  are applied. It is assumed the indexing tensors are applied in
  consecutive dimensions.
* â€‹num\_index\_tensors ([`Int`](/mojo/std/builtin/int/Int)): The number of indexing tensors.
* â€‹target (`StringSlice`): The target architecture to operation on.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.
* â€‹trace\_description (`StringSlice`): For profiling, the trace name the operation will
  appear under.
* â€‹updates\_tensor\_fn (`fn[width: Int](IndexList[updates_rank]) capturing -> SIMD[input_type, width]`): Fusion lambda for the update tensor.
* â€‹indices\_fn (`fn[indices_index: Int](IndexList[index_rank]) capturing -> Scalar[index_type]`): Fusion lambda for the indices tensors.

**Args:**

* â€‹input\_tensor ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor being indexed into and modified in-place.
* â€‹index\_tensor\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The shape of each index tensor.
* â€‹updates\_tensor\_strides ([`IndexList`](/mojo/std/utils/index_/IndexList)): The strides of the update tensor.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The DeviceContextPtr as prepared by the graph compiler.

</section>

---

## index_tensor

<section class='mojo-docs'>

## Functions

* [â€‹`advanced_indexing_getitem`](./advanced_indexing_getitem): Implement basic numpy-style advanced indexing.
* [â€‹`advanced_indexing_getitem_shape`](./advanced_indexing_getitem_shape): Calculate the output shape from advanced indexing.
* [â€‹`advanced_indexing_setitem_inplace`](./advanced_indexing_setitem_inplace): Implement basic numpy-style advanced indexing with assignment.
* [â€‹`index_tensor`](./index_tensor): Index\_tensor operation; based on modified implementation of gather\_nd.
* [â€‹`index_tensor_shape`](./index_tensor_shape): Compute the output shape of a `index_tensor` operation, and assert the inputs are compatible.

</section>

---

## index_tensor (Index_tensor)

<section class='mojo-docs'>

`index_tensor[dtype: DType, indices_type: DType, batch_dims: Int, target: StringSlice[StaticConstantOrigin] = "cpu", single_thread_blocking_override: Bool = False](data: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], indices: LayoutTensor[indices_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr)`

Index\_tensor operation; based on modified implementation of gather\_nd.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Type of data tensor.
* â€‹indices\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of indices tensor.
* â€‹batch\_dims ([`Int`](/mojo/std/builtin/int/Int)): Number of batch dimensions. The gather of indexing
  starts from dimension of data\[batch\_dims:].
* â€‹target (`StringSlice`): The target architecture to execute on.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.

**Args:**

* â€‹data ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor of rank data\_rank >= 1.
* â€‹indices ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor of rank indices\_rank >= 1. All index values are expected
  to be within bounds \[-s, s-1] along axis of size s. It is an
  error if any of the index values are out of bounds.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor of rank data\_rank + indices\_rank - indices\_shape\[-1] - 1 - b.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The DeviceContextPtr as prepared by the graph compiler.

</section>

---

## index_tensor_shape

<section class='mojo-docs'>

`index_tensor_shape[output_rank: Int, input_type: DType, indices_type: DType, batch_dims: Int, single_thread_blocking_override: Bool = True](input_buf: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], indices_buf: LayoutTensor[indices_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> IndexList[output_rank]`

Compute the output shape of a `index_tensor` operation, and assert the inputs are compatible.

**Parameters:**

* â€‹output\_rank ([`Int`](/mojo/std/builtin/int/Int)): Rank of the output tensor.
* â€‹input\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input tensor.
* â€‹indices\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the indices tensor.
* â€‹batch\_dims ([`Int`](/mojo/std/builtin/int/Int)): Batch dimensions.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then reduction is run
  synchronously using a single thread.

**Args:**

* â€‹input\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹indices\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The indices tensor.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## global_cache_insert

<section class='mojo-docs'>

`global_cache_insert(key: String, value: LegacyUnsafePointer[NoneType])`

</section>

---

## global_cache_lookup

<section class='mojo-docs'>

`global_cache_lookup(key: String) -> LegacyOpaquePointer`

**Returns:**

`LegacyOpaquePointer`

</section>

---

## irfft

<section class='mojo-docs'>

Inverse real FFT kernel using cuFFT.

## Functions

* [â€‹`global_cache_insert`](./global_cache_insert):
* [â€‹`global_cache_lookup`](./global_cache_lookup):
* [â€‹`irfft`](./irfft): Compute the inverse real FFT of the input tensor.

</section>

---

## irfft (Irfft)

<section class='mojo-docs'>

`irfft[input_type: DType, output_type: DType, alignment: Int](input: LayoutTensor[input_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[output_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], n: Int, buffer_size_mb: Int, ctx: DeviceContext)`

Compute the inverse real FFT of the input tensor.

Currently, only applies it to the last dimension.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Complex input tensor (NDBuffer).
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Real output tensor (NDBuffer).
* â€‹n ([`Int`](/mojo/std/builtin/int/Int)): Output signal size (if <= 0, computed as 2\*(input.size(axis) - 1)).
* â€‹buffer\_size\_mb ([`Int`](/mojo/std/builtin/int/Int)): Estimated buffer size in MB.
* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): Device context.

</section>

---

## generic_flash_attention_kv_cache_padded

<section class='mojo-docs'>

`generic_flash_attention_kv_cache_padded[collection_t: KVCollectionT, dtype: DType, //, *, target: StringSlice[StaticConstantOrigin], mask_str: StringSlice[StaticConstantOrigin], score_mod_str: StringSlice[StaticConstantOrigin], local_window_size: Int = -1, num_heads: Int = -1](q: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: collection_t, layer_idx: UInt32, valid_lengths: LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin], scale: Float32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr, sink_weights: OptionalReg[LayoutTensor[dtype, Layout.row_major(-1), MutAnyOrigin]] = None)`

</section>

---

## generic_flash_attention_kv_cache_padded_materialized_mask

<section class='mojo-docs'>

`generic_flash_attention_kv_cache_padded_materialized_mask[collection_t: KVCollectionT, dtype: DType, //, *, target: StringSlice[StaticConstantOrigin], score_mod_str: StringSlice[StaticConstantOrigin], local_window_size: Int = -1, num_heads: Int = -1](q: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: collection_t, layer_idx: UInt32, mask: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], valid_lengths: LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin], scale: Float32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr, sink_weights: OptionalReg[LayoutTensor[dtype, Layout.row_major(-1), MutAnyOrigin]] = None)`

</section>

---

## generic_fused_qk_rope_bshd_continuous_batch

<section class='mojo-docs'>

`generic_fused_qk_rope_bshd_continuous_batch[dtype: DType, //, *, interleaved: Bool, target: StringSlice[StaticConstantOrigin]](q_proj: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: ContinuousBatchingKVCacheCollection[dtype_, kv_params_], freqs_cis: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], layer_idx: UInt32, valid_lengths: LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr = DeviceContextPtr())`

Performs a fused RoPE projection for Q and K projections.

We have a manually fused QKV projection with mo.opaque dtypes in our Llama model.
Due to a limitation in custom op definitions, we can't declare both a tensor
and opaque dtype as output from a custom kernel. This requires us to only note
Q\_proj as an output from the QKV projection. If we immediately follow the
QKV proj kernel with a RoPE kernel applied to K, we'll get a race condition
because the graph compiler doesn't know about the dependency between these
kernels in the graph definition. Here we fuse the RoPE kernel applied to
Q\_proj with K\_proj, so K\_proj RoPE is only executed after QKV completes.

**Args:**

* â€‹q\_proj ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Query projection tensor of shape \[batch, seq\_len, n\_heads, head\_dim].
* â€‹kv\_collection ([`ContinuousBatchingKVCacheCollection`](/mojo/kernels/kv_cache/types/ContinuousBatchingKVCacheCollection)): The continuous batching KV cache collection.
* â€‹freqs\_cis ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Frequency tensor for RoPE of shape \[max\_seq\_len, head\_dim].
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The layer index for accessing the correct cache.
* â€‹valid\_lengths ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor of shape \[batch] containing the valid length for each
  sequence. RoPE is only applied to positions within these lengths.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output tensor for Q with RoPE applied, same shape as q\_proj.
* â€‹context ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): Device context pointer for execution.

</section>

---

## generic_fused_qk_rope_bshd_paged

<section class='mojo-docs'>

`generic_fused_qk_rope_bshd_paged[dtype: DType, //, *, interleaved: Bool, target: StringSlice[StaticConstantOrigin]](q_proj: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: PagedKVCacheCollection[dtype_, kv_params_, page_size], freqs_cis: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], layer_idx: UInt32, valid_lengths: LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr = DeviceContextPtr())`

Performs a fused RoPE projection for Q and K with paged KV cache.

This is the paged equivalent of generic\_fused\_qk\_rope\_bshd\_continuous\_batch.
It applies RoPE to both Q (returned) and K (in paged cache) to ensure
proper dependency ordering after fused\_qkv\_padded\_matmul.

**Args:**

* â€‹q\_proj ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Query projection tensor of shape \[batch, seq\_len, n\_heads, head\_dim].
* â€‹kv\_collection ([`PagedKVCacheCollection`](/mojo/kernels/kv_cache/types/PagedKVCacheCollection)): The paged KV cache collection.
* â€‹freqs\_cis ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Frequency tensor for RoPE of shape \[max\_seq\_len, head\_dim].
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The layer index for accessing the correct cache.
* â€‹valid\_lengths ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor of shape \[batch] containing the valid length for each
  sequence. RoPE is only applied to positions within these lengths.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output tensor for Q with RoPE applied, same shape as q\_proj.
* â€‹context ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): Device context pointer for execution.

</section>

---

## generic_fused_qkv_matmul_kv_cache_bshd_continuous_batch

<section class='mojo-docs'>

`generic_fused_qkv_matmul_kv_cache_bshd_continuous_batch[dtype: DType, target: StringSlice[StaticConstantOrigin] = "cpu"](hidden_state: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], weight: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: ContinuousBatchingKVCacheCollection[dtype_, kv_params_], layer_idx: UInt32, valid_lengths: LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr)`

Performs a fused QKV matmul. Q outputs are written to the output argument while K and V outputs are written in-place into k\_cache and v\_cache.

Only positions within valid\_lengths are written to the KV cache.

**Args:**

* â€‹hidden\_state ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (batch\_size, seq\_len, num\_heads \* head\_size).
* â€‹weight ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (num\_heads \* head\_size, num\_kv\_heads \* head\_size).
* â€‹kv\_collection ([`ContinuousBatchingKVCacheCollection`](/mojo/kernels/kv_cache/types/ContinuousBatchingKVCacheCollection)): The historical KVCache for keys and values. The KVCache for
  this layer is retrieved via layer\_idx.
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The index of the layer being executed. Used to retrieve the KVCache
  for the given layer from kv\_collection.
* â€‹valid\_lengths ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor of shape \[batch] containing the valid length for each
  sequence. K and V are only written to cache for positions within these lengths.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The pre-allocated output buffer for Q projections. K and V
  projections are written in-place to k\_cache and v\_cache.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The call context pointer, passed by the graph compiler.

</section>

---

## generic_fused_qkv_matmul_kv_cache_bshd_paged

<section class='mojo-docs'>

`generic_fused_qkv_matmul_kv_cache_bshd_paged[dtype: DType, target: StringSlice[StaticConstantOrigin] = "cpu"](hidden_state: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], weight: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: PagedKVCacheCollection[dtype_, kv_params_, page_size], layer_idx: UInt32, valid_lengths: LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr)`

Performs a fused QKV matmul. Q outputs are written to the output argument while K and V outputs are written in-place into k\_cache and v\_cache.

Only positions within valid\_lengths are written to the KV cache.

**Args:**

* â€‹hidden\_state ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (batch\_size, seq\_len, num\_heads \* head\_size).
* â€‹weight ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (num\_heads \* head\_size, num\_kv\_heads \* head\_size).
* â€‹kv\_collection ([`PagedKVCacheCollection`](/mojo/kernels/kv_cache/types/PagedKVCacheCollection)): The historical KVCache for keys and values. The KVCache for
  this layer is retrieved via layer\_idx.
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The index of the layer being executed. Used to retrieve the KVCache
  for the given layer from kv\_collection.
* â€‹valid\_lengths ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor of shape \[batch] containing the valid length for each
  sequence. K and V are only written to cache for positions within these lengths.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The pre-allocated output buffer for Q projections. K and V
  projections are written in-place to k\_cache and v\_cache.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The call context pointer, passed by the graph compiler.

</section>

---

## generic_get_continuous_cache

<section class='mojo-docs'>

`generic_get_continuous_cache[dtype: DType, kv_params: KVCacheStaticParams](blocks: LayoutTensor[dtype, Layout.row_major[6](), origin], cache_lengths: LayoutTensor[DType.uint32, Layout(IntTuple(-1)), origin], lookup_table: LayoutTensor[DType.uint32, Layout(IntTuple(-1)), origin], max_lengths: LayoutTensor[DType.uint32, Layout.row_major[2](), origin]) -> ContinuousBatchingKVCacheCollection[dtype, kv_params]`

**Returns:**

[`ContinuousBatchingKVCacheCollection`](/mojo/kernels/kv_cache/types/ContinuousBatchingKVCacheCollection)

</section>

---

## generic_get_paged_cache

<section class='mojo-docs'>

`generic_get_paged_cache[dtype: DType](blocks: ManagedTensorSlice[MutableInput, static_spec=static_spec], cache_lengths: ManagedTensorSlice[Input, static_spec=static_spec], lookup_table: ManagedTensorSlice[Input, static_spec=static_spec], max_lengths: ManagedTensorSlice[Input, static_spec=static_spec], out result: PagedKVCacheCollection[dtype, KVCacheStaticParams(UInt(static_spec.shape.get[4]()), UInt(static_spec.shape.get[5]()), (static_spec.shape.get[1]() == 1)), static_spec.shape.get[3]()])`

**Returns:**

[`PagedKVCacheCollection`](/mojo/kernels/kv_cache/types/PagedKVCacheCollection)

`generic_get_paged_cache[dtype: DType, kv_params: KVCacheStaticParams, page_size: Int](blocks: LayoutTensor[dtype, Layout.row_major[6](), origin], cache_lengths: LayoutTensor[DType.uint32, Layout(IntTuple(-1)), origin], lookup_table: LayoutTensor[DType.uint32, Layout.row_major[2](), origin], max_lengths: LayoutTensor[DType.uint32, Layout.row_major[2](), origin], out result: PagedKVCacheCollection[dtype, kv_params, page_size])`

**Returns:**

[`PagedKVCacheCollection`](/mojo/kernels/kv_cache/types/PagedKVCacheCollection)

</section>

---

## kv_cache (Kv_cache)

<section class='mojo-docs'>

## `comptime` values

### `embed_fn_type`

`comptime embed_fn_type = fn[dtype: DType, width: Int](IndexList[4], SIMD[dtype, width]) capturing -> SIMD[dtype, width]`

## Functions

* [â€‹`generic_flash_attention_kv_cache_padded`](./generic_flash_attention_kv_cache_padded):
* [â€‹`generic_flash_attention_kv_cache_padded_materialized_mask`](./generic_flash_attention_kv_cache_padded_materialized_mask):
* [â€‹`generic_fused_qk_rope_bshd_continuous_batch`](./generic_fused_qk_rope_bshd_continuous_batch): Performs a fused RoPE projection for Q and K projections.
* [â€‹`generic_fused_qk_rope_bshd_paged`](./generic_fused_qk_rope_bshd_paged): Performs a fused RoPE projection for Q and K with paged KV cache.
* [â€‹`generic_fused_qkv_matmul_kv_cache_bshd_continuous_batch`](./generic_fused_qkv_matmul_kv_cache_bshd_continuous_batch): Performs a fused QKV matmul. Q outputs are written to the output argument while K and V outputs are written in-place into k\_cache and v\_cache.
* [â€‹`generic_fused_qkv_matmul_kv_cache_bshd_paged`](./generic_fused_qkv_matmul_kv_cache_bshd_paged): Performs a fused QKV matmul. Q outputs are written to the output argument while K and V outputs are written in-place into k\_cache and v\_cache.
* [â€‹`generic_get_continuous_cache`](./generic_get_continuous_cache):
* [â€‹`generic_get_paged_cache`](./generic_get_paged_cache):
* [â€‹`print_kv_cache_cont_batch_generic_cpu`](./print_kv_cache_cont_batch_generic_cpu):
* [â€‹`print_kv_cache_cont_batch_generic_gpu`](./print_kv_cache_cont_batch_generic_gpu):
* [â€‹`print_kv_cache_paged_generic_cpu`](./print_kv_cache_paged_generic_cpu):
* [â€‹`print_kv_cache_paged_generic_gpu`](./print_kv_cache_paged_generic_gpu):
* [â€‹`rms_norm_kv_cache_ragged_continuous_batching`](./rms_norm_kv_cache_ragged_continuous_batching): Performs RMSNorm in place on new entries in the key cache.
* [â€‹`rms_norm_kv_cache_ragged_paged`](./rms_norm_kv_cache_ragged_paged): Performs RMSNorm in place on new entries in the key cache.

</section>

---

## print_kv_cache_cont_batch_generic_cpu

<section class='mojo-docs'>

`print_kv_cache_cont_batch_generic_cpu[target: StringSlice[StaticConstantOrigin], dtype: DType, kv_params: KVCacheStaticParams](valid_lengths: LayoutTensor[DType.uint32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: ContinuousBatchingKVCacheCollection[dtype, kv_params], layer_idx: UInt32, is_print_compact: Bool, context: DeviceContextPtr)`

</section>

---

## print_kv_cache_cont_batch_generic_gpu

<section class='mojo-docs'>

`print_kv_cache_cont_batch_generic_gpu[target: StringSlice[StaticConstantOrigin], dtype: DType, kv_params: KVCacheStaticParams](valid_lengths: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: ContinuousBatchingKVCacheCollection[dtype, kv_params], layer_idx: UInt32, is_print_compact: Bool, context: DeviceContextPtr)`

</section>

---

## print_kv_cache_paged_generic_cpu

<section class='mojo-docs'>

`print_kv_cache_paged_generic_cpu[target: StringSlice[StaticConstantOrigin], dtype: DType, kv_params: KVCacheStaticParams, page_size: Int](valid_lengths: LayoutTensor[DType.uint32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: PagedKVCacheCollection[dtype, kv_params, page_size], layer_idx: UInt32, is_print_compact: Bool, context: DeviceContextPtr)`

</section>

---

## print_kv_cache_paged_generic_gpu

<section class='mojo-docs'>

`print_kv_cache_paged_generic_gpu[target: StringSlice[StaticConstantOrigin], dtype: DType, kv_params: KVCacheStaticParams, page_size: Int](valid_lengths: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: PagedKVCacheCollection[dtype, kv_params, page_size], layer_idx: UInt32, is_print_compact: Bool, context: DeviceContextPtr)`

</section>

---

## rms_norm_kv_cache_ragged_continuous_batching

<section class='mojo-docs'>

`rms_norm_kv_cache_ragged_continuous_batching[dtype: DType, params: KVCacheStaticParams, //, target: StringSlice[StaticConstantOrigin], multiply_before_cast: Bool, per_head_norm: Bool](kv_collection: ContinuousBatchingKVCacheCollection[dtype, params], gamma: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon: Scalar[dtype], weight_offset: Scalar[dtype], layer_idx: UInt32, total_seq_len: UInt32, input_row_offsets: LayoutTensor[DType.uint32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr)`

Performs RMSNorm in place on new entries in the key cache.

This is done by first creating the ragged tensor weight\_shape
(total\_seq\_len, num\_heads, head\_dim) of the new token tensor.
To do this we need to pass in `total_seq_len` on host.
Then, using `input_row_offsets` we find the corresponding batch and token
index, and use that together with the static head and channel indices to
store to/load from the key cache.
This uses the input/output lambdas on the RMSNorm kernel.

This function could apply RMSNorm to a subset of dimensions in each head,
determined by the size of the gamma tensor. In this case, it operates on a
ragged tensor view of the key cache with shape (total\_seq\_len, num\_heads,
rms\_norm\_cols), where rms\_norm\_cols is the length of gamma and must be <=
head\_size.

`weight_offset` is a constant offset argument added to the learned weights
at runtime. Here, we don't use any offset, so we pass in a zero scalar.

`multiply_before_cast` is a boolean parameter that determines whether to
multiply the normalized values by the gamma tensor before casting to the
output dtype or not. We set it to `True` by default.

</section>

---

## rms_norm_kv_cache_ragged_paged

<section class='mojo-docs'>

`rms_norm_kv_cache_ragged_paged[dtype: DType, params: KVCacheStaticParams, page_size: Int, //, target: StringSlice[StaticConstantOrigin], multiply_before_cast: Bool, per_head_norm: Bool](kv_collection: PagedKVCacheCollection[dtype, params, page_size], gamma: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon: Scalar[dtype], weight_offset: Scalar[dtype], layer_idx: UInt32, total_seq_len: UInt32, input_row_offsets: LayoutTensor[DType.uint32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr)`

Performs RMSNorm in place on new entries in the key cache.

This is done by first creating the ragged tensor weight\_shape
(total\_seq\_len, num\_heads, head\_dim) of the new token tensor.
To do this we need to pass in `total_seq_len` on host.
Then, using `input_row_offsets` we find the corresponding batch and token
index, and use that together with the static head and channel indices to
store to/load from the key cache.
This uses the input/output lambdas on the RMSNorm kernel.

This function could apply RMSNorm to a subset of dimensions in each head,
determined by the size of the gamma tensor. In this case, it operates on a
ragged tensor view of the key cache with shape (total\_seq\_len, num\_heads,
rms\_norm\_cols), where rms\_norm\_cols is the length of gamma and must be <=
head\_size.

`weight_offset` is a constant offset argument added to the learned weights
at runtime. Here, we don't use any offset, so we pass in a zero scalar.

`multiply_before_cast` is a boolean parameter that determines whether to
multiply the normalized values by the gamma tensor before casting to the
output dtype or not. We set it to `True` by default.

</section>

---

## generic_cross_attention_kv_cache

<section class='mojo-docs'>

`generic_cross_attention_kv_cache[collection_t: KVCollectionT, dtype: DType, //, target: StringSlice[StaticConstantOrigin], mask_str: StringSlice[StaticConstantOrigin], score_mod_str: StringSlice[StaticConstantOrigin], local_window_size: Int = -1](q: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q_input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q_max_seq_len: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: collection_t, layer_idx: UInt32, scale: Float32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr, sink_weights: OptionalReg[LayoutTensor[dtype, Layout.row_major(-1), MutAnyOrigin]] = None)`

</section>

---

## generic_flare_mla_decode_kv_cache_ragged

<section class='mojo-docs'>

`generic_flare_mla_decode_kv_cache_ragged[collection_t: KVCollectionT, dtype: DType, //, mask_str: StringSlice[StaticConstantOrigin], score_mod_str: StringSlice[StaticConstantOrigin], target: StringSlice[StaticConstantOrigin], local_window_size: Int = -1](q: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: collection_t, layer_idx: UInt32, scale: Float32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr)`

</section>

---

## generic_flare_mla_decompress_k_cache_ragged_paged

<section class='mojo-docs'>

`generic_flare_mla_decompress_k_cache_ragged_paged[target: StringSlice[StaticConstantOrigin], dtype: DType](buffer_row_offsets_1d: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], cache_offsets_1d: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], buffer_length: Int32, weight: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: PagedKVCacheCollection[dtype_, kv_params_, page_size], layer_idx: UInt32, k_latent_buffer: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k_buffer: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr)`

</section>

---

## generic_flare_mla_prefill_kv_cache_ragged

<section class='mojo-docs'>

`generic_flare_mla_prefill_kv_cache_ragged[collection_t: KVCollectionT, dtype: DType, //, softmax_type: DType, write_softmax_info: Bool, use_cascade_attention: Bool, mask_str: StringSlice[StaticConstantOrigin], score_mod_str: StringSlice[StaticConstantOrigin], target: StringSlice[StaticConstantOrigin], local_window_size: Int = -1](q: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], v: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], buffer_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], cache_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: collection_t, layer_idx: UInt32, scale: Float32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], softmax_info: LayoutTensor[softmax_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr, prev_output: OptionalReg[LayoutTensor[dtype, Layout.row_major[3](), MutAnyOrigin]] = None, prev_softmax_info: OptionalReg[LayoutTensor[softmax_type, Layout.row_major[3](), MutAnyOrigin]] = None)`

</section>

---

## generic_flare_mla_prefill_ragged_paged_plan

<section class='mojo-docs'>

`generic_flare_mla_prefill_ragged_paged_plan[target: StringSlice[StaticConstantOrigin]](input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: PagedKVCacheCollection[dtype_, kv_params_, page_size], layer_idx: UInt32, buffer_token_size: UInt32, buffer_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], cache_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], buffer_lengths: LayoutTensor[DType.int32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr)`

</section>

---

## generic_flash_attention_kv_cache_ragged

<section class='mojo-docs'>

`generic_flash_attention_kv_cache_ragged[collection_t: KVCollectionT, dtype: DType, //, *, target: StringSlice[StaticConstantOrigin], mask_str: StringSlice[StaticConstantOrigin], score_mod_str: StringSlice[StaticConstantOrigin], local_window_size: Int = -1](q: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin], kv_collection: collection_t, layer_idx: UInt32, scale: Float32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr)`

</section>

---

## generic_flash_attention_kv_cache_ragged_sink

<section class='mojo-docs'>

`generic_flash_attention_kv_cache_ragged_sink[collection_t: KVCollectionT, dtype: DType, //, *, target: StringSlice[StaticConstantOrigin], mask_str: StringSlice[StaticConstantOrigin], score_mod_str: StringSlice[StaticConstantOrigin], local_window_size: Int = -1](q: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin], kv_collection: collection_t, layer_idx: UInt32, scale: Float32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr, sink_weights: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## generic_fused_qk_rope_bshd_continuous_batch_ragged

<section class='mojo-docs'>

`generic_fused_qk_rope_bshd_continuous_batch_ragged[dtype: DType, freq_dtype: DType, //, *, interleaved: Bool, has_position_ids: Bool, target: StringSlice[StaticConstantOrigin], mrope_section: Optional[IntTuple] = None](q_proj: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: ContinuousBatchingKVCacheCollection[dtype_, kv_params_], freqs_cis: LayoutTensor[freq_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], position_ids: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], layer_idx: UInt32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr)`

</section>

---

## generic_fused_qk_rope_bshd_paged_ragged

<section class='mojo-docs'>

`generic_fused_qk_rope_bshd_paged_ragged[dtype: DType, freq_dtype: DType, //, *, interleaved: Bool, has_position_ids: Bool, target: StringSlice[StaticConstantOrigin], mrope_section: Optional[IntTuple] = None](q_proj: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: PagedKVCacheCollection[dtype_, kv_params_, page_size], freqs_cis: LayoutTensor[freq_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], position_ids: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], layer_idx: UInt32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr = DeviceContextPtr())`

Performs a fused RoPE projection for Q and K projections.

We have a manually fused QKV projection with mo.opaque dtypes in our Llama model.
Due to a limitation in custom op definitions, we can't declare both a tensor
and opaque dtype as output from a custom kernel. This requires us to only note
Q\_proj as an output from the QKV projection. If we immediately follow the
QKV proj kernel with a RoPE kernel applied to K, we'll get a race condition
because the graph compiler doesn't know about the dependency between these
kernels in the graph definition. Here we fuse the RoPE kernel applied to
Q\_proj with K\_proj, so K\_proj RoPE is only executed after QKV completes.

</section>

---

## generic_fused_qkv_matmul_kv_cache_cont_batch_ragged

<section class='mojo-docs'>

`generic_fused_qkv_matmul_kv_cache_cont_batch_ragged[dtype: DType, //, target: StringSlice[StaticConstantOrigin] = "cpu"](hidden_state: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], weight: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: ContinuousBatchingKVCacheCollection[dtype_, kv_params_], layer_idx: UInt32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr)`

Performs a fused QKV matmul. Q outputs are written to the output argument while K and V outputs are written in-place into k\_cache and v\_cache.

**Args:**

* â€‹hidden\_state ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (sum(seq\_lens), num\_heads \* head\_size).
* â€‹input\_row\_offsets ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (batch\_size + 1,).
  The value at each index is the start\_idx of the corresponding batch in hidden\_state.
* â€‹weight ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (num\_heads \* head\_size, num\_kv\_heads \* head\_size).
* â€‹kv\_collection ([`ContinuousBatchingKVCacheCollection`](/mojo/kernels/kv_cache/types/ContinuousBatchingKVCacheCollection)): The object storing the KVCache for this layer.
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The current layer, used to retrieve the KVCache object from kv\_collection.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The pre-allocated output buffer for Q projections. K and V
  projections are written in-place to k\_cache and v\_cache.
  Shape: (sum(seq\_lens), num\_heads \* head\_size).
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The call context pointer, passed by the graph compiler.

</section>

---

## generic_fused_qkv_matmul_kv_cache_paged_ragged

<section class='mojo-docs'>

`generic_fused_qkv_matmul_kv_cache_paged_ragged[dtype: DType, weight_dtype: DType, target: StringSlice[StaticConstantOrigin] = "cpu", group_size: OptionalReg[Int] = None, has_zp: OptionalReg[Bool] = None](hidden_state: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], weight: LayoutTensor[weight_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: PagedKVCacheCollection[dtype_, kv_params_, page_size], layer_idx: UInt32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr)`

Performs a fused QKV matmul. Q outputs are written to the output argument while K and V outputs are written in-place into k\_cache and v\_cache.

**Args:**

* â€‹hidden\_state ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (sum(seq\_lens), num\_heads \* head\_size).
* â€‹input\_row\_offsets ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (batch\_size + 1,).
  The value at each index is the start\_idx of the corresponding batch in hidden\_state.
* â€‹weight ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (num\_heads \* head\_size, num\_kv\_heads \* head\_size).
* â€‹kv\_collection ([`PagedKVCacheCollection`](/mojo/kernels/kv_cache/types/PagedKVCacheCollection)): The object storing the KVCache for this layer.
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The current layer, used to retrieve the KVCache object from kv\_collection.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The pre-allocated output buffer for Q projections. K and V
  projections are written in-place to k\_cache and v\_cache.
  Shape: (sum(seq\_lens), num\_heads \* head\_size).
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The call context pointer, passed by the graph compiler.

</section>

---

## generic_fused_qkv_matmul_kv_cache_paged_ragged_bias

<section class='mojo-docs'>

`generic_fused_qkv_matmul_kv_cache_paged_ragged_bias[dtype: DType, weight_dtype: DType, target: StringSlice[StaticConstantOrigin] = "cpu", group_size: OptionalReg[Int] = None, has_zp: OptionalReg[Bool] = None](hidden_state: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], weight: LayoutTensor[weight_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: PagedKVCacheCollection[dtype_, kv_params_, page_size], layer_idx: UInt32, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], bias: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr)`

Performs a fused QKV matmul. Q outputs are written to the output argument while K and V outputs are written in-place into k\_cache and v\_cache.

**Args:**

* â€‹hidden\_state ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (sum(seq\_lens), num\_heads \* head\_size).
* â€‹input\_row\_offsets ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (batch\_size + 1,).
  The value at each index is the start\_idx of the corresponding batch in hidden\_state.
* â€‹weight ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (num\_heads \* head\_size, num\_kv\_heads \* head\_size).
* â€‹kv\_collection ([`PagedKVCacheCollection`](/mojo/kernels/kv_cache/types/PagedKVCacheCollection)): The object storing the KVCache for this layer.
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The current layer, used to retrieve the KVCache object from kv\_collection.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The pre-allocated output buffer for Q projections. K and V
  projections are written in-place to k\_cache and v\_cache.
  Shape: (sum(seq\_lens), num\_heads \* head\_size).
* â€‹bias ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Bias to be added to the QKV Tensor. Tensor is concatenated q + k + v. Rank 1.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The call context pointer, passed by the graph compiler.

</section>

---

## generic_fused_qkv_matmul_kv_cache_paged_ragged_scale

<section class='mojo-docs'>

`generic_fused_qkv_matmul_kv_cache_paged_ragged_scale[dtype: DType, weight_dtype: DType, output_dtype: DType, scale_dtype: DType, scales_granularity_mnk: IndexList[3], target: StringSlice[StaticConstantOrigin] = "cpu"](hidden_state: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], weight: LayoutTensor[weight_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_scale: LayoutTensor[scale_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], weight_scale: LayoutTensor[scale_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: PagedKVCacheCollection[dtype_, kv_params_, page_size], layer_idx: UInt32, output: LayoutTensor[output_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr, bias: OptionalReg[LayoutTensor[output_dtype, Layout.row_major(-1), ImmutAnyOrigin]] = None)`

Performs a fused QKV matmul. Q outputs are written to the output argument while K and V outputs are written in-place into k\_cache and v\_cache.

**Args:**

* â€‹hidden\_state ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (sum(seq\_lens), num\_heads \* head\_size).
* â€‹input\_row\_offsets ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (batch\_size + 1,).
  The value at each index is the start\_idx of the corresponding batch
  in hidden\_state.
* â€‹weight ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (num\_heads \* head\_size, num\_kv\_heads \*
  head\_size).
* â€‹input\_scale ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Scale to be multiplied to the input Tensor.
* â€‹weight\_scale ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Scale to be multiplied to the weight Tensor.
* â€‹kv\_collection ([`PagedKVCacheCollection`](/mojo/kernels/kv_cache/types/PagedKVCacheCollection)): The object storing the KVCache for this layer.
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The current layer, used to retrieve the KVCache object from
  kv\_collection.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The pre-allocated output buffer for Q projections. K and V
  projections are written in-place to k\_cache and v\_cache.
  Shape: (sum(seq\_lens), num\_heads \* head\_size).
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The call context pointer, passed by the graph compiler.
* â€‹bias ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional bias vector concatenated as \[q, k, v].

</section>

---

## generic_kv_cache_radd_dispatch

<section class='mojo-docs'>

`generic_kv_cache_radd_dispatch[dtype: DType, collection_t: KVCollectionT, //, target: StringSlice[StaticConstantOrigin]](a: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], cache: collection_t, input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], batch_offset: UInt32, layer_idx: UInt32, ctx: Optional[DeviceContext])`

</section>

---

## kv_cache_ragged

<section class='mojo-docs'>

## Functions

* [â€‹`generic_cross_attention_kv_cache`](./generic_cross_attention_kv_cache):
* [â€‹`generic_flare_mla_decode_kv_cache_ragged`](./generic_flare_mla_decode_kv_cache_ragged):
* [â€‹`generic_flare_mla_decompress_k_cache_ragged_paged`](./generic_flare_mla_decompress_k_cache_ragged_paged):
* [â€‹`generic_flare_mla_prefill_kv_cache_ragged`](./generic_flare_mla_prefill_kv_cache_ragged):
* [â€‹`generic_flare_mla_prefill_ragged_paged_plan`](./generic_flare_mla_prefill_ragged_paged_plan):
* [â€‹`generic_flash_attention_kv_cache_ragged`](./generic_flash_attention_kv_cache_ragged):
* [â€‹`generic_flash_attention_kv_cache_ragged_sink`](./generic_flash_attention_kv_cache_ragged_sink):
* [â€‹`generic_fused_qk_rope_bshd_continuous_batch_ragged`](./generic_fused_qk_rope_bshd_continuous_batch_ragged):
* [â€‹`generic_fused_qk_rope_bshd_paged_ragged`](./generic_fused_qk_rope_bshd_paged_ragged): Performs a fused RoPE projection for Q and K projections.
* [â€‹`generic_fused_qkv_matmul_kv_cache_cont_batch_ragged`](./generic_fused_qkv_matmul_kv_cache_cont_batch_ragged): Performs a fused QKV matmul. Q outputs are written to the output argument while K and V outputs are written in-place into k\_cache and v\_cache.
* [â€‹`generic_fused_qkv_matmul_kv_cache_paged_ragged`](./generic_fused_qkv_matmul_kv_cache_paged_ragged): Performs a fused QKV matmul. Q outputs are written to the output argument while K and V outputs are written in-place into k\_cache and v\_cache.
* [â€‹`generic_fused_qkv_matmul_kv_cache_paged_ragged_bias`](./generic_fused_qkv_matmul_kv_cache_paged_ragged_bias): Performs a fused QKV matmul. Q outputs are written to the output argument while K and V outputs are written in-place into k\_cache and v\_cache.
* [â€‹`generic_fused_qkv_matmul_kv_cache_paged_ragged_scale`](./generic_fused_qkv_matmul_kv_cache_paged_ragged_scale): Performs a fused QKV matmul. Q outputs are written to the output argument while K and V outputs are written in-place into k\_cache and v\_cache.
* [â€‹`generic_kv_cache_radd_dispatch`](./generic_kv_cache_radd_dispatch):
* [â€‹`k_matmul_ragged_paged`](./k_matmul_ragged_paged): Performs a matmul, writing the output into a mutable PagedKVCacheCollection object.
* [â€‹`k_matmul_ragged_paged_scale`](./k_matmul_ragged_paged_scale): Performs a matmul, writing the output into a mutable PagedKVCacheCollection object.
* [â€‹`kv_cache_2m_iadd_dispatch`](./kv_cache_2m_iadd_dispatch): In-place add to paged KV cache with concatenated K/V layout. This kernel is only used for LoRA.
* [â€‹`kv_cache_store_ragged`](./kv_cache_store_ragged):
* [â€‹`kv_matmul_ragged_paged`](./kv_matmul_ragged_paged): Performs a matmul, writing the output into a mutable ContinuousBatchingKVCacheCollection object.
* [â€‹`unfused_qkv_matmul_ragged_paged_gguf_quantized`](./unfused_qkv_matmul_ragged_paged_gguf_quantized): Performs a quantized matmul, writing the output into a mutable PagedKVCacheCollection object.

</section>

---

## k_matmul_ragged_paged

<section class='mojo-docs'>

`k_matmul_ragged_paged[dtype: DType, params: KVCacheStaticParams, page_size: Int, //, target: StringSlice[StaticConstantOrigin]](hidden_state: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], weight: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: PagedKVCacheCollection[dtype, params, page_size], layer_idx: UInt32, ctx: DeviceContextPtr)`

Performs a matmul, writing the output into a mutable PagedKVCacheCollection object.

**Args:**

* â€‹hidden\_state ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (sum(seq\_lens), num\_heads \* head\_size).
* â€‹input\_row\_offsets ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (batch\_size + 1,)
  denoting the start of each sequence along the seq\_len dimension.
* â€‹weight ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (num\_heads \* head\_size, num\_kv\_heads \* head\_size).
* â€‹kv\_collection ([`PagedKVCacheCollection`](/mojo/kernels/kv_cache/types/PagedKVCacheCollection)): The historical KVCache for keys and values. The KVCache for
  this layer is retrieved via layer\_idx.
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The index of the layer being executed. Used to retrieve the KVCache
  for the given layer from kv\_collection.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The call context pointer, passed by the graph compiler.

</section>

---

## k_matmul_ragged_paged_scale

<section class='mojo-docs'>

`k_matmul_ragged_paged_scale[dtype: DType, weight_dtype: DType, scale_dtype: DType, target: StringSlice[StaticConstantOrigin], scales_granularity_mnk: IndexList[3]](hidden_state: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], weight: LayoutTensor[weight_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_scale: LayoutTensor[scale_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], weight_scale: LayoutTensor[scale_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: PagedKVCacheCollection[dtype_, kv_params_, page_size], layer_idx: UInt32, ctx: DeviceContextPtr)`

Performs a matmul, writing the output into a mutable PagedKVCacheCollection object.

**Args:**

* â€‹hidden\_state ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (sum(seq\_lens), num\_heads \* head\_size).
* â€‹input\_row\_offsets ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (batch\_size + 1,)
  denoting the start of each sequence along the seq\_len dimension.
* â€‹weight ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (num\_heads \* head\_size, num\_kv\_heads \* head\_size).
* â€‹input\_scale ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Scale to be multiplied to the input Tensor.
* â€‹weight\_scale ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Scale to be multiplied to the weight Tensor.
* â€‹kv\_collection ([`PagedKVCacheCollection`](/mojo/kernels/kv_cache/types/PagedKVCacheCollection)): The historical KVCache for keys and values. The KVCache for
  this layer is retrieved via layer\_idx.
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The index of the layer being executed. Used to retrieve the KVCache
  for the given layer from kv\_collection.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The call context pointer, passed by the graph compiler.

</section>

---

## kv_cache_2m_iadd_dispatch

<section class='mojo-docs'>

`kv_cache_2m_iadd_dispatch[dtype: DType, collection_t: KVCollectionT, //, target: StringSlice[StaticConstantOrigin]](kv: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], cache: collection_t, input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], lora_end_idx: LayoutTensor[DType.int64, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], batch_seq_len: LayoutTensor[DType.int64, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], layer_idx: UInt32, ctx: Optional[DeviceContext])`

In-place add to paged KV cache with concatenated K/V layout. This kernel is only used for LoRA.

Performs an in-place addition of new key-value projections to paged KV cache.
The input tensor `a` uses a "2m" layout where keys and values are concatenated:
rows \[0, m) contain keys and rows \[m, 2m) contain values, where m is the number
of tokens. We use the `lora_end_idx` to index into the K or V tensor.
We call this value `m` since this value will be a subset of the
total tokens in the batch. We write tokens to K as \[0, m) and V as \[m, 2m).

</section>

---

## kv_cache_store_ragged

<section class='mojo-docs'>

`kv_cache_store_ragged[cache_t: KVCacheT, input_row_offsets_layout: Layout, //, target: StringSlice[StaticConstantOrigin], input_fn: fn[width: Int, alignment: Int](idx: IndexList[3]) capturing -> SIMD[cache_t.dtype, width]](cache: cache_t, input_shape: IndexList[3], input_row_offsets: LayoutTensor[DType.uint32, input_row_offsets_layout, MutAnyOrigin], context: Optional[DeviceContext])`

</section>

---

## kv_matmul_ragged_paged

<section class='mojo-docs'>

`kv_matmul_ragged_paged[dtype: DType, params: KVCacheStaticParams, page_size: Int, //, target: StringSlice[StaticConstantOrigin]](hidden_state: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], weight: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: PagedKVCacheCollection[dtype, params, page_size], layer_idx: UInt32, ctx: DeviceContextPtr)`

Performs a matmul, writing the output into a mutable ContinuousBatchingKVCacheCollection object.

**Args:**

* â€‹hidden\_state ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (sum(seq\_lens), num\_heads \* head\_size).
* â€‹input\_row\_offsets ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (batch\_size + 1,)
  denoting the start of each sequence along the seq\_len dimension.
* â€‹weight ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (num\_heads \* head\_size, num\_kv\_heads \* head\_size).
* â€‹kv\_collection ([`PagedKVCacheCollection`](/mojo/kernels/kv_cache/types/PagedKVCacheCollection)): The historical KVCache for keys and values. The KVCache for
  this layer is retrieved via layer\_idx.
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The index of the layer being executed. Used to retrieve the KVCache
  for the given layer from kv\_collection.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The call context pointer, passed by the graph compiler.

</section>

---

## unfused_qkv_matmul_ragged_paged_gguf_quantized

<section class='mojo-docs'>

`unfused_qkv_matmul_ragged_paged_gguf_quantized[dtype: DType, params: KVCacheStaticParams, page_size: Int, //, quantization_encoding_q: StringSlice[StaticConstantOrigin], quantization_encoding_k: StringSlice[StaticConstantOrigin], quantization_encoding_v: StringSlice[StaticConstantOrigin]](hidden_state: LayoutTensor[DType.float32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q_weight: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k_weight: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], v_weight: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: PagedKVCacheCollection[dtype, params, page_size], layer_idx: UInt32, output: LayoutTensor[DType.float32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr)`

Performs a quantized matmul, writing the output into a mutable PagedKVCacheCollection object.

Unlike the un-quantized version (kv\_matmul\_ragged\_continuous\_batching), this
implementation does not concat the q, k, and v weights together. Instead, it
performs three matmuls. This allows the q, k, and v weights to have different
quantization encodings.

This is only supported on CPU.

**Args:**

* â€‹hidden\_state ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (sum(seq\_lens), num\_heads \* head\_size).
* â€‹input\_row\_offsets ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (batch\_size + 1,)
  denoting the start of each sequence along the seq\_len dimension.
* â€‹q\_weight ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (num\_heads \* head\_size, num\_kv\_heads \* head\_size).
* â€‹k\_weight ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (num\_heads \* head\_size, num\_kv\_heads \* head\_size).
* â€‹v\_weight ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (num\_heads \* head\_size, num\_kv\_heads \* head\_size).
* â€‹kv\_collection ([`PagedKVCacheCollection`](/mojo/kernels/kv_cache/types/PagedKVCacheCollection)): The Collection object storing KVCache entries.
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): The index of the layer being executed. Used to retrieve the KVCache
  for the given layer from kv\_collection.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Tensor with shape (sum(seq\_lens), num\_kv\_heads \* head\_size).
  This is the output buffer for the Q matmul.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The call context pointer, passed by the graph compiler.

</section>

---

## depth_supported_by_gpu

<section class='mojo-docs'>

`depth_supported_by_gpu[depth: Int, mask_t: MHAMask, config: MHAConfig[dtype], info: GPUInfo]() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## flash_attention (Mha)

<section class='mojo-docs'>

`flash_attention[dtype: DType, q_layout: Layout, //, use_score_mod: Bool = False, config: MHAConfig[dtype] = MHAConfig[dtype](UInt(Int.__init__[IntTuple](q_layout.shape[2])), UInt(Int.__init__[IntTuple](q_layout.shape[3])), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), 4, 1, FlashAttentionAlgorithm(-1), TensorMapSwizzle.SWIZZLE_128B), decoding_warp_split_k: Bool = False, naive_kernel: Bool = False, sink: Bool = False](output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q: LayoutTensor[dtype, q_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], v: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mask: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scale: Float32, context: DeviceContextPtr = DeviceContextPtr(), num_partitions: OptionalReg[Int] = None, sink_weights: OptionalReg[LayoutTensor[dtype, Layout.row_major(-1), MutAnyOrigin]] = None)`

`flash_attention[cache_t: KVCacheT, mask_t: MHAMask, score_mod_t: ScoreModTrait, dtype: DType, q_layout: Layout, //, use_score_mod: Bool = False, config: MHAConfig[dtype] = MHAConfig[dtype](UInt(Int.__init__[IntTuple](q_layout.shape[(q_layout.rank() - 2)])), UInt(Int.__init__[IntTuple](q_layout.shape[(q_layout.rank() - 1)])), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), 4, 1, FlashAttentionAlgorithm(-1), TensorMapSwizzle.SWIZZLE_128B), ragged: Bool = False, sink: Bool = False, decoding_warp_split_k: Bool = False, naive_kernel: Bool = False](output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q: LayoutTensor[dtype, q_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: cache_t, v: cache_t, mask_functor: mask_t, score_mod_functor: score_mod_t, valid_length: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scale: Float32, ctx: DeviceContext, q_max_seq_len: OptionalReg[Int] = None, kv_input_row_offsets: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]] = None, num_partitions: OptionalReg[Int] = None, sink_weights: OptionalReg[LayoutTensor[dtype, Layout.row_major(-1), MutAnyOrigin]] = None)`

Flash attention 2 algorithm. Compute:     (1) Transpose (Q) BSHD -> BHSD;     (2) Transpose (K) BSHD -> BHSD;     (3) Transpose (V) BSHD -> BHSD;     (4) P = Bmm(Q, K), P is also called "score";     (5) P = P \* scale + mask;     (6) P = softmax(P);     (7) O = Bmm(P, V)     (8) Output = Transpose(O).

B, S, H, D denote batch size, sequence length, head count and depth, respectively.
(1), (2), (3) happens while loading the data into shared memory.
(8) happens when writing output to global memory.

All inputs (query, key, and value) must have BSHD layout. The mask can be
BSS or BHSS.

This kernel also handles grouped attention optimization. In this case the shape of
K and V are BShD where h = H / num\_groups.

This kernels handles batches with different valid lengths (i.e., before the
padding). Such lengths are passed in valid\_length argument.

`flash_attention[mask_t: MHAMask, score_mod_t: ScoreModTrait, dtype: DType, q_layout: Layout, //, use_score_mod: Bool = False, config: MHAConfig[dtype] = MHAConfig[dtype](UInt(Int.__init__[IntTuple](q_layout.shape[2])), UInt(Int.__init__[IntTuple](q_layout.shape[3])), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), 4, 1, FlashAttentionAlgorithm(-1), TensorMapSwizzle.SWIZZLE_128B), decoding_warp_split_k: Bool = False, _use_valid_length: Bool = False, _padded_ndbuffer: Bool = False, naive_kernel: Bool = False, sink: Bool = False](output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q: LayoutTensor[dtype, q_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], v: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mask_functor: mask_t, score_mod_functor: score_mod_t, scale: Float32, ctx: DeviceContext, num_partitions: OptionalReg[Int] = None, valid_length: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]] = None, sink_weights: OptionalReg[LayoutTensor[dtype, Layout.row_major(-1), MutAnyOrigin]] = None)`

</section>

---

## flash_attention_dispatch

<section class='mojo-docs'>

`flash_attention_dispatch[k_t: MHAOperand, v_t: MHAOperand, mask_t: MHAMask, score_mod_t: ScoreModTrait, dtype: DType, q_layout: Layout, //, kv_num_heads: Int, use_score_mod: Bool = False, config: MHAConfig[dtype] = MHAConfig[dtype](UInt(Int.__init__[IntTuple](q_layout.shape[(q_layout.rank() - 2)])), UInt(Int.__init__[IntTuple](q_layout.shape[(q_layout.rank() - 1)])), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), 4, 1, FlashAttentionAlgorithm(-1), TensorMapSwizzle.SWIZZLE_128B), ragged: Bool = False, sink: Bool = False, _is_flash_attention_applicable: Bool = True, _is_cache_length_accurate: Bool = False, _use_valid_length: Bool = True, _padded_ndbuffer: Bool = False, decoding_warp_split_k: Bool = False](output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q: LayoutTensor[dtype, q_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: k_t, v: v_t, mask_functor: mask_t, score_mod_functor: score_mod_t, max_prompt_len: Int, max_cache_valid_length: Int, scale: Float32, is_token_generation: Bool, ctx: DeviceContext, valid_length: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]] = None, kv_input_row_offsets: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]] = None, num_partitions: OptionalReg[Int] = None, sink_weights: OptionalReg[LayoutTensor[dtype, Layout.row_major(-1), MutAnyOrigin]] = None)`

</section>

---

## flash_attention_hw_supported

<section class='mojo-docs'>

`flash_attention_hw_supported[qkv_type: DType]() -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## flash_attention_ragged

<section class='mojo-docs'>

`flash_attention_ragged[mask_t: MHAMask, score_mod_t: ScoreModTrait, type: DType, q_layout: Layout, //, use_score_mod: Bool = False, config: MHAConfig[type] = MHAConfig[type](UInt(Int.__init__[IntTuple](q_layout.shape[(q_layout.rank() - 2)])), UInt(Int.__init__[IntTuple](q_layout.shape[(q_layout.rank() - 1)])), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), 4, 1, FlashAttentionAlgorithm(-1), TensorMapSwizzle.SWIZZLE_128B), decoding_warp_split_k: Bool = False, naive_kernel: Bool = False](output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q: LayoutTensor[type, q_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], v: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin], max_prompt_len: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mask_functor: mask_t, score_mod_functor: score_mod_t, scale: Float32, ctx: DeviceContext, num_partitions: OptionalReg[Int] = None)`

</section>

---

## get_mha_decoding_num_partitions

<section class='mojo-docs'>

`get_mha_decoding_num_partitions[num_heads: Int, group: Int](batch_size: Int, num_keys: Int, ctx: DeviceContext) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## mha

<section class='mojo-docs'>

## Functions

* [â€‹`depth_supported_by_gpu`](./depth_supported_by_gpu):
* [â€‹`flash_attention`](./flash_attention):
* [â€‹`flash_attention_dispatch`](./flash_attention_dispatch):
* [â€‹`flash_attention_hw_supported`](./flash_attention_hw_supported):
* [â€‹`flash_attention_ragged`](./flash_attention_ragged):
* [â€‹`get_mha_decoding_num_partitions`](./get_mha_decoding_num_partitions):
* [â€‹`mha`](./mha):
* [â€‹`mha_decoding`](./mha_decoding):
* [â€‹`mha_decoding_single_batch`](./mha_decoding_single_batch): Flash attention v2 algorithm.
* [â€‹`mha_decoding_single_batch_pipelined`](./mha_decoding_single_batch_pipelined): Flash attention v2 algorithm.
* [â€‹`mha_gpu_naive`](./mha_gpu_naive):
* [â€‹`mha_single_batch`](./mha_single_batch): MHA for token gen where seqlen = 1 and num\_keys >= 1.
* [â€‹`mha_single_batch_pipelined`](./mha_single_batch_pipelined): MHA for token gen where seqlen = 1 and num\_keys >= 1.
* [â€‹`mha_splitk_reduce`](./mha_splitk_reduce):
* [â€‹`q_num_matrix_view_rows`](./q_num_matrix_view_rows):
* [â€‹`scale_and_mask_helper`](./scale_and_mask_helper):

</section>

---

## mha (Mha)

<section class='mojo-docs'>

`mha[q_type: DType, k_t: MHAOperand, v_t: MHAOperand, output_type: DType, mask_t: MHAMask, score_mod_t: ScoreModTrait, valid_length_layout: Layout, config: MHAConfig[dtype], group: Int = 1, use_score_mod: Bool = False, ragged: Bool = False, is_shared_kv: Bool = False, sink: Bool = False, _use_valid_length: Bool = False, _is_cache_length_accurate: Bool = False, _padded_ndbuffer: Bool = False](q_ptr: LegacyUnsafePointer[Scalar[q_type]], k: k_t, v: v_t, output_ptr: LegacyUnsafePointer[Scalar[output_type]], scale: Float32, batch_size: Int, seq_len_arg: Int, num_keys_arg: Int, valid_length: LayoutTensor[DType.uint32, valid_length_layout, MutAnyOrigin], kv_input_row_offsets: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]], sink_weights: OptionalReg[LayoutTensor[q_type, Layout.row_major(-1), MutAnyOrigin]], mask: mask_t, score_mod: score_mod_t)`

</section>

---

## mha_decoding

<section class='mojo-docs'>

`mha_decoding[q_type: DType, k_t: MHAOperand, v_t: MHAOperand, output_type: DType, mask_t: MHAMask, score_mod_t: ScoreModTrait, valid_length_layout: Layout, BM: UInt, BN: UInt, BK: UInt, WM: UInt, WN: UInt, depth: UInt, num_heads: UInt, num_threads: UInt, num_pipeline_stages: UInt, group: UInt = 1, use_score_mod: Bool = False, ragged: Bool = False, is_shared_kv: Bool = False, sink: Bool = False, _use_valid_length: Bool = False, _is_cache_length_accurate: Bool = False, decoding_warp_split_k: Bool = False](q_ptr: LegacyUnsafePointer[Scalar[q_type]], k: k_t, v: v_t, output_ptr: LegacyUnsafePointer[Scalar[output_type]], exp_sum_ptr: LegacyUnsafePointer[Scalar[get_accum_type[q_type]()]], qk_max_ptr: LegacyUnsafePointer[Scalar[get_accum_type[q_type]()]], scale: Float32, batch_size: Int, num_partitions: Int, max_cache_valid_length: Int, valid_length: LayoutTensor[DType.uint32, valid_length_layout, MutAnyOrigin], sink_weights: OptionalReg[LayoutTensor[q_type, Layout.row_major(-1), MutAnyOrigin]], mask: mask_t, score_mod: score_mod_t)`

</section>

---

## mha_decoding_single_batch

<section class='mojo-docs'>

`mha_decoding_single_batch[q_type: DType, k_t: MHAOperand, v_t: MHAOperand, output_type: DType, mask_t: MHAMask, score_mod_t: ScoreModTrait, *, BM: UInt, BN: UInt, BK: UInt, WM: UInt, WN: UInt, depth: UInt, num_heads: UInt, num_threads: UInt, num_pipeline_stages: UInt, group: UInt = 1, use_score_mod: Bool = False, decoding_warp_split_k: Bool = False, sink: Bool = False](q_ptr: LegacyUnsafePointer[Scalar[q_type]], k: k_t, v: v_t, output_ptr: LegacyUnsafePointer[Scalar[output_type]], exp_sum_ptr: LegacyUnsafePointer[Scalar[get_accum_type[q_type]()]], qk_max_ptr: LegacyUnsafePointer[Scalar[get_accum_type[q_type]()]], scale: Float32, num_keys: UInt, num_partitions: UInt, max_cache_valid_length: UInt, mask: mask_t, score_mod: score_mod_t, batch_idx: Int, sink_weights: OptionalReg[LayoutTensor[q_type, Layout.row_major(-1), MutAnyOrigin]])`

Flash attention v2 algorithm.

</section>

---

## mha_decoding_single_batch_pipelined

<section class='mojo-docs'>

`mha_decoding_single_batch_pipelined[q_type: DType, k_t: MHAOperand, v_t: MHAOperand, output_type: DType, mask_t: MHAMask, score_mod_t: ScoreModTrait, *, BM: UInt, BN: UInt, BK: UInt, WM: UInt, WN: UInt, depth: UInt, num_heads: UInt, num_threads: UInt, num_pipeline_stages: UInt, group: UInt = 1, use_score_mod: Bool = False, decoding_warp_split_k: Bool = False, sink: Bool = False](q_ptr: LegacyUnsafePointer[Scalar[q_type]], k: k_t, v: v_t, output_ptr: LegacyUnsafePointer[Scalar[output_type]], exp_sum_ptr: LegacyUnsafePointer[Scalar[get_accum_type[q_type]()]], qk_max_ptr: LegacyUnsafePointer[Scalar[get_accum_type[q_type]()]], scale: Float32, num_keys: UInt, num_partitions: UInt, max_cache_valid_length: UInt, sink_weights: OptionalReg[LayoutTensor[q_type, Layout.row_major(-1), MutAnyOrigin]], mask: mask_t, score_mod: score_mod_t, batch_idx: Int)`

Flash attention v2 algorithm.

</section>

---

## mha_gpu_naive

<section class='mojo-docs'>

`mha_gpu_naive[output_type: DType, k_t: MHAOperand, v_t: MHAOperand, mask_t: MHAMask, //, ragged: Bool = False, sink: Bool = False, _use_valid_length: Bool = False, _is_cache_length_accurate: Bool = False](q: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: k_t, v: v_t, mask_functor: mask_t, output: LayoutTensor[output_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], valid_length: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scale: Float32, batch_size: Int, max_prompt_len: Int, max_cache_size: Int, num_heads: Int, depth: Int, group: Int, ctx: DeviceContext, sink_weights: OptionalReg[LayoutTensor[dtype, Layout.row_major(-1), MutAnyOrigin]] = None)`

`mha_gpu_naive[q_type: DType, k_type: DType, v_type: DType, output_type: DType, mask_type: DType, //, sink: Bool = False](q: LayoutTensor[q_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: LayoutTensor[k_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], v: LayoutTensor[v_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mask: LayoutTensor[mask_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[output_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scale: Float32, batch_size: Int, seq_len: Int, num_keys: Int, num_heads: Int, depth: Int, group: Int, ctx: DeviceContext, sink_weights: OptionalReg[LayoutTensor[q_type, Layout.row_major(-1), MutAnyOrigin]] = None)`

`mha_gpu_naive[q_type: DType, output_type: DType, cache_t: KVCacheT, mask_t: MHAMask, //, ragged: Bool = False, sink: Bool = False](q: LayoutTensor[q_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: cache_t, v: cache_t, mask_functor: mask_t, output: LayoutTensor[output_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], valid_length: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scale: Float32, batch_size: Int, max_prompt_len: Int, max_cache_size: Int, num_heads: Int, depth: Int, group: Int, ctx: DeviceContext, sink_weights: OptionalReg[LayoutTensor[q_type, Layout.row_major(-1), MutAnyOrigin]] = None)`

</section>

---

## mha_single_batch

<section class='mojo-docs'>

`mha_single_batch[q_type: DType, k_t: MHAOperand, v_t: MHAOperand, output_type: DType, mask_t: MHAMask, score_mod_t: ScoreModTrait, *, config: MHAConfig[dtype], group: Int = 1, use_score_mod: Bool = False, sink: Bool = False](q_ptr: LegacyUnsafePointer[Scalar[q_type]], k: k_t, v: v_t, output_ptr: LegacyUnsafePointer[Scalar[output_type]], scale: Float32, seq_len: Int, max_seq_len: Int, start_pos: UInt32, num_keys: Int, mask_tensor_col: Int, mask: mask_t, score_mod: score_mod_t, batch_idx: Int, sink_weights: OptionalReg[LayoutTensor[q_type, Layout.row_major(-1), MutAnyOrigin]])`

MHA for token gen where seqlen = 1 and num\_keys >= 1.

The general data layout and steps conform to flash attention. Two exceptions:

1 Partition across B, H, and num\_keys (TODO).  The last one is split-K and
will need a separate reduction kernel at the end.

2 First bmm becomes gemv and second bmm becomes gevm.
TODO: use more optimized kernels for them

</section>

---

## mha_single_batch_pipelined

<section class='mojo-docs'>

`mha_single_batch_pipelined[q_type: DType, k_t: MHAOperand, v_t: MHAOperand, output_type: DType, mask_t: MHAMask, score_mod_t: ScoreModTrait, *, config: MHAConfig[dtype], group: Int = 1, use_score_mod: Bool = False, sink: Bool = False](q_ptr: LegacyUnsafePointer[Scalar[q_type]], k: k_t, v: v_t, output_ptr: LegacyUnsafePointer[Scalar[output_type]], scale: Float32, seq_len: Int, max_seq_len: Int, start_pos: UInt32, num_keys: Int, mask_tensor_col: Int, mask: mask_t, score_mod: score_mod_t, batch_idx: Int, sink_weights: OptionalReg[LayoutTensor[q_type, Layout.row_major(-1), MutAnyOrigin]])`

MHA for token gen where seqlen = 1 and num\_keys >= 1.

The general data layout and steps conform to flash attention. Two exceptions:

1 Partition across B, H, and num\_keys (TODO).  The last one is split-K and
will need a separate reduction kernel at the end.

2 First bmm becomes gemv and second bmm becomes gevm.
TODO: use more optimized kernels for them

</section>

---

## mha_splitk_reduce

<section class='mojo-docs'>

`mha_splitk_reduce[output_type: DType, depth: UInt, num_heads: UInt, num_threads: UInt, group: UInt = 1, use_exp2: Bool = False](intermediate_ptr: LegacyUnsafePointer[Scalar[output_type]], output_ptr: LegacyUnsafePointer[Scalar[output_type]], exp_sum_ptr: LegacyUnsafePointer[Scalar[get_accum_type[output_type]()]], qk_max_ptr: LegacyUnsafePointer[Scalar[get_accum_type[output_type]()]], batch_size: Int, num_partitions: Int)`

</section>

---

## q_num_matrix_view_rows

<section class='mojo-docs'>

`q_num_matrix_view_rows[dtype: DType, //](q: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## scale_and_mask_helper

<section class='mojo-docs'>

`scale_and_mask_helper[p_type: DType, p_layout: Layout, mask_t: MHAMask, score_mod_t: ScoreModTrait, group: Int, num_n_mmas: Int, WN: Int, MMA_N: Int, simd_width: Int, use_score_mod: Bool = False](p_reg_tile: LayoutTensor[p_type, p_layout, origin, address_space=AddressSpace.LOCAL], scale_log2e: Float32, num_keys: UInt, bound: UInt, lane: UInt, warp: UInt, mask: mask_t, score_mod: score_mod_t, kv_tile_start_row: Int, mask_stride: UInt, max_seq_len: Int)`

</section>

---

## mha_cross

<section class='mojo-docs'>

## Functions

* [â€‹`mha_cross_gpu_naive`](./mha_cross_gpu_naive): Naive cross attention on GPU.

</section>

---

## mha_cross_gpu_naive

<section class='mojo-docs'>

`mha_cross_gpu_naive[cache_t: KVCacheT, mask_t: MHAMask, dtype: DType, //, rank: Int](output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q_input_row_offsets: LayoutTensor[DType.uint32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q_max_seq_len: Int, k: cache_t, v: cache_t, kv_input_row_offsets: LayoutTensor[DType.uint32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mask_functor: mask_t, scale: Float32, ctx: DeviceContext)`

Naive cross attention on GPU.

Note that this assumes ragged tensor inputs and uses a mask functor.

Computes:
(1) Transpose (Q) BSHD -> BHSD;
(2) Transpose (K) BSHD -> BHSD;
(3) Transpose (V) BSHD -> BHSD;
(4) P = Bmm(Q, K), P is also called "score";
(5) P = P \* scale + mask;
(6) P = softmax(P);
(7) O = Bmm(P, V)
(8) Output = Transpose(O).

B, S, H, D denote batch size, sequence length, head count and depth, respectively.
(1), (2), (3) happens while loading the data into shared memory.
(8) happens when writing output to global memory.

All inputs (query, key, and value) must have BSHD layout. The mask can be
BSS or BHSS.

This kernel also handles grouped attention optimization. In this case the shape of
K and V are BShD where h = H / num\_groups.

</section>

---

## MHAPosition

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MHAPosition[BM: Int, BN: Int, depth: Int, padded_depth: Int, q_num_heads: Int, group: Int, decoding: Bool]`

Position of the MHA-kernel. When `decoding=False`, `q_head_stride == q_num_heads`. When `decoding=True`, `q_head_stride == 1`.

## Fields

* â€‹q\_row (`UInt32`):
* â€‹q\_col (`UInt32`):
* â€‹q\_out\_offset (`Int`):
* â€‹num\_keys (`UInt32`):
* â€‹start\_pos (`UInt32`):
* â€‹seq\_len (`UInt32`):
* â€‹head\_idx (`UInt32`):
* â€‹prompt\_offset (`UInt32`):
* â€‹prompt\_idx (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `num_q_heads_per_thread`

`comptime num_q_heads_per_thread = min(2, ceildiv(group, 8)) if decoding else 1`

### `q_output_gmem_layout`

`comptime q_output_gmem_layout = Layout(IntTuple(BM, depth), IntTuple(MHAPosition[BM, BN, depth, padded_depth, q_num_heads, group, decoding].q_stride, 1))`

### `q_stride`

`comptime q_stride = depth if decoding else (depth * q_num_heads)`

### `split_gmem_layout`

`comptime split_gmem_layout = Layout(IntTuple((BM // 2), depth), IntTuple(MHAPosition[BM, BN, depth, padded_depth, q_num_heads, group, decoding].q_stride, 1))`

## Methods

### `__init__`

`__init__(q_row: UInt32, q_col: UInt32, q_out_offset: Int, num_keys: UInt32, start_pos: UInt32, seq_info: SeqInfo) -> Self`

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `q_head_idx`

`q_head_idx(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `kv_head_idx`

`kv_head_idx(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `write_to`

`write_to(self, mut writer: T)`

### `q_tile_num_rows`

`q_tile_num_rows(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `q_out_gmem_tensor`

`q_out_gmem_tensor[dtype: DType](self, ptr: LegacyUnsafePointer[Scalar[dtype]]) -> LayoutTensor[dtype, MHAPosition[BM, BN, depth, padded_depth, q_num_heads, group, decoding].q_output_gmem_layout, MutAnyOrigin, layout_int_type=DType.int32, linear_idx_type=DType.int32, masked=True]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `mask_status`

`mask_status[MaskType: MHAMask](self, mask: MaskType, kv_tile_start_row: UInt32) -> TileMaskStatus`

**Returns:**

`TileMaskStatus`

### `get_score_row`

`get_score_row(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `exp_sum_qk_max_ptr`

`exp_sum_qk_max_ptr[partition_t: MHAPartitionScheme](self, partition: partition_t, batch_size: UInt32) -> Tuple[LegacyUnsafePointer[Scalar[partition_t.accum_dtype]], LegacyUnsafePointer[Scalar[partition_t.accum_dtype]]]`

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

### `get_start_and_end_for_partitions`

`get_start_and_end_for_partitions[partition_t: MHAPartitionScheme, //](self, partition: partition_t) -> Tuple[UInt32, UInt32]`

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

### `get_q_gmem_row`

`static get_q_gmem_row[MaxSeqLenType: OptionallyStaticInt, //, ragged: Bool](seq_info: SeqInfo, max_seq_len: MaxSeqLenType) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

`static get_q_gmem_row[ragged: Bool](seq_info: SeqInfo, max_seq_len: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

</section>

---

## NonNullPointer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct NonNullPointer[dtype_: DType]`

## Fields

* â€‹ptr (`LegacyUnsafePointer[Scalar[NonNullPointer[dtype_].dtype]]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`OptionalPointer`](/mojo/kernels/nn/mha_fa3_utils/OptionalPointer)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `dtype`

`comptime dtype = dtype_`

### `is_null`

`comptime is_null = False`

## Methods

### `__init__`

`__init__(ptr: LegacyUnsafePointer[Scalar[NonNullPointer[dtype_].dtype]]) -> Self`

`__init__(ptr: DeviceBuffer[NonNullPointer[dtype_].dtype]) -> Self`

### `value`

`value(self) -> LegacyUnsafePointer[Scalar[NonNullPointer[dtype_].dtype]]`

**Returns:**

`LegacyUnsafePointer`

</section>

---

## NullPointer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct NullPointer[dtype_: DType]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`OptionalPointer`](/mojo/kernels/nn/mha_fa3_utils/OptionalPointer)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `dtype`

`comptime dtype = dtype_`

### `is_null`

`comptime is_null = True`

## Methods

### `__init__`

`__init__() -> Self`

### `value`

`value(self) -> LegacyUnsafePointer[Scalar[NullPointer[dtype_].dtype]]`

**Returns:**

`LegacyUnsafePointer`

</section>

---

## OptionalPointer

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__copyinit__` is trivial.

The implementation of `__copyinit__` is considered to be trivial if:

* The struct has a compiler-generated trivial `__copyinit__` and all its fields
  have a trivial `__copyinit__` method.

In practice, it means the value can be copied by copying the bits from
one location to another without side effects.

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__moveinit__` is trivial.

The implementation of `__moveinit__` is considered to be trivial if:

* The struct has a compiler-generated `__moveinit__` and all its fields
  have a trivial `__moveinit__` method.

In practice, it means the value can be moved by moving the bits from
one location to another without side effects.

### `dtype`

`comptime dtype`

### `is_null`

`comptime is_null`

## Required methods

### `__copyinit__`

`__copyinit__(out self: _Self, existing: _Self, /)`

Create a new instance of the value by copying an existing one.

**Args:**

* â€‹existing (`_Self`): The value to copy.

**Returns:**

`_Self`

### `__moveinit__`

`__moveinit__(out self: _Self, deinit existing: _Self, /)`

Create a new instance of the value by moving the value of another.

**Args:**

* â€‹existing (`_Self`): The value to move.

**Returns:**

`_Self`

### `value`

`value(self: _Self) -> LegacyUnsafePointer[Scalar[_Self.dtype]]`

**Returns:**

`LegacyUnsafePointer`

## Provided methods

### `copy`

`copy(self: _Self) -> _Self`

Explicitly construct a copy of self.

**Returns:**

`_Self`: A copy of this value.

</section>

---

## Pack

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct Pack[MaskType: MHAMask, ScoreModType: ScoreModTrait, SchedulerType: MHATileScheduler, ValidLengthType: OptionalPointer, SinkType: OptionalPointer, KVRowOffsetsType: OptionalPointer, MaxSeqLenType: OptionallyStaticInt, PartitionType: MHAPartitionScheme]`

## Fields

* â€‹mask (`MaskType`):
* â€‹score\_mod (`ScoreModType`):
* â€‹scheduler (`SchedulerType`):
* â€‹valid\_length (`ValidLengthType`):
* â€‹sink\_weights (`SinkType`):
* â€‹kv\_input\_row\_offsets (`KVRowOffsetsType`):
* â€‹max\_seq\_len (`MaxSeqLenType`):
* â€‹partition (`PartitionType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = PartitionType.__copyinit__is_trivial if MaxSeqLenType.__copyinit__is_trivial if KVRowOffsetsType.__copyinit__is_trivial if SinkType.__copyinit__is_trivial if ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SinkType.__copyinit__is_trivial if ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else KVRowOffsetsType.__copyinit__is_trivial if SinkType.__copyinit__is_trivial if ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SinkType.__copyinit__is_trivial if ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else MaxSeqLenType.__copyinit__is_trivial if KVRowOffsetsType.__copyinit__is_trivial if SinkType.__copyinit__is_trivial if ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SinkType.__copyinit__is_trivial if ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else KVRowOffsetsType.__copyinit__is_trivial if SinkType.__copyinit__is_trivial if ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SinkType.__copyinit__is_trivial if ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ValidLengthType.__copyinit__is_trivial if SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else SchedulerType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial`

### `__del__is_trivial`

`comptime __del__is_trivial = PartitionType.__del__is_trivial if MaxSeqLenType.__del__is_trivial if KVRowOffsetsType.__del__is_trivial if SinkType.__del__is_trivial if ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SinkType.__del__is_trivial if ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else KVRowOffsetsType.__del__is_trivial if SinkType.__del__is_trivial if ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SinkType.__del__is_trivial if ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else MaxSeqLenType.__del__is_trivial if KVRowOffsetsType.__del__is_trivial if SinkType.__del__is_trivial if ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SinkType.__del__is_trivial if ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else KVRowOffsetsType.__del__is_trivial if SinkType.__del__is_trivial if ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SinkType.__del__is_trivial if ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ValidLengthType.__del__is_trivial if SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else SchedulerType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = PartitionType.__moveinit__is_trivial if MaxSeqLenType.__moveinit__is_trivial if KVRowOffsetsType.__moveinit__is_trivial if SinkType.__moveinit__is_trivial if ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SinkType.__moveinit__is_trivial if ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else KVRowOffsetsType.__moveinit__is_trivial if SinkType.__moveinit__is_trivial if ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SinkType.__moveinit__is_trivial if ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else MaxSeqLenType.__moveinit__is_trivial if KVRowOffsetsType.__moveinit__is_trivial if SinkType.__moveinit__is_trivial if ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SinkType.__moveinit__is_trivial if ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else KVRowOffsetsType.__moveinit__is_trivial if SinkType.__moveinit__is_trivial if ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SinkType.__moveinit__is_trivial if ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ValidLengthType.__moveinit__is_trivial if SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else SchedulerType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial`

### `device_type`

`comptime device_type = Pack[MaskType, ScoreModType, SchedulerType, ValidLengthType, SinkType, KVRowOffsetsType, MaxSeqLenType, PartitionType]`

## Methods

### `__init__`

`__init__(mask: MaskType, score_mod: ScoreModType, scheduler: SchedulerType, valid_length: ValidLengthType, sink_weights: SinkType, kv_input_row_offsets: KVRowOffsetsType, max_seq_len: MaxSeqLenType, partition: PartitionType) -> Self`

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

</section>

---

## PositionSummary

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct PositionSummary`

## Fields

* â€‹num\_keys (`UInt32`):
* â€‹score\_row (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(num_keys: UInt32, score_row: UInt32) -> Self`

### `get_start_pos`

`static get_start_pos[KVLUTType: MHAOperand, //, ragged: Bool, _is_cache_length_accurate: Bool](kv_lut: KVLUTType, seq_info: SeqInfo, num_keys_arg: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `get_num_keys`

`static get_num_keys[MaxSeqLenType: OptionallyStaticInt, KVInputRowOffsetsType: OptionalPointer, //, ragged: Bool, _is_cache_length_accurate: Bool](kv_input_row_offsets: KVInputRowOffsetsType, seq_info: SeqInfo, max_seq_len: MaxSeqLenType, num_keys_arg: UInt32, start_pos: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `get_score_row`

`static get_score_row[*, ragged: Bool, _is_cache_length_accurate: Bool, decoding: Bool](seq_info: SeqInfo, num_keys: UInt32, start_pos: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `create`

`static create[KVLUTType: MHAOperand, KVRowOffsetsType: OptionalPointer, MaxSeqLenType: OptionallyStaticInt, //, ragged: Bool, _is_cache_length_accurate: Bool](kv_lut: KVLUTType, seq_info: SeqInfo, num_keys_arg: UInt32, kv_input_row_offsets: KVRowOffsetsType, max_seq_len: MaxSeqLenType) -> Self`

</section>

---

## get_q_head_idx

<section class='mojo-docs'>

`get_q_head_idx[BM: Int, BN: Int, depth: Int, padded_depth: Int, num_heads: Int, group: Int, decoding: Bool, //](position: MHAPosition[BM, BN, depth, padded_depth, num_heads, group, decoding], lane: UInt32) -> StaticTuple[UInt32, MHAPosition[BM, BN, depth, padded_depth, num_heads, group, decoding].num_q_heads_per_thread]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

</section>

---

## get_seq_info

<section class='mojo-docs'>

`get_seq_info[MaxSeqLenType: OptionallyStaticInt, ValidLengthType: OptionalPointer, PartitionType: MHAPartitionScheme, //, BM: Int, num_heads: Int](batch_size: UInt32, max_seq_len: MaxSeqLenType, valid_length: ValidLengthType, partition: PartitionType) -> SeqInfo`

**Returns:**

`SeqInfo`

</section>

---

## mha_fa3_utils

<section class='mojo-docs'>

## `comptime` values

### `KVTMATile`

`comptime KVTMATile[dtype: DType, swizzle_mode: TensorMapSwizzle, *, BN: Int, depth: Int, BK: Int = depth] = TMATensorTile[dtype, _split_last_layout[dtype](IndexList[3, DType.int64](BN, 1, BK, Tuple[]()), swizzle_mode, True), _ragged_desc_layout[dtype](IndexList[3, DType.int64](BN, 1, BK, Tuple[]()), swizzle_mode)]`

#### Parameters

* â€‹dtype ([`DType`](/std/builtin/dtype/DType)):
* â€‹swizzle\_mode (`TensorMapSwizzle`):
* â€‹BN ([`Int`](/std/builtin/int/Int)):
* â€‹depth ([`Int`](/std/builtin/int/Int)):
* â€‹BK ([`Int`](/std/builtin/int/Int)):

### `QTMATile`

`comptime QTMATile[dtype: DType, swizzle_mode: TensorMapSwizzle, *, BM: Int, depth: Int, group: Int, decoding: Bool] = TMATensorTile[dtype, _split_last_layout[dtype](q_smem_shape[dtype, swizzle_mode, BM=BM, group=group, depth=depth, decoding=decoding](), swizzle_mode, True), _ragged_desc_layout[dtype](q_smem_shape[dtype, swizzle_mode, BM=BM, group=group, depth=depth, decoding=decoding](), swizzle_mode)]`

#### Parameters

* â€‹dtype ([`DType`](/std/builtin/dtype/DType)):
* â€‹swizzle\_mode (`TensorMapSwizzle`):
* â€‹BM ([`Int`](/std/builtin/int/Int)):
* â€‹depth ([`Int`](/std/builtin/int/Int)):
* â€‹group ([`Int`](/std/builtin/int/Int)):
* â€‹decoding ([`Bool`](/std/builtin/bool/Bool)):

## Structs

* [â€‹`MHAPosition`](./MHAPosition): Position of the MHA-kernel. When `decoding=False`, `q_head_stride == q_num_heads`. When `decoding=True`, `q_head_stride == 1`.
* [â€‹`NonNullPointer`](./NonNullPointer):
* [â€‹`NullPointer`](./NullPointer):
* [â€‹`Pack`](./Pack):
* [â€‹`PositionSummary`](./PositionSummary):

## Traits

* [â€‹`OptionalPointer`](./OptionalPointer):

## Functions

* [â€‹`get_q_head_idx`](./get_q_head_idx):
* [â€‹`get_seq_info`](./get_seq_info):
* [â€‹`kv_coord`](./kv_coord):
* [â€‹`output_reg_to_smem`](./output_reg_to_smem):
* [â€‹`output_reg_to_smem_st_matrix`](./output_reg_to_smem_st_matrix):
* [â€‹`produce`](./produce):
* [â€‹`q_coord`](./q_coord): Returns the coordinates for a tma load on the `Q` matrix. This load can be 3D, 4D, or 5D.
* [â€‹`q_gmem_shape`](./q_gmem_shape):
* [â€‹`q_smem_shape`](./q_smem_shape):
* [â€‹`q_tma`](./q_tma):

</section>

---

## kv_coord

<section class='mojo-docs'>

`kv_coord[*, depth: Int, swizzle_granularity: Int](row: UInt32, head_idx: UInt32) -> StaticTuple[UInt32, 4 if _should_split_last_dim(depth, swizzle_granularity) else 3]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

</section>

---

## output_reg_to_smem

<section class='mojo-docs'>

`output_reg_to_smem[output_type: DType, accum_type: DType, num_m_mmas: Int, o_frag_size: Int, //, BM: Int, BN: Int, padded_depth: Int, swizzle: Swizzle, num_consumer: Int](tid: UInt32, local_warp_group_idx: UInt32, warp_y: UInt32, q_smem: LegacyUnsafePointer[Scalar[output_type], address_space=AddressSpace.SHARED], output_reg_tile: LayoutTensor[accum_type, Layout.row_major(num_m_mmas, o_frag_size), MutAnyOrigin, address_space=AddressSpace.LOCAL]) -> LayoutTensor[output_type, Layout.row_major(BM, padded_depth), MutAnyOrigin, address_space=AddressSpace.SHARED]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

</section>

---

## output_reg_to_smem_st_matrix

<section class='mojo-docs'>

`output_reg_to_smem_st_matrix[output_type: DType, accum_type: DType, num_m_mmas: Int, o_frag_size: Int, //, BM: Int, padded_depth: Int, swizzle: Swizzle, num_consumer: Int](warp_group_thread_idx: UInt32, local_warp_group_idx: UInt32, output_reg_tile: LayoutTensor[accum_type, Layout.row_major(num_m_mmas, o_frag_size), MutAnyOrigin, address_space=AddressSpace.LOCAL], accum_smem_tile: LayoutTensor[output_type, Layout.row_major(BM, padded_depth), MutAnyOrigin, address_space=AddressSpace.SHARED])`

</section>

---

## produce

<section class='mojo-docs'>

`produce[qkv_type: DType, BM: Int, BN: Int, q_smem_layout: Layout, q_desc_layout: Layout, k_smem_layout: Layout, k_desc_layout: Layout, v_smem_layout: Layout, v_desc_layout: Layout, depth: Int, padded_depth: Int, num_heads: Int, group: Int, PartitionType: MHAPartitionScheme, MaxSeqLenType: OptionallyStaticInt, SchedulerType: MHATileScheduler, KVLUTType: MHAOperand, MaskType: MHAMask, KVInputRowOffsetsType: OptionalPointer, ValidLengthType: OptionalPointer, //, swizzle_mode: TensorMapSwizzle, *, pipeline_stages: Int, ragged: Bool, _is_cache_length_accurate: Bool](q_tma_op: TMATensorTile[qkv_type, q_smem_layout, q_desc_layout], k_tma_op: TMATensorTile[qkv_type, k_smem_layout, k_desc_layout], v_tma_op: TMATensorTile[qkv_type, v_smem_layout, v_desc_layout], q_smem: LegacyUnsafePointer[Scalar[qkv_type], address_space=AddressSpace.SHARED], kv_smem: LegacyUnsafePointer[Scalar[qkv_type], address_space=AddressSpace.SHARED], produced_mbar_kv: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], consumed_mbar_kv: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], produced_mbar_q: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], consumed_mbar_q: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], kv_lut: KVLUTType, initial_position: MHAPosition[BM, BN, depth, padded_depth, num_heads, group, _is_decoding[MaxSeqLenType]()], partition: PartitionType, scheduler: SchedulerType, mask: MaskType, tile_summary: MHATileSummary[ValidLengthType], tile_state_arg: MHATileState, max_seq_len: MaxSeqLenType, num_keys_arg: UInt32, kv_input_row_offsets: KVInputRowOffsetsType)`

</section>

---

## q_coord

<section class='mojo-docs'>

`q_coord[*, depth: Int, swizzle_granularity: Int, decoding: Bool](row: UInt32, head_idx: UInt32) -> StaticTuple[UInt32, (4 if decoding else 3 + Int.__init__[Bool](_should_split_last_dim(depth, swizzle_granularity)))]`

Returns the coordinates for a tma load on the `Q` matrix. This load can be 3D, 4D, or 5D.

Arguments:
row: the row to load from.
head\_idx: q\_head\_idx if prefill, kv\_head\_idx if decoding.

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

</section>

---

## q_gmem_shape

<section class='mojo-docs'>

`q_gmem_shape[dtype: DType, swizzle_mode: TensorMapSwizzle, *, group: Int, q_num_heads: Int, depth: Int, decoding: Bool]() -> IndexList[3 if (xor decoding._mlir_value, True) else 5 if _should_split_last_dim[dtype](depth, swizzle_mode) else 4]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## q_smem_shape

<section class='mojo-docs'>

`q_smem_shape[dtype: DType, swizzle_mode: TensorMapSwizzle, *, BM: Int, group: Int, depth: Int, decoding: Bool]() -> IndexList[3 if (xor decoding._mlir_value, True) else 5 if _should_split_last_dim[dtype](depth, swizzle_mode) else 4]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## q_tma

<section class='mojo-docs'>

`q_tma[dtype: DType, //, swizzle_mode: TensorMapSwizzle, *, BM: Int, depth: Int, q_num_heads: Int, group: Int, decoding: Bool](ctx: DeviceContext, ptr: LegacyUnsafePointer[Scalar[dtype]], rows: Int) -> TMATensorTile[dtype, _split_last_layout[dtype](q_smem_shape[dtype, swizzle_mode, BM=BM, group=group, depth=depth, decoding=decoding](), swizzle_mode, True), _ragged_desc_layout[dtype](q_smem_shape[dtype, swizzle_mode, BM=BM, group=group, depth=depth, decoding=decoding](), swizzle_mode)]`

**Returns:**

[`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)

</section>

---

## AndMask

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct AndMask[T: MHAMask, S: MHAMask, //, lhs: T, rhs: S]`

Mask that's the AND of two masks.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHAMask`](/mojo/kernels/nn/mha_mask/MHAMask),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `apply_log2e_after_mask`

`comptime apply_log2e_after_mask = T.apply_log2e_after_mask if T.apply_log2e_after_mask else S.apply_log2e_after_mask`

### `check_mask_during_decoding`

`comptime check_mask_during_decoding = S.check_mask_during_decoding if T.check_mask_during_decoding else T.check_mask_during_decoding`

### `device_type`

`comptime device_type = AndMask[lhs, rhs]`

### `mask_out_of_bound`

`comptime mask_out_of_bound = T.mask_out_of_bound if T.mask_out_of_bound else S.mask_out_of_bound`

### `mask_safe_out_of_bounds`

`comptime mask_safe_out_of_bounds = S.mask_safe_out_of_bounds if T.mask_safe_out_of_bounds else T.mask_safe_out_of_bounds`

## Methods

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `name`

`static name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `mask`

`mask[dtype: DType, width: Int, //, *, element_type: DType = DType.uint32](self, coord: IndexList[4, element_type=element_type], score_vec: SIMD[dtype, width]) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

### `status`

`status[*, element_type: DType = DType.uint32](self, tile_offset: IndexList[2, element_type=element_type], tile_size: IndexList[2, element_type=element_type]) -> TileMaskStatus`

**Returns:**

`TileMaskStatus`

### `start_column`

`start_column[BM: Int, BN: Int, page_size: Int](self, row: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `total_iters`

`total_iters[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `count_nonfull_sets`

`static count_nonfull_sets(BM: Int, BN: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `last_masked_set_end`

`last_masked_set_end[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `masked_set_ends`

`masked_set_ends[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> StaticTuple[UInt32, AndMask.count_nonfull_sets[T, S, lhs, rhs](BM, BN)]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

### `nonfull_sets`

`static nonfull_sets[BM: Int, BN: Int]() -> StaticTuple[TileMaskStatus, AndMask.count_nonfull_sets[T, S, lhs, rhs](BM, BN)]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

</section>

---

## CausalMask

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct CausalMask`

MHA causal mask ensures a token is only affected by previous tokens.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHAMask`](/mojo/kernels/nn/mha_mask/MHAMask),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `apply_log2e_after_mask`

`comptime apply_log2e_after_mask = False`

### `check_mask_during_decoding`

`comptime check_mask_during_decoding = False`

### `device_type`

`comptime device_type = CausalMask`

### `mask_out_of_bound`

`comptime mask_out_of_bound = is_nvidia_gpu()`

### `mask_safe_out_of_bounds`

`comptime mask_safe_out_of_bounds = True`

## Methods

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `name`

`static name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `mask`

`mask[dtype: DType, width: Int, //, *, element_type: DType = DType.uint32](self, coord: IndexList[4, element_type=element_type], score_vec: SIMD[dtype, width]) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

### `status`

`status[*, element_type: DType = DType.uint32](self, tile_offset: IndexList[2, element_type=element_type], tile_size: IndexList[2, element_type=element_type]) -> TileMaskStatus`

**Returns:**

`TileMaskStatus`

### `start_column`

`start_column[BM: Int, BN: Int, page_size: Int](self, row: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `total_iters`

`total_iters[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `count_nonfull_sets`

`static count_nonfull_sets(BM: Int, BN: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `last_masked_set_end`

`last_masked_set_end[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `masked_set_ends`

`masked_set_ends[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> StaticTuple[UInt32, CausalMask.count_nonfull_sets(BM, BN)]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

### `nonfull_sets`

`static nonfull_sets[BM: Int, BN: Int]() -> StaticTuple[TileMaskStatus, CausalMask.count_nonfull_sets(BM, BN)]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

</section>

---

## ChunkedCausalMask

<section class='mojo-docs'>

`ChunkedCausalMask[local_window_size: Int]() -> OrMask[CausalMask(), ChunkedMask[local_window_size]()]`

Mask implementing Chunked Causal attention for Llama4 models.

This groups the mask into chunks of size `local_window_size` and performs causal
attention within each local chunk. Considering the following case:

* Q\_len = 7
* K\_len = 10
* start\_pos = 3
* local\_window\_size = 4

The mask will be applied as follows:
K > 0 1 2 3 4 5 6 7 8 9
Q v x--------------------x
0 | 1 1 1 1 0 0 0 0 0 0
1 | 0 0 0 0 1 0 0 0 0 0
2 | 0 0 0 0 1 1 0 0 0 0
3 | 0 0 0 0 1 1 1 0 0 0
4 | 0 0 0 0 1 1 1 1 0 0
5 | 0 0 0 0 0 0 0 0 1 0
6 | 0 0 0 0 0 0 0 0 1 1

**Returns:**

`OrMask`

</section>

---

## ChunkedMask

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ChunkedMask[local_window_size: Int]`

Mask implementing Chunked attention.

This groups the mask into chunks of size `local_window_size`.
Considering the following case:

* Q\_len = 7
* K\_len = 10
* local\_window\_size = 4

The mask will be applied as follows:
K > 0 1 2 3 4 5 6 7 8 9
Q v x--------------------x
0 | 1 1 1 1 0 0 0 0 0 0
1 | 0 0 0 0 1 1 1 1 0 0
2 | 0 0 0 0 1 1 1 1 0 0
3 | 0 0 0 0 1 1 1 1 0 0
4 | 0 0 0 0 1 1 1 1 0 0
5 | 0 0 0 0 0 0 0 0 1 1
6 | 0 0 0 0 0 0 0 0 1 1

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHAMask`](/mojo/kernels/nn/mha_mask/MHAMask),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `apply_log2e_after_mask`

`comptime apply_log2e_after_mask = False`

### `check_mask_during_decoding`

`comptime check_mask_during_decoding = True`

### `device_type`

`comptime device_type = ChunkedMask[local_window_size]`

### `mask_out_of_bound`

`comptime mask_out_of_bound = True`

### `mask_safe_out_of_bounds`

`comptime mask_safe_out_of_bounds = True`

## Methods

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `name`

`static name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `mask`

`mask[dtype: DType, width: Int, //, *, element_type: DType = DType.uint32](self, coord: IndexList[4, element_type=element_type], score_vec: SIMD[dtype, width]) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

### `status`

`status[*, element_type: DType = DType.uint32](self, tile_offset: IndexList[2, element_type=element_type], tile_size: IndexList[2, element_type=element_type]) -> TileMaskStatus`

**Returns:**

`TileMaskStatus`

### `start_column`

`start_column[BM: Int, BN: Int, page_size: Int](self, row: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `total_iters`

`total_iters[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `count_nonfull_sets`

`static count_nonfull_sets(BM: Int, BN: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `last_masked_set_end`

`last_masked_set_end[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `masked_set_ends`

`masked_set_ends[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> StaticTuple[UInt32, ChunkedMask.count_nonfull_sets[local_window_size](BM, BN)]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

### `nonfull_sets`

`static nonfull_sets[BM: Int, BN: Int]() -> StaticTuple[TileMaskStatus, ChunkedMask.count_nonfull_sets[local_window_size](BM, BN)]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

</section>

---

## MHAMask

<section class='mojo-docs'>

The MHAMask trait describes masks for MHA kernels, such as the causal mask.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__copyinit__` is trivial.

The implementation of `__copyinit__` is considered to be trivial if:

* The struct has a compiler-generated trivial `__copyinit__` and all its fields
  have a trivial `__copyinit__` method.

In practice, it means the value can be copied by copying the bits from
one location to another without side effects.

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__moveinit__` is trivial.

The implementation of `__moveinit__` is considered to be trivial if:

* The struct has a compiler-generated `__moveinit__` and all its fields
  have a trivial `__moveinit__` method.

In practice, it means the value can be moved by moving the bits from
one location to another without side effects.

### `apply_log2e_after_mask`

`comptime apply_log2e_after_mask`

Does the mask require `log2e` to be applied after the mask, or can it be fused with the scaling?

### `check_mask_during_decoding`

`comptime check_mask_during_decoding`

Should we check the mask during decoding, or should we assume that it does not return `FULL_MASK`?

### `device_type`

`comptime device_type`

Indicate the type being used on accelerator devices.

### `mask_out_of_bound`

`comptime mask_out_of_bound`

### `mask_safe_out_of_bounds`

`comptime mask_safe_out_of_bounds`

Is the mask safe to read out of bounds?

## Required methods

### `__copyinit__`

`__copyinit__(out self: _Self, existing: _Self, /)`

Create a new instance of the value by copying an existing one.

**Args:**

* â€‹existing (`_Self`): The value to copy.

**Returns:**

`_Self`

### `__moveinit__`

`__moveinit__(out self: _Self, deinit existing: _Self, /)`

Create a new instance of the value by moving the value of another.

**Args:**

* â€‹existing (`_Self`): The value to move.

**Returns:**

`_Self`

### `mask`

`mask[dtype: DType, width: Int, //, *, element_type: DType = DType.uint32](self: _Self, coord: IndexList[4, element_type=element_type], score_vec: SIMD[dtype, width]) -> SIMD[dtype, width]`

Return mask vector at given coordinates.

Arguments:
coord is (seq\_id, head, q\_idx, k\_idx)
score\_vec is at `coord` of the score matrix

The functor could capture an mask tensor and add to the score e.g. Replit.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

### `status`

`status[*, element_type: DType = DType.uint32](self: _Self, tile_offset: IndexList[2, element_type=element_type], tile_size: IndexList[2, element_type=element_type]) -> TileMaskStatus`

Given a tile's index range, return its masking status.

**Returns:**

`TileMaskStatus`

### `start_column`

`start_column[BM: Int, BN: Int, page_size: Int](self: _Self, row: UInt32) -> UInt32`

Returns the first column for which this mask does not return `TileMaskStatus.FULL_MASK`. This may not be a multiple of `BN`, in which case iterating using `start_column` and `masked_set_ends` will not necessarilly produce the same set or number of iterations as iterating from `0` and checking `status` to skip. The return value of `total_iters` should be less than or equal to the number of non-skipped iterations. The practical consequence is that all warp group specializations within a kernel that loop over columns need to be in agreement. Either they all loop over all columns and check status to skip, or they loop using the `masked_set_ends`.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `total_iters`

`total_iters[BM: Int, BN: Int, page_size: Int](self: _Self, row: UInt32, num_cols: UInt32) -> UInt32`

The total number of column iterations for which this mask returns either `TileMaskStatus.NO_MASK' or 'TileMaskStatus.PARTIAL_MASK'. This is to be used by warp specializations that do not need to use `kv\_row\`.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `count_nonfull_sets`

`static count_nonfull_sets(BM: Int, BN: Int) -> Int`

The number of blocks that are all partial-masks or not masked.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `masked_set_ends`

`masked_set_ends[BM: Int, BN: Int, page_size: Int](self: _Self, row: UInt32, num_cols: UInt32) -> StaticTuple[UInt32, _Self.count_nonfull_sets(::Int,::Int)(BM, BN)]`

For each set of iterations in `nonfull_sets`, indicate the end idx belonging to that set (i.e., the last idx would be `end - 1`). Note that the final `masked_set_ends` may not necessarilly equal `total_iters`, if we have `UNKNOWN_MASK`s. In case of `UNKNOWN_MASK`s, `masked_set_ends` with tile-skipping must be used to have the correct kv\_row values at each iteration.

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

### `last_masked_set_end`

`last_masked_set_end[BM: Int, BN: Int, page_size: Int](self: _Self, row: UInt32, num_cols: UInt32) -> UInt32`

Equivalent to `masked_set_ends[BM,BN,page_size](row, num_cols)[-1]`.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `nonfull_sets`

`static nonfull_sets[BM: Int, BN: Int]() -> StaticTuple[TileMaskStatus, _Self.count_nonfull_sets(::Int,::Int)(BM, BN)]`

For each set of iterations that are either partially masked or not masked, this indicates the mask status. `UNKNOWN_MASK` here is an indicator meaning that we should check the status at runtime. It is semantically equivalent to `partial`, but with the optimization hint that it's worth checking on each iteration at runtime for `FULL_MASK` (in which case we can skip the tile) or `NO_MASK` (in which case we can unswitch and avoid masking in an inner loop).

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

### `name`

`static name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_type_name`

`static get_type_name() -> String`

Gets the name of the host type (the one implementing this trait). For example, Int would return "Int", DeviceBuffer\[DType.float32] would return "DeviceBuffer\[DType.float32]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): The host type's name.

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name. For example, because DeviceBuffer's device\_type is UnsafePointer, DeviceBuffer\[DType.float32]'s get\_device\_type\_name() should return something like "UnsafePointer\[Scalar\[DType.float32]]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): The device type's name.

## Provided methods

### `copy`

`copy(self: _Self) -> _Self`

Explicitly construct a copy of self.

**Returns:**

`_Self`: A copy of this value.

</section>

---

## MaskName

<section class='mojo-docs'>

`struct MaskName`

A tile's masking status.

## Fields

* â€‹name (`String`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Stringable`](/mojo/std/builtin/str/Stringable)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = False`

### `CAUSAL`

`comptime CAUSAL = MaskName("causal")`

### `CHUNKED`

`comptime CHUNKED = MaskName("chunked")`

### `CHUNKED_CAUSAL`

`comptime CHUNKED_CAUSAL = MaskName("chunked_causal")`

### `MATERIALIZED`

`comptime MATERIALIZED = MaskName("materialized")`

### `NULL`

`comptime NULL = MaskName("null")`

### `SLIDING_WINDOW_CAUSAL`

`comptime SLIDING_WINDOW_CAUSAL = MaskName("sliding_window_causal")`

## Methods

### `__init__`

`__init__(out self, name: String)`

### `__eq__`

`__eq__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

`__eq__(self, rhs: String) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

</section>

---

## MaterializedMask

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MaterializedMask[dtype_: DType, layout_: Layout]`

Mask that's backed by a materialized tensor.

## Fields

* â€‹mask\_tensor (`MaterializedMask[dtype_, layout_].MaskType`):
* â€‹start\_pos (`OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]]`):
* â€‹is\_multiple\_of\_2 (`Bool`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHAMask`](/mojo/kernels/nn/mha_mask/MHAMask),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `apply_log2e_after_mask`

`comptime apply_log2e_after_mask = True`

### `check_mask_during_decoding`

`comptime check_mask_during_decoding = True`

### `device_type`

`comptime device_type = MaterializedMask[dtype_, layout_]`

### `mask_out_of_bound`

`comptime mask_out_of_bound = True`

### `mask_safe_out_of_bounds`

`comptime mask_safe_out_of_bounds = False`

### `MaskType`

`comptime MaskType = LayoutTensor[dtype_, layout_, MutAnyOrigin]`

## Methods

### `__init__`

`__init__(mask_tensor: LayoutTensor[dtype_, layout_, MutAnyOrigin], start_pos: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]] = None) -> Self`

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `name`

`static name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_start_pos`

`get_start_pos(self, batch_idx: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `mask`

`mask[dtype: DType, width: Int, //, *, element_type: DType = DType.uint32](self, coord: IndexList[4, element_type=element_type], score_vec: SIMD[dtype, width]) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

### `status`

`status[*, element_type: DType = DType.uint32](self, tile_offset: IndexList[2, element_type=element_type], tile_size: IndexList[2, element_type=element_type]) -> TileMaskStatus`

**Returns:**

`TileMaskStatus`

### `start_column`

`start_column[BM: Int, BN: Int, page_size: Int](self, row: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `total_iters`

`total_iters[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `count_nonfull_sets`

`static count_nonfull_sets(BM: Int, BN: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `last_masked_set_end`

`last_masked_set_end[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `masked_set_ends`

`masked_set_ends[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> StaticTuple[UInt32, MaterializedMask.count_nonfull_sets[dtype_, layout_](BM, BN)]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

### `nonfull_sets`

`static nonfull_sets[BM: Int, BN: Int]() -> StaticTuple[TileMaskStatus, MaterializedMask.count_nonfull_sets[dtype_, layout_](BM, BN)]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

</section>

---

## NullMask

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct NullMask`

Mask that's effectively a noop.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHAMask`](/mojo/kernels/nn/mha_mask/MHAMask),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `apply_log2e_after_mask`

`comptime apply_log2e_after_mask = False`

### `check_mask_during_decoding`

`comptime check_mask_during_decoding = False`

### `device_type`

`comptime device_type = NullMask`

### `mask_out_of_bound`

`comptime mask_out_of_bound = True`

### `mask_safe_out_of_bounds`

`comptime mask_safe_out_of_bounds = True`

## Methods

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `name`

`static name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `mask`

`mask[dtype: DType, width: Int, //, *, element_type: DType = DType.uint32](self, coord: IndexList[4, element_type=element_type], score_vec: SIMD[dtype, width]) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

### `status`

`status[*, element_type: DType = DType.uint32](self, tile_offset: IndexList[2, element_type=element_type], tile_size: IndexList[2, element_type=element_type]) -> TileMaskStatus`

**Returns:**

`TileMaskStatus`

### `start_column`

`start_column[BM: Int, BN: Int, page_size: Int](self, row: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `total_iters`

`total_iters[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> UInt32`

The total number of column iterations for which this mask returns either \`TileMaskStatus.NO\_MASK' or 'TileMaskStatus.PARTIAL\_MASK'.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `last_masked_set_end`

`last_masked_set_end[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `count_nonfull_sets`

`static count_nonfull_sets(BM: Int, BN: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `masked_set_ends`

`masked_set_ends[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> StaticTuple[UInt32, NullMask.count_nonfull_sets(BM, BN)]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

### `nonfull_sets`

`static nonfull_sets[BM: Int, BN: Int]() -> StaticTuple[TileMaskStatus, NullMask.count_nonfull_sets(BM, BN)]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

</section>

---

## OrMask

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct OrMask[T: MHAMask, S: MHAMask, //, lhs: T, rhs: S]`

Mask that's the OR of two masks.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHAMask`](/mojo/kernels/nn/mha_mask/MHAMask),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `apply_log2e_after_mask`

`comptime apply_log2e_after_mask = T.apply_log2e_after_mask if T.apply_log2e_after_mask else S.apply_log2e_after_mask`

### `check_mask_during_decoding`

`comptime check_mask_during_decoding = T.check_mask_during_decoding if T.check_mask_during_decoding else S.check_mask_during_decoding`

### `device_type`

`comptime device_type = OrMask[lhs, rhs]`

### `mask_out_of_bound`

`comptime mask_out_of_bound = S.mask_out_of_bound if T.mask_out_of_bound else T.mask_out_of_bound`

### `mask_safe_out_of_bounds`

`comptime mask_safe_out_of_bounds = S.mask_safe_out_of_bounds if T.mask_safe_out_of_bounds else T.mask_safe_out_of_bounds`

## Methods

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `name`

`static name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `mask`

`mask[dtype: DType, width: Int, //, *, element_type: DType = DType.uint32](self, coord: IndexList[4, element_type=element_type], score_vec: SIMD[dtype, width]) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

### `status`

`status[*, element_type: DType = DType.uint32](self, tile_offset: IndexList[2, element_type=element_type], tile_size: IndexList[2, element_type=element_type]) -> TileMaskStatus`

**Returns:**

`TileMaskStatus`

### `start_column`

`start_column[BM: Int, BN: Int, page_size: Int](self, row: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `total_iters`

`total_iters[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `count_nonfull_sets`

`static count_nonfull_sets(BM: Int, BN: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `last_masked_set_end`

`last_masked_set_end[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `masked_set_ends`

`masked_set_ends[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> StaticTuple[UInt32, OrMask.count_nonfull_sets[T, S, lhs, rhs](BM, BN)]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

### `nonfull_sets`

`static nonfull_sets[BM: Int, BN: Int]() -> StaticTuple[TileMaskStatus, OrMask.count_nonfull_sets[T, S, lhs, rhs](BM, BN)]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

</section>

---

## SlidingWindowCausalMask

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SlidingWindowCausalMask[window_size: Int]`

Mask implementing Sliding Window attention.

Considering the following case:

* Q\_len = 7
* K\_len = 7
* window\_size = 3

The mask will be applied as follows:
K > 0 1 2 3 4 5 6
Q v x------------x
0 | 1 0 0 0 0 0 0
1 | 1 1 0 0 0 0 0
2 | 1 1 1 0 0 0 0
3 | 0 1 1 1 0 0 0
4 | 0 0 1 1 1 0 0
5 | 0 0 0 1 1 1 0
6 | 0 0 0 0 1 1 1

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHAMask`](/mojo/kernels/nn/mha_mask/MHAMask),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `apply_log2e_after_mask`

`comptime apply_log2e_after_mask = False`

### `check_mask_during_decoding`

`comptime check_mask_during_decoding = True`

### `device_type`

`comptime device_type = SlidingWindowCausalMask[window_size]`

### `mask_out_of_bound`

`comptime mask_out_of_bound = True`

### `mask_safe_out_of_bounds`

`comptime mask_safe_out_of_bounds = True`

## Methods

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `name`

`static name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `mask`

`mask[dtype: DType, width: Int, //, *, element_type: DType = DType.uint32](self, coord: IndexList[4, element_type=element_type], score_vec: SIMD[dtype, width]) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

### `status`

`status[*, element_type: DType = DType.uint32](self, tile_offset: IndexList[2, element_type=element_type], tile_size: IndexList[2, element_type=element_type]) -> TileMaskStatus`

**Returns:**

`TileMaskStatus`

### `start_column`

`start_column[BM: Int, BN: Int, page_size: Int](self, row: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `total_iters`

`total_iters[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `count_nonfull_sets`

`static count_nonfull_sets(BM: Int, BN: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `masked_set_ends`

`masked_set_ends[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> StaticTuple[UInt32, SlidingWindowCausalMask.count_nonfull_sets[window_size](BM, BN)]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

### `last_masked_set_end`

`last_masked_set_end[BM: Int, BN: Int, page_size: Int](self, row: UInt32, num_cols: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `nonfull_sets`

`static nonfull_sets[BM: Int, BN: Int]() -> StaticTuple[TileMaskStatus, SlidingWindowCausalMask.count_nonfull_sets[window_size](BM, BN)]`

**Returns:**

[`StaticTuple`](/mojo/std/utils/static_tuple/StaticTuple)

</section>

---

## TileMaskStatus

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileMaskStatus`

A tile's masking status.

## Fields

* â€‹status (`UInt8`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Equatable`](/mojo/std/builtin/comparable/Equatable),
[`Identifiable`](/mojo/std/builtin/identifiable/Identifiable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `FULL_MASK`

`comptime FULL_MASK = TileMaskStatus(3)`

### `NO_MASK`

`comptime NO_MASK = TileMaskStatus(0)`

### `PARTIAL_MASK`

`comptime PARTIAL_MASK = TileMaskStatus(1)`

### `UNKNOWN_MASK`

`comptime UNKNOWN_MASK = TileMaskStatus(4)`

## Methods

### `__eq__`

`__eq__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__is__`

`__is__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__and__`

`__and__(self, rhs: Self) -> Self`

### `__or__`

`__or__(self, rhs: Self) -> Self`

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

</section>

---

## mha_mask

<section class='mojo-docs'>

## `comptime` values

### `MASK_VALUE`

`comptime MASK_VALUE = -10000`

## Structs

* [â€‹`AndMask`](./AndMask): Mask that's the AND of two masks.
* [â€‹`CausalMask`](./CausalMask): MHA causal mask ensures a token is only affected by previous tokens.
* [â€‹`ChunkedMask`](./ChunkedMask): Mask implementing Chunked attention.
* [â€‹`MaskName`](./MaskName): A tile's masking status.
* [â€‹`MaterializedMask`](./MaterializedMask): Mask that's backed by a materialized tensor.
* [â€‹`NullMask`](./NullMask): Mask that's effectively a noop.
* [â€‹`OrMask`](./OrMask): Mask that's the OR of two masks.
* [â€‹`SlidingWindowCausalMask`](./SlidingWindowCausalMask): Mask implementing Sliding Window attention.
* [â€‹`TileMaskStatus`](./TileMaskStatus): A tile's masking status.

## Traits

* [â€‹`MHAMask`](./MHAMask): The MHAMask trait describes masks for MHA kernels, such as the causal mask.

## Functions

* [â€‹`ChunkedCausalMask`](./ChunkedCausalMask): Mask implementing Chunked Causal attention for Llama4 models.
* [â€‹`naively_compute_total_iters`](./naively_compute_total_iters):
* [â€‹`naively_get_first_nonempty_mask_col`](./naively_get_first_nonempty_mask_col):

</section>

---

## naively_compute_total_iters

<section class='mojo-docs'>

`naively_compute_total_iters[MaskType: MHAMask, //, BM: Int, BN: Int](mask: MaskType, q_row: UInt32, end: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

</section>

---

## naively_get_first_nonempty_mask_col

<section class='mojo-docs'>

`naively_get_first_nonempty_mask_col[MaskType: MHAMask, //, BM: Int, BN: Int](mask: MaskType, q_row: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

</section>

---

## KVCacheMHAOperand

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct KVCacheMHAOperand[cache_t: KVCacheT]`

An implementation for `mo.opaque` KVCacheT arguments to MHA kernels.

We can eventually remove this trait and just add it as a sub-trait in the
KVCacheT type, but we need to solve some cyclic dependencies first.

## Fields

* â€‹cache (`cache_t`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHAOperand`](/mojo/kernels/nn/mha_operand/MHAOperand),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = cache_t.__copyinit__is_trivial`

### `__del__is_trivial`

`comptime __del__is_trivial = cache_t.__del__is_trivial`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = cache_t.__moveinit__is_trivial`

### `device_type`

`comptime device_type = KVCacheMHAOperand[cache_t]`

### `dtype`

`comptime dtype = cache_t.dtype`

### `page_size`

`comptime page_size = cache_t.page_size_`

## Methods

### `__init__`

`__init__(cache: cache_t) -> Self`

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `block_paged_ptr`

`block_paged_ptr[tile_size: Int](self, batch_idx: UInt32, start_tok_idx: UInt32, head_idx: UInt32, head_dim_idx: UInt32 = 0) -> LegacyUnsafePointer[Scalar[KVCacheMHAOperand[cache_t].dtype]]`

**Returns:**

`LegacyUnsafePointer`

### `cache_length`

`cache_length(self, batch_idx: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `max_context_length`

`max_context_length(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `row_idx`

`row_idx(self, batch_idx: UInt32, start_tok_idx: UInt32) -> UInt32`

Returns the row idx when viewing the memory as a matrix.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `create_tma_tile`

`create_tma_tile[BN: Int, depth: Int, swizzle_mode: TensorMapSwizzle, BK: Int = depth](self, ctx: DeviceContext, out tma: TMATensorTile[KVCacheMHAOperand[cache_t].dtype, _split_last_layout[KVCacheMHAOperand[cache_t].dtype](IndexList[3, DType.int64](BN, 1, BK, Tuple[]()), swizzle_mode, True), _ragged_desc_layout[KVCacheMHAOperand[cache_t].dtype](IndexList[3, DType.int64](BN, 1, BK, Tuple[]()), swizzle_mode)])`

Creates a TMA tile for efficient GPU memory transfers.

**Returns:**

[`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)

</section>

---

## LayoutTensorMHAOperand

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct LayoutTensorMHAOperand[dtype_: DType, layout: Layout]`

An implementation for NDBuffer arguments to MHA kernels.

## Fields

* â€‹buffer (`LayoutTensor[LayoutTensorMHAOperand[dtype_, layout].dtype, layout, MutAnyOrigin]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHAOperand`](/mojo/kernels/nn/mha_operand/MHAOperand),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `device_type`

`comptime device_type = LayoutTensorMHAOperand[dtype_, layout]`

### `dtype`

`comptime dtype = dtype_`

### `page_size`

`comptime page_size = 0`

## Methods

### `__init__`

`__init__(buffer: LayoutTensor[LayoutTensorMHAOperand[dtype_, layout].dtype, layout, MutAnyOrigin]) -> Self`

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `block_paged_ptr`

`block_paged_ptr[tile_size: Int](self, batch_idx: UInt32, start_tok_idx: UInt32, head_idx: UInt32, head_dim_idx: UInt32 = 0) -> LegacyUnsafePointer[Scalar[LayoutTensorMHAOperand[dtype_, layout].dtype]]`

**Returns:**

`LegacyUnsafePointer`

### `cache_length`

`cache_length(self, batch_idx: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `max_context_length`

`max_context_length(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `row_idx`

`row_idx(self, batch_idx: UInt32, start_tok_idx: UInt32) -> UInt32`

Returns the row idx when viewing the memory as a matrix.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `create_tma_tile`

`create_tma_tile[BN: Int, depth: Int, swizzle_mode: TensorMapSwizzle, BK: Int = depth](self, ctx: DeviceContext, out tma: TMATensorTile[LayoutTensorMHAOperand[dtype_, layout].dtype, _split_last_layout[LayoutTensorMHAOperand[dtype_, layout].dtype](IndexList[3, DType.int64](BN, 1, BK, Tuple[]()), swizzle_mode, True), _ragged_desc_layout[LayoutTensorMHAOperand[dtype_, layout].dtype](IndexList[3, DType.int64](BN, 1, BK, Tuple[]()), swizzle_mode)])`

Creates a TMA tile for efficient GPU memory transfers.

**Returns:**

[`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)

</section>

---

## MHAOperand

<section class='mojo-docs'>

This serves as the trait to support arguments to our MHA kernel.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `device_type`

`comptime device_type`

Indicate the type being used on accelerator devices.

### `dtype`

`comptime dtype`

### `page_size`

`comptime page_size`

## Required methods

### `block_paged_ptr`

`block_paged_ptr[tile_size: Int](self: _Self, batch_idx: UInt32, start_tok_idx: UInt32, head_idx: UInt32, head_dim_idx: UInt32 = 0) -> LegacyUnsafePointer[Scalar[_Self.dtype]]`

**Returns:**

`LegacyUnsafePointer`

### `cache_length`

`cache_length(self: _Self, batch_idx: Int) -> Int`

Returns the length of the cache for a given batch index.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `max_context_length`

`max_context_length(self: _Self) -> UInt32`

Returns the maximum cache length in a given batch index.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `row_idx`

`row_idx(self: _Self, batch_idx: UInt32, start_tok_idx: UInt32) -> UInt32`

Returns the row idx when viewing the memory as a matrix.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `create_tma_tile`

`create_tma_tile[BN: Int, depth: Int, swizzle_mode: TensorMapSwizzle, BK: Int = depth](self: _Self, ctx: DeviceContext) -> TMATensorTile[_Self.dtype, _split_last_layout[_Self.dtype](IndexList[3, DType.int64](BN, 1, BK, Tuple[]()), swizzle_mode, True), _ragged_desc_layout[_Self.dtype](IndexList[3, DType.int64](BN, 1, BK, Tuple[]()), swizzle_mode)]`

Creates a TMA tile for efficient GPU memory transfers.

**Returns:**

[`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)

### `get_type_name`

`static get_type_name() -> String`

Gets the name of the host type (the one implementing this trait). For example, Int would return "Int", DeviceBuffer\[DType.float32] would return "DeviceBuffer\[DType.float32]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): The host type's name.

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name. For example, because DeviceBuffer's device\_type is UnsafePointer, DeviceBuffer\[DType.float32]'s get\_device\_type\_name() should return something like "UnsafePointer\[Scalar\[DType.float32]]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): The device type's name.

</section>

---

## RaggedMHAOperand

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct RaggedMHAOperand[dtype_: DType, layout: Layout, cache_layout: Layout]`

An implementation for ragged NDBuffer arguments to MHA kernels.

## Fields

* â€‹buffer (`LayoutTensor[RaggedMHAOperand[dtype_, layout, cache_layout].dtype, layout, MutAnyOrigin]`):
* â€‹cache\_row\_offsets (`LayoutTensor[DType.uint32, cache_layout, MutAnyOrigin]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHAOperand`](/mojo/kernels/nn/mha_operand/MHAOperand),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `device_type`

`comptime device_type = RaggedMHAOperand[dtype_, layout, cache_layout]`

### `dtype`

`comptime dtype = dtype_`

### `page_size`

`comptime page_size = 0`

## Methods

### `__init__`

`__init__(buffer: LayoutTensor[RaggedMHAOperand[dtype_, layout, cache_layout].dtype, layout, MutAnyOrigin], cache_row_offsets: LayoutTensor[DType.uint32, cache_layout, MutAnyOrigin]) -> Self`

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `block_paged_ptr`

`block_paged_ptr[tile_size: Int](self, batch_idx: UInt32, start_tok_idx: UInt32, head_idx: UInt32, head_dim_idx: UInt32 = 0) -> LegacyUnsafePointer[Scalar[RaggedMHAOperand[dtype_, layout, cache_layout].dtype]]`

**Returns:**

`LegacyUnsafePointer`

### `cache_length`

`cache_length(self, batch_idx: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `max_context_length`

`max_context_length(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `row_idx`

`row_idx(self, batch_idx: UInt32, start_tok_idx: UInt32) -> UInt32`

Returns the row idx when viewing the memory as a matrix.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `create_tma_tile`

`create_tma_tile[BN: Int, depth: Int, swizzle_mode: TensorMapSwizzle, BK: Int = depth](self, ctx: DeviceContext, out tma: TMATensorTile[RaggedMHAOperand[dtype_, layout, cache_layout].dtype, _split_last_layout[RaggedMHAOperand[dtype_, layout, cache_layout].dtype](IndexList[3, DType.int64](BN, 1, BK, Tuple[]()), swizzle_mode, True), _ragged_desc_layout[RaggedMHAOperand[dtype_, layout, cache_layout].dtype](IndexList[3, DType.int64](BN, 1, BK, Tuple[]()), swizzle_mode)])`

Creates a TMA tile for efficient GPU memory transfers.

**Returns:**

[`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)

</section>

---

## mha_operand

<section class='mojo-docs'>

## Structs

* [â€‹`KVCacheMHAOperand`](./KVCacheMHAOperand): An implementation for `mo.opaque` KVCacheT arguments to MHA kernels.
* [â€‹`LayoutTensorMHAOperand`](./LayoutTensorMHAOperand): An implementation for NDBuffer arguments to MHA kernels.
* [â€‹`RaggedMHAOperand`](./RaggedMHAOperand): An implementation for ragged NDBuffer arguments to MHA kernels.

## Traits

* [â€‹`MHAOperand`](./MHAOperand): This serves as the trait to support arguments to our MHA kernel.

</section>

---

## AlibiScoreMod

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct AlibiScoreMod[num_heads: Int]`

AlibiScoreMod adds the appropriate ALiBi constant bias to attention score.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`ScoreModTrait`](/mojo/kernels/nn/mha_score_mod/ScoreModTrait)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `device_type`

`comptime device_type = AlibiScoreMod[num_heads]`

### `name_str`

`comptime name_str = "alibi"`

## Methods

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `score_mod`

`score_mod[dtype: DType, width: Int, //, *, element_type: DType = DType.int32](self, coord: IndexList[4, element_type=element_type], score_vec: SIMD[dtype, width], max_prompt_len: Int) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## IdentityScoreMod

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct IdentityScoreMod`

IdentityScoreMod simply returns attention score.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`ScoreModTrait`](/mojo/kernels/nn/mha_score_mod/ScoreModTrait)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `device_type`

`comptime device_type = IdentityScoreMod`

### `name_str`

`comptime name_str = "no_pos"`

## Methods

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `score_mod`

`score_mod[dtype: DType, width: Int, //, *, element_type: DType = DType.int32](self, coord: IndexList[4, element_type=element_type], score_vec: SIMD[dtype, width], max_prompt_len: Int = 0) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## ScoreModTrait

<section class='mojo-docs'>

The ScoreMod trait desctribes score\_mod for mha kernel like alibi bias.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__copyinit__` is trivial.

The implementation of `__copyinit__` is considered to be trivial if:

* The struct has a compiler-generated trivial `__copyinit__` and all its fields
  have a trivial `__copyinit__` method.

In practice, it means the value can be copied by copying the bits from
one location to another without side effects.

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__moveinit__` is trivial.

The implementation of `__moveinit__` is considered to be trivial if:

* The struct has a compiler-generated `__moveinit__` and all its fields
  have a trivial `__moveinit__` method.

In practice, it means the value can be moved by moving the bits from
one location to another without side effects.

### `device_type`

`comptime device_type`

Indicate the type being used on accelerator devices.

### `name_str`

`comptime name_str`

## Required methods

### `__copyinit__`

`__copyinit__(out self: _Self, existing: _Self, /)`

Create a new instance of the value by copying an existing one.

**Args:**

* â€‹existing (`_Self`): The value to copy.

**Returns:**

`_Self`

### `__moveinit__`

`__moveinit__(out self: _Self, deinit existing: _Self, /)`

Create a new instance of the value by moving the value of another.

**Args:**

* â€‹existing (`_Self`): The value to move.

**Returns:**

`_Self`

### `score_mod`

`score_mod[dtype: DType, width: Int, //, *, element_type: DType = DType.int32](self: _Self, coord: IndexList[4, element_type=element_type], score_vec: SIMD[dtype, width], max_prompt_len: Int = 0) -> SIMD[dtype, width]`

Return score vector at given coordinates given a score\_mod.

Arguments:
coord is (seq\_id, head, q\_idx, k\_idx)
score\_vec is at `coord` of the score matrix

Score\_mod calculates a tensor given the functor and adds to score\_vec.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

### `get_type_name`

`static get_type_name() -> String`

Gets the name of the host type (the one implementing this trait). For example, Int would return "Int", DeviceBuffer\[DType.float32] would return "DeviceBuffer\[DType.float32]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): The host type's name.

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name. For example, because DeviceBuffer's device\_type is UnsafePointer, DeviceBuffer\[DType.float32]'s get\_device\_type\_name() should return something like "UnsafePointer\[Scalar\[DType.float32]]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): The device type's name.

## Provided methods

### `copy`

`copy(self: _Self) -> _Self`

Explicitly construct a copy of self.

**Returns:**

`_Self`: A copy of this value.

</section>

---

## mha_score_mod

<section class='mojo-docs'>

## Structs

* [â€‹`AlibiScoreMod`](./AlibiScoreMod): AlibiScoreMod adds the appropriate ALiBi constant bias to attention score.
* [â€‹`IdentityScoreMod`](./IdentityScoreMod): IdentityScoreMod simply returns attention score.

## Traits

* [â€‹`ScoreModTrait`](./ScoreModTrait): The ScoreMod trait desctribes score\_mod for mha kernel like alibi bias.

</section>

---

## AccumulatorTile

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__copyinit__` is trivial.

The implementation of `__copyinit__` is considered to be trivial if:

* The struct has a compiler-generated trivial `__copyinit__` and all its fields
  have a trivial `__copyinit__` method.

In practice, it means the value can be copied by copying the bits from
one location to another without side effects.

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__moveinit__` is trivial.

The implementation of `__moveinit__` is considered to be trivial if:

* The struct has a compiler-generated `__moveinit__` and all its fields
  have a trivial `__moveinit__` method.

In practice, it means the value can be moved by moving the bits from
one location to another without side effects.

### `dtype`

`comptime dtype`

### `element_layout`

`comptime element_layout`

### `rows_of_frags_layout`

`comptime rows_of_frags_layout`

### `vec_output_layout`

`comptime vec_output_layout`

## Required methods

### `__copyinit__`

`__copyinit__(out self: _Self, existing: _Self, /)`

Create a new instance of the value by copying an existing one.

**Args:**

* â€‹existing (`_Self`): The value to copy.

**Returns:**

`_Self`

### `__moveinit__`

`__moveinit__(out self: _Self, deinit existing: _Self, /)`

Create a new instance of the value by moving the value of another.

**Args:**

* â€‹existing (`_Self`): The value to move.

**Returns:**

`_Self`

### `rows_of_frags`

`static rows_of_frags(src: LayoutTensor[_Self.dtype, _Self.vec_output_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=_Self.element_layout]) -> LayoutTensor[_Self.dtype, _Self.rows_of_frags_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `allocate_register_tile`

`static allocate_register_tile() -> LayoutTensor[_Self.dtype, _Self.vec_output_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=_Self.element_layout]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `copy_from`

`copy_from(self: _Self, src: LayoutTensor[_Self.dtype, _Self.vec_output_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=_Self.element_layout])`

### `copy_to`

`copy_to(self: _Self, dst: LayoutTensor[_Self.dtype, _Self.vec_output_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=_Self.element_layout])`

## Provided methods

### `copy`

`copy(self: _Self) -> _Self`

Explicitly construct a copy of self.

**Returns:**

`_Self`: A copy of this value.

</section>

---

## DescriptorPair

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `a_t`

`comptime a_t`

### `b_t`

`comptime b_t`

## Required methods

### `get_a`

`get_a(self: _Self) -> _Self.a_t`

**Returns:**

`_Self.a_t`

### `get_b`

`get_b(self: _Self) -> _Self.b_t`

**Returns:**

`_Self.b_t`

</section>

---

## DescriptorPairTS

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `a_t`

`comptime a_t`

### `b_t`

`comptime b_t`

## Required methods

### `get_a`

`get_a(self: _Self) -> _Self.a_t`

**Returns:**

`_Self.a_t`

### `get_b`

`get_b(self: _Self) -> _Self.b_t`

**Returns:**

`_Self.b_t`

</section>

---

## MMAOperandOffsetFn

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MMAOperandOffsetFn[dtype: DType, BMN: Int, BK: Int, swizzle: TensorMapSwizzle, is_k_major: Bool, WMMA_MN: Int, WMMA_K: Int]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `canonical_K`

`comptime canonical_K = (swizzle.bytes() // size_of[dtype]()) if (swizzle != TensorMapSwizzle(Int32(0))) else BK`

### `canonical_layout`

`comptime canonical_layout = tile_to_descriptor[dtype, MMAOperandOffsetFn[dtype, BMN, BK, swizzle, is_k_major, WMMA_MN, WMMA_K].canonical_layout_flat, is_k_major]()`

### `canonical_layout_flat`

`comptime canonical_layout_flat = tile_layout_k_major[dtype, BMN, MMAOperandOffsetFn[dtype, BMN, BK, swizzle, is_k_major, WMMA_MN, WMMA_K].canonical_K, swizzle]() if is_k_major else MMAOperandOffsetFn[dtype, BMN, BK, swizzle, is_k_major, WMMA_MN, WMMA_K].layout`

### `canonical_layout_size`

`comptime canonical_layout_size = MMAOperandOffsetFn[dtype, BMN, BK, swizzle, is_k_major, WMMA_MN, WMMA_K].canonical_layout.size()`

### `layout`

`comptime layout = tile_layout_k_major[dtype, BMN, BK, swizzle]() if is_k_major else tile_layout_mn_major[dtype, BMN, BK, swizzle]()`

### `layout_size`

`comptime layout_size = MMAOperandOffsetFn[dtype, BMN, BK, swizzle, is_k_major, WMMA_MN, WMMA_K].layout.size()`

## Methods

### `__init__`

`__init__() -> Self`

</section>

---

## RegisterAccumulatorDescription

<section class='mojo-docs'>

`struct RegisterAccumulatorDescription`

## Fields

* â€‹num\_mmas (`Int`):
* â€‹frag\_size (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

## Methods

### `__init__`

`__init__(out self, num_mmas: Int, frag_size: Int)`

</section>

---

## RegisterAccumulatorLayout

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct RegisterAccumulatorLayout[MMA_M: Int, MMA_N: Int, num_m_mmas: Int, num_n_mmas: Int, consumer_group_size: Int, *, frag_simdwidth: Int = 2]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `element_layout`

`comptime element_layout = Layout.row_major(1, frag_simdwidth)`

### `frag_size`

`comptime frag_size = ((MMA_M * MMA_N) // consumer_group_size)`

### `num_row_blocks_per_mma`

`comptime num_row_blocks_per_mma = 2`

### `rows_of_frags_layout`

`comptime rows_of_frags_layout = Layout.row_major((num_m_mmas * num_n_mmas), RegisterAccumulatorLayout[MMA_M, MMA_N, num_m_mmas, num_n_mmas, consumer_group_size, frag_simdwidth=frag_simdwidth].frag_size)`

### `vec_output_layout`

`comptime vec_output_layout = Layout(IntTuple(IntTuple(2, num_m_mmas), IntTuple((RegisterAccumulatorLayout[MMA_M, MMA_N, num_m_mmas, num_n_mmas, consumer_group_size, frag_simdwidth=frag_simdwidth].frag_size // (2 * frag_simdwidth)), num_n_mmas), Tuple[]()), IntTuple(IntTuple(frag_simdwidth, RegisterAccumulatorLayout[MMA_M, MMA_N, num_m_mmas, num_n_mmas, consumer_group_size, frag_simdwidth=frag_simdwidth].frag_size), IntTuple((2 * frag_simdwidth), (num_m_mmas * RegisterAccumulatorLayout[MMA_M, MMA_N, num_m_mmas, num_n_mmas, consumer_group_size, frag_simdwidth=frag_simdwidth].frag_size)), Tuple[]()))`

## Methods

### `description`

`static description() -> RegisterAccumulatorDescription`

**Returns:**

`RegisterAccumulatorDescription`

</section>

---

## SM100TensorAccumulatorSS

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SM100TensorAccumulatorSS[operand_type: DType, accum_type: DType, MMA_M: Int, MMA_N: Int, BM: Int, BN: Int, BK: Int, compute_BK: Int, num_softmax_threads: Int, swizzle_a: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, swizzle_b: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, *, transpose_b: Bool = True, cta_group: Int = 1, pipeline_stages: Int = 1]`

## Fields

* â€‹mbar (`LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]`):
* â€‹pipeline (`PipelineState[pipeline_stages]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `a_offset`

`comptime a_offset = MMAOperandOffsetFn[SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].operand_t, BM, BK, swizzle_a, True, MMA_M, 16]()`

### `a_t`

`comptime a_t = SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].ab_t.a_t`

### `ab_t`

`comptime ab_t = UMMADescriptorSS[SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].operand_t]`

### `accum_t`

`comptime accum_t = accum_type`

### `b_offset`

`comptime b_offset = MMAOperandOffsetFn[SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].operand_t, BN, BK, swizzle_b, transpose_b, MMA_N, 16]()`

### `b_t`

`comptime b_t = SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].ab_t.b_t`

### `c_t`

`comptime c_t = TMemAccumulator[SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].accum_t, (BM // SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].num_m_blocks_per_warp), MMA_N, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].num_m_blocks_per_warp, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].num_n_mmas, num_softmax_threads]`

### `idesc`

`comptime idesc = UMMAInsDescriptor.create[UMMAKind.KIND_F16, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].accum_t, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].operand_t, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].operand_t, Index[dtype=DType.uint32](MMA_M, MMA_N), transpose_b=transpose_b]()`

### `MMA_K`

`comptime MMA_K = 16`

### `num_k_mmas`

`comptime num_k_mmas = (compute_BK // 16)`

### `num_m_blocks_per_warp`

`comptime num_m_blocks_per_warp = ((2 * BM) // num_softmax_threads)`

### `num_m_mmas`

`comptime num_m_mmas = (BM // MMA_M)`

### `num_n_mmas`

`comptime num_n_mmas = (BN // MMA_N)`

### `operand_t`

`comptime operand_t = operand_type`

### `smem_ptr_t`

`comptime smem_ptr_t = LegacyUnsafePointer[Scalar[SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].operand_t], address_space=AddressSpace.SHARED]`

## Methods

### `__init__`

`__init__(smem: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]) -> Self`

### `check_constraints`

`static check_constraints()`

### `init`

`init(self)`

### `mma_descriptors`

`static mma_descriptors[dtype_a: DType, dtype_b: DType](p_a: LegacyUnsafePointer[Scalar[dtype_a], address_space=AddressSpace.SHARED], p_b: LegacyUnsafePointer[Scalar[dtype_b], address_space=AddressSpace.SHARED]) -> SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].ab_t`

**Returns:**

`SM100TensorAccumulatorSS`

### `mma`

`mma(mut self, a: MMASmemDescriptor, b: MMASmemDescriptor, c_base: TMemAccumulator[SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].accum_t, (BM // SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].num_m_blocks_per_warp), MMA_N, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].num_m_blocks_per_warp, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].num_n_mmas, num_softmax_threads], scale_c: UInt32)`

### `wait_for_tmem`

`wait_for_tmem(self)`

Wait for the accumulator tmem to finish being read.

### `wait_for_mma`

`wait_for_mma(self, c_base: TMemAccumulator[SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].accum_t, (BM // SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].num_m_blocks_per_warp), MMA_N, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].num_m_blocks_per_warp, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].num_n_mmas, num_softmax_threads]) -> SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, compute_BK, num_softmax_threads, swizzle_a, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, pipeline_stages=pipeline_stages].c_t`

Wait for the accumulator tmem to finish being read.

**Returns:**

`SM100TensorAccumulatorSS`

### `tmem_arrive_init`

`tmem_arrive_init(self)`

### `tmem_arrive`

`tmem_arrive(mut self)`

Indicate that the accumulator is ready to be updated.

</section>

---

## SM100TensorAccumulatorTS

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SM100TensorAccumulatorTS[operand_type: DType, accum_type: DType, MMA_M: Int, MMA_N: Int, BM: Int, BN: Int, BK: Int, num_softmax_threads: Int, swizzle_b: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, transpose_b: Bool = True, cta_group: Int = 1]`

## Fields

* â€‹mbar (`LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]`):
* â€‹phase (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `a_frag_size`

`comptime a_frag_size = ((MMA_M * 16) // num_softmax_threads)`

### `a_t`

`comptime a_t = SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].ab_t.a_t`

### `ab_t`

`comptime ab_t = UMMADescriptorTS[SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].operand_t, SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].num_m_blocks_per_warp, SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].num_n_mmas, MMA_M=(BM // SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].num_m_blocks_per_warp), MMA_N=BK, MMA_K=16, consumer_group_size=num_softmax_threads]`

### `accum_t`

`comptime accum_t = accum_type`

### `b_offset`

`comptime b_offset = MMAOperandOffsetFn[SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].operand_t, BN, BK, swizzle_b, transpose_b, MMA_N, 16]()`

### `b_t`

`comptime b_t = SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].ab_t.b_t`

### `c_frag_size`

`comptime c_frag_size = ((MMA_M * MMA_N) // num_softmax_threads)`

### `c_t`

`comptime c_t = TMemAccumulator[SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].accum_t, (BM // SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].num_m_blocks_per_warp), MMA_N, SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].num_m_blocks_per_warp, SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].num_n_mmas, num_softmax_threads]`

### `idesc`

`comptime idesc = UMMAInsDescriptor.create[UMMAKind.KIND_F16, SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].accum_t, SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].operand_t, SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].operand_t, Index[dtype=DType.uint32](MMA_M, MMA_N), transpose_b=transpose_b]()`

### `MMA_K`

`comptime MMA_K = 16`

### `num_k_mmas`

`comptime num_k_mmas = (BK // 16)`

### `num_m_blocks_per_warp`

`comptime num_m_blocks_per_warp = ((2 * BM) // num_softmax_threads)`

### `num_m_mmas`

`comptime num_m_mmas = (BM // MMA_M)`

### `num_n_mmas`

`comptime num_n_mmas = (BN // MMA_N)`

### `operand_t`

`comptime operand_t = operand_type`

### `smem_ptr_t`

`comptime smem_ptr_t = LegacyUnsafePointer[Scalar[SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].operand_t], address_space=AddressSpace.SHARED]`

## Methods

### `__init__`

`__init__(smem: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]) -> Self`

### `check_constraints`

`static check_constraints()`

### `init`

`init(self)`

### `a_mma_descriptor`

`static a_mma_descriptor(a_tmem: UInt32) -> SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].ab_t.a_t`

**Returns:**

`SM100TensorAccumulatorTS`

### `b_mma_descriptor`

`static b_mma_descriptor[dtype_b: DType](p_b: LegacyUnsafePointer[Scalar[dtype_b], address_space=AddressSpace.SHARED]) -> SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].ab_t.b_t`

**Returns:**

`SM100TensorAccumulatorTS`

### `mma`

`mma(self, a: TMemOperand[SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].operand_t, SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].num_m_blocks_per_warp, SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].num_n_mmas, (BM // SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].num_m_blocks_per_warp), BK, 16, num_softmax_threads], b: MMASmemDescriptor, c: TMemAccumulator[SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].accum_t, (BM // SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].num_m_blocks_per_warp), MMA_N, SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].num_m_blocks_per_warp, SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BM, BN, BK, num_softmax_threads, swizzle_b, transpose_b, cta_group].num_n_mmas, num_softmax_threads], c_scale: UInt32)`

### `wait`

`wait(mut self, idx: UInt32)`

### `wait_for_mma`

`wait_for_mma(mut self)`

Wait for the mma to be complete.

### `wait_for_tmem`

`wait_for_tmem(mut self)`

Wait for the `output` and `A` tmem to be ready.

### `tmem_arrive`

`tmem_arrive(self)`

Indicate that the accumulator and the tensor memory arguments are ready for the MMA to begin.

</section>

---

## TMemAccumulator

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TMemAccumulator[dtype_: DType, MMA_M: Int, MMA_N: Int, num_m_mmas: Int, num_n_mmas: Int, num_softmax_threads: Int]`

## Fields

* â€‹tmem\_addr (`UInt32`):

## Implemented traits

[`AccumulatorTile`](/mojo/kernels/nn/mha_sm100_1q/AccumulatorTile),
[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `dtype`

`comptime dtype = dtype_`

### `element_layout`

`comptime element_layout = TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].layout_t.element_layout`

### `frag_size`

`comptime frag_size = TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].layout_t.frag_size`

### `layout_t`

`comptime layout_t = RegisterAccumulatorLayout[MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads]`

### `rows_of_frags_layout`

`comptime rows_of_frags_layout = TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].layout_t.rows_of_frags_layout`

### `vec_output_layout`

`comptime vec_output_layout = TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].layout_t.vec_output_layout`

## Methods

### `__init__`

`__init__(tmem_addr: UInt32) -> Self`

### `__getitem__`

`__getitem__(self, i: UInt32) -> Self`

### `check_constraints`

`static check_constraints()`

### `offset`

`offset[m_mma: Int, n_mma: Int](self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `rows_of_frags`

`static rows_of_frags(src: LayoutTensor[TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].dtype, TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].vec_output_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].layout_t.element_layout]) -> LayoutTensor[TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].dtype, TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].rows_of_frags_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `allocate_register_tile`

`static allocate_register_tile() -> LayoutTensor[TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].dtype, TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].vec_output_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].layout_t.element_layout]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `copy_from`

`copy_from(self, src: LayoutTensor[TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].dtype, TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].vec_output_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].layout_t.element_layout])`

### `copy_to`

`copy_to(self, dst: LayoutTensor[TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].dtype, TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].vec_output_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=TMemAccumulator[dtype_, MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads].layout_t.element_layout])`

</section>

---

## TMemOperand

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TMemOperand[dtype: DType, num_m_mmas: Int, num_n_mmas: Int, MMA_M: Int, MMA_N: Int, MMA_K: Int, num_softmax_threads: Int]`

## Fields

* â€‹tmem\_addr (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`WriteableMMAOperandDescriptor`](/mojo/kernels/nn/mha_sm100_1q/WriteableMMAOperandDescriptor)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `frag_size`

`comptime frag_size = TMemOperand[dtype, num_m_mmas, num_n_mmas, MMA_M, MMA_N, MMA_K, num_softmax_threads].reg_layout.frag_size`

### `reg_layout`

`comptime reg_layout = RegisterAccumulatorLayout[MMA_M, MMA_N, num_m_mmas, num_n_mmas, num_softmax_threads]`

### `reg_tile_t`

`comptime reg_tile_t = LayoutTensor[dtype, TMemOperand[dtype, num_m_mmas, num_n_mmas, MMA_M, MMA_N, MMA_K, num_softmax_threads].vec_output_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=TMemOperand[dtype, num_m_mmas, num_n_mmas, MMA_M, MMA_N, MMA_K, num_softmax_threads].reg_layout.element_layout]`

### `vec_output_layout`

`comptime vec_output_layout = TMemOperand[dtype, num_m_mmas, num_n_mmas, MMA_M, MMA_N, MMA_K, num_softmax_threads].reg_layout.vec_output_layout`

## Methods

### `__init__`

`__init__(tmem_addr: UInt32) -> Self`

### `offset`

`offset[m_mma: Int, k_mma: Int](self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `copy_from`

`copy_from[src_type: DType, src_layout: Layout, src_element_layout: Layout, //](self, src: LayoutTensor[src_type, src_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=src_element_layout])`

### `copy_to`

`copy_to[dst_type: DType, dst_layout: Layout, dst_element_layout: Layout, //](self, dst: LayoutTensor[dst_type, dst_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=dst_element_layout])`

</section>

---

## UMMADescriptorSS

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct UMMADescriptorSS[operand_type: DType]`

## Fields

* â€‹a (`UMMADescriptorSS[operand_type].a_t`):
* â€‹b (`UMMADescriptorSS[operand_type].b_t`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DescriptorPair`](/mojo/kernels/nn/mha_sm100_1q/DescriptorPair),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `a_t`

`comptime a_t = MMASmemDescriptor`

### `b_t`

`comptime b_t = MMASmemDescriptor`

### `operand_t`

`comptime operand_t = operand_type`

## Methods

### `__init__`

`__init__(a: MMASmemDescriptor, b: MMASmemDescriptor) -> Self`

### `get_a`

`get_a(self) -> UMMADescriptorSS[operand_type].a_t`

**Returns:**

`UMMADescriptorSS`

### `get_b`

`get_b(self) -> UMMADescriptorSS[operand_type].b_t`

**Returns:**

`UMMADescriptorSS`

</section>

---

## UMMADescriptorTS

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct UMMADescriptorTS[operand_type: DType, num_m_mmas: Int, num_n_mmas: Int, *, MMA_M: Int, MMA_N: Int, MMA_K: Int, consumer_group_size: Int]`

## Fields

* â€‹a (`UMMADescriptorTS[operand_type, num_m_mmas, num_n_mmas, MMA_M=MMA_M, MMA_N=MMA_N, MMA_K=MMA_K, consumer_group_size=consumer_group_size].a_t`):
* â€‹b (`UMMADescriptorTS[operand_type, num_m_mmas, num_n_mmas, MMA_M=MMA_M, MMA_N=MMA_N, MMA_K=MMA_K, consumer_group_size=consumer_group_size].b_t`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DescriptorPairTS`](/mojo/kernels/nn/mha_sm100_1q/DescriptorPairTS),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `a_t`

`comptime a_t = TMemOperand[operand_type, num_m_mmas, num_n_mmas, MMA_M, MMA_N, MMA_K, consumer_group_size]`

### `b_t`

`comptime b_t = MMASmemDescriptor`

### `operand_t`

`comptime operand_t = operand_type`

## Methods

### `__init__`

`__init__(a: TMemOperand[operand_type, num_m_mmas, num_n_mmas, MMA_M, MMA_N, MMA_K, consumer_group_size], b: MMASmemDescriptor) -> Self`

### `get_a`

`get_a(self) -> UMMADescriptorTS[operand_type, num_m_mmas, num_n_mmas, MMA_M=MMA_M, MMA_N=MMA_N, MMA_K=MMA_K, consumer_group_size=consumer_group_size].a_t`

**Returns:**

`UMMADescriptorTS`

### `get_b`

`get_b(self) -> UMMADescriptorTS[operand_type, num_m_mmas, num_n_mmas, MMA_M=MMA_M, MMA_N=MMA_N, MMA_K=MMA_K, consumer_group_size=consumer_group_size].b_t`

**Returns:**

`UMMADescriptorTS`

</section>

---

## WriteableMMAOperandDescriptor

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

## Required methods

### `copy_from`

`copy_from[src_type: DType, src_layout: Layout, src_element_layout: Layout, //](self: _Self, src: LayoutTensor[src_type, src_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=src_element_layout])`

</section>

---

## mha_sm100_1q

<section class='mojo-docs'>

## `comptime` values

### `logger`

`comptime logger = Logger[DEFAULT_LEVEL](stdout, "", False)`

## Structs

* [â€‹`MMAOperandOffsetFn`](./MMAOperandOffsetFn):
* [â€‹`RegisterAccumulatorDescription`](./RegisterAccumulatorDescription):
* [â€‹`RegisterAccumulatorLayout`](./RegisterAccumulatorLayout):
* [â€‹`SM100TensorAccumulatorSS`](./SM100TensorAccumulatorSS):
* [â€‹`SM100TensorAccumulatorTS`](./SM100TensorAccumulatorTS):
* [â€‹`TMemAccumulator`](./TMemAccumulator):
* [â€‹`TMemOperand`](./TMemOperand):
* [â€‹`UMMADescriptorSS`](./UMMADescriptorSS):
* [â€‹`UMMADescriptorTS`](./UMMADescriptorTS):

## Traits

* [â€‹`AccumulatorTile`](./AccumulatorTile):
* [â€‹`DescriptorPair`](./DescriptorPair):
* [â€‹`DescriptorPairTS`](./DescriptorPairTS):
* [â€‹`WriteableMMAOperandDescriptor`](./WriteableMMAOperandDescriptor):

## Functions

* [â€‹`local_tensor_type`](./local_tensor_type):
* [â€‹`mha_sm100_dispatch`](./mha_sm100_dispatch):

</section>

---

## local_tensor_type

<section class='mojo-docs'>

`local_tensor_type[dtype: DType, layout: Layout, element_layout: Layout]() -> LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

</section>

---

## mha_sm100_dispatch

<section class='mojo-docs'>

`mha_sm100_dispatch[q_type: DType, KVType: MHAOperand, MaskType: MHAMask, ScoreModType: ScoreModTrait, output_type: DType, MaxPromptLenType: OptionallyStaticInt, PartitionType: MHAPartitionScheme, //, config: MHAConfig[dtype], group: Int, use_score_mod: Bool, ragged: Bool, sink: Bool, _is_cache_length_accurate: Bool](output: DeviceBuffer[output_type], q_arg: DeviceBuffer[q_type], k: KVType, v: KVType, num_rows_q: Int, mask: MaskType, score_mod: ScoreModType, valid_length: DeviceBuffer[DType.uint32], max_prompt_len_arg: MaxPromptLenType, max_cache_valid_length_arg: Int, scale: Float32, kv_input_row_offsets: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]], batch_size_arg: Int, partition: PartitionType, ctx: DeviceContext, sink_weights: OptionalReg[LayoutTensor[q_type, Layout.row_major(-1), MutAnyOrigin]])`

</section>

---

## ConsumerPipeline

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ConsumerPipeline[number_of_stages: Int]`

## Fields

* â€‹mbar (`MBarType`):
* â€‹state (`PipelineState[ConsumerPipeline[number_of_stages].num_stages]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `num_stages`

`comptime num_stages = number_of_stages`

## Methods

### `__init__`

`__init__(mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]) -> Self`

### `producer_mbar`

`producer_mbar(self) -> MBarType`

**Returns:**

`MBarType`

### `consumer_mbar`

`consumer_mbar(self) -> MBarType`

**Returns:**

`MBarType`

### `wait`

`wait(self)`

### `release`

`release(mut self)`

### `step`

`step(mut self)`

</section>

---

## FA4Config

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct FA4Config`

## Fields

* â€‹MMA\_M (`Int`):
* â€‹BM (`Int`):
* â€‹BN (`Int`):
* â€‹BK0 (`Int`):
* â€‹BK1 (`Int`):
* â€‹depth (`Int`):
* â€‹padded\_depth (`Int`):
* â€‹group (`Int`):
* â€‹num\_q\_heads (`Int`):
* â€‹num\_kv\_heads (`Int`):
* â€‹TMEM\_S1 (`Int`):
* â€‹TMEM\_O0 (`Int`):
* â€‹TMEM\_O1 (`Int`):
* â€‹TMEM\_P0 (`Int`):
* â€‹TMEM\_P1 (`Int`):
* â€‹TMEM\_C0 (`Int`):
* â€‹TMEM\_C1 (`Int`):
* â€‹tmem\_used (`Int`):
* â€‹num\_kv\_stages (`Int`):
* â€‹num\_mma\_stages (`Int`):
* â€‹smem\_used (`Int`):
* â€‹dtype\_size (`Int`):
* â€‹split\_m (`Bool`):
* â€‹swizzle\_mode (`TensorMapSwizzle`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `mbar_size`

`comptime mbar_size = size_of[DType.int64]()`

### `MMA_K`

`comptime MMA_K = 16`

### `num_correction_cols`

`comptime num_correction_cols = 1`

### `num_threads`

`comptime num_threads = 512`

### `sm100_smem_carveout`

`comptime sm100_smem_carveout = (B200 - 1024)`

### `sm100_tmem_cols`

`comptime sm100_tmem_cols = 512`

### `TMEM_S0`

`comptime TMEM_S0 = 0`

## Methods

### `__init__`

`__init__(*, num_q_heads: Int, group: Int, depth: Int, dtype_size: Int, swizzle_mode: TensorMapSwizzle, page_size: Int, is_mla: Bool = False) -> Self`

### `num_qo`

`num_qo(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `supported`

`supported(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `use_tmem_for_correction`

`use_tmem_for_correction(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `correction_smem_elements`

`correction_smem_elements(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `num_active_warps_per_group`

`num_active_warps_per_group(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `num_active_threads_per_group`

`num_active_threads_per_group(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## FA4MiscMBars

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct FA4MiscMBars`

## Fields

* â€‹mbar\_base (`MBarType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `C0_offset`

`comptime C0_offset = 4`

### `C1_offset`

`comptime C1_offset = 6`

### `order_offset`

`comptime order_offset = 8`

### `Q1SyncIdx`

`comptime Q1SyncIdx = 10`

### `S0_offset`

`comptime S0_offset = 0`

### `S1_offset`

`comptime S1_offset = 2`

### `size`

`comptime size = 11`

## Methods

### `__init__`

`__init__(mbar_base: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]) -> Self`

### `init`

`init(self)`

### `producer_s0`

`producer_s0(self) -> ProducerPipeline[1]`

**Returns:**

`ProducerPipeline`

### `producer_s1`

`producer_s1(self) -> ProducerPipeline[1]`

**Returns:**

`ProducerPipeline`

### `consumer_s`

`consumer_s(self, wg_idx: UInt32) -> ConsumerPipeline[1]`

**Returns:**

`ConsumerPipeline`

### `consumer_c0`

`consumer_c0(self) -> ConsumerPipeline[1]`

**Returns:**

`ConsumerPipeline`

### `consumer_c1`

`consumer_c1(self) -> ConsumerPipeline[1]`

**Returns:**

`ConsumerPipeline`

### `producer_c`

`producer_c(self, wg_idx: UInt32) -> ProducerPipeline[1]`

**Returns:**

`ProducerPipeline`

### `pipeline_order_wait`

`pipeline_order_wait(self, wg_idx: UInt32) -> MBarType`

**Returns:**

`MBarType`

### `pipeline_order_arrive`

`pipeline_order_arrive(self, wg_idx: UInt32) -> MBarType`

**Returns:**

`MBarType`

### `q1_wait_mbar`

`q1_wait_mbar(self) -> ref [MutAnyOrigin, 3] SharedMemBarrier`

**Returns:**

`ref`

### `end`

`end(self) -> MBarType`

**Returns:**

`MBarType`

</section>

---

## KVConsumerPipeline

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct KVConsumerPipeline[dtype: DType, config: FA4Config]`

Pipeline for managing the consumption of K and V. This follows the order of Tri Dao and Cutlass implementations (modulo any rotation of the ops through the iterations).

We consume/produce in the following order:
0\. S0 <- Q0 @ Kn'
1\. O1 <- O1 + P1 @ V{n-1}
2\. S1 <- Q1 @ Kn'
3\. O0 <- O0 + P0 @ Vn

Note that we have two MMA between calculating Si and consuming Pi,
maximizing the overlap between MMAs and softmax calculation.
Oi + Pi @ V also depends on the correction, which is computed
asynchronously with the softmax in a correction warpgroup (as soon
as the softmax writes the correction factor).

# wait on K0

S0 <- Q0 @ K0'
S1 <- Q1 @ K0'

# release K0

# wait on V0

O0 <- P0 @ V0
for n in range(1,num\_iters):
\# wait on Kn
S0 <- Q0 @ Kn'
O1 <- O1 + P1\@V{n-1}
\# release V{n-1}
S1 <- Q1 @ Kn'
\# release Kn
\# wait on Vn
O0 <- P0 @ Vn
O1 <- O1 + P1\@V{num\_iters-1}

wK0, rK0, wV0
wK1, rV0, rK1, wV1
wK2, rV1, rK2, wV2
wK3, rV2, rK3, wV3

wKn(state)
wK0(0), rK0(0), wV0(1)
wK1(2), rV0(1), rK1(2), wV1(3)
wK2(4), rV1(3), rK2(4), wV2(5)
wK3(6), rV2(5), rK3(6), wV3(7)

Rules:
wK backs up and increments prior to waiting, except K0
rK increments after releasing
rV uses backup

wK0(0), rK0(0), wV0(1)
wK1(2), rV0(1), rK1(2), wV1(3)
wK2(4), rV1(3), rK2(4), wV2(5)
rV2(5)

## Fields

* â€‹kv\_pipeline (`KVPipeline[config.num_kv_stages, config.num_mma_stages]`):
* â€‹k\_smem\_descriptor (`MMASmemDescriptorPair`):
* â€‹v\_smem\_descriptor (`MMASmemDescriptorPair`):
* â€‹v\_pipeline\_release\_index (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `full_kv_bytes`

`comptime full_kv_bytes = ((config * config) * size_of[dtype]())`

### `mma_kv_bytes`

`comptime mma_kv_bytes = ((config * config) * size_of[dtype]())`

## Methods

### `__init__`

`__init__(kv_pipeline: KVPipeline[config.num_kv_stages, config.num_mma_stages], smem: LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED]) -> Self`

`__init__(mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], smem: LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED]) -> Self`

### `init`

`init(self)`

Only one of the producer or consumer should call `init()`.

### `wait`

`wait[*, mma_stage: Int](self) -> UInt32`

Wait on `k` from the producer, and return the `k` smem descriptor.

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `wait_k`

`wait_k[*, mma_stage: Int = (config - 1), pre_increment: Bool = True](mut self) -> MMASmemDescriptorPair`

Wait on `k` from the producer, and return the `k` smem descriptor. If `pre-increment` is true.

**Returns:**

[`MMASmemDescriptorPair`](/mojo/std/gpu/compute/arch/mma_nvidia_sm100/MMASmemDescriptorPair)

### `wait_v`

`wait_v[*, mma_stage: Int = (config - 1)](self) -> MMASmemDescriptorPair`

**Returns:**

[`MMASmemDescriptorPair`](/mojo/std/gpu/compute/arch/mma_nvidia_sm100/MMASmemDescriptorPair)

### `release_k`

`release_k[*, mma_stage: Int = (config - 1)](mut self, e: Int32)`

Must call `producer_commit` on the tmem resource before calling `consumer_release`. `release_k` does increment the pipeline step.

### `release_v`

`release_v[*, mma_stage: Int = (config - 1)](self, e: Int32)`

Must call `producer_commit` on the tmem resource before calling `consumer_release`. `release_v` does not increment the pipeline step.

</section>

---

## KVPipeline

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct KVPipeline[num_kv_stages: Int, num_mma_stages: Int]`

KVPipeline has `num_kv_stages * num_mma_stages` stages. `num_kv_stages` refers to how many `K` and `V` tiles we pipeline for performing the `S = Q@K'` and `O += P@V` MMAs. Each of these MMAs is broken up into `num_mma_stages` pipelined MMAs. We set `step=False` for all but the last MMA that completes the operation. An alternative implementation would separate the two, and potentially allow for more overall stages at the cost of slightly more bookkeeping.

## Fields

* â€‹mbar (`MBarType`):
* â€‹state (`PipelineState[num_kv_stages]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `num_stages`

`comptime num_stages = (num_kv_stages * num_mma_stages)`

## Methods

### `__init__`

`__init__(mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]) -> Self`

### `init`

`init(self)`

### `producer_mbar`

`producer_mbar[mma_stage: Int](self) -> MBarType`

**Returns:**

`MBarType`

### `consumer_mbar`

`consumer_mbar[mma_stage: Int](self, idx: UInt32) -> MBarType`

**Returns:**

`MBarType`

`consumer_mbar[mma_stage: Int](self) -> MBarType`

**Returns:**

`MBarType`

### `producer_acquire`

`producer_acquire[mma_stage: Int = (num_mma_stages - 1)](self)`

Returns the dynamic pipe idx.

### `consumer_wait`

`consumer_wait[mma_stage: Int = (num_mma_stages - 1)](self)`

### `consumer_release`

`consumer_release[mma_stage: Int = (num_mma_stages - 1)](mut self, e: Int32)`

### `num_mbars`

`static num_mbars() -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

</section>

---

## KVProducerPipeline

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct KVProducerPipeline[dtype: DType, config: FA4Config]`

## Fields

* â€‹kv\_pipeline (`KVPipeline[config.num_kv_stages, config.num_mma_stages]`):
* â€‹smem (`KVProducerPipeline[dtype, config].SMemType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `KPairType`

`comptime KPairType = TMADestination[dtype, tile_layout_k_major[dtype, config.BN, config.BK0, config.swizzle_mode]()]`

### `KType`

`comptime KType = LayoutTensor[dtype, tile_layout_k_major[dtype, config.BN, config.BK0, config.swizzle_mode](), MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=DType.int32, linear_idx_type=DType.int32, alignment=128]`

### `kv_bytes`

`comptime kv_bytes = (KVProducerPipeline[dtype, config].kv_elements * size_of[dtype]())`

### `kv_elements`

`comptime kv_elements = tile_layout_k_major[dtype, config.BN, config.BK0, config.swizzle_mode]().size()`

### `SMemType`

`comptime SMemType = LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED]`

### `VPairType`

`comptime VPairType = TMADestination[dtype, tile_layout_mn_major[dtype, config.padded_depth, config.BK1, config.swizzle_mode]()]`

### `VType`

`comptime VType = LayoutTensor[dtype, tile_layout_mn_major[dtype, config.padded_depth, config.BK1, config.swizzle_mode](), MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=DType.int32, linear_idx_type=DType.int32, alignment=128]`

## Methods

### `__init__`

`__init__(mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], smem: LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED]) -> Self`

`__init__(kv_pipeline: KVPipeline[config.num_kv_stages, config.num_mma_stages], smem: LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED]) -> Self`

### `init`

`init(self)`

Only one of the producer or consumer should call `init()`.

### `get_kv_smem`

`get_kv_smem[*, mma_stage: Int](self) -> KVProducerPipeline[dtype, config].SMemType`

**Returns:**

`KVProducerPipeline`

### `get_k`

`get_k[*, mma_stage: Int, expect: Bool = True](self) -> KVProducerPipeline[dtype, config].KPairType`

**Returns:**

`KVProducerPipeline`

### `get_v`

`get_v[*, mma_stage: Int](self) -> KVProducerPipeline[dtype, config].VPairType`

**Returns:**

`KVProducerPipeline`

### `acquire_kv`

`acquire_kv[*, mma_stage: Int = (config - 1)](self)`

### `commit_kv_step`

`commit_kv_step(mut self)`

Step the kv pipeline. The does not perform the commit on the mbars; that should be handled by the `tma_op.async_copy`.

</section>

---

## MBarPipeline

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MBarPipeline[number_of_stages: Int]`

## Fields

* â€‹mbar (`MBarType`):
* â€‹state (`PipelineState[MBarPipeline[number_of_stages].num_stages]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `num_stages`

`comptime num_stages = number_of_stages`

## Methods

### `__init__`

`__init__(mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]) -> Self`

### `init`

`init[*, num_producer: UInt32 = 1, num_consumer: UInt32 = 1](self)`

### `num_mbars`

`static num_mbars() -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

</section>

---

## ProducerPipeline

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ProducerPipeline[number_of_stages: Int]`

## Fields

* â€‹mbar (`MBarType`):
* â€‹state (`PipelineState[ProducerPipeline[number_of_stages].num_stages]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `num_stages`

`comptime num_stages = number_of_stages`

## Methods

### `__init__`

`__init__(mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]) -> Self`

### `producer_mbar`

`producer_mbar(self) -> MBarType`

**Returns:**

`MBarType`

### `consumer_mbar`

`consumer_mbar(self) -> MBarType`

**Returns:**

`MBarType`

### `acquire`

`acquire(self)`

### `commit`

`commit(mut self)`

### `commit_mma`

`commit_mma(self)`

`commit_mma(self, elect: Int32)`

### `step`

`step(mut self)`

</section>

---

## SM100MHA2Q

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SM100MHA2Q[KVLUTType: MHAOperand, output_type: DType, MaskType: MHAMask, ScoreModType: ScoreModTrait, SchedulerType: MHATileScheduler, config: FA4Config, use_score_mod: Bool, ValidLengthType: OptionalPointer, SinkType: OptionalPointer, KVRowOffsetsType: OptionalPointer, _is_cache_length_accurate: Bool, MaxSeqLenType: OptionallyStaticInt, PartitionType: MHAPartitionScheme, descriptor_shape: IndexList[3], remaining_global_dim_rank: Int]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `accum_type`

`comptime accum_type = get_accum_type[SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_type]()`

### `BM`

`comptime BM = config.BM`

### `BN`

`comptime BN = config.BN`

### `cta_group`

`comptime cta_group = 1`

### `depth`

`comptime depth = config.depth`

### `group`

`comptime group = config.group`

### `k_bytes`

`comptime k_bytes = SIMD[DType.uint32, 1]((SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].swizzle_granularity * config)).__rmul__[DType.uint32, 1](SIMD[DType.uint32, 1](SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_dt_size))`

### `k_elements`

`comptime k_elements = SIMD[DType.uint32, 1]((SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].swizzle_granularity * config))`

### `KVPipelineType`

`comptime KVPipelineType = KVPipeline[config.num_kv_stages, config.num_mma_stages]`

### `MMA_K`

`comptime MMA_K = 16`

### `MMA_M`

`comptime MMA_M = (config // 2)`

### `num_m_mmas`

`comptime num_m_mmas = 2`

### `num_mma_stages`

`comptime num_mma_stages = config.num_mma_stages`

### `num_q_heads`

`comptime num_q_heads = config.num_q_heads`

### `OPipelineType`

`comptime OPipelineType = MBarPipeline[2]`

### `padded_depth`

`comptime padded_depth = config.padded_depth`

### `page_size`

`comptime page_size = KVLUTType.page_size`

### `PositionType`

`comptime PositionType = MHAPosition[config.BM, config.BN, config.depth, config.padded_depth, config.num_q_heads, config.group, _is_decoding[MaxSeqLenType]()]`

### `qkv_dt_size`

`comptime qkv_dt_size = size_of[SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_type]()`

### `qkv_type`

`comptime qkv_type = KVLUTType.dtype`

### `qo_bytes`

`comptime qo_bytes = SIMD[DType.uint32, 1]((SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_dt_size * SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qo_elements))`

### `qo_elements`

`comptime qo_elements = (SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].padded_depth * SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].MMA_M)`

### `ragged`

`comptime ragged = ValidLengthType.is_null.__bool__().__invert__()`

### `simd_size`

`comptime simd_size = simd_width_of[SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_type]()`

### `swizzle_granularity`

`comptime swizzle_granularity = (config.swizzle_mode.bytes() // SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_dt_size)`

### `UMMA0Type`

`comptime UMMA0Type = SM100TensorAccumulatorSS[SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_type, SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].accum_type, SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].MMA_M, SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].BN, align_up(SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].depth, 16), swizzle_a=config.swizzle_mode, swizzle_b=config.swizzle_mode, num_stages=SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].num_mma_stages]`

### `UMMA1Type`

`comptime UMMA1Type = SM100TensorAccumulatorTS[SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_type, SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].accum_type, SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].MMA_M, config.padded_depth, SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].BN, config.swizzle_mode, transpose_b=False, num_stages=SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].num_mma_stages]`

### `v_bytes_per_mma`

`comptime v_bytes_per_mma = SIMD[DType.uint32, 1](((SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_dt_size * 16) * config))`

## Methods

### `kernel`

`static kernel(q_tma_op: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](q_smem_shape[KVLUTType.dtype, config.swizzle_mode, BM=(config // 2), group=config.group, depth=config.depth, decoding=False](), config, True), _ragged_desc_layout[KVLUTType.dtype](q_smem_shape[KVLUTType.dtype, config.swizzle_mode, BM=(config // 2), group=config.group, depth=config.depth, decoding=False](), config)], k_tma_op: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, config.depth, Tuple[]()), config, True), _ragged_desc_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, config.depth, Tuple[]()), config)], v_tma_op: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, config.depth, Tuple[]()), config, True), _ragged_desc_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, config.depth, Tuple[]()), config)], o_ptr_arg: LegacyUnsafePointer[Scalar[output_type]], ragged_tma_store: RaggedTensorMap[output_type, descriptor_shape, remaining_global_dim_rank, config.swizzle_mode], kv_lut: KVLUTType, scale: Float32, batch_size: UInt32, num_keys_arg: UInt32, pack: Pack[MaskType, ScoreModType, SchedulerType, ValidLengthType, SinkType, KVRowOffsetsType, MaxSeqLenType, PartitionType])`

### `mask_status`

`static mask_status(mask: MaskType, score_row: UInt32, kv_row: UInt32) -> TileMaskStatus`

**Returns:**

`TileMaskStatus`

### `scale_write_output`

`static scale_write_output(local_row: UInt32, inv_row_sum: Scalar[SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].accum_type], o_smem: LegacyUnsafePointer[Scalar[output_type], address_space=AddressSpace.SHARED], o_tmem: TMemTile[SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].accum_type, (SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].BM // 2), SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].padded_depth], o_ptr: LegacyUnsafePointer[Scalar[output_type]], ragged_tma_store: RaggedTensorMap[output_type, descriptor_shape, remaining_global_dim_rank, config.swizzle_mode], warp_idx: UInt32, consumer_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], current_seq: Int, num_output_rows: Int32)`

### `softmax`

`static softmax(tmem_addr: UInt32, warp_idx: UInt32, mbars: FA4MiscMBars, o_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], score_row: UInt32, seq_info: SeqInfo, mask: MaskType, num_keys: UInt32, scale: Scalar[SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].accum_type], score_mod: ScoreModType, max_seq_len: UInt32, o_ptr_arg: LegacyUnsafePointer[Scalar[output_type]], ragged_tma_store: RaggedTensorMap[output_type, descriptor_shape, remaining_global_dim_rank, config.swizzle_mode], o_smem: LegacyUnsafePointer[Scalar[output_type], address_space=AddressSpace.SHARED], sink_weights: SinkType)`

### `correction`

`static correction(tmem_addr: UInt32, mbars: FA4MiscMBars, o_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], score_row: UInt32, num_keys: UInt32, mask: MaskType)`

### `load`

`static load(mbars: FA4MiscMBars, kv_pipeline_arg: KVPipeline[config.num_kv_stages, config.num_mma_stages], score_row: UInt32, num_keys: UInt32, seq_info: SeqInfo, max_seq_len: MaxSeqLenType, mask: MaskType, q_tma_op: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](q_smem_shape[KVLUTType.dtype, config.swizzle_mode, BM=(config // 2), group=config.group, depth=config.depth, decoding=False](), config, True), _ragged_desc_layout[KVLUTType.dtype](q_smem_shape[KVLUTType.dtype, config.swizzle_mode, BM=(config // 2), group=config.group, depth=config.depth, decoding=False](), config)], k_tma_op: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, config.depth, Tuple[]()), config, True), _ragged_desc_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, config.depth, Tuple[]()), config)], v_tma_op: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, config.depth, Tuple[]()), config, True), _ragged_desc_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, config.depth, Tuple[]()), config)], kv_lut: KVLUTType, q_smem: LegacyUnsafePointer[Scalar[KVLUTType.dtype], address_space=AddressSpace.SHARED])`

### `descriptor_q`

`static descriptor_q(q_smem: LegacyUnsafePointer[Scalar[SM100MHA2Q[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_type], address_space=AddressSpace.SHARED]) -> MMASmemDescriptorPair`

**Returns:**

[`MMASmemDescriptorPair`](/mojo/std/gpu/compute/arch/mma_nvidia_sm100/MMASmemDescriptorPair)

### `mma`

`static mma(tmem_addr: UInt32, mbars: FA4MiscMBars, kv_pipeline_arg: KVPipeline[config.num_kv_stages, config.num_mma_stages], o_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], score_row: UInt32, num_keys: UInt32, mask: MaskType, q_smem: LegacyUnsafePointer[Scalar[KVLUTType.dtype], address_space=AddressSpace.SHARED])`

</section>

---

## SM100TensorAccumulatorSS (Mha_sm100_2q)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SM100TensorAccumulatorSS[operand_type: DType, accum_type: DType, MMA_M: Int, MMA_N: Int, BK: Int, *, swizzle_a: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, swizzle_b: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, transpose_b: Bool = True, cta_group: Int = 1, num_stages: Int = 1]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `a_layout`

`comptime a_layout = tile_layout_k_major[SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].operand_t, align_up(MMA_M, 8), SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].padded_BK, swizzle_a]()`

### `accum_t`

`comptime accum_t = accum_type`

### `AType`

`comptime AType = MMASmemDescriptorPair`

### `b_layout`

`comptime b_layout = tile_layout_k_major[SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].operand_t, MMA_N, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].padded_BK, swizzle_b]() if transpose_b else tile_layout_mn_major[SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].operand_t, MMA_N, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].padded_BK, swizzle_b]()`

### `BType`

`comptime BType = MMASmemDescriptorPair`

### `CType`

`comptime CType = TMemTile[SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].accum_t, MMA_M, MMA_N]`

### `idesc`

`comptime idesc = UMMAInsDescriptor.create[UMMAKind.KIND_F16, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].accum_t, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].operand_t, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].operand_t, Index[dtype=DType.uint32](MMA_M, MMA_N), transpose_b=transpose_b]()`

### `MMA_K`

`comptime MMA_K = 16`

### `num_k_blocks`

`comptime num_k_blocks = (SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].padded_BK // 16)`

### `num_k_blocks_per_stage`

`comptime num_k_blocks_per_stage = (SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].num_k_blocks // num_stages)`

### `num_k_mmas`

`comptime num_k_mmas = ceildiv(BK, 16)`

### `operand_size`

`comptime operand_size = size_of[SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].operand_t]()`

### `operand_t`

`comptime operand_t = operand_type`

### `padded_BK`

`comptime padded_BK = align_up(BK, SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].swizzle_granularity)`

### `swizzle_granularity`

`comptime swizzle_granularity = (max(swizzle_a.bytes(), swizzle_b.bytes()) // size_of[SM100TensorAccumulatorSS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_a=swizzle_a, swizzle_b=swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages].operand_t]())`

## Methods

### `mma`

`static mma[*, stage_idx: Int = 0](a: MMASmemDescriptorPair, b: MMASmemDescriptorPair, c: UInt32, *, c_scale: UInt32, elect: Int32)`

</section>

---

## SM100TensorAccumulatorTS (Mha_sm100_2q)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SM100TensorAccumulatorTS[operand_type: DType, accum_type: DType, MMA_M: Int, MMA_N: Int, BK: Int, swizzle_b: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B, *, transpose_b: Bool = True, cta_group: Int = 1, num_stages: Int = 1, padded_BK: Int = BK]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `accum_t`

`comptime accum_t = accum_type`

### `AType`

`comptime AType = TMemTile[operand_type, MMA_M, BK]`

### `b_layout`

`comptime b_layout = tile_layout_k_major[SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages, padded_BK=padded_BK].operand_t, MMA_N, BK, swizzle_b]() if transpose_b else tile_layout_mn_major[SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages, padded_BK=padded_BK].operand_t, MMA_N, BK, swizzle_b]()`

### `BType`

`comptime BType = MMASmemDescriptorPair`

### `CType`

`comptime CType = TMemTile[SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages, padded_BK=padded_BK].accum_t, MMA_M, MMA_N]`

### `idesc`

`comptime idesc = UMMAInsDescriptor.create[UMMAKind.KIND_F16, SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages, padded_BK=padded_BK].accum_t, SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages, padded_BK=padded_BK].operand_t, SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages, padded_BK=padded_BK].operand_t, Index[dtype=DType.uint32](MMA_M, MMA_N), transpose_b=transpose_b]()`

### `MMA_K`

`comptime MMA_K = 16`

### `num_k_blocks`

`comptime num_k_blocks = (padded_BK // 16)`

### `num_k_blocks_per_stage`

`comptime num_k_blocks_per_stage = (SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages, padded_BK=padded_BK].num_k_blocks // num_stages)`

### `num_k_mmas`

`comptime num_k_mmas = (BK // 16)`

### `operand_size`

`comptime operand_size = size_of[operand_type]()`

### `operand_t`

`comptime operand_t = operand_type`

### `swizzle_granularity`

`comptime swizzle_granularity = (swizzle_b.bytes() // SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages, padded_BK=padded_BK].operand_size)`

## Methods

### `descriptor_a`

`static descriptor_a(a_tmem: UInt32) -> SM100TensorAccumulatorTS[operand_type, accum_type, MMA_M, MMA_N, BK, swizzle_b, transpose_b=transpose_b, cta_group=cta_group, num_stages=num_stages, padded_BK=padded_BK].AType`

**Returns:**

`SM100TensorAccumulatorTS`

### `mma`

`static mma[*, stage_idx: Int = 0](a: UInt32, b: MMASmemDescriptorPair, c: UInt32, *, c_scale: UInt32, elect: Int32)`

</section>

---

## STMatrixLayout

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct STMatrixLayout[BM: Int, BN: Int, *, num_threads: Int, accum_type_size: Int]`

Layout for using `st_matrix` for writing the final accumulator to smem.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `bits`

`comptime bits = (64 * accum_type_size)`

### `bits_per_byte`

`comptime bits_per_byte = 8`

### `element_layout`

`comptime element_layout = Layout.row_major(1, STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size].frag_simdwidth)`

### `elements_per_repeat`

`comptime elements_per_repeat = 4`

### `frag_simdwidth`

`comptime frag_simdwidth = 2`

### `frag_size`

`comptime frag_size = ((BN * 2) // 4)`

### `num_m_tiles`

`comptime num_m_tiles = (STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size].num_m_tiles_total // STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size].num_warpgroups)`

### `num_m_tiles_total`

`comptime num_m_tiles_total = ceildiv((2 * BM), 128)`

### `num_row_blocks_per_mma`

`comptime num_row_blocks_per_mma = 2`

### `num_warpgroups`

`comptime num_warpgroups = ceildiv(num_threads, 128)`

### `repeat`

`comptime repeat = (BN // 8)`

### `row_of_frags_layout`

`comptime row_of_frags_layout = Layout.row_major(STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size].num_m_tiles, STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size].frag_size)`

### `TensorType`

`comptime TensorType[dtype: DType] = LayoutTensor[dtype, STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size].vec_local_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size].element_layout]`

#### Parameters

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)):

### `thread_cols`

`comptime thread_cols = 4`

### `vec_local_layout`

`comptime vec_local_layout = Layout(IntTuple(IntTuple(2, STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size].num_m_tiles), IntTuple(STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size].repeat), Tuple[]()), IntTuple(IntTuple(STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size].frag_simdwidth, STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size].frag_size), IntTuple(4), Tuple[]()))`

## Methods

### `__init__`

`__init__() -> Self`

</section>

---

## STMatrixOffsets

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct STMatrixOffsets[BM: Int, BN: Int, *, num_threads: Int, accum_type_size: Int, curr_repeat: Int, cumulative_repeat: Int, m_mma: Int]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `b32_per_repeat`

`comptime b32_per_repeat = ((STMatrixOffsets[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size, curr_repeat=curr_repeat, cumulative_repeat=cumulative_repeat, m_mma=m_mma].STLayout.elements_per_repeat * accum_type_size) // 4)`

### `local_frag_size_b32`

`comptime local_frag_size_b32 = (curr_repeat * STMatrixOffsets[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size, curr_repeat=curr_repeat, cumulative_repeat=cumulative_repeat, m_mma=m_mma].b32_per_repeat)`

### `ptr_offset`

`comptime ptr_offset = (STMatrixOffsets[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size, curr_repeat=curr_repeat, cumulative_repeat=cumulative_repeat, m_mma=m_mma].b32_per_repeat * ((STMatrixOffsets[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size, curr_repeat=curr_repeat, cumulative_repeat=cumulative_repeat, m_mma=m_mma].STLayout.repeat * m_mma) + cumulative_repeat))`

### `STLayout`

`comptime STLayout = STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size]`

### `tmem_col_offset`

`comptime tmem_col_offset = ((cumulative_repeat * STMatrixOffsets[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size, curr_repeat=curr_repeat, cumulative_repeat=cumulative_repeat, m_mma=m_mma].STLayout.frag_simdwidth) * 4)`

### `tmem_offset`

`comptime tmem_offset = ((STMatrixOffsets[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size, curr_repeat=curr_repeat, cumulative_repeat=cumulative_repeat, m_mma=m_mma].tmem_row_offset << 16) + STMatrixOffsets[BM, BN, num_threads=num_threads, accum_type_size=accum_type_size, curr_repeat=curr_repeat, cumulative_repeat=cumulative_repeat, m_mma=m_mma].tmem_col_offset)`

### `tmem_row_offset`

`comptime tmem_row_offset = (16 * m_mma)`

## Methods

### `__init__`

`__init__() -> Self`

</section>

---

## TMADestination

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TMADestination[dtype: DType, layout: Layout]`

## Fields

* â€‹mbar (`MBarType`):
* â€‹smem (`LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=DType.int32, linear_idx_type=DType.int32, alignment=128]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], smem: LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=DType.int32, linear_idx_type=DType.int32, alignment=128]) -> Self`

### `split_smem`

`split_smem[first: Layout, second: Layout](self) -> Tuple[LayoutTensor[dtype, first, MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=DType.int32, linear_idx_type=DType.int32, alignment=128], LayoutTensor[dtype, second, MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=DType.int32, linear_idx_type=DType.int32, alignment=128]]`

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

</section>

---

## TMemTile

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TMemTile[dtype_: DType, BM: Int, BN: Int]`

## Fields

* â€‹tmem\_addr (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `dtype`

`comptime dtype = dtype_`

### `dtype_size`

`comptime dtype_size = size_of[TMemTile[dtype_, BM, BN].dtype]()`

### `num_m_tiles`

`comptime num_m_tiles = (BM // 64)`

## Methods

### `__init__`

`__init__(tmem_addr: UInt32) -> Self`

### `__getitem__`

`__getitem__(self, i: UInt32) -> Self`

### `offset`

`offset[m_mma: Int, n_mma: Int](self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `allocate_register_tile`

`static allocate_register_tile[*, num_threads: Int]() -> LayoutTensor[TMemTile[dtype_, BM, BN].dtype, STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=TMemTile[dtype_, BM, BN].dtype_size].vec_local_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=TMemTile[dtype_, BM, BN].dtype_size].element_layout]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `store_async`

`store_async[*, num_threads: Int](self, src: LayoutTensor[TMemTile[dtype_, BM, BN].dtype, STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=TMemTile[dtype_, BM, BN].dtype_size].vec_local_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=TMemTile[dtype_, BM, BN].dtype_size].element_layout])`

`store_async[src_type: DType](self, src: LayoutTensor[src_type, Layout.row_major(BN), MutAnyOrigin, address_space=AddressSpace.LOCAL])`

### `store`

`store[*, num_threads: Int](self, src: LayoutTensor[TMemTile[dtype_, BM, BN].dtype, STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=TMemTile[dtype_, BM, BN].dtype_size].vec_local_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=TMemTile[dtype_, BM, BN].dtype_size].element_layout])`

`store[src_type: DType](self, src: LayoutTensor[src_type, Layout.row_major(BN), MutAnyOrigin, address_space=AddressSpace.LOCAL])`

### `load_async_with_st_matrix_layout`

`load_async_with_st_matrix_layout[*, num_threads: Int](self) -> LayoutTensor[TMemTile[dtype_, BM, BN].dtype, STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=TMemTile[dtype_, BM, BN].dtype_size].vec_local_layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=STMatrixLayout[BM, BN, num_threads=num_threads, accum_type_size=TMemTile[dtype_, BM, BN].dtype_size].element_layout]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

### `load_async`

`load_async(self) -> LayoutTensor[TMemTile[dtype_, BM, BN].dtype, Layout.row_major(BN), MutAnyOrigin, address_space=AddressSpace.LOCAL]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

</section>

---

## apply_mask

<section class='mojo-docs'>

`apply_mask[dtype: DType, BN: Int, MaskType: MHAMask, ScoreModType: ScoreModTrait, //, *, use_score_mod: Bool, masked: Bool, last_iter: Bool, decoding: Bool = False](srow: LayoutTensor[dtype, Layout.row_major(BN), MutAnyOrigin, address_space=AddressSpace.LOCAL], mask: MaskType, score_mod: ScoreModType, scale_log2e: Scalar[dtype], *, prompt_idx: UInt32, q_head_idx: UInt32, kv_tile_start_row: UInt32, max_seq_len: UInt32, num_keys: UInt32, score_row: UInt32)`

</section>

---

## break_into_powers_of_two

<section class='mojo-docs'>

`break_into_powers_of_two[origins: OriginSet, //, func: fn[pow_two: Int, offset: Int]() capturing -> None, N: Int, *, max_value: Int = 128]()`

</section>

---

## build_mma_ss

<section class='mojo-docs'>

`build_mma_ss(kind: String, layout_a: Layout, layout_b: Layout, *, operand_size: Int, num_k_mmas: Int) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

</section>

---

## build_mma_ts

<section class='mojo-docs'>

`build_mma_ts(kind: String, layout_b: Layout, *, operand_size: Int, num_k_mmas: Int) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

</section>

---

## bulk_mma

<section class='mojo-docs'>

`bulk_mma[kind: UMMAKind, //, layout_a: Layout, layout_b: Layout, *, num_k_mmas: Int, operand_size: Int](idesc: UMMAInsDescriptor[kind], a: MMASmemDescriptorPair, b: MMASmemDescriptorPair, c_tmem: UInt32, c_scale: UInt32, elect: Int32)`

`bulk_mma[kind: UMMAKind, //, layout_b: Layout, *, num_k_mmas: Int, operand_size: Int](idesc: UMMAInsDescriptor[kind], a: UInt32, b: MMASmemDescriptorPair, c_tmem: UInt32, c_scale: UInt32, elect: Int32)`

</section>

---

## cumulative_power_of_two

<section class='mojo-docs'>

`cumulative_power_of_two(N: Int, i: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## elect

<section class='mojo-docs'>

`elect() -> Int32`

**Returns:**

[`Int32`](/mojo/std/builtin/simd/#int32)

</section>

---

## elect_mma_arrive

<section class='mojo-docs'>

`elect_mma_arrive[cta_group: Int = 1](mbar_ptr: LegacyUnsafePointer[type, address_space=AddressSpace.SHARED, mut=mut, origin=origin], elect: Int32)`

Arrive at the mbar pointer for the MMA instruction.

**Parameters:**

* â€‹cta\_group ([`Int`](/mojo/std/builtin/int/Int)): Number of ctas used by MMA.

**Args:**

* â€‹mbar\_ptr (`LegacyUnsafePointer`): Pointer to the mbar.
* â€‹elect ([`Int32`](/mojo/std/builtin/simd/#int32)): `elect()`.

</section>

---

## extract_power_of_two

<section class='mojo-docs'>

`extract_power_of_two(N: Int, i: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## mha_sm100_2q

<section class='mojo-docs'>

## `comptime` values

### `LocalTensor`

`comptime LocalTensor[dtype: DType, layout: Layout, element_layout: Layout = Layout(IntTuple(1), IntTuple(1))] = LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.LOCAL, element_layout=element_layout]`

#### Parameters

* â€‹dtype ([`DType`](/std/builtin/dtype/DType)):
* â€‹layout ([`Layout`](/kernels/layout/layout/Layout)):
* â€‹element\_layout ([`Layout`](/kernels/layout/layout/Layout)):

### `logger`

`comptime logger = Logger[DEFAULT_LEVEL](stdout, "", False)`

### `MBarType`

`comptime MBarType = LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED]`

### `SharedMemPointer`

`comptime SharedMemPointer[type: AnyType] = LegacyUnsafePointer[type, address_space=AddressSpace.SHARED]`

#### Parameters

* â€‹type ([`AnyType`](/std/builtin/anytype/AnyType)):

### `SharedMemTensor`

`comptime SharedMemTensor[dtype: DType, layout: Layout] = LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=DType.int32, linear_idx_type=DType.int32, alignment=128]`

#### Parameters

* â€‹dtype ([`DType`](/std/builtin/dtype/DType)):
* â€‹layout ([`Layout`](/kernels/layout/layout/Layout)):

## Structs

* [â€‹`ConsumerPipeline`](./ConsumerPipeline):
* [â€‹`FA4Config`](./FA4Config):
* [â€‹`FA4MiscMBars`](./FA4MiscMBars):
* [â€‹`KVConsumerPipeline`](./KVConsumerPipeline): Pipeline for managing the consumption of K and V. This follows the order of Tri Dao and Cutlass implementations (modulo any rotation of the ops through the iterations).
* [â€‹`KVPipeline`](./KVPipeline): KVPipeline has `num_kv_stages * num_mma_stages` stages. `num_kv_stages` refers to how many `K` and `V` tiles we pipeline for performing the `S = Q@K'` and `O += P@V` MMAs. Each of these MMAs is broken up into `num_mma_stages` pipelined MMAs. We set `step=False` for all but the last MMA that completes the operation. An alternative implementation would separate the two, and potentially allow for more overall stages at the cost of slightly more bookkeeping.
* [â€‹`KVProducerPipeline`](./KVProducerPipeline):
* [â€‹`MBarPipeline`](./MBarPipeline):
* [â€‹`ProducerPipeline`](./ProducerPipeline):
* [â€‹`SM100MHA2Q`](./SM100MHA2Q):
* [â€‹`SM100TensorAccumulatorSS`](./SM100TensorAccumulatorSS):
* [â€‹`SM100TensorAccumulatorTS`](./SM100TensorAccumulatorTS):
* [â€‹`STMatrixLayout`](./STMatrixLayout): Layout for using `st_matrix` for writing the final accumulator to smem.
* [â€‹`STMatrixOffsets`](./STMatrixOffsets):
* [â€‹`TMADestination`](./TMADestination):
* [â€‹`TMemTile`](./TMemTile):

## Functions

* [â€‹`apply_mask`](./apply_mask):
* [â€‹`break_into_powers_of_two`](./break_into_powers_of_two):
* [â€‹`build_mma_ss`](./build_mma_ss):
* [â€‹`build_mma_ts`](./build_mma_ts):
* [â€‹`bulk_mma`](./bulk_mma):
* [â€‹`cumulative_power_of_two`](./cumulative_power_of_two):
* [â€‹`elect`](./elect):
* [â€‹`elect_mma_arrive`](./elect_mma_arrive): Arrive at the mbar pointer for the MMA instruction.
* [â€‹`extract_power_of_two`](./extract_power_of_two):
* [â€‹`maximum`](./maximum):
* [â€‹`mha_sm100_dispatch`](./mha_sm100_dispatch):
* [â€‹`sum`](./sum):

</section>

---

## maximum

<section class='mojo-docs'>

`maximum[dtype: DType, BN: Int, //, *, width: Int = 8](x: LayoutTensor[dtype, Layout.row_major(BN), MutAnyOrigin, address_space=AddressSpace.LOCAL]) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

`maximum[dtype: DType, BN: Int, width: Int, //](x: LayoutTensor[dtype, Layout.row_major(BN), MutAnyOrigin, address_space=AddressSpace.LOCAL], init: SIMD[dtype, width]) -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## mha_sm100_dispatch (Mha_sm100_2q)

<section class='mojo-docs'>

`mha_sm100_dispatch[q_type: DType, KVType: MHAOperand, MaskType: MHAMask, ScoreModType: ScoreModTrait, output_type: DType, MaxPromptLenType: OptionallyStaticInt, PartitionType: MHAPartitionScheme, //, config: MHAConfig[dtype], group: Int, use_score_mod: Bool, ragged: Bool, sink: Bool, _is_cache_length_accurate: Bool](output: DeviceBuffer[output_type], q_arg: LegacyUnsafePointer[Scalar[q_type]], k: KVType, v: KVType, num_rows_q: Int, mask: MaskType, score_mod: ScoreModType, valid_length: LegacyUnsafePointer[UInt32], max_prompt_len_arg: MaxPromptLenType, max_cache_valid_length_arg: Int, scale: Float32, kv_input_row_offsets: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]], batch_size_arg: Int, partition: PartitionType, ctx: DeviceContext, sink_weights: OptionalReg[LayoutTensor[q_type, Layout.row_major(-1), MutAnyOrigin]])`

</section>

---

## sum (Mha_sm100_2q)

<section class='mojo-docs'>

`sum[dtype: DType, BN: Int, //, *, width: Int = 8](x: LayoutTensor[dtype, Layout.row_major(BN), MutAnyOrigin, address_space=AddressSpace.LOCAL]) -> SIMD[dtype, 2]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## mha_sm90

<section class='mojo-docs'>

## Functions

* [â€‹`mha_sm90_dispatch`](./mha_sm90_dispatch):

</section>

---

## mha_sm90_dispatch

<section class='mojo-docs'>

`mha_sm90_dispatch[q_type: DType, KVType: MHAOperand, MaskType: MHAMask, ScoreModType: ScoreModTrait, output_type: DType, MaxPromptLenType: OptionallyStaticInt, PartitionType: MHAPartitionScheme, //, config: MHAConfig[dtype], group: Int, use_score_mod: Bool, ragged: Bool, sink: Bool, _is_cache_length_accurate: Bool](output: DeviceBuffer[output_type], q_arg: DeviceBuffer[q_type], k: KVType, v: KVType, num_rows_q: Int, mask_functor: MaskType, score_mod: ScoreModType, valid_length: DeviceBuffer[DType.uint32], max_prompt_len_arg: MaxPromptLenType, max_cache_valid_length_arg: Int, scale: Float32, kv_input_row_offsets: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]], batch_size_arg: Int, partition: PartitionType, ctx: DeviceContext, sink_weights: OptionalReg[LayoutTensor[q_type, Layout.row_major(-1), MutAnyOrigin]])`

</section>

---

## MHASchedule

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MHASchedule`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `DEFAULT`

`comptime DEFAULT = MHASchedule(0)`

### `PROMPT_ROTATE`

`comptime PROMPT_ROTATE = MHASchedule(1)`

## Methods

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## MHASchedulerSynchronization

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MHASchedulerSynchronization`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ALL`

`comptime ALL = MHASchedulerSynchronization(2)`

### `DEFAULT`

`comptime DEFAULT = MHASchedulerSynchronization.PRODUCER`

### `NONE`

`comptime NONE = MHASchedulerSynchronization(0)`

### `PRODUCER`

`comptime PRODUCER = MHASchedulerSynchronization(1)`

## Methods

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## MHATileScheduler

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__copyinit__` is trivial.

The implementation of `__copyinit__` is considered to be trivial if:

* The struct has a compiler-generated trivial `__copyinit__` and all its fields
  have a trivial `__copyinit__` method.

In practice, it means the value can be copied by copying the bits from
one location to another without side effects.

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__moveinit__` is trivial.

The implementation of `__moveinit__` is considered to be trivial if:

* The struct has a compiler-generated `__moveinit__` and all its fields
  have a trivial `__moveinit__` method.

In practice, it means the value can be moved by moving the bits from
one location to another without side effects.

### `device_type`

`comptime device_type`

Indicate the type being used on accelerator devices.

### `may_advance`

`comptime may_advance`

### `mha_schedule`

`comptime mha_schedule`

The MHATileScheduler trait describes a schedule for the persistent kernel.

## Required methods

### `__copyinit__`

`__copyinit__(out self: _Self, existing: _Self, /)`

Create a new instance of the value by copying an existing one.

**Args:**

* â€‹existing (`_Self`): The value to copy.

**Returns:**

`_Self`

### `__moveinit__`

`__moveinit__(out self: _Self, deinit existing: _Self, /)`

Create a new instance of the value by moving the value of another.

**Args:**

* â€‹existing (`_Self`): The value to move.

**Returns:**

`_Self`

### `get_current_work_info`

`get_current_work_info[ValidLengthType: OptionalPointer, //](self: _Self, ts: MHATileSummary[ValidLengthType], state: MHATileState) -> WorkInfo`

Returns the current `WorkInfo`.

**Returns:**

[`WorkInfo`](/mojo/kernels/nn/mha_tile_scheduler/WorkInfo)

### `advance`

`advance[ValidLengthType: OptionalPointer, //, producer: Bool, sync: MHASchedulerSynchronization = MHASchedulerSynchronization.DEFAULT](self: _Self, ts: MHATileSummary[ValidLengthType], mut state: MHATileState, pipeline_idx: UInt32) -> OptionalReg[SeqInfo]`

Advance state to the next work item. `func` must return a `Bool` indicating whether there is more work. Returns `True` if there is more work.

**Returns:**

[`OptionalReg`](/mojo/std/collections/optional/OptionalReg)

### `grid_dim`

`static grid_dim(batch_size: UInt32, max_num_prompt_tiles: UInt32) -> Tuple[Int, Int, Int]`

Return the grid\_dim required for the kernel.

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

### `initial_state`

`initial_state[ValidLengthType: OptionalPointer, //](self: _Self, ptr: LegacyUnsafePointer[UInt32, address_space=AddressSpace.SHARED], tile_summary: MHATileSummary[ValidLengthType]) -> MHATileState`

Create the initial state object.

**Returns:**

[`MHATileState`](/mojo/kernels/nn/mha_tile_scheduler/MHATileState)

### `unsafe_seq_info`

`unsafe_seq_info[ValidLengthType: OptionalPointer, //](self: _Self, ts: MHATileSummary[ValidLengthType], state: MHATileState) -> SeqInfo`

**Returns:**

[`SeqInfo`](/mojo/kernels/nn/mha_tile_scheduler/SeqInfo)

### `get_type_name`

`static get_type_name() -> String`

Gets the name of the host type (the one implementing this trait). For example, Int would return "Int", DeviceBuffer\[DType.float32] would return "DeviceBuffer\[DType.float32]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): The host type's name.

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name. For example, because DeviceBuffer's device\_type is UnsafePointer, DeviceBuffer\[DType.float32]'s get\_device\_type\_name() should return something like "UnsafePointer\[Scalar\[DType.float32]]". This is used for error messages when passing types to the device. TODO: This method will be retired soon when better kernel call error messages arrive.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): The device type's name.

## Provided methods

### `copy`

`copy(self: _Self) -> _Self`

Explicitly construct a copy of self.

**Returns:**

`_Self`: A copy of this value.

</section>

---

## MHATileState

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MHATileState`

## Fields

* â€‹idx (`UInt32`):
* â€‹sidx\_ptr (`LegacyUnsafePointer[UInt32, address_space=AddressSpace.SHARED]`):
* â€‹max\_idx (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(idx: UInt32, sidx_ptr: LegacyUnsafePointer[UInt32, address_space=AddressSpace.SHARED], max_idx: UInt32) -> Self`

### `is_valid`

`is_valid(self, idx: UInt32) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

`is_valid(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## MHATileSummary

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MHATileSummary[ValidLengthType: OptionalPointer]`

## Fields

* â€‹batch\_size (`UInt32`):
* â€‹max\_num\_prompt\_tiles (`UInt32`):
* â€‹valid\_length (`ValidLengthType`):
* â€‹max\_seq\_len (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True if ValidLengthType.__copyinit__is_trivial else ValidLengthType.__copyinit__is_trivial`

### `__del__is_trivial`

`comptime __del__is_trivial = True if ValidLengthType.__del__is_trivial else ValidLengthType.__del__is_trivial`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True if ValidLengthType.__moveinit__is_trivial else ValidLengthType.__moveinit__is_trivial`

## Methods

### `__init__`

`__init__(batch_size: UInt32, max_num_prompt_tiles: UInt32, valid_length: ValidLengthType, max_seq_len: UInt32) -> Self`

### `get_current_work_info`

`get_current_work_info[tile_shape: UInt32, num_heads: UInt32, schedule: MHASchedule](self, idx: UInt32) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/nn/mha_tile_scheduler/WorkInfo)

`get_current_work_info[tile_shape: UInt32, num_heads: UInt32, schedule: MHASchedule](self, idx: MHATileState) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/nn/mha_tile_scheduler/WorkInfo)

### `unsafe_get_current_work_info`

`unsafe_get_current_work_info[tile_shape: UInt32, num_heads: UInt32, schedule: MHASchedule](self, idx: UInt32) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/nn/mha_tile_scheduler/WorkInfo)

### `max_idx`

`max_idx(self, num_heads: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `grid_dim`

`static grid_dim[num_heads: UInt32](max_num_prompt_tiles: UInt32, batch_size: UInt32) -> Tuple[Int, Int, Int]`

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

### `seq_info`

`seq_info(self, work: WorkInfo) -> SeqInfo`

**Returns:**

[`SeqInfo`](/mojo/kernels/nn/mha_tile_scheduler/SeqInfo)

### `unsafe_seq_info`

`unsafe_seq_info[tile_shape: UInt32, num_heads: UInt32, schedule: MHASchedule](self, idx: UInt32) -> SeqInfo`

**Returns:**

[`SeqInfo`](/mojo/kernels/nn/mha_tile_scheduler/SeqInfo)

`unsafe_seq_info[tile_shape: UInt32, num_heads: UInt32, schedule: MHASchedule](self, state: MHATileState) -> SeqInfo`

**Returns:**

[`SeqInfo`](/mojo/kernels/nn/mha_tile_scheduler/SeqInfo)

</section>

---

## QueuedTileScheduler

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct QueuedTileScheduler[tile_shape: UInt32, num_heads: UInt32, /, decoding: Bool, num_ctas: UInt32 = H100.sm_count, schedule: MHASchedule = MHASchedule.DEFAULT]`

If `decoding == False`, then `num_heads` is `q_num_heads`. If `decoding == True`, then `num_heads` is `kv_num_heads`.

## Fields

* â€‹gidx\_ptr (`LegacyUnsafePointer[UInt32, address_space=AddressSpace.GLOBAL]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHATileScheduler`](/mojo/kernels/nn/mha_tile_scheduler/MHATileScheduler),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `device_type`

`comptime device_type = QueuedTileScheduler[tile_shape, num_heads, decoding, num_ctas, schedule]`

### `may_advance`

`comptime may_advance = True`

### `mha_schedule`

`comptime mha_schedule = schedule`

## Methods

### `__init__`

`__init__(gidx_ptr: LegacyUnsafePointer[UInt32]) -> Self`

### `get_current_work_info`

`get_current_work_info[ValidLengthType: OptionalPointer, //](self, ts: MHATileSummary[ValidLengthType], state: MHATileState) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/nn/mha_tile_scheduler/WorkInfo)

### `advance`

`advance[ValidLengthType: OptionalPointer, //, producer: Bool, sync: MHASchedulerSynchronization = MHASchedulerSynchronization.DEFAULT](self, ts: MHATileSummary[ValidLengthType], mut state: MHATileState, pipeline_idx: UInt32) -> OptionalReg[SeqInfo]`

The parameter `func` must return a `Bool` indicating whether the `WorkInfo` arg is valid. This function returns whether the current idx corresponds to a valid `WorkInfo`. Note that if `MHASchedulerSynchronization` is `NONE`, then we assume it is only called by `thread_idx.x==0`.

**Returns:**

[`OptionalReg`](/mojo/std/collections/optional/OptionalReg)

### `grid_dim`

`static grid_dim(batch_size: UInt32, max_num_prompt_tiles: UInt32) -> Tuple[Int, Int, Int]`

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

### `initial_state`

`initial_state[ValidLengthType: OptionalPointer, //](self, ptr: LegacyUnsafePointer[UInt32, address_space=AddressSpace.SHARED], tile_summary: MHATileSummary[ValidLengthType]) -> MHATileState`

**Returns:**

[`MHATileState`](/mojo/kernels/nn/mha_tile_scheduler/MHATileState)

### `unsafe_seq_info`

`unsafe_seq_info[ValidLengthType: OptionalPointer, //](self, ts: MHATileSummary[ValidLengthType], state: MHATileState) -> SeqInfo`

**Returns:**

[`SeqInfo`](/mojo/kernels/nn/mha_tile_scheduler/SeqInfo)

### `get_type_name`

`static get_type_name() -> String`

Gets the name of the host type (the one implementing this trait).

**Returns:**

[`String`](/mojo/std/collections/string/string/String): The host type's name.

### `get_device_type_name`

`static get_device_type_name() -> String`

Gets device\_type's name.

**Returns:**

[`String`](/mojo/std/collections/string/string/String): The device type's name.

</section>

---

## SeqInfo

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SeqInfo`

## Fields

* â€‹seq\_len (`UInt32`):
* â€‹start\_of\_seq (`UInt32`):
* â€‹prompt\_offset (`UInt32`):
* â€‹head\_idx (`UInt32`):
* â€‹prompt\_idx (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(seq_len: UInt32, start_of_seq: UInt32, work: WorkInfo) -> Self`

### `is_valid`

`is_valid(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `create`

`static create[ValidLengthType: OptionalPointer, //](work: WorkInfo, valid_length: ValidLengthType, max_seq_len: UInt32) -> Self`

</section>

---

## TileScheduler (Mha_tile_scheduler)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TileScheduler[tile_shape: UInt32, num_heads: UInt32, /, num_ctas: UInt32 = H100.sm_count, schedule: MHASchedule = MHASchedule.DEFAULT]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHATileScheduler`](/mojo/kernels/nn/mha_tile_scheduler/MHATileScheduler),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `device_type`

`comptime device_type = TileScheduler[tile_shape, num_heads, num_ctas, schedule]`

### `may_advance`

`comptime may_advance = True`

### `mha_schedule`

`comptime mha_schedule = schedule`

## Methods

### `__init__`

`__init__() -> Self`

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_current_work_info`

`get_current_work_info[ValidLengthType: OptionalPointer, //](self, ts: MHATileSummary[ValidLengthType], state: MHATileState) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/nn/mha_tile_scheduler/WorkInfo)

### `fetch_next_work`

`fetch_next_work(self, ts: MHATileSummary[ValidLengthType], mut state: MHATileState) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/nn/mha_tile_scheduler/WorkInfo)

### `advance`

`advance[ValidLengthType: OptionalPointer, //, producer: Bool, sync: MHASchedulerSynchronization = MHASchedulerSynchronization.DEFAULT](self, ts: MHATileSummary[ValidLengthType], mut state: MHATileState, pipeline_idx: UInt32) -> OptionalReg[SeqInfo]`

**Returns:**

[`OptionalReg`](/mojo/std/collections/optional/OptionalReg)

### `grid_dim`

`static grid_dim(batch_size: UInt32, max_num_prompt_tiles: UInt32) -> Tuple[Int, Int, Int]`

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

### `initial_state`

`initial_state[ValidLengthType: OptionalPointer, //](self, ptr: LegacyUnsafePointer[UInt32, address_space=AddressSpace.SHARED], tile_summary: MHATileSummary[ValidLengthType]) -> MHATileState`

**Returns:**

[`MHATileState`](/mojo/kernels/nn/mha_tile_scheduler/MHATileState)

### `unsafe_seq_info`

`unsafe_seq_info[ValidLengthType: OptionalPointer, //](self, ts: MHATileSummary[ValidLengthType], state: MHATileState) -> SeqInfo`

**Returns:**

[`SeqInfo`](/mojo/kernels/nn/mha_tile_scheduler/SeqInfo)

</section>

---

## TransientScheduler

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TransientScheduler[tile_shape: UInt32, num_heads: UInt32]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHATileScheduler`](/mojo/kernels/nn/mha_tile_scheduler/MHATileScheduler),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `device_type`

`comptime device_type = TransientScheduler[tile_shape, num_heads]`

### `may_advance`

`comptime may_advance = False`

### `mha_schedule`

`comptime mha_schedule = MHASchedule.DEFAULT`

## Methods

### `__init__`

`__init__() -> Self`

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_current_work_info`

`get_current_work_info(self) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/nn/mha_tile_scheduler/WorkInfo)

`get_current_work_info[ValidLengthType: OptionalPointer, //](self, ts: MHATileSummary[ValidLengthType], state: MHATileState) -> WorkInfo`

**Returns:**

[`WorkInfo`](/mojo/kernels/nn/mha_tile_scheduler/WorkInfo)

### `advance`

`advance[ValidLengthType: OptionalPointer, //, producer: Bool, sync: MHASchedulerSynchronization = MHASchedulerSynchronization.DEFAULT](self, ts: MHATileSummary[ValidLengthType], mut state: MHATileState, pipeline_idx: UInt32) -> OptionalReg[SeqInfo]`

**Returns:**

[`OptionalReg`](/mojo/std/collections/optional/OptionalReg)

### `grid_dim`

`static grid_dim(batch_size: UInt32, max_num_prompt_tiles: UInt32) -> Tuple[Int, Int, Int]`

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

### `initial_state`

`initial_state[ValidLengthType: OptionalPointer, //](self, ptr: LegacyUnsafePointer[UInt32, address_space=AddressSpace.SHARED], tile_summary: MHATileSummary[ValidLengthType]) -> MHATileState`

**Returns:**

[`MHATileState`](/mojo/kernels/nn/mha_tile_scheduler/MHATileState)

### `unsafe_seq_info`

`unsafe_seq_info[ValidLengthType: OptionalPointer, //](self, ts: MHATileSummary[ValidLengthType], state: MHATileState) -> SeqInfo`

**Returns:**

[`SeqInfo`](/mojo/kernels/nn/mha_tile_scheduler/SeqInfo)

</section>

---

## WorkInfo (Mha_tile_scheduler)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct WorkInfo`

## Fields

* â€‹prompt\_offset (`UInt32`):
* â€‹head\_idx (`UInt32`):
* â€‹prompt\_idx (`UInt32`):
* â€‹is\_valid\_tile (`Bool`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `is_valid`

`is_valid(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

</section>

---

## mha_tile_scheduler

<section class='mojo-docs'>

## Structs

* [â€‹`MHASchedule`](./MHASchedule):
* [â€‹`MHASchedulerSynchronization`](./MHASchedulerSynchronization):
* [â€‹`MHATileState`](./MHATileState):
* [â€‹`MHATileSummary`](./MHATileSummary):
* [â€‹`QueuedTileScheduler`](./QueuedTileScheduler): If `decoding == False`, then `num_heads` is `q_num_heads`. If `decoding == True`, then `num_heads` is `kv_num_heads`.
* [â€‹`SeqInfo`](./SeqInfo):
* [â€‹`TileScheduler`](./TileScheduler):
* [â€‹`TransientScheduler`](./TransientScheduler):
* [â€‹`WorkInfo`](./WorkInfo):

## Traits

* [â€‹`MHATileScheduler`](./MHATileScheduler):

</section>

---

## DynamicInt

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DynamicInt`

## Fields

* â€‹value (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Intable`](/mojo/std/builtin/int/Intable),
[`Movable`](/mojo/std/builtin/value/Movable),
[`OptionallyStaticInt`](/mojo/kernels/nn/mha_utils/OptionallyStaticInt)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `static_value`

`comptime static_value = OptionalReg[Int](None)`

## Methods

### `__init__`

`__init__(value: Int) -> Self`

### `__int__`

`__int__(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `as_uint32`

`as_uint32(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

</section>

---

## FlashAttentionAlgorithm

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct FlashAttentionAlgorithm`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `FLASH_ATTENTION_1`

`comptime FLASH_ATTENTION_1 = FlashAttentionAlgorithm(1)`

### `FLASH_ATTENTION_2`

`comptime FLASH_ATTENTION_2 = FlashAttentionAlgorithm(2)`

### `FLASH_ATTENTION_3`

`comptime FLASH_ATTENTION_3 = FlashAttentionAlgorithm(3)`

### `NAIVE`

`comptime NAIVE = FlashAttentionAlgorithm(0)`

## Methods

### `__init__`

`__init__() -> Self`

`__init__(value: Int) -> Self`

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

`__eq__(self, version: Int) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `init`

`init(self, dtype: DType) -> Self`

### `write_to`

`write_to(self, mut writer: T)`

</section>

---

## MHAConfig

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MHAConfig[dtype: DType]`

## Fields

* â€‹num\_heads (`UInt`):
* â€‹depth (`UInt`):
* â€‹padded\_depth (`UInt`):
* â€‹num\_queries\_per\_block (`UInt`):
* â€‹num\_keys\_per\_block (`UInt`):
* â€‹BK (`UInt`):
* â€‹WM (`UInt`):
* â€‹WN (`UInt`):
* â€‹num\_pipeline\_stages (`UInt`):
* â€‹k\_group\_size (`UInt`):
* â€‹algorithm (`FlashAttentionAlgorithm`):
* â€‹swizzle\_mode (`TensorMapSwizzle`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(num_heads: UInt, depth: UInt, num_queries_per_block: OptionalReg[UInt] = None, num_keys_per_block: OptionalReg[UInt] = None, BK: OptionalReg[UInt] = None, WM: OptionalReg[UInt] = None, WN: OptionalReg[UInt] = None, num_pipeline_stages: UInt = 4, k_group_size: UInt = 1, algorithm: FlashAttentionAlgorithm = FlashAttentionAlgorithm(-1), swizzle_mode: TensorMapSwizzle = TensorMapSwizzle.SWIZZLE_128B) -> Self`

### `block_m`

`block_m(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `block_n`

`block_n(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `block_k`

`block_k(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `warp_m`

`warp_m(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `warp_n`

`warp_n(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `num_warps_m`

`num_warps_m(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `num_warps_n`

`num_warps_n(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `num_consumer_threads`

`num_consumer_threads(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `num_producer_threads`

`num_producer_threads[producer_consumer_kernel: Bool = False](self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `num_threads`

`num_threads[producer_consumer_kernel: Bool = False](self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `swizzle_granularity`

`swizzle_granularity(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `q_smem_size`

`q_smem_size(self, fa3: Bool = False, persistent: Bool = False) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `kv_smem_size`

`kv_smem_size(self, fa3: Bool = False) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `k_smem_size`

`k_smem_size(self, fa3: Bool = False) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `v_smem_size`

`v_smem_size(self, fa3: Bool = False) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `p_smem_size`

`p_smem_size(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `warp_scratch_smem_size`

`warp_scratch_smem_size(self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `shared_mem_bytes`

`shared_mem_bytes[shared_kv: Bool = False, sm_90: Bool = False](self) -> UInt`

**Returns:**

[`UInt`](/mojo/std/builtin/uint/UInt)

### `__str__`

`__str__(self) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `write_to`

`write_to(self, mut writer: T)`

</section>

---

## MHAPartitionScheme

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__copyinit__` is trivial.

The implementation of `__copyinit__` is considered to be trivial if:

* The struct has a compiler-generated trivial `__copyinit__` and all its fields
  have a trivial `__copyinit__` method.

In practice, it means the value can be copied by copying the bits from
one location to another without side effects.

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__moveinit__` is trivial.

The implementation of `__moveinit__` is considered to be trivial if:

* The struct has a compiler-generated `__moveinit__` and all its fields
  have a trivial `__moveinit__` method.

In practice, it means the value can be moved by moving the bits from
one location to another without side effects.

### `accum_dtype`

`comptime accum_dtype`

### `do_partition`

`comptime do_partition`

## Required methods

### `__copyinit__`

`__copyinit__(out self: _Self, existing: _Self, /)`

Create a new instance of the value by copying an existing one.

**Args:**

* â€‹existing (`_Self`): The value to copy.

**Returns:**

`_Self`

### `__moveinit__`

`__moveinit__(out self: _Self, deinit existing: _Self, /)`

Create a new instance of the value by moving the value of another.

**Args:**

* â€‹existing (`_Self`): The value to move.

**Returns:**

`_Self`

### `num_partitions`

`num_partitions(self: _Self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `get_exp_sum_qk_max_pointer`

`get_exp_sum_qk_max_pointer(self: _Self) -> LegacyUnsafePointer[Scalar[_Self.accum_dtype]]`

**Returns:**

`LegacyUnsafePointer`

## Provided methods

### `copy`

`copy(self: _Self) -> _Self`

Explicitly construct a copy of self.

**Returns:**

`_Self`: A copy of this value.

</section>

---

## NoPartition

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct NoPartition[dtype: DType]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHAPartitionScheme`](/mojo/kernels/nn/mha_utils/MHAPartitionScheme),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `accum_dtype`

`comptime accum_dtype = dtype`

### `do_partition`

`comptime do_partition = False`

## Methods

### `__init__`

`__init__() -> Self`

### `num_partitions`

`num_partitions(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `get_exp_sum_qk_max_pointer`

`get_exp_sum_qk_max_pointer(self) -> LegacyUnsafePointer[Scalar[NoPartition[dtype].accum_dtype]]`

**Returns:**

`LegacyUnsafePointer`

</section>

---

## OptionallyStaticInt

<section class='mojo-docs'>

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Intable`](/mojo/std/builtin/int/Intable),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__copyinit__` is trivial.

The implementation of `__copyinit__` is considered to be trivial if:

* The struct has a compiler-generated trivial `__copyinit__` and all its fields
  have a trivial `__copyinit__` method.

In practice, it means the value can be copied by copying the bits from
one location to another without side effects.

### `__del__is_trivial`

`comptime __del__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__del__` is trivial.

The implementation of `__del__` is considered to be trivial if:

* The struct has a compiler-generated trivial destructor and all its fields
  have a trivial `__del__` method.

In practice, it means that the `__del__` can be considered as no-op.

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial`

A flag (often compiler generated) to indicate whether the implementation of `__moveinit__` is trivial.

The implementation of `__moveinit__` is considered to be trivial if:

* The struct has a compiler-generated `__moveinit__` and all its fields
  have a trivial `__moveinit__` method.

In practice, it means the value can be moved by moving the bits from
one location to another without side effects.

### `static_value`

`comptime static_value`

## Required methods

### `__copyinit__`

`__copyinit__(out self: _Self, existing: _Self, /)`

Create a new instance of the value by copying an existing one.

**Args:**

* â€‹existing (`_Self`): The value to copy.

**Returns:**

`_Self`

### `__moveinit__`

`__moveinit__(out self: _Self, deinit existing: _Self, /)`

Create a new instance of the value by moving the value of another.

**Args:**

* â€‹existing (`_Self`): The value to move.

**Returns:**

`_Self`

### `as_uint32`

`as_uint32(self: _Self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `__int__`

`__int__(self: _Self) -> Int`

Get the integral representation of the value.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The integral representation of the value.

## Provided methods

### `copy`

`copy(self: _Self) -> _Self`

Explicitly construct a copy of self.

**Returns:**

`_Self`: A copy of this value.

</section>

---

## SplitKPartition

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SplitKPartition[dtype: DType]`

## Fields

* â€‹ptr (`LegacyUnsafePointer[Scalar[SplitKPartition[dtype].accum_dtype]]`):
* â€‹num\_partitions\_value (`UInt32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`MHAPartitionScheme`](/mojo/kernels/nn/mha_utils/MHAPartitionScheme),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `accum_dtype`

`comptime accum_dtype = dtype`

### `do_partition`

`comptime do_partition = True`

## Methods

### `__init__`

`__init__(ptr: LegacyUnsafePointer[Scalar[SplitKPartition[dtype].accum_dtype]], num_partitions_value: UInt32) -> Self`

### `num_partitions`

`num_partitions(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `get_exp_sum_qk_max_pointer`

`get_exp_sum_qk_max_pointer(self) -> LegacyUnsafePointer[Scalar[SplitKPartition[dtype].accum_dtype]]`

**Returns:**

`LegacyUnsafePointer`

</section>

---

## StaticInt

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct StaticInt[value: Int]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Intable`](/mojo/std/builtin/int/Intable),
[`Movable`](/mojo/std/builtin/value/Movable),
[`OptionallyStaticInt`](/mojo/kernels/nn/mha_utils/OptionallyStaticInt)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `static_value`

`comptime static_value = OptionalReg[Int](value)`

## Methods

### `__init__`

`__init__() -> Self`

### `__int__`

`__int__(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `as_uint32`

`as_uint32(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

</section>

---

## dispatch_mask_and_score_mod

<section class='mojo-docs'>

`dispatch_mask_and_score_mod[mask_type: String, score_mod_type: String, callback_fn: callback_fn_type, local_window_size: Int = -1, num_heads: Int = -1]()`

</section>

---

## dispatch_materialized_mask_and_score_mod

<section class='mojo-docs'>

`dispatch_materialized_mask_and_score_mod[dtype: DType, layout: Layout, //, score_mod_type: String, callback_fn: callback_fn_type, num_heads: Int = -1](mask_nd: LayoutTensor[dtype, layout, MutAnyOrigin], start_pos_nd: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]] = None)`

</section>

---

## get_start_and_end_for_partitions

<section class='mojo-docs'>

`get_start_and_end_for_partitions[tile_size: Int](num_keys: Int, num_partitions: Int, partition_idx: Int) -> Tuple[Int, Int]`

Calculate start and end indices for a partition.

**Args:**

* â€‹num\_keys ([`Int`](/mojo/std/builtin/int/Int)): Total number of keys (sequence length).
* â€‹num\_partitions ([`Int`](/mojo/std/builtin/int/Int)): Number of partitions to split keys into.
* â€‹partition\_idx ([`Int`](/mojo/std/builtin/int/Int)): Index of current partition (0 to num\_partitions-1).

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple): Tuple of (start\_idx, end\_idx) for the partition, aligned to tile\_size.

</section>

---

## mha_utils

<section class='mojo-docs'>

## `comptime` values

### `callback_fn_type`

`comptime callback_fn_type = fn[mask_t: MHAMask, score_mod_t: ScoreModTrait](mask: mask_t, score_mod: score_mod_t) raises capturing -> None`

### `is_sm100`

`comptime is_sm100 = String(_accelerator_arch()).__contains__("sm_100")`

### `is_sm90`

`comptime is_sm90 = String(_accelerator_arch()).__contains__("sm_90")`

### `is_sm90or100`

`comptime is_sm90or100 = is_sm90 if String(_accelerator_arch()).__contains__("sm_90") else is_sm100`

## Structs

* [â€‹`DynamicInt`](./DynamicInt):
* [â€‹`FlashAttentionAlgorithm`](./FlashAttentionAlgorithm):
* [â€‹`MHAConfig`](./MHAConfig):
* [â€‹`NoPartition`](./NoPartition):
* [â€‹`SplitKPartition`](./SplitKPartition):
* [â€‹`StaticInt`](./StaticInt):

## Traits

* [â€‹`MHAPartitionScheme`](./MHAPartitionScheme):
* [â€‹`OptionallyStaticInt`](./OptionallyStaticInt):

## Functions

* [â€‹`dispatch_mask_and_score_mod`](./dispatch_mask_and_score_mod):
* [â€‹`dispatch_materialized_mask_and_score_mod`](./dispatch_materialized_mask_and_score_mod):
* [â€‹`get_start_and_end_for_partitions`](./get_start_and_end_for_partitions): Calculate start and end indices for a partition.

</section>

---

## flare_mla_decoding

<section class='mojo-docs'>

`flare_mla_decoding[rank: Int, cache_t: KVCacheT, mask_t: MHAMask, score_mod_t: ScoreModTrait, dtype: DType, q_layout: Layout, //, use_score_mod: Bool = False, config: MHAConfig[dtype] = MHAConfig[dtype](UInt(Int.__init__[IntTuple](q_layout.shape[(rank - 2)])), UInt(Int.__init__[IntTuple](q_layout.shape[(rank - 1)])), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), 4, 1, FlashAttentionAlgorithm(-1), TensorMapSwizzle.SWIZZLE_128B), ragged: Bool = False, decoding_warp_split_k: Bool = False](output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q: LayoutTensor[dtype, q_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: cache_t, mask_functor: mask_t, score_mod_functor: score_mod_t, valid_length: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scale: Float32, ctx: DeviceContext, q_max_seq_len: OptionalReg[Int] = None, kv_input_row_offsets: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]] = None, num_partitions: OptionalReg[Int] = None)`

MLA decoding kernel that would only be called in the optimized compute graph.

The Q input has a shape of \[seq\_len, num\_heads, depth].
The K input has a shape of \[seq\_len, 1, depth].
The V tensor is derived by reusing K, where V = K\[:, :, :depth\_v].

Specifically, for DeepSeek V2/3, depth = 576 and depth\_v = 512.

This kernel computes attention without needing to load V twice. This kernel
only handles decoding requests. In this case q\_max\_seq\_len = 1.

This kernel handles batches with different valid lengths (i.e., before the
padding). Such lengths are passed in valid\_length argument.

`flare_mla_decoding[mask_t: MHAMask, score_mod_t: ScoreModTrait, dtype: DType, q_layout: Layout, //, use_score_mod: Bool = False, config: MHAConfig[dtype] = MHAConfig[dtype](UInt(Int.__init__[IntTuple](q_layout.shape[2])), UInt(Int.__init__[IntTuple](q_layout.shape[3])), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), 4, 1, FlashAttentionAlgorithm(-1), TensorMapSwizzle.SWIZZLE_128B), decoding_warp_split_k: Bool = False](output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q: LayoutTensor[dtype, q_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mask_functor: mask_t, score_mod_functor: score_mod_t, scale: Float32, ctx: DeviceContext, num_partitions: OptionalReg[Int] = None)`

</section>

---

## flare_mla_decoding_dispatch

<section class='mojo-docs'>

`flare_mla_decoding_dispatch[k_t: MHAOperand, mask_t: MHAMask, score_mod_t: ScoreModTrait, dtype: DType, q_layout: Layout, //, kv_num_heads: Int, use_score_mod: Bool = False, config: MHAConfig[dtype] = MHAConfig[dtype](UInt(Int.__init__[IntTuple](q_layout.shape[(q_layout.rank() - 2)])), UInt(Int.__init__[IntTuple](q_layout.shape[(q_layout.rank() - 1)])), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), 4, 1, FlashAttentionAlgorithm(-1), TensorMapSwizzle.SWIZZLE_128B), ragged: Bool = False, _is_cache_length_accurate: Bool = False, _use_valid_length: Bool = True, decoding_warp_split_k: Bool = False](output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q: LayoutTensor[dtype, q_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: k_t, mask_functor: mask_t, score_mod_functor: score_mod_t, valid_length: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], max_prompt_len: Int, max_cache_valid_length: Int, scale: Float32, ctx: DeviceContext, kv_input_row_offsets: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]] = None, num_partitions: OptionalReg[Int] = None)`

</section>

---

## flare_mla_prefill

<section class='mojo-docs'>

`flare_mla_prefill[rank: Int, cache_t: KVCacheT, mask_t: MHAMask, score_mod_t: ScoreModTrait, dtype: DType, output_type: DType, softmax_type: DType, q_layout: Layout, //, use_score_mod: Bool = False, write_softmax_info: Bool = False, use_cascade_attention: Bool = False, use_fa4: Bool = False](output: LayoutTensor[output_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q: LayoutTensor[dtype, q_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], v: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k_rope: cache_t, mask_functor: mask_t, score_mod_functor: score_mod_t, valid_length: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], cache_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scale: Float32, ctx: DeviceContext, q_max_seq_len: OptionalReg[Int] = None, softmax_info: OptionalReg[LayoutTensor[softmax_type, Layout.row_major[3](), MutAnyOrigin]] = None, cache_offsets: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]] = None, prev_output: OptionalReg[LayoutTensor[output_type, Layout.row_major[rank](), MutAnyOrigin]] = None, prev_softmax_info: OptionalReg[LayoutTensor[softmax_type, Layout.row_major[3](), MutAnyOrigin]] = None)`

MLA prefill kernel that would only be called in the optimized compute graph. Only supports ragged Q/K/V inputs.

The Q input has a shape of \[seq\_len, num\_heads, q\_depth].
The K and V input has a shape of \[cache\_len, num\_heads, depth].
The K\_rope input is retrieved from the KV cache, with a shape of
\[cache\_len, 1, q\_depth - depth].

Specifically, for DeepSeek V2/3, depth = 128 and q\_depth = 192.

When computing attention scores (Q @ K), each head of K is smaller than Q
head. The missing 64 elements of K are retrieved from the K cache, and
broadcasted to all the heads. This kernel also handles that output has
reduced dimension compared to input Q.

This kernel handles batches with different valid lengths (i.e., before the
padding). Such lengths are passed in valid\_length argument.

`flare_mla_prefill[rank: Int, mask_t: MHAMask, score_mod_t: ScoreModTrait, dtype: DType, softmax_type: DType, q_layout: Layout, //, use_score_mod: Bool = False, write_softmax_info: Bool = False, use_cascade_attention: Bool = False, use_fa4: Bool = False](output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q: LayoutTensor[dtype, q_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], v: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k_rope: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mask_functor: mask_t, score_mod_functor: score_mod_t, valid_length: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], cache_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scale: Float32, ctx: DeviceContext, q_max_seq_len: OptionalReg[Int] = None, softmax_info: OptionalReg[LayoutTensor[softmax_type, Layout.row_major[3](), MutAnyOrigin]] = None, cache_offsets: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]] = None)`

</section>

---

## flare_mla_prefill_dispatch

<section class='mojo-docs'>

`flare_mla_prefill_dispatch[rank: Int, k_t: MHAOperand, v_t: MHAOperand, k_rope_t: MHAOperand, mask_t: MHAMask, score_mod_t: ScoreModTrait, dtype: DType, output_type: DType, softmax_type: DType, q_layout: Layout, //, kv_num_heads: Int, use_score_mod: Bool = False, write_softmax_info: Bool = False, use_cascade_attention: Bool = False, q_depth: Int = 192, cache_depth: Int = 576, config: MHAConfig[dtype] = MHAConfig[dtype](UInt(Int.__init__[IntTuple](q_layout.shape[(q_layout.rank() - 2)])), UInt(Int.__init__[IntTuple](q_layout.shape[(q_layout.rank() - 1)])), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), OptionalReg[UInt](None), 4, 1, FlashAttentionAlgorithm(-1), TensorMapSwizzle.SWIZZLE_128B), _ndbuffer_mha_operand: Bool = False, use_fa4: Bool = False](output: LayoutTensor[output_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q: LayoutTensor[dtype, q_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: k_t, v: v_t, k_rope: k_rope_t, mask_functor: mask_t, score_mod_functor: score_mod_t, valid_length: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], max_prompt_len: Int, scale: Float32, ctx: DeviceContext, softmax_info: OptionalReg[LayoutTensor[softmax_type, Layout.row_major[3](), MutAnyOrigin]] = None, cache_offsets: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]] = None, prev_output: OptionalReg[LayoutTensor[output_type, Layout.row_major[rank](), MutAnyOrigin]] = None, prev_softmax_info: OptionalReg[LayoutTensor[softmax_type, Layout.row_major[3](), MutAnyOrigin]] = None)`

</section>

---

## mla (Mla)

<section class='mojo-docs'>

## Functions

* [â€‹`flare_mla_decoding`](./flare_mla_decoding): MLA decoding kernel that would only be called in the optimized compute graph.
* [â€‹`flare_mla_decoding_dispatch`](./flare_mla_decoding_dispatch):
* [â€‹`flare_mla_prefill`](./flare_mla_prefill): MLA prefill kernel that would only be called in the optimized compute graph. Only supports ragged Q/K/V inputs.
* [â€‹`flare_mla_prefill_dispatch`](./flare_mla_prefill_dispatch):
* [â€‹`mla_decoding`](./mla_decoding):
* [â€‹`mla_decoding_single_batch`](./mla_decoding_single_batch): Flash attention v2 algorithm.
* [â€‹`mla_prefill`](./mla_prefill):
* [â€‹`mla_prefill_plan`](./mla_prefill_plan): This calls a GPU kernel that plans how to process a batch of sequences with varying lengths using a fixed-size buffer.
* [â€‹`mla_prefill_plan_kernel`](./mla_prefill_plan_kernel):
* [â€‹`mla_prefill_single_batch`](./mla_prefill_single_batch): MLA for encoding where seqlen > 1.
* [â€‹`set_buffer_lengths_to_zero`](./set_buffer_lengths_to_zero):

</section>

---

## mla_decoding

<section class='mojo-docs'>

`mla_decoding[q_type: DType, k_t: MHAOperand, output_type: DType, mask_t: MHAMask, score_mod_t: ScoreModTrait, valid_layout: Layout, BM: UInt, BN: UInt, BK: UInt, WM: UInt, WN: UInt, depth: UInt, num_heads: UInt, num_threads: UInt, num_pipeline_stages: UInt, group: UInt = 1, use_score_mod: Bool = False, ragged: Bool = False, _use_valid_length: Bool = False, _is_cache_length_accurate: Bool = False, decoding_warp_split_k: Bool = False](q_ptr: LegacyUnsafePointer[Scalar[q_type]], k: k_t, output_ptr: LegacyUnsafePointer[Scalar[output_type]], exp_sum_ptr: LegacyUnsafePointer[Scalar[get_accum_type[q_type]()]], qk_max_ptr: LegacyUnsafePointer[Scalar[get_accum_type[q_type]()]], scale: Float32, batch_size: Int, num_partitions: Int, max_cache_valid_length: Int, valid_length: LayoutTensor[DType.uint32, valid_layout, MutAnyOrigin], mask: mask_t, score_mod: score_mod_t)`

</section>

---

## mla_decoding_single_batch

<section class='mojo-docs'>

`mla_decoding_single_batch[q_type: DType, k_t: MHAOperand, output_type: DType, mask_t: MHAMask, score_mod_t: ScoreModTrait, *, BM: UInt, BN: UInt, BK: UInt, WM: UInt, WN: UInt, depth: UInt, depth_v: UInt, num_heads: UInt, num_threads: UInt, num_pipeline_stages: UInt, group: UInt = 1, use_score_mod: Bool = False, decoding_warp_split_k: Bool = False](q_ptr: LegacyUnsafePointer[Scalar[q_type]], k: k_t, output_ptr: LegacyUnsafePointer[Scalar[output_type]], exp_sum_ptr: LegacyUnsafePointer[Scalar[get_accum_type[q_type]()]], qk_max_ptr: LegacyUnsafePointer[Scalar[get_accum_type[q_type]()]], scale: Float32, num_keys: UInt, num_partitions: UInt, max_cache_valid_length: UInt, mask: mask_t, score_mod: score_mod_t, batch_idx: Int)`

Flash attention v2 algorithm.

</section>

---

## mla_prefill

<section class='mojo-docs'>

`mla_prefill[q_type: DType, k_t: MHAOperand, v_t: MHAOperand, k_rope_t: MHAOperand, output_type: DType, softmax_type: DType, mask_t: MHAMask, score_mod_t: ScoreModTrait, valid_layout: Layout, config: MHAConfig[dtype], group: Int = 128, q_depth: Int = 192, cache_depth: Int = 576, use_score_mod: Bool = False, write_softmax_info: Bool = False, use_cascade_attention: Bool = False, _ndbuffer_mha_operand: Bool = False](q_ptr: LegacyUnsafePointer[Scalar[q_type]], k: k_t, v: v_t, k_rope: k_rope_t, output_ptr: LegacyUnsafePointer[Scalar[output_type]], softmax_info_ptr: LegacyUnsafePointer[Scalar[softmax_type]], prev_output_ptr: LegacyUnsafePointer[Scalar[output_type]], prev_softmax_info_ptr: LegacyUnsafePointer[Scalar[softmax_type]], scale: Float32, batch_size: Int, seq_len_arg: Int, valid_length: LayoutTensor[DType.uint32, valid_layout, MutAnyOrigin], cache_offsets: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1), MutAnyOrigin]], mask: mask_t, score_mod: score_mod_t)`

</section>

---

## mla_prefill_plan

<section class='mojo-docs'>

`mla_prefill_plan[cache_t: KVCacheT](buffer_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], cache_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], buffer_lengths: LayoutTensor[DType.int32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k_cache: cache_t, buffer_token_size: UInt32, ctx: DeviceContext)`

This calls a GPU kernel that plans how to process a batch of sequences with varying lengths using a fixed-size buffer.

Each sequence in the batch has some existing cached tokens and new input
tokens. The kernel divides the total tokens into chunks of buffer\_token\_size.

For each chunk (iteration), it calculates:
1\. Buffer offsets for each sequence in each chunk
2\. Cache offsets for each sequence in each chunk
3\. Total buffer lengths for each processing iteration

</section>

---

## mla_prefill_plan_kernel

<section class='mojo-docs'>

`mla_prefill_plan_kernel[buffer_row_offsets_layout: Layout, cache_offsets_layout: Layout, buffer_lengths_layout: Layout, input_row_offsets_layout: Layout, cache_t: KVCacheT](buffer_row_offsets: LayoutTensor[DType.uint32, buffer_row_offsets_layout, MutAnyOrigin], cache_offsets: LayoutTensor[DType.uint32, cache_offsets_layout, MutAnyOrigin], buffer_lengths: LayoutTensor[DType.int32, buffer_lengths_layout, MutAnyOrigin], input_row_offsets: LayoutTensor[DType.uint32, input_row_offsets_layout, MutAnyOrigin], k_cache: cache_t, buffer_token_size: UInt32)`

</section>

---

## mla_prefill_single_batch

<section class='mojo-docs'>

`mla_prefill_single_batch[q_type: DType, k_t: MHAOperand, v_t: MHAOperand, k_rope_t: MHAOperand, output_type: DType, mask_t: MHAMask, score_mod_t: ScoreModTrait, *, config: MHAConfig[dtype], group: Int = 1, q_depth: Int = 192, cache_depth: Int = 576, use_score_mod: Bool = False, write_softmax_info: Bool = False, use_cascade_attention: Bool = False](q_ptr: LegacyUnsafePointer[Scalar[q_type]], k: k_t, v: v_t, k_rope: k_rope_t, output_ptr: LegacyUnsafePointer[Scalar[output_type]], softmax_info_ptr: LegacyUnsafePointer[Scalar[get_accum_type[q_type]()]], prev_output_ptr: LegacyUnsafePointer[Scalar[output_type]], prev_softmax_info_ptr: LegacyUnsafePointer[Scalar[get_accum_type[q_type]()]], scale: Float32, seq_len: Int, max_seq_len: Int, start_pos: UInt32, cache_start_pos: UInt32, num_keys: Int, mask: mask_t, score_mod: score_mod_t, batch_idx: Int)`

MLA for encoding where seqlen > 1.

</section>

---

## set_buffer_lengths_to_zero

<section class='mojo-docs'>

`set_buffer_lengths_to_zero[buffer_lengths_layout: Layout](buffer_lengths: LayoutTensor[DType.int32, buffer_lengths_layout, MutAnyOrigin])`

</section>

---

## DecodeCConsumer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodeCConsumer`

## Fields

* â€‹pipe (`ConsumerPipeline[1]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `CNumStages`

`comptime CNumStages = 1`

## Methods

### `__init__`

`__init__(pipe: ConsumerPipeline[1]) -> Self`

### `wait`

`wait(self)`

### `release`

`release(mut self)`

</section>

---

## DecodeCProducer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodeCProducer`

## Fields

* â€‹pipe (`ProducerPipeline[1]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `CNumStages`

`comptime CNumStages = 1`

## Methods

### `__init__`

`__init__(pipe: ProducerPipeline[1]) -> Self`

### `acquire`

`acquire(self)`

### `commit`

`commit(mut self)`

</section>

---

## DecodeKVConsumer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodeKVConsumer[dtype: DType, config: MLA_SM100_Decode_Config]`

## Fields

* â€‹pipe (`DecodeKVConsumer[dtype, config].KVPipeType`):
* â€‹smem (`UnsafePointer[Scalar[dtype], MutAnyOrigin, address_space=AddressSpace.SHARED]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `kv_stage_elems`

`comptime kv_stage_elems = (config * config)`

### `KVPipeType`

`comptime KVPipeType = KVPipelineGeneric[config.num_kv_stages, 1, 1, 2]`

## Methods

### `__init__`

`__init__(pipe: KVPipelineGeneric[config.num_kv_stages, 1, 1, 2], smem: UnsafePointer[Scalar[dtype], MutAnyOrigin, address_space=AddressSpace.SHARED]) -> Self`

### `stage_base_ptr`

`stage_base_ptr[*, mma_stage: Int = 0](self) -> UnsafePointer[Scalar[dtype], MutAnyOrigin, address_space=AddressSpace.SHARED]`

**Returns:**

[`UnsafePointer`](/mojo/std/memory/unsafe_pointer/UnsafePointer)

### `stage_index`

`stage_index[*, mma_stage: Int = 0](self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `wait`

`wait[*, mma_stage: Int = 0](self)`

### `release`

`release[*, mma_stage: Int = 0](mut self, e: Int32)`

</section>

---

## DecodeKVProducer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodeKVProducer[dtype: DType, config: MLA_SM100_Decode_Config]`

## Fields

* â€‹pipe (`DecodeKVProducer[dtype, config].KVPipeType`):
* â€‹smem (`UnsafePointer[Scalar[dtype], MutAnyOrigin, address_space=AddressSpace.SHARED]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `kv_stage_bytes`

`comptime kv_stage_bytes = (DecodeKVProducer[dtype, config].kv_stage_elems * size_of[dtype]())`

### `kv_stage_elems`

`comptime kv_stage_elems = (config * config)`

### `KVPipeType`

`comptime KVPipeType = KVPipelineGeneric[config.num_kv_stages, 1, 1, 2]`

## Methods

### `__init__`

`__init__(pipe: KVPipelineGeneric[config.num_kv_stages, 1, 1, 2], smem: UnsafePointer[Scalar[dtype], MutAnyOrigin, address_space=AddressSpace.SHARED]) -> Self`

### `init`

`init(self)`

### `stage_base_ptr`

`stage_base_ptr[*, mma_stage: Int = 0](self) -> UnsafePointer[Scalar[dtype], MutAnyOrigin, address_space=AddressSpace.SHARED]`

**Returns:**

[`UnsafePointer`](/mojo/std/memory/unsafe_pointer/UnsafePointer)

### `stage_index`

`stage_index[*, mma_stage: Int = 0](self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `producer_mbar`

`producer_mbar[*, mma_stage: Int = 0](self) -> MBarType`

**Returns:**

`MBarType`

### `acquire`

`acquire[*, mma_stage: Int = 0](self)`

### `commit_step`

`commit_step(mut self)`

</section>

---

## DecodeOConsumer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodeOConsumer`

## Fields

* â€‹pipe (`ConsumerPipeline[1]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ONumStages`

`comptime ONumStages = 1`

## Methods

### `__init__`

`__init__(pipe: ConsumerPipeline[1]) -> Self`

### `wait`

`wait(self)`

### `release`

`release(mut self)`

</section>

---

## DecodeOProducer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodeOProducer`

## Fields

* â€‹pipe (`ProducerPipeline[1]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ONumStages`

`comptime ONumStages = 1`

## Methods

### `__init__`

`__init__(pipe: ProducerPipeline[1]) -> Self`

### `acquire`

`acquire(self)`

### `slot_index`

`slot_index(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `commit_mma`

`commit_mma(mut self, elect: Int32)`

</section>

---

## DecodeOutConsumer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodeOutConsumer[dtype: DType, config: MLA_SM100_Decode_Config]`

## Fields

* â€‹pipe (`DecodeOutConsumer[dtype, config].OutPipeType`):
* â€‹smem (`UnsafePointer[Scalar[dtype], MutAnyOrigin, address_space=AddressSpace.SHARED]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `num_out_stages`

`comptime num_out_stages = 1 if config.decoding_warp_split_k else (config // config)`

### `out_stage_elems`

`comptime out_stage_elems = (config * config)`

### `OutPipeType`

`comptime OutPipeType = OutPipeline[DecodeOutConsumer[dtype, config].num_out_stages, WARPGROUP_SIZE, 1]`

## Methods

### `__init__`

`__init__(pipe: OutPipeline[DecodeOutConsumer[dtype, config].num_out_stages, WARPGROUP_SIZE, 1], smem: UnsafePointer[Scalar[dtype], MutAnyOrigin, address_space=AddressSpace.SHARED]) -> Self`

### `stage_base_ptr`

`stage_base_ptr(self) -> UnsafePointer[Scalar[dtype], MutAnyOrigin, address_space=AddressSpace.SHARED]`

**Returns:**

[`UnsafePointer`](/mojo/std/memory/unsafe_pointer/UnsafePointer)

### `wait`

`wait(self)`

### `release`

`release(mut self, e: Int32)`

</section>

---

## DecodeOutProducer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodeOutProducer[dtype: DType, config: MLA_SM100_Decode_Config]`

## Fields

* â€‹pipe (`DecodeOutProducer[dtype, config].OutPipeType`):
* â€‹smem (`UnsafePointer[Scalar[dtype], MutAnyOrigin, address_space=AddressSpace.SHARED]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `num_out_stages`

`comptime num_out_stages = 1 if config.decoding_warp_split_k else (config // config)`

### `out_stage_bytes`

`comptime out_stage_bytes = (DecodeOutProducer[dtype, config].out_stage_elems * size_of[dtype]())`

### `out_stage_elems`

`comptime out_stage_elems = (config * config)`

### `OutPipeType`

`comptime OutPipeType = OutPipeline[DecodeOutProducer[dtype, config].num_out_stages, WARPGROUP_SIZE, 1]`

## Methods

### `__init__`

`__init__(pipe: OutPipeline[DecodeOutProducer[dtype, config].num_out_stages, WARPGROUP_SIZE, 1], smem: UnsafePointer[Scalar[dtype], MutAnyOrigin, address_space=AddressSpace.SHARED]) -> Self`

### `init`

`init(self)`

### `stage_base_ptr`

`stage_base_ptr(self) -> UnsafePointer[Scalar[dtype], MutAnyOrigin, address_space=AddressSpace.SHARED]`

**Returns:**

[`UnsafePointer`](/mojo/std/memory/unsafe_pointer/UnsafePointer)

### `producer_mbar`

`producer_mbar(self) -> MBarType`

**Returns:**

`MBarType`

### `acquire`

`acquire(self)`

### `commit_step`

`commit_step(mut self)`

</section>

---

## DecodePConsumer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodePConsumer`

## Fields

* â€‹pipe (`ConsumerPipeline[2]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `PNumStages`

`comptime PNumStages = 2`

## Methods

### `__init__`

`__init__(pipe: ConsumerPipeline[2]) -> Self`

### `wait`

`wait(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `release_mma`

`release_mma(mut self, elect: Int32)`

</section>

---

## DecodePProducer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodePProducer`

## Fields

* â€‹pipe (`ProducerPipeline[2]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `PNumStages`

`comptime PNumStages = 2`

## Methods

### `__init__`

`__init__(pipe: ProducerPipeline[2]) -> Self`

### `acquire`

`acquire(self)`

### `commit`

`commit(mut self)`

### `stage_index`

`stage_index(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

</section>

---

## DecodeSConsumer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodeSConsumer`

## Fields

* â€‹pipe (`ConsumerPipeline[2]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `SNumStages`

`comptime SNumStages = 2`

## Methods

### `__init__`

`__init__(pipe: ConsumerPipeline[2]) -> Self`

### `wait`

`wait(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `release`

`release(mut self)`

</section>

---

## DecodeSM100MiscMBars

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodeSM100MiscMBars[num_stages: Int, num_producer: Int, num_consumer: Int]`

## Fields

* â€‹mbar\_base (`MBarType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(mbar_base: UnsafePointer[SharedMemBarrier, MutAnyOrigin, address_space=AddressSpace.SHARED]) -> Self`

### `init`

`init(self)`

### `producer`

`producer(self) -> ProducerPipeline[num_stages]`

**Returns:**

`ProducerPipeline`

### `consumer`

`consumer(self) -> ConsumerPipeline[num_stages]`

**Returns:**

`ConsumerPipeline`

### `end`

`end(self) -> MBarType`

**Returns:**

`MBarType`

</section>

---

## DecodeSM100PVSS

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodeSM100PVSS[operand_type: DType, accum_type: DType, *, config: MLA_SM100_Decode_Config]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ALayout`

`comptime ALayout = tile_layout_k_major[operand_type, DecodeSM100PVSS[operand_type, accum_type, config=config].TensorAccumulatorSS.BM, DecodeSM100PVSS[operand_type, accum_type, config=config].TensorAccumulatorSS.BN, DecodeSM100PVSS[operand_type, accum_type, config=config].TensorAccumulatorSS.a_swizzle]()`

### `BLayout`

`comptime BLayout = tile_layout_mn_major[operand_type, DecodeSM100PVSS[operand_type, accum_type, config=config].TensorAccumulatorSS.BM, DecodeSM100PVSS[operand_type, accum_type, config=config].TensorAccumulatorSS.BN, DecodeSM100PVSS[operand_type, accum_type, config=config].TensorAccumulatorSS.a_swizzle]()`

### `CTileType`

`comptime CTileType = TMemTile[accum_type, DecodeSM100PVSS[operand_type, accum_type, config=config].TensorAccumulatorSS.O_M, DecodeSM100PVSS[operand_type, accum_type, config=config].TensorAccumulatorSS.O_N]`

### `TensorAccumulatorSS`

`comptime TensorAccumulatorSS = DecodeSM100TensorAccumulatorSS[operand_type, accum_type, config=config]`

### `UMMAPVSS`

`comptime UMMAPVSS = UMMAInsDescriptor.create[UMMAKind.KIND_F16, accum_type, operand_type, operand_type, Index[dtype=DType.uint32](DecodeSM100PVSS[operand_type, accum_type, config=config].TensorAccumulatorSS.MMA_M, DecodeSM100PVSS[operand_type, accum_type, config=config].TensorAccumulatorSS.MMA_N), transpose_b=False]()`

## Methods

### `descriptor_v_block`

`static descriptor_v_block(kv_smem: UnsafePointer[Scalar[operand_type], MutAnyOrigin, address_space=AddressSpace.SHARED]) -> MMASmemDescriptorPair`

**Returns:**

[`MMASmemDescriptorPair`](/mojo/std/gpu/compute/arch/mma_nvidia_sm100/MMASmemDescriptorPair)

### `descriptor_p_block`

`static descriptor_p_block(p_smem: UnsafePointer[Scalar[operand_type], MutAnyOrigin, address_space=AddressSpace.SHARED]) -> MMASmemDescriptorPair`

**Returns:**

[`MMASmemDescriptorPair`](/mojo/std/gpu/compute/arch/mma_nvidia_sm100/MMASmemDescriptorPair)

### `mma`

`static mma[*, stage_idx: Int = 0](a: MMASmemDescriptorPair, b: MMASmemDescriptorPair, c: UInt32, *, c_scale: UInt32, elect: Int32)`

</section>

---

## DecodeSM100QKTSS

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodeSM100QKTSS[operand_type: DType, accum_type: DType, *, config: MLA_SM100_Decode_Config]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ALayout`

`comptime ALayout = tile_layout_k_major[operand_type, DecodeSM100QKTSS[operand_type, accum_type, config=config].TensorAccumulatorSS.BM, DecodeSM100QKTSS[operand_type, accum_type, config=config].TensorAccumulatorSS.BN, DecodeSM100QKTSS[operand_type, accum_type, config=config].TensorAccumulatorSS.a_swizzle]()`

### `BLayout`

`comptime BLayout = tile_layout_k_major[operand_type, DecodeSM100QKTSS[operand_type, accum_type, config=config].TensorAccumulatorSS.BM, DecodeSM100QKTSS[operand_type, accum_type, config=config].TensorAccumulatorSS.BN, DecodeSM100QKTSS[operand_type, accum_type, config=config].TensorAccumulatorSS.b_swizzle]()`

### `CTileType`

`comptime CTileType = TMemTile[accum_type, DecodeSM100QKTSS[operand_type, accum_type, config=config].TensorAccumulatorSS.S_M, DecodeSM100QKTSS[operand_type, accum_type, config=config].TensorAccumulatorSS.S_N]`

### `TensorAccumulatorSS`

`comptime TensorAccumulatorSS = DecodeSM100TensorAccumulatorSS[operand_type, accum_type, config=config]`

### `UMMAInstDesc`

`comptime UMMAInstDesc = UMMAInsDescriptor.create[UMMAKind.KIND_F16, accum_type, operand_type, operand_type, Index[dtype=DType.uint32](DecodeSM100QKTSS[operand_type, accum_type, config=config].TensorAccumulatorSS.MMA_M, DecodeSM100QKTSS[operand_type, accum_type, config=config].TensorAccumulatorSS.MMA_N)]()`

## Methods

### `descriptor_q_block`

`static descriptor_q_block(q_smem: UnsafePointer[Scalar[operand_type], MutAnyOrigin, address_space=AddressSpace.SHARED]) -> MMASmemDescriptorPair`

**Returns:**

[`MMASmemDescriptorPair`](/mojo/std/gpu/compute/arch/mma_nvidia_sm100/MMASmemDescriptorPair)

### `descriptor_k_block`

`static descriptor_k_block(kv_smem: UnsafePointer[Scalar[operand_type], MutAnyOrigin, address_space=AddressSpace.SHARED]) -> MMASmemDescriptorPair`

**Returns:**

[`MMASmemDescriptorPair`](/mojo/std/gpu/compute/arch/mma_nvidia_sm100/MMASmemDescriptorPair)

### `mma`

`static mma[*, stage_idx: Int = 0](a: MMASmemDescriptorPair, b: MMASmemDescriptorPair, c: UInt32, *, c_scale: UInt32, elect: Int32)`

</section>

---

## DecodeSM100TensorAccumulatorSS

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodeSM100TensorAccumulatorSS[operand_type: DType, accum_type: DType, *, config: MLA_SM100_Decode_Config]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `a_swizzle`

`comptime a_swizzle = config.swizzle_mode`

### `b_swizzle`

`comptime b_swizzle = config.kv_swizzle_mode`

### `BK`

`comptime BK = config.BN`

### `BM`

`comptime BM = config.BM`

### `BN`

`comptime BN = config.BN`

### `MMA_K`

`comptime MMA_K = MLA_SM100_Decode_Config.MMA_K`

### `MMA_M`

`comptime MMA_M = config.BM`

### `MMA_N`

`comptime MMA_N = config.BN`

### `num_k_mmas`

`comptime num_k_mmas = (DecodeSM100TensorAccumulatorSS[operand_type, accum_type, config=config].BK // 16)`

### `O_M`

`comptime O_M = (config * 2)`

### `O_N`

`comptime O_N = (config // 2)`

### `operand_size`

`comptime operand_size = size_of[operand_type]()`

### `S_M`

`comptime S_M = (config * 2)`

### `S_N`

`comptime S_N = (config // 2)`

</section>

---

## DecodeSProducer

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct DecodeSProducer`

## Fields

* â€‹pipe (`ProducerPipeline[2]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `SNumStages`

`comptime SNumStages = 2`

## Methods

### `__init__`

`__init__(pipe: ProducerPipeline[2]) -> Self`

### `acquire`

`acquire(self)`

### `slot_index`

`slot_index(self) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

### `commit_mma`

`commit_mma(mut self, elect: Int32)`

</section>

---

## KVPipelineGeneric

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct KVPipelineGeneric[num_kv_stages: Int, num_mma_stages: Int, num_producer: Int, num_consumer: Int]`

KVPipeline has `num_kv_stages * num_mma_stages` stages. `num_kv_stages` refers to how many `K` and `V` tiles we pipeline for performing the `S = Q@K'` and `O += P@V` MMAs. Each of these MMAs is broken up into `num_mma_stages` pipelined MMAs. We set `step=False` for all but the last MMA that completes the operation. An alternative implementation would separate the two, and potentially allow for more overall stages at the cost of slightly more bookkeeping.

## Fields

* â€‹mbar (`MBarType`):
* â€‹state (`PipelineState[num_kv_stages]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `num_stages`

`comptime num_stages = (num_kv_stages * num_mma_stages)`

## Methods

### `__init__`

`__init__(mbar: UnsafePointer[SharedMemBarrier, MutAnyOrigin, address_space=AddressSpace.SHARED]) -> Self`

### `init`

`init(self)`

### `producer_mbar`

`producer_mbar[mma_stage: Int](self) -> MBarType`

**Returns:**

`MBarType`

### `consumer_mbar`

`consumer_mbar[mma_stage: Int](self, idx: UInt32) -> MBarType`

**Returns:**

`MBarType`

`consumer_mbar[mma_stage: Int](self) -> MBarType`

**Returns:**

`MBarType`

### `producer_acquire`

`producer_acquire[mma_stage: Int = (num_mma_stages - 1)](self)`

Returns the dynamic pipe idx.

### `consumer_wait`

`consumer_wait[mma_stage: Int = (num_mma_stages - 1)](self)`

### `consumer_release`

`consumer_release[mma_stage: Int = (num_mma_stages - 1)](mut self, e: Int32)`

### `num_mbars`

`static num_mbars() -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

</section>

---

## MLA_Decode_Pack

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MLA_Decode_Pack[ValidLengthType: OptionalPointer, MaskType: MHAMask, ScoreModType: ScoreModTrait]`

## Fields

* â€‹mask (`MaskType`):
* â€‹score\_mod (`ScoreModType`):
* â€‹valid\_length (`ValidLengthType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`DevicePassable`](/mojo/std/builtin/device_passable/DevicePassable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = ValidLengthType.__copyinit__is_trivial if ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial else ScoreModType.__copyinit__is_trivial if MaskType.__copyinit__is_trivial else MaskType.__copyinit__is_trivial`

### `__del__is_trivial`

`comptime __del__is_trivial = ValidLengthType.__del__is_trivial if ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial else ScoreModType.__del__is_trivial if MaskType.__del__is_trivial else MaskType.__del__is_trivial`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = ValidLengthType.__moveinit__is_trivial if ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial else ScoreModType.__moveinit__is_trivial if MaskType.__moveinit__is_trivial else MaskType.__moveinit__is_trivial`

### `device_type`

`comptime device_type = MLA_Decode_Pack[ValidLengthType, MaskType, ScoreModType]`

## Methods

### `__init__`

`__init__(mask: MaskType, score_mod: ScoreModType, valid_length: ValidLengthType) -> Self`

### `get_type_name`

`static get_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

### `get_device_type_name`

`static get_device_type_name() -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

</section>

---

## MLA_SM100_Decode

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MLA_SM100_Decode[KVLUTType: MHAOperand, output_type: DType, MaskType: MHAMask, ScoreModType: ScoreModTrait, config: MLA_SM100_Decode_Config, use_score_mod: Bool, ValidLengthType: OptionalPointer, _is_cache_length_accurate: Bool = False, _use_valid_length: Bool = False, ragged: Bool = False]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `AccumType`

`comptime AccumType = get_accum_type[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type]()`

### `BlockElems`

`comptime BlockElems = (config * config)`

### `bytes_per_element`

`comptime bytes_per_element = size_of[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type]()`

### `KVStageElems`

`comptime KVStageElems = (MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].NumQKBlocks * MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].BlockElems)`

### `NumQKBlocks`

`comptime NumQKBlocks = (config // config)`

### `NumVOBlocks`

`comptime NumVOBlocks = (config // config)`

### `O_M`

`comptime O_M = (config * 2)`

### `O_N`

`comptime O_N = (config // 2)`

### `OTMemTile`

`comptime OTMemTile = TMemTile[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].AccumType, MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].O_M, MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].O_N]`

### `output_tile_width`

`comptime output_tile_width = ((config // 2) * (4 // size_of[output_type]()))`

### `qkv_type`

`comptime qkv_type = KVLUTType.dtype`

### `S_M`

`comptime S_M = (config * 2)`

### `S_N`

`comptime S_N = (config // 2)`

### `STMemTile`

`comptime STMemTile = TMemTile[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].AccumType, MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].S_M, MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].S_N]`

### `UMMAPVSS`

`comptime UMMAPVSS = DecodeSM100PVSS[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].AccumType, config=config]`

### `UMMAQKTSS`

`comptime UMMAQKTSS = DecodeSM100QKTSS[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].AccumType, config=config]`

## Methods

### `kernel`

`static kernel(q_tma: TMATensorTile[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, tile_layout_k_major[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, config.BM, config.BN, config.swizzle_mode](), _tma_desc_tile_layout[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, 2, IndexList[2, DType.int64](config.BM, config.BN, Tuple[]()), config.swizzle_mode]()], k_tma: TMATensorTile[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, _split_last_layout[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type](IndexList[3, DType.int64](config.BM, 1, config.BN, Tuple[]()), config, True), _ragged_desc_layout[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type](IndexList[3, DType.int64](config.BM, 1, config.BN, Tuple[]()), config)], o_tma: TMATensorTile[output_type, tile_layout_k_major[output_type, config.out_rows, config.BN, config.swizzle_mode](), _tma_desc_tile_layout[output_type, 2, IndexList[2, DType.int64](config.out_rows, config.BN, Tuple[]()), config.swizzle_mode]()], kv_lut: KVLUTType, scale: Float32, batch_size: Int, num_partitions: Int, max_cache_valid_length: Int, mla_decode_pack: MLA_Decode_Pack[ValidLengthType, MaskType, ScoreModType])`

### `load_kv`

`static load_kv(tma: TMATensorTile[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, _split_last_layout[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type](IndexList[3, DType.int64](config.BM, 1, config.BN, Tuple[]()), config, True), _ragged_desc_layout[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type](IndexList[3, DType.int64](config.BM, 1, config.BN, Tuple[]()), config)], smem: UnsafePointer[Scalar[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type], MutAnyOrigin, address_space=AddressSpace.SHARED], mbar: UnsafePointer[SharedMemBarrier, MutAnyOrigin, address_space=AddressSpace.SHARED], col_start: UInt, row_start: UInt)`

### `load_q`

`static load_q(tma: TMATensorTile[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, tile_layout_k_major[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, config.BM, config.BN, config.swizzle_mode](), _tma_desc_tile_layout[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, 2, IndexList[2, DType.int64](config.BM, config.BN, Tuple[]()), config.swizzle_mode]()], smem: UnsafePointer[Scalar[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type], MutAnyOrigin, address_space=AddressSpace.SHARED], mbar: UnsafePointer[SharedMemBarrier, MutAnyOrigin, address_space=AddressSpace.SHARED], col_start: UInt, row_start: UInt)`

### `load`

`static load(q_tma: TMATensorTile[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, tile_layout_k_major[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, config.BM, config.BN, config.swizzle_mode](), _tma_desc_tile_layout[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, 2, IndexList[2, DType.int64](config.BM, config.BN, Tuple[]()), config.swizzle_mode]()], k_tma: TMATensorTile[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, _split_last_layout[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type](IndexList[3, DType.int64](config.BM, 1, config.BN, Tuple[]()), config, True), _ragged_desc_layout[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type](IndexList[3, DType.int64](config.BM, 1, config.BN, Tuple[]()), config)], kv_lut: KVLUTType, q_smem: UnsafePointer[Scalar[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type], MutAnyOrigin, address_space=AddressSpace.SHARED], kv_smem: UnsafePointer[Scalar[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type], MutAnyOrigin, address_space=AddressSpace.SHARED], mbar_q: UnsafePointer[SharedMemBarrier, MutAnyOrigin, address_space=AddressSpace.SHARED], mut kv_prod: DecodeKVProducer[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, config], offset_position: OffsetPosition[config, KVLUTType, ragged, _use_valid_length, _is_cache_length_accurate, ValidLengthType])`

### `mmaQK`

`static mmaQK(tmem_addr: UInt32, q_tma: TMATensorTile[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, tile_layout_k_major[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, config.BM, config.BN, config.swizzle_mode](), _tma_desc_tile_layout[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, 2, IndexList[2, DType.int64](config.BM, config.BN, Tuple[]()), config.swizzle_mode]()], k_tma: TMATensorTile[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, _split_last_layout[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type](IndexList[3, DType.int64](config.BM, 1, config.BN, Tuple[]()), config, True), _ragged_desc_layout[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type](IndexList[3, DType.int64](config.BM, 1, config.BN, Tuple[]()), config)], kv_lut: KVLUTType, q_smem: UnsafePointer[Scalar[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type], MutAnyOrigin, address_space=AddressSpace.SHARED], kv_smem: UnsafePointer[Scalar[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type], MutAnyOrigin, address_space=AddressSpace.SHARED], mbar_q: UnsafePointer[SharedMemBarrier, MutAnyOrigin, address_space=AddressSpace.SHARED], s_bars: DecodeSM100MiscMBars[2, 1, WARPGROUP_SIZE], mut kv_cons: DecodeKVConsumer[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, config], offset_position: OffsetPosition[config, KVLUTType, ragged, _use_valid_length, _is_cache_length_accurate, ValidLengthType])`

### `mmaPV`

`static mmaPV(tmem_addr: UInt32, k_tma: TMATensorTile[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, _split_last_layout[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type](IndexList[3, DType.int64](config.BM, 1, config.BN, Tuple[]()), config, True), _ragged_desc_layout[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type](IndexList[3, DType.int64](config.BM, 1, config.BN, Tuple[]()), config)], kv_lut: KVLUTType, kv_smem: UnsafePointer[Scalar[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type], MutAnyOrigin, address_space=AddressSpace.SHARED], p_bars: DecodeSM100MiscMBars[2, WARPGROUP_SIZE, 1], o_bars: DecodeSM100MiscMBars[1, 1, WARPGROUP_SIZE], mut kv_cons: DecodeKVConsumer[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type, config], offset_position: OffsetPosition[config, KVLUTType, ragged, _use_valid_length, _is_cache_length_accurate, ValidLengthType])`

### `clamped_index_coordinate`

`static clamped_index_coordinate(var prompt_idx: UInt32, var q_head_idx: UInt32, var q_idx_abs: UInt32, var col: UInt32, var tile_key_base: UInt32, var num_keys: Int, var cache_start_pos: UInt32) -> IndexList[4, element_type=DType.uint32]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

### `apply_mask`

`static apply_mask[half_load: Int, masked: Bool](tiles_done: Int, col0: Int, num_keys: Int, s_row: LayoutTensor[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].AccumType, Layout.row_major(half_load), MutAnyOrigin, address_space=AddressSpace.LOCAL], mask: MaskType, score_mod: ScoreModType, prompt_idx: UInt32, q_head_idx: UInt32, score_row: UInt32, max_seq_len: UInt32, start_pos: UInt32, cache_start_pos: UInt32) -> Scalar[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].AccumType]`

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar)

### `Softmax`

`static Softmax(tmem_addr: UInt32, s_bars: DecodeSM100MiscMBars[2, 1, WARPGROUP_SIZE], p_bars: DecodeSM100MiscMBars[2, WARPGROUP_SIZE, 1], kv_smem: UnsafePointer[Scalar[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].qkv_type], MutAnyOrigin, address_space=AddressSpace.SHARED], max_smem: UnsafePointer[Scalar[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].AccumType], MutAnyOrigin, address_space=AddressSpace.SHARED], li_smem: UnsafePointer[Scalar[MLA_SM100_Decode[KVLUTType, output_type, MaskType, ScoreModType, config, use_score_mod, ValidLengthType, _is_cache_length_accurate, _use_valid_length, ragged].AccumType], MutAnyOrigin, address_space=AddressSpace.SHARED], c_bars: DecodeSM100MiscMBars[1, WARPGROUP_SIZE, WARPGROUP_SIZE], li_bars: DecodeSM100MiscMBars[1, WARPGROUP_SIZE, WARPGROUP_SIZE], num_k_tiles: Int, num_keys: Int, scale: Float32, mask: MaskType, score_mod: ScoreModType, prompt_idx: UInt32, max_seq_len: UInt32)`

### `Correction`

`static Correction(tmem_addr: UInt32, o_bars: DecodeSM100MiscMBars[1, 1, WARPGROUP_SIZE], mut out_prod: DecodeOutProducer[output_type, config], c_bars: DecodeSM100MiscMBars[1, WARPGROUP_SIZE, WARPGROUP_SIZE], li_bars: DecodeSM100MiscMBars[1, WARPGROUP_SIZE, WARPGROUP_SIZE], num_k_tiles: Int)`

### `store`

`static store(mut out_cons: DecodeOutConsumer[output_type, config], o_tma: TMATensorTile[output_type, tile_layout_k_major[output_type, config.out_rows, config.BN, config.swizzle_mode](), _tma_desc_tile_layout[output_type, 2, IndexList[2, DType.int64](config.out_rows, config.BN, Tuple[]()), config.swizzle_mode]()])`

</section>

---

## MLA_SM100_Decode_Config

<section class='mojo-docs'>

`struct MLA_SM100_Decode_Config`

## Fields

* â€‹MMA\_M (`Int`):
* â€‹BM (`Int`):
* â€‹BN (`Int`):
* â€‹BK0 (`Int`):
* â€‹BK1 (`Int`):
* â€‹q\_depth (`Int`):
* â€‹depth (`Int`):
* â€‹padded\_depth (`Int`):
* â€‹padded\_q\_depth (`Int`):
* â€‹rope\_depth (`Int`):
* â€‹group (`Int`):
* â€‹num\_q\_heads (`Int`):
* â€‹num\_kv\_heads (`Int`):
* â€‹tmem\_used (`Int`):
* â€‹num\_kv\_stages (`Int`):
* â€‹smem\_used (`Int`):
* â€‹dtype\_size (`Int`):
* â€‹swizzle\_mode (`TensorMapSwizzle`):
* â€‹kv\_swizzle\_mode (`TensorMapSwizzle`):
* â€‹decoding\_warp\_split\_k (`Bool`):
* â€‹out\_rows (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `cta_group`

`comptime cta_group = 1`

### `mbar_size`

`comptime mbar_size = size_of[DType.int64]()`

### `MMA_K`

`comptime MMA_K = 16`

### `num_threads`

`comptime num_threads = 384`

### `sm100_smem_carveout`

`comptime sm100_smem_carveout = (B200 - 1024)`

### `sm100_tmem_cols`

`comptime sm100_tmem_cols = 512`

### `TMEM_CORR_LI`

`comptime TMEM_CORR_LI = 321`

### `TMEM_CORR_SCALE`

`comptime TMEM_CORR_SCALE = 320`

### `TMEM_O`

`comptime TMEM_O = 0`

### `TMEM_S0`

`comptime TMEM_S0 = 256`

### `TMEM_S1`

`comptime TMEM_S1 = 288`

## Methods

### `__init__`

`__init__(out self, *, num_q_heads: Int, group: Int, depth: Int, q_depth: Int, dtype_size: Int, swizzle_mode: TensorMapSwizzle, kv_swizzle_mode: TensorMapSwizzle, page_size: Int, decoding_warp_split_k: Bool)`

### `supported`

`supported(self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## OffsetPosition

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct OffsetPosition[config: MLA_SM100_Decode_Config, KVLUTType: MHAOperand, ragged: Bool, use_valid_length: Bool, is_cache_length_accurate: Bool, ValidLengthType: OptionalPointer]`

## Fields

* â€‹start\_of\_seq (`Int`):
* â€‹end\_of\_seq (`Int`):
* â€‹seq\_len (`Int`):
* â€‹q\_batch\_offset (`Int`):
* â€‹num\_keys (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(k: KVLUTType, valid_length: UnsafePointer[Scalar[ValidLengthType.dtype], MutAnyOrigin]) -> Self`

</section>

---

## OutPipeline

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct OutPipeline[num_out_stages: Int, num_producer: Int, num_consumer: Int]`

OutPipeline has `num_out_stages` stages. `num_out_stages` refers to how many output stages we pipeline for performing the output store.

## Fields

* â€‹mbar (`MBarType`):
* â€‹state (`PipelineState[OutPipeline[num_out_stages, num_producer, num_consumer].num_stages]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `num_stages`

`comptime num_stages = num_out_stages`

## Methods

### `__init__`

`__init__(mbar: UnsafePointer[SharedMemBarrier, MutAnyOrigin, address_space=AddressSpace.SHARED]) -> Self`

### `init`

`init(self)`

### `producer_mbar`

`producer_mbar(self) -> MBarType`

**Returns:**

`MBarType`

### `consumer_mbar`

`consumer_mbar(self) -> MBarType`

**Returns:**

`MBarType`

### `producer_acquire`

`producer_acquire(self)`

Returns the dynamic pipe idx.

### `consumer_wait`

`consumer_wait(self)`

### `consumer_release`

`consumer_release(mut self, e: Int32)`

### `producer_commit`

`producer_commit(mut self)`

### `num_mbars`

`static num_mbars() -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

</section>

---

## TMADestination (Mla_decode_sm100)

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TMADestination[dtype: DType, layout: Layout]`

## Fields

* â€‹mbar (`MBarType`):
* â€‹smem (`LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=DType.int32, linear_idx_type=DType.int32, alignment=128]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(mbar: UnsafePointer[SharedMemBarrier, MutAnyOrigin, address_space=AddressSpace.SHARED], smem: LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=DType.int32, linear_idx_type=DType.int32, alignment=128]) -> Self`

</section>

---

## build_mma_ss_ws

<section class='mojo-docs'>

`build_mma_ss_ws(kind: String, layout_a: Layout, layout_b: Layout, *, operand_size: Int, num_k_mmas: Int, tcgen05_mma_type: String) -> String`

**Returns:**

[`String`](/mojo/std/collections/string/string/String)

</section>

---

## bulk_mma_ws

<section class='mojo-docs'>

`bulk_mma_ws[kind: UMMAKind, //, layout_a: Layout, layout_b: Layout, *, num_k_mmas: Int, operand_size: Int, tcgen05_mma_type: String](idesc: UMMAInsDescriptor[kind], a: MMASmemDescriptorPair, b: MMASmemDescriptorPair, c_tmem: UInt32, c_scale: UInt32, elect: Int32)`

</section>

---

## mla_decode_sm100

<section class='mojo-docs'>

## `comptime` values

### `logger`

`comptime logger = Logger[DEFAULT_LEVEL](stdout, "", False)`

### `MBarType`

`comptime MBarType = UnsafePointer[SharedMemBarrier, MutAnyOrigin, address_space=AddressSpace.SHARED]`

### `QOTMATile`

`comptime QOTMATile[dtype: DType, BM: Int, BK: Int, swizzle_mode: TensorMapSwizzle] = TMATensorTile[dtype, tile_layout_k_major[dtype, BM, BK, swizzle_mode](), _tma_desc_tile_layout[dtype, 2, IndexList[2, DType.int64](BM, BK, Tuple[]()), swizzle_mode]()]`

#### Parameters

* â€‹dtype ([`DType`](/std/builtin/dtype/DType)):
* â€‹BM ([`Int`](/std/builtin/int/Int)):
* â€‹BK ([`Int`](/std/builtin/int/Int)):
* â€‹swizzle\_mode ([`TensorMapSwizzle`](/std/gpu/host/nvidia/tma/TensorMapSwizzle)):

### `SharedMemPointer`

`comptime SharedMemPointer[type: AnyType] = UnsafePointer[type, MutAnyOrigin, address_space=AddressSpace.SHARED]`

#### Parameters

* â€‹type ([`AnyType`](/std/builtin/anytype/AnyType)):

### `SharedMemTensor`

`comptime SharedMemTensor[dtype: DType, layout: Layout] = LayoutTensor[dtype, layout, MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=DType.int32, linear_idx_type=DType.int32, alignment=128]`

#### Parameters

* â€‹dtype ([`DType`](/std/builtin/dtype/DType)):
* â€‹layout ([`Layout`](/kernels/layout/layout/Layout)):

## Structs

* [â€‹`DecodeCConsumer`](./DecodeCConsumer):
* [â€‹`DecodeCProducer`](./DecodeCProducer):
* [â€‹`DecodeKVConsumer`](./DecodeKVConsumer):
* [â€‹`DecodeKVProducer`](./DecodeKVProducer):
* [â€‹`DecodeOConsumer`](./DecodeOConsumer):
* [â€‹`DecodeOProducer`](./DecodeOProducer):
* [â€‹`DecodeOutConsumer`](./DecodeOutConsumer):
* [â€‹`DecodeOutProducer`](./DecodeOutProducer):
* [â€‹`DecodePConsumer`](./DecodePConsumer):
* [â€‹`DecodePProducer`](./DecodePProducer):
* [â€‹`DecodeSConsumer`](./DecodeSConsumer):
* [â€‹`DecodeSM100MiscMBars`](./DecodeSM100MiscMBars):
* [â€‹`DecodeSM100PVSS`](./DecodeSM100PVSS):
* [â€‹`DecodeSM100QKTSS`](./DecodeSM100QKTSS):
* [â€‹`DecodeSM100TensorAccumulatorSS`](./DecodeSM100TensorAccumulatorSS):
* [â€‹`DecodeSProducer`](./DecodeSProducer):
* [â€‹`KVPipelineGeneric`](./KVPipelineGeneric): KVPipeline has `num_kv_stages * num_mma_stages` stages. `num_kv_stages` refers to how many `K` and `V` tiles we pipeline for performing the `S = Q@K'` and `O += P@V` MMAs. Each of these MMAs is broken up into `num_mma_stages` pipelined MMAs. We set `step=False` for all but the last MMA that completes the operation. An alternative implementation would separate the two, and potentially allow for more overall stages at the cost of slightly more bookkeeping.
* [â€‹`MLA_Decode_Pack`](./MLA_Decode_Pack):
* [â€‹`MLA_SM100_Decode`](./MLA_SM100_Decode):
* [â€‹`MLA_SM100_Decode_Config`](./MLA_SM100_Decode_Config):
* [â€‹`OffsetPosition`](./OffsetPosition):
* [â€‹`OutPipeline`](./OutPipeline): OutPipeline has `num_out_stages` stages. `num_out_stages` refers to how many output stages we pipeline for performing the output store.
* [â€‹`TMADestination`](./TMADestination):

## Functions

* [â€‹`build_mma_ss_ws`](./build_mma_ss_ws):
* [â€‹`bulk_mma_ws`](./bulk_mma_ws):
* [â€‹`launch_mla_sm100_decode_enqueue_kernel`](./launch_mla_sm100_decode_enqueue_kernel):
* [â€‹`mla_decode_sm100`](./mla_decode_sm100):
* [â€‹`num_matrix_view_rows_decode`](./num_matrix_view_rows_decode):
* [â€‹`tma_tile_qo`](./tma_tile_qo):
* [â€‹`write_bf16x2_row_to_smem_fast`](./write_bf16x2_row_to_smem_fast):

</section>

---

## launch_mla_sm100_decode_enqueue_kernel

<section class='mojo-docs'>

`launch_mla_sm100_decode_enqueue_kernel[KVLUTType: MHAOperand, output_type: DType, MaskType: MHAMask, ScoreModType: ScoreModTrait, config: MLA_SM100_Decode_Config, use_score_mod: Bool, ValidLengthType: OptionalPointer, _use_valid_length: Bool = False, _is_cache_length_accurate: Bool = False, ragged: Bool = False](q_tma: TMATensorTile[KVLUTType.dtype, tile_layout_k_major[KVLUTType.dtype, config.BM, config.BN, config.swizzle_mode](), _tma_desc_tile_layout[KVLUTType.dtype, 2, IndexList[2, DType.int64](config.BM, config.BN, Tuple[]()), config.swizzle_mode]()], k_tma: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BM, 1, config.BN, Tuple[]()), config, True), _ragged_desc_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BM, 1, config.BN, Tuple[]()), config)], o_tma: TMATensorTile[output_type, tile_layout_k_major[output_type, config.out_rows, config.BN, config.swizzle_mode](), _tma_desc_tile_layout[output_type, 2, IndexList[2, DType.int64](config.out_rows, config.BN, Tuple[]()), config.swizzle_mode]()], kv_lut: KVLUTType, scale: Float32, batch_size: Int, num_partitions: Int, max_cache_valid_length: Int, valid_len: ValidLengthType, mask: MaskType, score_mod: ScoreModType, ctx: DeviceContext)`

</section>

---

## mla_decode_sm100 (Mla_decode_sm100)

<section class='mojo-docs'>

`mla_decode_sm100[q_type: DType, q_layout: Layout, k_t: MHAOperand, output_type: DType, mask_t: MHAMask, score_mod_t: ScoreModTrait, valid_layout: Layout, config: MHAConfig[dtype], depth: Int, num_heads: Int, group: Int = 1, *, use_score_mod: Bool = False, ragged: Bool = False, _use_valid_length: Bool = False, _is_cache_length_accurate: Bool = False, decoding_warp_split_k: Bool = False](q: LayoutTensor[q_type, q_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: k_t, output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scale: Float32, batch_size: Int, num_partitions: Int, max_cache_valid_length: Int, valid_length: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mask: mask_t, score_mod: score_mod_t, ctx: DeviceContext)`

</section>

---

## num_matrix_view_rows_decode

<section class='mojo-docs'>

`num_matrix_view_rows_decode[dtype: DType, //](q: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## tma_tile_qo

<section class='mojo-docs'>

`tma_tile_qo[dtype: DType, //, swizzle_mode: TensorMapSwizzle, *, BM: Int, BK: Int, depth: Int](ctx: DeviceContext, ptr: UnsafePointer[Scalar[dtype], origin], rows: Int, out res: TMATensorTile[dtype, tile_layout_k_major[dtype, BM, BK, swizzle_mode](), _tma_desc_tile_layout[dtype, 2, IndexList[2, DType.int64](BM, BK, Tuple[]()), swizzle_mode]()])`

**Returns:**

[`TMATensorTile`](/mojo/kernels/layout/tma_async/TMATensorTile)

</section>

---

## write_bf16x2_row_to_smem_fast

<section class='mojo-docs'>

`write_bf16x2_row_to_smem_fast[layout: Layout, *, out_dtype: DType, in_dtype: DType, config: MLA_SM100_Decode_Config, local_tile_size: Int](shared_mem: UnsafePointer[Scalar[out_dtype], MutAnyOrigin, address_space=AddressSpace.SHARED], local_mem: LayoutTensor[in_dtype, layout, MutAnyOrigin, address_space=AddressSpace.LOCAL], col_start: Int, row_start: Int)`

</section>

---

## mla_graph

<section class='mojo-docs'>

## Functions

* [â€‹`mla_decode_branch_fp8`](./mla_decode_branch_fp8): This is a manually fused kernel that performs the following operations: - Project q\_nope to kv\_latent\_dim through a fp8 batched matmul:     q\_nope\_proj = q\_nope\_t @ w\_uk. - Concatenate q\_nope\_proj and q\_rope:     q\_full = concat(q\_nope\_proj, q\_rope, axis=2). - Perform MLA decode. - Project raw\_output to v\_head\_dim through another fp8 batched matmul:     output = raw\_output\_t @ w\_uv.
* [â€‹`mla_prefill_branch_fp8`](./mla_prefill_branch_fp8): This is a manually fused kernel that performs the following operations: - Copy the KV latent values from PagedKVCache to a contiguous buffer. - Quantize the KV latent values to fp8. - Up-project the latent KV values to full K and V through a matmul. - Split the concatenated KV into K and V. - Perform MLA prefill.
* [â€‹`mla_prefill_decode_graph_fp8`](./mla_prefill_decode_graph_fp8): This is a manually fused kernel that performs the following operations: - Perform MLA prefill or decode based on the maximum sequence length.
* [â€‹`quantize_and_bmm_fp8_helper`](./quantize_and_bmm_fp8_helper): Helper function to quantize and perform a batched matrix multiplication. This function uses the transposed view of the input tensor `a`.
* [â€‹`transpose_helper`](./transpose_helper): Helper function to transpose a tensor from \[B, N, K] to \[N, B, K] (or vice versa).

</section>

---

## mla_decode_branch_fp8

<section class='mojo-docs'>

`mla_decode_branch_fp8[dtype: DType, fp8_dtype: DType, fp8_scale_dtype: DType, collection_t: KVCollectionT, //, m_scale_granularity: Int, n_scale_granularity: Int, k_scale_granularity: Int, mask_str: StringSlice[StaticConstantOrigin], score_mod_str: StringSlice[StaticConstantOrigin], target: StringSlice[StaticConstantOrigin] = "cpu"](output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q_nope: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q_rope: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: collection_t, layer_idx: UInt32, scale: Float32, w_uk: LayoutTensor[fp8_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], w_uk_scale: LayoutTensor[fp8_scale_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], w_uv: LayoutTensor[fp8_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], w_uv_scale: LayoutTensor[fp8_scale_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

This is a manually fused kernel that performs the following operations: - Project q\_nope to kv\_latent\_dim through a fp8 batched matmul:     q\_nope\_proj = q\_nope\_t @ w\_uk. - Concatenate q\_nope\_proj and q\_rope:     q\_full = concat(q\_nope\_proj, q\_rope, axis=2). - Perform MLA decode. - Project raw\_output to v\_head\_dim through another fp8 batched matmul:     output = raw\_output\_t @ w\_uv.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the input and output tensors.
* â€‹fp8\_dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the fp8 input and output tensors.
* â€‹fp8\_scale\_dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the fp8 scale input and output tensors.
* â€‹collection\_t ([`KVCollectionT`](/mojo/kernels/kv_cache/types/KVCollectionT)): Type of the KV collection.
* â€‹m\_scale\_granularity ([`Int`](/mojo/std/builtin/int/Int)): Granularity of the scale for M dimension of the
  matrix multiplication.
* â€‹n\_scale\_granularity ([`Int`](/mojo/std/builtin/int/Int)): Granularity of the scale for N dimension of the
  matrix multiplication.
* â€‹k\_scale\_granularity ([`Int`](/mojo/std/builtin/int/Int)): Granularity of the scale for K dimension of the
  matrix multiplication.
* â€‹mask\_str (`StringSlice`): Mask variant.
* â€‹score\_mod\_str (`StringSlice`): Positional encoding variant.
* â€‹target (`StringSlice`): Target device.

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output tensor of shape \[tot\_seq\_len, num\_heads, v\_head\_dim].
* â€‹q\_nope ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Query tensor of shape \[tot\_seq\_len, num\_heads,
  qk\_nope\_head\_dim].
* â€‹q\_rope ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Rope query tensor of shape \[tot\_seq\_len, num\_heads,
  qk\_rope\_head\_dim].
* â€‹input\_row\_offsets ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Indicates where each request starts and ends in
  `q`. Shape: [num\_batches + 1].
* â€‹kv\_collection (`collection_t`): Paged KV Cache object.
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Layer index.
* â€‹scale ([`Float32`](/mojo/std/builtin/simd/#float32)): Scale for the attention calculation.
* â€‹w\_uk ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Weight matrix for projecting the non-rope part of each query head to
  KV latent space. Shape: [num\_heads, kv\_latent\_dim, qk\_nope\_head\_dim].
* â€‹w\_uk\_scale ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The scale for the w\_uk weight matrix. Shape varies
  depending on the float8\_config.
* â€‹w\_uv ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Weight matrix for projecting the output of the attention back to
  each head's original space. Shape: [num\_heads, v\_head\_dim, kv\_latent\_dim].
* â€‹w\_uv\_scale ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The scale for the w\_uv weight matrix. Shape varies
  depending on the float8\_config.
* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): Device context.

</section>

---

## mla_prefill_branch_fp8

<section class='mojo-docs'>

`mla_prefill_branch_fp8[dtype: DType, fp8_dtype: DType, fp8_scale_dtype: DType, collection_t: KVCollectionT, //, m_scale_granularity: Int, n_scale_granularity: Int, k_scale_granularity: Int, mask_str: StringSlice[StaticConstantOrigin], score_mod_str: StringSlice[StaticConstantOrigin], target: StringSlice[StaticConstantOrigin] = "cpu"](output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q_nope: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q_rope: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: collection_t, layer_idx: UInt32, scale: Float32, buffer_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], cache_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], buffer_length: Int, kv_b_proj: LayoutTensor[fp8_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_b_proj_scale: LayoutTensor[fp8_scale_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

This is a manually fused kernel that performs the following operations: - Copy the KV latent values from PagedKVCache to a contiguous buffer. - Quantize the KV latent values to fp8. - Up-project the latent KV values to full K and V through a matmul. - Split the concatenated KV into K and V. - Perform MLA prefill.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the input and output tensors.
* â€‹fp8\_dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the fp8 input and output tensors.
* â€‹fp8\_scale\_dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the fp8 scale input and output tensors.
* â€‹collection\_t ([`KVCollectionT`](/mojo/kernels/kv_cache/types/KVCollectionT)): Type of the KV collection.
* â€‹m\_scale\_granularity ([`Int`](/mojo/std/builtin/int/Int)): Granularity of the scale for M dimension of the
  matrix multiplication.
* â€‹n\_scale\_granularity ([`Int`](/mojo/std/builtin/int/Int)): Granularity of the scale for N dimension of the
  matrix multiplication.
* â€‹k\_scale\_granularity ([`Int`](/mojo/std/builtin/int/Int)): Granularity of the scale for K dimension of the
  matrix multiplication.
* â€‹mask\_str (`StringSlice`): Mask variant.
* â€‹score\_mod\_str (`StringSlice`): Positional encoding variant.
* â€‹target (`StringSlice`): Target device.

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output tensor of shape \[tot\_seq\_len, num\_heads, v\_head\_dim].
* â€‹q\_nope ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Query tensor of shape \[tot\_seq\_len, num\_heads,
  qk\_nope\_head\_dim].
* â€‹q\_rope ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Query tensor of shape \[tot\_seq\_len, num\_heads,
  qk\_rope\_head\_dim].
* â€‹input\_row\_offsets ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Indicates where each request starts and ends in
  `q`. Shape: [num\_batches + 1].
* â€‹kv\_collection (`collection_t`): Paged KV Cache object.
* â€‹layer\_idx ([`UInt32`](/mojo/std/builtin/simd/#uint32)): Layer index.
* â€‹scale ([`Float32`](/mojo/std/builtin/simd/#float32)): Scale for the attention calculation.
* â€‹buffer\_row\_offsets ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Indicates where each request's KV latent values
  should be stored in the contiguous K buffer. This is a 1D tensor
  of shape \[num\_batches + 1].
* â€‹cache\_offsets ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Indicates the starting token position in the KV cache
  from which to copy KV latent values for each request. This is a 1D
  tensor of shape \[num\_batches + 1].
* â€‹buffer\_length ([`Int`](/mojo/std/builtin/int/Int)): The total number of tokens in the KV cache. Scalar.
* â€‹kv\_b\_proj ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Weight matrix for up-projecting the KV latent values to full
  K and V. Shape: [num\_heads \* (qk\_nope\_head\_dim + v\_head\_dim),
  kv\_latent\_dim].
* â€‹kv\_b\_proj\_scale ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The scale for the weight matrix. Shape varies
  depending on the float8\_config.
* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): Device context.

</section>

---

## mla_prefill_decode_graph_fp8

<section class='mojo-docs'>

`mla_prefill_decode_graph_fp8[dtype: DType, fp8_dtype: DType, fp8_scale_dtype: DType, collection_t: KVCollectionT, //, m_scale_granularity: Int, n_scale_granularity: Int, k_scale_granularity: Int, mask_str: StringSlice[StaticConstantOrigin], score_mod_str: StringSlice[StaticConstantOrigin], target: StringSlice[StaticConstantOrigin] = "cpu"](output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q_nope: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q_rope: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_collection: collection_t, layer_idx: UInt32, scale: Float32, buffer_row_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], cache_offsets: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], buffer_length: Int, max_seq_len: Int, kv_b_proj: LayoutTensor[fp8_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], kv_b_proj_scale: LayoutTensor[fp8_scale_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], w_uk: LayoutTensor[fp8_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], w_uk_scale: LayoutTensor[fp8_scale_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], w_uv: LayoutTensor[fp8_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], w_uv_scale: LayoutTensor[fp8_scale_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

This is a manually fused kernel that performs the following operations: - Perform MLA prefill or decode based on the maximum sequence length.

</section>

---

## quantize_and_bmm_fp8_helper

<section class='mojo-docs'>

`quantize_and_bmm_fp8_helper[dtype: DType, fp8_dtype: DType, fp8_scale_dtype: DType, m_scale_granularity: Int, n_scale_granularity: Int, k_scale_granularity: Int, target: StringSlice[StaticConstantOrigin] = "cpu"](c: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[fp8_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_scales: LayoutTensor[fp8_scale_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

Helper function to quantize and perform a batched matrix multiplication. This function uses the transposed view of the input tensor `a`.

</section>

---

## transpose_helper

<section class='mojo-docs'>

`transpose_helper[dtype: DType](output_tensor: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_tensor: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContext)`

Helper function to transpose a tensor from \[B, N, K] to \[N, B, K] (or vice versa).

</section>

---

## MLAKVProducerPipeline

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct MLAKVProducerPipeline[dtype: DType, config: FA4Config]`

## Fields

* â€‹kv\_pipeline (`KVPipeline[config.num_kv_stages, config.num_mma_stages]`):
* â€‹smem (`MLAKVProducerPipeline[dtype, config].SMemType`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `k_bytes`

`comptime k_bytes = (MLAKVProducerPipeline[dtype, config].k_elements * size_of[dtype]())`

### `k_elements`

`comptime k_elements = tile_layout_k_major[dtype, config.BN, config.BK0, config.swizzle_mode]().size()`

### `k_layout`

`comptime k_layout = tile_layout_k_major[dtype, config.BN, 128, config.swizzle_mode]()`

### `k_rope_layout`

`comptime k_rope_layout = tile_layout_k_major[dtype, config.BN, 64, config.swizzle_mode]()`

### `KPairType`

`comptime KPairType = TMADestination[dtype, tile_layout_k_major[dtype, config.BN, config.BK0, config.swizzle_mode]()]`

### `KType`

`comptime KType = LayoutTensor[dtype, tile_layout_k_major[dtype, config.BN, config.BK0, config.swizzle_mode](), MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=DType.int32, linear_idx_type=DType.int32, alignment=128]`

### `SMemType`

`comptime SMemType = LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED]`

### `v_bytes`

`comptime v_bytes = (MLAKVProducerPipeline[dtype, config].v_elements * size_of[dtype]())`

### `v_elements`

`comptime v_elements = tile_layout_mn_major[dtype, 128, config.BK1, config.swizzle_mode]().size()`

### `VPairType`

`comptime VPairType = TMADestination[dtype, tile_layout_mn_major[dtype, 128, config.BK1, config.swizzle_mode]()]`

### `VType`

`comptime VType = LayoutTensor[dtype, tile_layout_mn_major[dtype, 128, config.BK1, config.swizzle_mode](), MutAnyOrigin, address_space=AddressSpace.SHARED, layout_int_type=DType.int32, linear_idx_type=DType.int32, alignment=128]`

## Methods

### `__init__`

`__init__(mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], smem: LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED]) -> Self`

`__init__(kv_pipeline: KVPipeline[config.num_kv_stages, config.num_mma_stages], smem: LegacyUnsafePointer[Scalar[dtype], address_space=AddressSpace.SHARED]) -> Self`

### `init`

`init(self)`

Only one of the producer or consumer should call `init()`.

### `get_kv_smem`

`get_kv_smem[*, mma_stage: Int](self) -> MLAKVProducerPipeline[dtype, config].SMemType`

**Returns:**

`MLAKVProducerPipeline`

### `get_k`

`get_k[*, mma_stage: Int, expect: Bool = True](self) -> MLAKVProducerPipeline[dtype, config].KPairType`

**Returns:**

`MLAKVProducerPipeline`

### `get_v`

`get_v[*, mma_stage: Int](self) -> MLAKVProducerPipeline[dtype, config].VPairType`

**Returns:**

`MLAKVProducerPipeline`

### `acquire_kv`

`acquire_kv[*, mma_stage: Int = (config - 1)](self)`

### `commit_kv_step`

`commit_kv_step(mut self)`

Step the kv pipeline. The does not perform the commit on the mbars; that should be handled by the `tma_op.async_copy`.

</section>

---

## SM100MLA

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct SM100MLA[KVLUTType: MHAOperand, output_type: DType, MaskType: MHAMask, ScoreModType: ScoreModTrait, SchedulerType: MHATileScheduler, config: FA4Config, use_score_mod: Bool, ValidLengthType: OptionalPointer, SinkType: OptionalPointer, KVRowOffsetsType: OptionalPointer, _is_cache_length_accurate: Bool, MaxSeqLenType: OptionallyStaticInt, PartitionType: MHAPartitionScheme, descriptor_shape: IndexList[3], remaining_global_dim_rank: Int]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `accum_type`

`comptime accum_type = get_accum_type[SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_type]()`

### `BM`

`comptime BM = config.BM`

### `BN`

`comptime BN = config.BN`

### `cache_depth`

`comptime cache_depth = 576`

### `cta_group`

`comptime cta_group = 1`

### `depth`

`comptime depth = config.depth`

### `group`

`comptime group = config.group`

### `k_bytes`

`comptime k_bytes = SIMD[DType.uint32, 1]((SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].swizzle_granularity * config)).__rmul__[DType.uint32, 1](SIMD[DType.uint32, 1](SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_dt_size))`

### `k_elements`

`comptime k_elements = SIMD[DType.uint32, 1]((SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].swizzle_granularity * config))`

### `k_rope_depth`

`comptime k_rope_depth = 64`

### `kv_depth`

`comptime kv_depth = (config - 64)`

### `KVPipelineType`

`comptime KVPipelineType = KVPipeline[config.num_kv_stages, config.num_mma_stages]`

### `MMA_K`

`comptime MMA_K = 16`

### `MMA_M`

`comptime MMA_M = (config // 2)`

### `num_m_mmas`

`comptime num_m_mmas = 2`

### `num_mma_stages`

`comptime num_mma_stages = config.num_mma_stages`

### `num_q_heads`

`comptime num_q_heads = config.num_q_heads`

### `OPipelineType`

`comptime OPipelineType = MBarPipeline[2]`

### `padded_depth`

`comptime padded_depth = config.padded_depth`

### `page_size`

`comptime page_size = KVLUTType.page_size`

### `PositionType`

`comptime PositionType = MHAPosition[config.BM, config.BN, config.depth, config.padded_depth, config.num_q_heads, config.group, _is_decoding[MaxSeqLenType]()]`

### `qkv_dt_size`

`comptime qkv_dt_size = size_of[SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_type]()`

### `qkv_type`

`comptime qkv_type = KVLUTType.dtype`

### `qo_bytes`

`comptime qo_bytes = SIMD[DType.uint32, 1]((SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_dt_size * SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qo_elements))`

### `qo_elements`

`comptime qo_elements = (SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].padded_depth * SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].MMA_M)`

### `ragged`

`comptime ragged = ValidLengthType.is_null.__bool__().__invert__()`

### `simd_size`

`comptime simd_size = simd_width_of[SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_type]()`

### `swizzle_granularity`

`comptime swizzle_granularity = (config.swizzle_mode.bytes() // SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_dt_size)`

### `UMMA0Type`

`comptime UMMA0Type = SM100TensorAccumulatorSS[SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_type, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].accum_type, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].MMA_M, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].BN, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].depth, swizzle_a=config.swizzle_mode, swizzle_b=config.swizzle_mode, num_stages=SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].num_mma_stages]`

### `UMMA1Type`

`comptime UMMA1Type = SM100TensorAccumulatorTS[SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_type, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].accum_type, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].MMA_M, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].kv_depth, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].BN, config.swizzle_mode, transpose_b=False, num_stages=SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].num_mma_stages]`

### `v_bytes_per_mma`

`comptime v_bytes_per_mma = SIMD[DType.uint32, 1](((SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_dt_size * 16) * config))`

## Methods

### `mla_prefill_kernel`

`static mla_prefill_kernel[KRopeType: MHAOperand](q_tma_op: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](q_smem_shape[KVLUTType.dtype, config.swizzle_mode, BM=(config // 2), group=config.group, depth=config.BK0, decoding=False](), config, True), _ragged_desc_layout[KVLUTType.dtype](q_smem_shape[KVLUTType.dtype, config.swizzle_mode, BM=(config // 2), group=config.group, depth=config.BK0, decoding=False](), config)], k_tma_op: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].kv_depth, Tuple[]()), config, True), _ragged_desc_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].kv_depth, Tuple[]()), config)], k_rope_tma_op: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, 64, Tuple[]()), TensorMapSwizzle.SWIZZLE_128B, True), _ragged_desc_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, 64, Tuple[]()), TensorMapSwizzle.SWIZZLE_128B)], v_tma_op: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].kv_depth, Tuple[]()), config, True), _ragged_desc_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].kv_depth, Tuple[]()), config)], o_ptr_arg: LegacyUnsafePointer[Scalar[output_type]], ragged_tma_store: RaggedTensorMap[output_type, descriptor_shape, remaining_global_dim_rank, config.swizzle_mode], kv_lut: KVLUTType, scale: Float32, batch_size: UInt32, num_keys_arg: UInt32, pack: Pack[MaskType, ScoreModType, SchedulerType, ValidLengthType, SinkType, KVRowOffsetsType, MaxSeqLenType, PartitionType])`

### `correction`

`static correction(tmem_addr: UInt32, mbars: FA4MiscMBars, o_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], score_row: UInt32, num_keys: UInt32, mask: MaskType)`

### `softmax`

`static softmax(tmem_addr: UInt32, warp_idx: UInt32, mbars: FA4MiscMBars, o_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], score_row: UInt32, seq_info: SeqInfo, mask: MaskType, num_keys: UInt32, scale: Scalar[SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].accum_type], score_mod: ScoreModType, max_seq_len: UInt32, o_ptr_arg: LegacyUnsafePointer[Scalar[output_type]], ragged_tma_store: RaggedTensorMap[output_type, descriptor_shape, remaining_global_dim_rank, config.swizzle_mode], o_smem: LegacyUnsafePointer[Scalar[output_type], address_space=AddressSpace.SHARED], sink_weights: SinkType)`

### `scale_write_output`

`static scale_write_output(local_row: UInt32, inv_row_sum: Scalar[SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].accum_type], o_smem: LegacyUnsafePointer[Scalar[output_type], address_space=AddressSpace.SHARED], o_tmem: TMemTile[SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].accum_type, (SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].BM // 2), SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].kv_depth], o_ptr: LegacyUnsafePointer[Scalar[output_type]], ragged_tma_store: RaggedTensorMap[output_type, descriptor_shape, remaining_global_dim_rank, config.swizzle_mode], warp_group_idx: UInt32, consumer_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], current_seq: Int, num_output_rows: Int32)`

### `mask_status`

`static mask_status(mask: MaskType, score_row: UInt32, kv_row: UInt32) -> TileMaskStatus`

**Returns:**

`TileMaskStatus`

### `load`

`static load(mbars: FA4MiscMBars, kv_pipeline_arg: KVPipeline[config.num_kv_stages, config.num_mma_stages], score_row: UInt32, num_keys: UInt32, seq_info: SeqInfo, max_seq_len: MaxSeqLenType, mask: MaskType, q_tma_op: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](q_smem_shape[KVLUTType.dtype, config.swizzle_mode, BM=(config // 2), group=config.group, depth=config.BK0, decoding=False](), config, True), _ragged_desc_layout[KVLUTType.dtype](q_smem_shape[KVLUTType.dtype, config.swizzle_mode, BM=(config // 2), group=config.group, depth=config.BK0, decoding=False](), config)], k_tma_op: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].kv_depth, Tuple[]()), config, True), _ragged_desc_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].kv_depth, Tuple[]()), config)], k_rope_tma_op: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, 64, Tuple[]()), TensorMapSwizzle.SWIZZLE_128B, True), _ragged_desc_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, 64, Tuple[]()), TensorMapSwizzle.SWIZZLE_128B)], v_tma_op: TMATensorTile[KVLUTType.dtype, _split_last_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].kv_depth, Tuple[]()), config, True), _ragged_desc_layout[KVLUTType.dtype](IndexList[3, DType.int64](config.BN, 1, SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].kv_depth, Tuple[]()), config)], kv_lut: KVLUTType, q_smem: LegacyUnsafePointer[Scalar[KVLUTType.dtype], address_space=AddressSpace.SHARED])`

### `descriptor_q`

`static descriptor_q(q_smem: LegacyUnsafePointer[Scalar[SM100MLA[KVLUTType, output_type, MaskType, ScoreModType, SchedulerType, config, use_score_mod, ValidLengthType, SinkType, KVRowOffsetsType, _is_cache_length_accurate, MaxSeqLenType, PartitionType, descriptor_shape, remaining_global_dim_rank].qkv_type], address_space=AddressSpace.SHARED]) -> MMASmemDescriptorPair`

**Returns:**

[`MMASmemDescriptorPair`](/mojo/std/gpu/compute/arch/mma_nvidia_sm100/MMASmemDescriptorPair)

### `mma`

`static mma(tmem_addr: UInt32, mbars: FA4MiscMBars, kv_pipeline_arg: KVPipeline[config.num_kv_stages, config.num_mma_stages], o_mbar: LegacyUnsafePointer[SharedMemBarrier, address_space=AddressSpace.SHARED], score_row: UInt32, num_keys: UInt32, mask: MaskType, q_smem: LegacyUnsafePointer[Scalar[KVLUTType.dtype], address_space=AddressSpace.SHARED])`

</section>

---

## mla_prefill_sm100

<section class='mojo-docs'>

## Structs

* [â€‹`MLAKVProducerPipeline`](./MLAKVProducerPipeline):
* [â€‹`SM100MLA`](./SM100MLA):

## Functions

* [â€‹`mla_sm100_prefill`](./mla_sm100_prefill):

</section>

---

## mla_sm100_prefill

<section class='mojo-docs'>

`mla_sm100_prefill[output_type: DType, q_type: DType, KVType: MHAOperand, KRopeType: MHAOperand, MaskType: MHAMask, ScoreModType: ScoreModTrait, MaxPromptLenType: OptionallyStaticInt, //, config: MHAConfig[dtype], group: Int, q_depth: Int, cache_depth: Int, use_score_mod: Bool, _is_cache_length_accurate: Bool](output: LayoutTensor[output_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], q: LayoutTensor[q_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: KVType, v: KVType, k_rope: KRopeType, mask_functor: MaskType, score_mod_functor: ScoreModType, valid_length: LayoutTensor[DType.uint32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], max_prompt_len: MaxPromptLenType, scale: Float32, batch_size: Int, ctx: DeviceContext)`

</section>

---

## calculate_warp_offset

<section class='mojo-docs'>

`calculate_warp_offset[MaskType: DType](state: Bool) -> Tuple[UInt64, UInt64]`

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

</section>

---

## group_limited_router_kernel

<section class='mojo-docs'>

`group_limited_router_kernel[scores_type: DType, bias_type: DType, expert_indices_layout: Layout, expert_weights_layout: Layout, expert_scores_layout: Layout, expert_bias_layout: Layout, n_routed_experts: Int, n_experts_per_tok: Int, n_groups: Int, topk_group: Int, norm_weights: Bool, num_threads: Int](expert_indices: LayoutTensor[DType.int32, expert_indices_layout, MutAnyOrigin], expert_weights: LayoutTensor[scores_type, expert_weights_layout, MutAnyOrigin], expert_scores: LayoutTensor[scores_type, expert_scores_layout, ImmutAnyOrigin], expert_bias: LayoutTensor[bias_type, expert_bias_layout, ImmutAnyOrigin], routed_scaling_factor: Float32)`

A manually fused MoE router with the group-limited strategy. It divides all the experts into `n_groups` groups and then finds the top `topk_group` groups with the highest scores. The final experts for each token are selected from the experts in the selected groups. The bias will be applied to the scores during the selection process, but the final weights will not include the bias.

</section>

---

## moe

<section class='mojo-docs'>

## Functions

* [â€‹`calculate_warp_offset`](./calculate_warp_offset):
* [â€‹`group_limited_router_kernel`](./group_limited_router_kernel): A manually fused MoE router with the group-limited strategy. It divides all the experts into `n_groups` groups and then finds the top `topk_group` groups with the highest scores. The final experts for each token are selected from the experts in the selected groups. The bias will be applied to the scores during the selection process, but the final weights will not include the bias.
* [â€‹`moe_create_indices`](./moe_create_indices):
* [â€‹`moe_create_indices_bucket_group_kernel`](./moe_create_indices_bucket_group_kernel): Create indices for MoE routing using bucket sort algorithm.
* [â€‹`moe_create_indices_kernel`](./moe_create_indices_kernel):
* [â€‹`router_group_limited`](./router_group_limited): A manually fused MoE router with the group-limited strategy.

</section>

---

## moe_create_indices

<section class='mojo-docs'>

`moe_create_indices[input_type: DType, //, target: StringSlice[StaticConstantOrigin], expected_count: Int = 8192](token_expert_order: LayoutTensor[DType.uint32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], expert_start_indices: LayoutTensor[DType.uint32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], restore_token_order: LayoutTensor[DType.uint32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], expert_ids: LayoutTensor[DType.int32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], expert_usage_stats: LayoutTensor[DType.uint32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], topk_ids: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr)`

</section>

---

## moe_create_indices_bucket_group_kernel

<section class='mojo-docs'>

`moe_create_indices_bucket_group_kernel[input_type: DType, token_expert_order_layout: Layout, expert_start_indices_layout: Layout, restore_token_order_layout: Layout, expert_ids_layout: Layout, expert_usage_stats_layout: Layout, topk_ids_layout: Layout, num_threads: Int = WARP_SIZE, expected_count: Int = 8192](token_expert_order: LayoutTensor[DType.uint32, token_expert_order_layout, MutAnyOrigin], lock: LayoutTensor[DType.uint32, Layout.row_major(1), MutAnyOrigin], expert_start_indices: LayoutTensor[DType.uint32, expert_start_indices_layout, MutAnyOrigin], restore_token_order: LayoutTensor[DType.uint32, restore_token_order_layout, MutAnyOrigin], expert_ids: LayoutTensor[DType.int32, expert_ids_layout, MutAnyOrigin], expert_usage_stats: LayoutTensor[DType.uint32, expert_usage_stats_layout, MutAnyOrigin], topk_ids: LayoutTensor[input_type, topk_ids_layout, MutAnyOrigin])`

Create indices for MoE routing using bucket sort algorithm.

The main goal of this kernel is to group tokens that use the same expert together.
This allows for efficient batching when used by other kernels such as grouped matmul.

This is a GPU-optimized bucket sort implementation that uses:

* Warp-level voting to count matching tokens
* Shared memory for temporary storage
* Atomic operations for thread-safe global memory updates

topk\_ids: a 1D tensor of expert ids, the index of each expert\_id corresponds to a token.
For example if topk\_ids is \[1, 0, 1, 3, 4, 2], then the corresponding tokens are \[0, 1, 2, 3, 4, 5]

token\_expert\_order: a 1D tensor of tokens grouped together by expert id.
Using the previous topk\_ids, the token expert order could be \[0, 2, 1, 3, 4, 5]

expert\_ids: a 1D tensor of all the experts that are being used. Using the previous topk\_ids the
our expert\_ids would be \[1, 0, 3, 4, 2]

expert\_start\_indices: tells us where each expert starts and end in the token\_expert\_order. Based on the
order of our expert\_ids our expert\_start\_indices would be \[0, 2, 3, 4, 5, 6]. So if you wanted to see where
expert 1 starts and ends you would get the index 'i' of expert 1 in expert\_ids and would query expert\_start\_indices\[i]
and query expert\_start\_indices\[i + 1] which is 0 and 2 respectively.

lock: a 1D tensor that holds a single scalar value, this single integer will be used to atomically
synchronize the writes back to global memory. It will do this by storing how many blocks have finished
writing and the current global memory offset.

expert\_usage\_stats: contains two values, the maximum number of tokens assigned to any expert and the
number of active experts. For our example the stats would be \[2, 5]

restore\_token\_order: a 1D tensor where each index represents a cooresponding token and holds the new index of the token
in the token\_expert\_order tensor. For our example the restore\_token\_order would be \[0, 2, 1, 3, 4, 5]

</section>

---

## moe_create_indices_kernel

<section class='mojo-docs'>

`moe_create_indices_kernel[input_type: DType, num_threads: Int, token_expert_order_layout: Layout, expert_start_indices_layout: Layout, restore_token_order_layout: Layout, expert_ids_layout: Layout, expert_usage_stats_layout: Layout, indices_padded_layout: Layout, padded_input_layout: Layout, topk_ids_layout: Layout](token_expert_order: LayoutTensor[DType.uint32, token_expert_order_layout, MutAnyOrigin], expert_start_indices: LayoutTensor[DType.uint32, expert_start_indices_layout, MutAnyOrigin], restore_token_order: LayoutTensor[DType.uint32, restore_token_order_layout, MutAnyOrigin], expert_ids: LayoutTensor[DType.int32, expert_ids_layout, MutAnyOrigin], expert_usage_stats: LayoutTensor[DType.uint32, expert_usage_stats_layout, MutAnyOrigin], indices_padded: LayoutTensor[DType.uint32, indices_padded_layout, MutAnyOrigin], topk_ids_padded: LayoutTensor[input_type, padded_input_layout, MutAnyOrigin], topk_ids: LayoutTensor[input_type, topk_ids_layout, MutAnyOrigin])`

</section>

---

## router_group_limited

<section class='mojo-docs'>

`router_group_limited[scores_type: DType, bias_type: DType, //, n_routed_experts: Int, n_experts_per_tok: Int, n_groups: Int, topk_group: Int, norm_weights: Bool, target: StringSlice[StaticConstantOrigin]](expert_indices: LayoutTensor[DType.int32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], expert_weights: LayoutTensor[scores_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], expert_scores: LayoutTensor[scores_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], expert_bias: LayoutTensor[bias_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], routed_scaling_factor: Float32, context: DeviceContextPtr)`

A manually fused MoE router with the group-limited strategy.

Reference: <https://github.com/deepseek-ai/DeepSeek-V3/blob/9b4e9788e4a3a731f7567338ed15d3ec549ce03b/inference/model.py#L566>.

Inputs:
expert\_indices: The indices of the routed experts for each token.
Shape: [num\_tokens, num\_experts\_per\_tok].
expert\_weights: The weights of the routed experts for each token.
Shape: [num\_tokens, num\_experts\_per\_tok].
expert\_scores: The scores for each expert for each token. Shape:
\[num\_tokens, n\_routed\_experts].
expert\_bias: The bias for each expert. Shape: [n\_routed\_experts].
routed\_scaling\_factor: The scaling factor for the routed expert weights.
context: DeviceContextPtr.

**Parameters:**

* â€‹scores\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the scores and the output weights.
* â€‹bias\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the expert bias.
* â€‹n\_routed\_experts ([`Int`](/mojo/std/builtin/int/Int)): The number of experts to route to.
* â€‹n\_experts\_per\_tok ([`Int`](/mojo/std/builtin/int/Int)): The number of experts to be selected per token.
* â€‹n\_groups ([`Int`](/mojo/std/builtin/int/Int)): The number of expert groups.
* â€‹topk\_group ([`Int`](/mojo/std/builtin/int/Int)): The number of expert groups to be selected per token.
* â€‹norm\_weights ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to normalize the selected weights.
* â€‹target (`StringSlice`): The target device to run the kernel on.

</section>

---

## BoundingBox

<section class='mojo-docs'>

`struct BoundingBox[dtype: DType]`

Represents a 2D bounding box for object detection.

The box is stored using two corner points: `nw` and `se`.
**Note:** In this implementation, `nw` stores the maximum coordinates (max y, max x)
and `se` stores the minimum coordinates (min y, min x). This differs from the typical
interpretation of "northwest" (usually min x, max y) and "southeast" (usually max x, min y).
This representation allows efficient computation of intersection and union areas.

Fields:
nw: Corner storing the maximum coordinates (max y, max x).
se: Corner storing the minimum coordinates (min y, min x).

## Parameters

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type for coordinate values.

## Fields

* â€‹nw (`SIMD[dtype, 2]`):
* â€‹se (`SIMD[dtype, 2]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(out self, y1: Scalar[dtype], x1: Scalar[dtype], y2: Scalar[dtype], x2: Scalar[dtype])`

Initialize a bounding box from two diagonal corner coordinates.

Note:
The corners are automatically ordered to ensure nw contains the
maximum coordinates and se contains the minimum coordinates.

**Args:**

* â€‹y1 ([`Scalar`](/mojo/std/builtin/simd/#scalar)): Y-coordinate of first corner.
* â€‹x1 ([`Scalar`](/mojo/std/builtin/simd/#scalar)): X-coordinate of first corner.
* â€‹y2 ([`Scalar`](/mojo/std/builtin/simd/#scalar)): Y-coordinate of second corner.
* â€‹x2 ([`Scalar`](/mojo/std/builtin/simd/#scalar)): X-coordinate of second corner.

### `iou`

`iou(self, other: Self) -> Scalar[dtype]`

Calculate Intersection over Union (IoU) with another bounding box.

**Args:**

* â€‹other (`Self`): The other bounding box to compare with.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The IoU value, ranging from 0 (no overlap) to 1 (perfect overlap).

### `intersection_area`

`intersection_area(self, other: Self) -> Scalar[dtype]`

Calculate the area of intersection with another bounding box.

**Args:**

* â€‹other (`Self`): The other bounding box to intersect with.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The intersection area, or 0 if boxes don't overlap.

### `area`

`area(self) -> Scalar[dtype]`

Calculate the area of this bounding box.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar): The area of the box.

</section>

---

## nms

<section class='mojo-docs'>

## Structs

* [â€‹`BoundingBox`](./BoundingBox): Represents a 2D bounding box for object detection.

## Functions

* [â€‹`non_max_suppression`](./non_max_suppression): Perform Non-Maximum Suppression (NMS) on bounding boxes.
* [â€‹`non_max_suppression_shape_func`](./non_max_suppression_shape_func): Compute the output shape for NMS without allocating the output buffer.

</section>

---

## non_max_suppression

<section class='mojo-docs'>

`non_max_suppression[dtype: DType](boxes: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scores: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[DType.int64, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], max_output_boxes_per_class: Int, iou_threshold: Float32, score_threshold: Float32)`

Perform Non-Maximum Suppression (NMS) on bounding boxes.

This is a buffer semantic overload that writes results directly to an output tensor.
NMS iteratively selects boxes with highest scores while suppressing nearby boxes
with high overlap (IoU).

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type for box coordinates and scores.

**Args:**

* â€‹boxes ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Rank-3 tensor of bounding boxes with shape (batch, num\_boxes, 4).
  Each box is \[y1, x1, y2, x2].
* â€‹scores ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Rank-3 tensor of scores with shape (batch, num\_classes, num\_boxes).
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Rank-2 output tensor to store selected boxes as (N, 3) where each
  row is \[batch\_idx, class\_idx, box\_idx].
* â€‹max\_output\_boxes\_per\_class ([`Int`](/mojo/std/builtin/int/Int)): Maximum number of boxes to select per class.
* â€‹iou\_threshold ([`Float32`](/mojo/std/builtin/simd/#float32)): IoU threshold for suppression. Boxes with IoU > threshold
  are suppressed.
* â€‹score\_threshold ([`Float32`](/mojo/std/builtin/simd/#float32)): Minimum score threshold. Boxes with score < threshold
  are filtered out.

`non_max_suppression[dtype: DType, func: fn(Int64, Int64, Int64) capturing -> None](boxes: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scores: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], max_output_boxes_per_class: Int, iou_threshold: Float32, score_threshold: Float32)`

Implements the NonMaxSuppression operator from the ONNX spec <https://github.com/onnx/onnx/blob/main/docs/Operators.md#nonmaxsuppression>.

</section>

---

## non_max_suppression_shape_func

<section class='mojo-docs'>

`non_max_suppression_shape_func[dtype: DType](boxes: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], scores: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], max_output_boxes_per_class: Int, iou_threshold: Float32, score_threshold: Float32) -> IndexList[2]`

Compute the output shape for NMS without allocating the output buffer.

This function performs a dry-run of NMS to determine how many boxes will be
selected, allowing proper output buffer allocation. Can be removed once the
graph compiler supports value semantic kernels that allocate their own output.

**Args:**

* â€‹boxes ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Rank-3 tensor of bounding boxes with shape (batch, num\_boxes, 4).
* â€‹scores ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Rank-3 tensor of scores with shape (batch, num\_classes, num\_boxes).
* â€‹max\_output\_boxes\_per\_class ([`Int`](/mojo/std/builtin/int/Int)): Maximum number of boxes to select per class.
* â€‹iou\_threshold ([`Float32`](/mojo/std/builtin/simd/#float32)): IoU threshold for suppression.
* â€‹score\_threshold ([`Float32`](/mojo/std/builtin/simd/#float32)): Minimum score threshold.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): A 2-element IndexList specifying the output shape (num\_selected\_boxes, 3).

</section>

---

## block_reduce

<section class='mojo-docs'>

`block_reduce[dtype: DType, max_warps_per_block: Int](val: Scalar[dtype]) -> Scalar[dtype]`

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar)

</section>

---

## group_norm

<section class='mojo-docs'>

`group_norm[dtype: DType, rank: Int, input_fn: fn[width: Int, _rank: Int](IndexList[_rank]) capturing -> SIMD[dtype, width], gamma_fn: fn[width: Int](IndexList[1]) capturing -> SIMD[dtype, width], beta_fn: fn[width: Int](IndexList[1]) capturing -> SIMD[dtype, width], /, target: StringSlice[StaticConstantOrigin] = "gpu"](shape: IndexList[rank], epsilon: Scalar[dtype], groups: Int32, output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr)`

</section>

---

## group_norm_gpu

<section class='mojo-docs'>

`group_norm_gpu[dtype: DType, rank: Int, //, input_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], gamma_fn: fn[width: Int](IndexList[1]) capturing -> SIMD[dtype, width], beta_fn: fn[width: Int](IndexList[1]) capturing -> SIMD[dtype, width]](shape: IndexList[rank, element_type=element_type], epsilon: Scalar[dtype], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], num_groups: Int, ctx: DeviceContext)`

</section>

---

## group_norm_gpu_block

<section class='mojo-docs'>

`group_norm_gpu_block[mut: Bool, origin: Origin[mut=mut], layout: Layout, //, dtype: DType, simd_width: UInt, input_fn: fn[width: Int](row: Int, col: Int) capturing -> SIMD[dtype, width], gamma_fn: fn[width: Int](IndexList[1]) capturing -> SIMD[dtype, width], beta_fn: fn[width: Int](IndexList[1]) capturing -> SIMD[dtype, width]](output: LayoutTensor[dtype, layout, origin], epsilon: Scalar[dtype], num_groups: Int, channels_per_group: Int, spatial: Int)`

</section>

---

## group_norm_gpu_warp_tiling

<section class='mojo-docs'>

`group_norm_gpu_warp_tiling[mut: Bool, origin: Origin[mut=mut], layout: Layout, //, dtype: DType, simd_width: Int, input_fn: fn[width: Int](row: Int, col: Int) capturing -> SIMD[dtype, width], gamma_fn: fn[width: Int](IndexList[1]) capturing -> SIMD[dtype, width], beta_fn: fn[width: Int](IndexList[1]) capturing -> SIMD[dtype, width]](output: LayoutTensor[dtype, layout, origin], epsilon: Scalar[dtype], num_groups: Int, channels_per_group: Int, spatial: Int)`

</section>

---

## group_norm_reshape

<section class='mojo-docs'>

`group_norm_reshape[dtype: DType, rank: Int](shape: IndexList[rank, element_type=element_type], buf: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], channels_per_group: Int, spatial: Int) -> LayoutTensor[dtype, Layout.row_major[2](), origin, address_space=address_space]`

Reshapes an input buffer for group normalization by flattening all dimensions except the group dimension. Returns a 2D buffer of shape (num\_groups \* N, group\_size), where group\_size is the product of channels\_per\_group and spatial.

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

</section>

---

## group_norm_shape

<section class='mojo-docs'>

`group_norm_shape[dtype: DType, single_thread_blocking_override: Bool](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], gamma: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], beta: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon: Scalar[dtype], num_groups: Int32) -> IndexList[LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## normalization

<section class='mojo-docs'>

## Functions

* [â€‹`block_reduce`](./block_reduce):
* [â€‹`group_norm`](./group_norm):
* [â€‹`group_norm_gpu`](./group_norm_gpu):
* [â€‹`group_norm_gpu_block`](./group_norm_gpu_block):
* [â€‹`group_norm_gpu_warp_tiling`](./group_norm_gpu_warp_tiling):
* [â€‹`group_norm_reshape`](./group_norm_reshape): Reshapes an input buffer for group normalization by flattening all dimensions except the group dimension. Returns a 2D buffer of shape (num\_groups \* N, group\_size), where group\_size is the product of channels\_per\_group and spatial.
* [â€‹`group_norm_shape`](./group_norm_shape):
* [â€‹`layer_norm`](./layer_norm):
* [â€‹`layer_norm_cpu`](./layer_norm_cpu): Computes layernorm(elementwise\_fn(x)) across the last dimension of x, where layernorm is defined as $(x-mean(x))/(sqrt(var(x)+eps)*gamma_fn + beta$.
* [â€‹`layer_norm_gpu`](./layer_norm_gpu):
* [â€‹`layer_norm_gpu_block`](./layer_norm_gpu_block):
* [â€‹`layer_norm_gpu_warp_tiling`](./layer_norm_gpu_warp_tiling):
* [â€‹`layer_norm_reshape`](./layer_norm_reshape):
* [â€‹`layer_norm_shape`](./layer_norm_shape): Compute the output shape of a `layer_norm` operation.
* [â€‹`rms_norm`](./rms_norm):
* [â€‹`rms_norm_cpu`](./rms_norm_cpu):
* [â€‹`rms_norm_fused_residual_add`](./rms_norm_fused_residual_add):
* [â€‹`rms_norm_fused_residual_add_cpu`](./rms_norm_fused_residual_add_cpu):
* [â€‹`rms_norm_fused_residual_add_gpu`](./rms_norm_fused_residual_add_gpu):
* [â€‹`rms_norm_fused_residual_add_gpu_block`](./rms_norm_fused_residual_add_gpu_block):
* [â€‹`rms_norm_fused_residual_add_gpu_warp_tiling`](./rms_norm_fused_residual_add_gpu_warp_tiling):
* [â€‹`rms_norm_gpu`](./rms_norm_gpu):
* [â€‹`rms_norm_gpu_block`](./rms_norm_gpu_block):
* [â€‹`rms_norm_gpu_warp_tiling`](./rms_norm_gpu_warp_tiling):
* [â€‹`rms_norm_gpu_warp_tiling_128`](./rms_norm_gpu_warp_tiling_128):
* [â€‹`rms_norm_shape`](./rms_norm_shape):
* [â€‹`welford_block_all_reduce`](./welford_block_all_reduce):
* [â€‹`welford_combine`](./welford_combine):
* [â€‹`welford_update`](./welford_update):
* [â€‹`welford_warp_all_reduce`](./welford_warp_all_reduce):
* [â€‹`welford_warp_reduce`](./welford_warp_reduce):

</section>

---

## layer_norm

<section class='mojo-docs'>

`layer_norm[dtype: DType, rank: Int, input_0_fn: fn[_width: Int, _rank: Int](IndexList[_rank]) capturing -> SIMD[dtype, _width], input_1_fn: fn[_width: Int, _rank: Int](IndexList[_rank]) capturing -> SIMD[dtype, _width], output_0_fn: fn[width: Int, rank: Int, alignment: Int](idx: IndexList[rank], val: SIMD[dtype, width]) capturing -> None, /, target: StringSlice[StaticConstantOrigin] = "cpu"](shape: IndexList[rank], gamma_shape: IndexList[1], beta: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon: Scalar[dtype], ctx: DeviceContextPtr)`

</section>

---

## layer_norm_cpu

<section class='mojo-docs'>

`layer_norm_cpu[dtype: DType, //, input_fn: fn[width: Int](Int, Int) capturing -> SIMD[dtype, width], gamma_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], output_fn: fn[width: Int, alignment: Int](row: Int, col: Int, val: SIMD[dtype, width]) capturing -> None](num_rows: Int, num_cols: Int, beta: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon: Scalar[dtype])`

Computes layernorm(elementwise\_fn(x)) across the last dimension of x, where layernorm is defined as $(x-mean(x))/(sqrt(var(x)+eps)*gamma_fn + beta$.

Currently performs 3 passes over the input data. This can be reduced to 2 by
fusing the add, mean, and variance loops using Welford's algorithm.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The x and out buffers' elements dtype.
* â€‹input\_fn (`fn[width: Int](Int, Int) capturing -> SIMD[dtype, width]`): Function called to generate an input value.
* â€‹gamma\_fn (`fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width]`): Function called to generate a gamma value.
* â€‹output\_fn (`fn[width: Int, alignment: Int](row: Int, col: Int, val: SIMD[dtype, width]) capturing -> None`): Function called to store the output value.

**Args:**

* â€‹num\_rows ([`Int`](/mojo/std/builtin/int/Int)): The number of rows in the input tensor.
* â€‹num\_cols ([`Int`](/mojo/std/builtin/int/Int)): The number of columns in the input tensor.
* â€‹beta ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The beta value to use in the layernorm calculation.
* â€‹epsilon ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The eps value to use in the layernorm calculation.

`layer_norm_cpu[dtype: DType, rank: Int, //, input_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], gamma_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], output_fn: fn[width: Int, rank: Int, alignment: Int](idx: IndexList[rank], val: SIMD[dtype, width]) capturing -> None](shape: IndexList[rank], beta: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon: Scalar[dtype])`

</section>

---

## layer_norm_gpu

<section class='mojo-docs'>

`layer_norm_gpu[dtype: DType, rank: Int, //, input_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], gamma_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], output_fn: fn[width: Int, rank: Int, alignment: Int](idx: IndexList[rank], val: SIMD[dtype, width]) capturing -> None](shape: IndexList[rank, element_type=element_type], beta: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon: Scalar[dtype], *, ctx: DeviceContext)`

</section>

---

## layer_norm_gpu_block

<section class='mojo-docs'>

`layer_norm_gpu_block[mut: Bool, origin: Origin[mut=mut], layout: Layout, dtype: DType, //, simd_width: UInt, input_fn: fn[width: Int](row: Int, col: Int) capturing -> SIMD[dtype, width], gamma_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], output_fn: fn[width: Int, alignment: Int](row: Int, col: Int, val: SIMD[dtype, width]) capturing -> None](shape: IndexList[2], beta: LayoutTensor[dtype, layout, origin], epsilon: Scalar[dtype])`

</section>

---

## layer_norm_gpu_warp_tiling

<section class='mojo-docs'>

`layer_norm_gpu_warp_tiling[mut: Bool, origin: Origin[mut=mut], layout: Layout, dtype: DType, //, simd_width: UInt, input_fn: fn[width: Int](row: Int, col: Int) capturing -> SIMD[dtype, width], gamma_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], output_fn: fn[width: Int, alignment: Int](row: Int, col: Int, val: SIMD[dtype, width]) capturing -> None](shape: IndexList[2], beta: LayoutTensor[dtype, layout, origin], epsilon: Scalar[dtype])`

</section>

---

## layer_norm_reshape

<section class='mojo-docs'>

`layer_norm_reshape[rank: Int, //, output_rank: Int](shape: IndexList[rank, element_type=element_type]) -> IndexList[output_rank]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## layer_norm_shape

<section class='mojo-docs'>

`layer_norm_shape[dtype: DType, single_thread_blocking_override: Bool](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], gamma: LayoutTensor[dtype, Layout.row_major(1), origin], beta: LayoutTensor[dtype, Layout.row_major(1), origin], epsilon: Scalar[dtype]) -> IndexList[LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

Compute the output shape of a `layer_norm` operation.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input tensors.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹gamma ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The tensor for gamma coefficient.
* â€‹beta ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The tensor for beta coefficient.
* â€‹epsilon ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The tensor for epsilon coefficient.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## rms_norm

<section class='mojo-docs'>

`rms_norm[dtype: DType, rank: Int, input_0_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], output_0_fn: fn[width: Int, rank: Int, alignment: Int](idx: IndexList[rank], val: SIMD[dtype, width]) capturing -> None, /, target: StringSlice[StaticConstantOrigin] = "cpu", multiply_before_cast: Bool = True](shape: IndexList[rank], gamma: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon: Scalar[dtype], weight_offset: Scalar[dtype], ctx: DeviceContextPtr)`

</section>

---

## rms_norm_cpu

<section class='mojo-docs'>

`rms_norm_cpu[dtype: DType, //, input_fn: fn[width: Int](Int, Int) capturing -> SIMD[dtype, width], output_fn: fn[width: Int, alignment: Int](Int, Int, SIMD[dtype, width]) capturing -> None, multiply_before_cast: Bool](gamma: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon: Scalar[dtype], weight_offset: Scalar[dtype], out_shape: IndexList[2])`

`rms_norm_cpu[dtype: DType, rank: Int, //, input_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], output_fn: fn[width: Int, alignment: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None, multiply_before_cast: Bool](shape: IndexList[rank], gamma: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon: Scalar[dtype], weight_offset: Scalar[dtype])`

</section>

---

## rms_norm_fused_residual_add

<section class='mojo-docs'>

`rms_norm_fused_residual_add[dtype: DType, rank: Int, //, input_0_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], input_1_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], output_0_fn: fn[width: Int, rank: Int, alignment: Int](idx: IndexList[rank], val: SIMD[dtype, width]) capturing -> None, output_residual_fn: fn[width: Int, rank: Int, alignment: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None, /, target: StringSlice[StaticConstantOrigin] = "cpu", multiply_before_cast: Bool = True](shape: IndexList[rank], gamma1: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon1: Scalar[dtype], weight_offset1: Scalar[dtype], gamma2: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon2: Scalar[dtype], weight_offset2: Scalar[dtype], ctx: DeviceContextPtr)`

</section>

---

## rms_norm_fused_residual_add_cpu

<section class='mojo-docs'>

`rms_norm_fused_residual_add_cpu[dtype: DType, rank: Int, //, input_0_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], residual_input_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], output_0_fn: fn[width: Int, alignment: Int](idx: IndexList[rank], val: SIMD[dtype, width]) capturing -> None, output_residual_fn: fn[width: Int, alignment: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None, /, multiply_before_cast: Bool = True](shape: IndexList[rank], gamma1: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon1: Scalar[dtype], weight_offset1: Scalar[dtype], gamma2: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon2: Scalar[dtype], weight_offset2: Scalar[dtype])`

</section>

---

## rms_norm_fused_residual_add_gpu

<section class='mojo-docs'>

`rms_norm_fused_residual_add_gpu[dtype: DType, rank: Int, //, input_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], residual_input_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], output_residual_fn: fn[width: Int, alignment: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None, output_fn: fn[width: Int, alignment: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None, multiply_before_cast: Bool](shape: IndexList[rank, element_type=element_type], gamma1: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon1: Scalar[dtype], weight_offset1: Scalar[dtype], gamma2: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon2: Scalar[dtype], weight_offset2: Scalar[dtype], ctx: DeviceContext)`

</section>

---

## rms_norm_fused_residual_add_gpu_block

<section class='mojo-docs'>

`rms_norm_fused_residual_add_gpu_block[mut1: Bool, origin1: Origin[mut=mut1], layout1: Layout, mut2: Bool, origin2: Origin[mut=mut2], layout2: Layout, dtype: DType, //, simd_width: Int, max_warps_per_block: Int, input_fn: fn[width: Int](row: Int, col: Int) capturing -> SIMD[dtype, width], residual_input_fn: fn[width: Int](row: Int, col: Int) capturing -> SIMD[dtype, width], output_fn: fn[width: Int, alignment: Int](row: Int, col: Int, val: SIMD[dtype, width]) capturing -> None, output_residual_fn: fn[width: Int, alignment: Int](row: Int, col: Int, val: SIMD[dtype, width]) capturing -> None, multiply_before_cast: Bool](gamma1: LayoutTensor[dtype, layout1, origin1], epsilon1: Scalar[dtype], weight_offset1: Scalar[dtype], gamma2: LayoutTensor[dtype, layout2, origin2], epsilon2: Scalar[dtype], weight_offset2: Scalar[dtype], num_cols: Int)`

</section>

---

## rms_norm_fused_residual_add_gpu_warp_tiling

<section class='mojo-docs'>

`rms_norm_fused_residual_add_gpu_warp_tiling[mut1: Bool, origin1: Origin[mut=mut1], layout1: Layout, mut2: Bool, origin2: Origin[mut=mut2], layout2: Layout, dtype: DType, //, simd_width: Int, max_warps_per_block: Int, input_fn: fn[width: Int](row: Int, col: Int) capturing -> SIMD[dtype, width], residual_input_fn: fn[width: Int](row: Int, col: Int) capturing -> SIMD[dtype, width], output_fn: fn[width: Int, alignment: Int](row: Int, col: Int, val: SIMD[dtype, width]) capturing -> None, output_residual_fn: fn[width: Int, alignment: Int](row: Int, col: Int, val: SIMD[dtype, width]) capturing -> None, multiply_before_cast: Bool](gamma1: LayoutTensor[dtype, layout1, origin1], epsilon1: Scalar[dtype], weight_offset1: Scalar[dtype], gamma2: LayoutTensor[dtype, layout2, origin2], epsilon2: Scalar[dtype], weight_offset2: Scalar[dtype], num_cols: Int)`

</section>

---

## rms_norm_gpu

<section class='mojo-docs'>

`rms_norm_gpu[dtype: DType, rank: Int, //, input_fn: fn[width: Int, rank: Int](IndexList[rank]) capturing -> SIMD[dtype, width], output_fn: fn[width: Int, alignment: Int](IndexList[rank], SIMD[dtype, width]) capturing -> None, multiply_before_cast: Bool](shape: IndexList[rank, element_type=element_type], gamma: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon: Scalar[dtype], weight_offset: Scalar[dtype], ctx: DeviceContext)`

</section>

---

## rms_norm_gpu_block

<section class='mojo-docs'>

`rms_norm_gpu_block[mut: Bool, origin: Origin[mut=mut], layout: Layout, dtype: DType, //, simd_width: Int, max_warps_per_block: Int, input_fn: fn[width: Int](row: Int, col: Int) capturing -> SIMD[dtype, width], output_fn: fn[width: Int, alignment: Int](row: Int, col: Int, val: SIMD[dtype, width]) capturing -> None, multiply_before_cast: Bool](gamma: LayoutTensor[dtype, layout, origin], epsilon: Scalar[dtype], weight_offset: Scalar[dtype], num_cols: Int)`

</section>

---

## rms_norm_gpu_warp_tiling

<section class='mojo-docs'>

`rms_norm_gpu_warp_tiling[mut: Bool, origin: Origin[mut=mut], layout: Layout, dtype: DType, //, simd_width: Int, max_warps_per_block: Int, input_fn: fn[width: Int](row: Int, col: Int) capturing -> SIMD[dtype, width], output_fn: fn[width: Int, alignment: Int](row: Int, col: Int, val: SIMD[dtype, width]) capturing -> None, multiply_before_cast: Bool](gamma: LayoutTensor[dtype, layout, origin], epsilon: Scalar[dtype], weight_offset: Scalar[dtype], num_cols: Int)`

</section>

---

## rms_norm_gpu_warp_tiling_128

<section class='mojo-docs'>

`rms_norm_gpu_warp_tiling_128[mut: Bool, origin: Origin[mut=mut], layout: Layout, dtype: DType, //, simd_width: Int, warps_per_block: Int, input_fn: fn[width: Int](row: Int, col: Int) capturing -> SIMD[dtype, width], output_fn: fn[width: Int, alignment: Int](row: Int, col: Int, val: SIMD[dtype, width]) capturing -> None, multiply_before_cast: Bool](gamma: LayoutTensor[dtype, layout, origin], epsilon: Scalar[dtype], weight_offset: Scalar[dtype], num_rows: Int, num_cols: Int)`

</section>

---

## rms_norm_shape

<section class='mojo-docs'>

`rms_norm_shape[dtype: DType, single_thread_blocking_override: Bool](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], gamma: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], epsilon: Scalar[dtype], weight_offset: Scalar[dtype]) -> IndexList[LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## welford_block_all_reduce

<section class='mojo-docs'>

`welford_block_all_reduce[dtype: DType, //](thread_mean: Scalar[dtype], thread_m2: Scalar[dtype], thread_count: Scalar[dtype], mut res_mean: Scalar[dtype], mut res_m2: Scalar[dtype], mut res_count: Scalar[dtype])`

</section>

---

## welford_combine

<section class='mojo-docs'>

`welford_combine[dtype: DType, //](mean: Scalar[dtype], m2: Scalar[dtype], count: Scalar[dtype], mut res_mean: Scalar[dtype], mut res_m2: Scalar[dtype], mut res_count: Scalar[dtype])`

</section>

---

## welford_update

<section class='mojo-docs'>

`welford_update[dtype: DType, //](val: Scalar[dtype], mut mean: Scalar[dtype], mut m2: Scalar[dtype], mut count: Scalar[dtype])`

</section>

---

## welford_warp_all_reduce

<section class='mojo-docs'>

`welford_warp_all_reduce[dtype: DType, //](thread_mean: Scalar[dtype], thread_m2: Scalar[dtype], thread_count: Scalar[dtype], mut res_mean: Scalar[dtype], mut res_m2: Scalar[dtype], mut res_count: Scalar[dtype])`

</section>

---

## welford_warp_reduce

<section class='mojo-docs'>

`welford_warp_reduce[dtype: DType, //](thread_mean: Scalar[dtype], thread_m2: Scalar[dtype], thread_count: Scalar[dtype], mut res_mean: Scalar[dtype], mut res_m2: Scalar[dtype], mut res_count: Scalar[dtype])`

</section>

---

## pad (Pad)

<section class='mojo-docs'>

## Functions

* [â€‹`pad_constant`](./pad_constant): Fill `output` with values from `input`, and edges padded with `constant` based on `paddings`.
* [â€‹`pad_reflect`](./pad_reflect): Fill `output` with values from `input`, and edges padded with reflected values from the unpadded region.
* [â€‹`pad_repeat`](./pad_repeat): Fill `output` with values from `input`, and edges padded boundary values from the unpadded region.
* [â€‹`pad_shape`](./pad_shape): Compute the output shape of a `pad` operation, and assert the inputs are compatible.

</section>

---

## pad_constant

<section class='mojo-docs'>

`pad_constant[output_layout: Layout, input_layout: Layout, dtype: DType, paddings_type: DType, constant_type: DType](output: LayoutTensor[dtype, output_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, input_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], paddings: LegacyUnsafePointer[Scalar[paddings_type]], constant: Scalar[constant_type])`

Fill `output` with values from `input`, and edges padded with `constant` based on `paddings`.

Example:
var input\_shape = (X, Y, Z)
var paddings = [x0, x1, y0, y1, z0, z1]

out\[x, y, z] =
input\[x - x0, y - y0, z - z0] if x âˆˆ \[x0, x0 + X] &&
y âˆˆ \[y0, y0 + Y] &&
z âˆˆ \[z0, z0 + Z]
else constant

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output buffer.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input buffer.
* â€‹paddings (`LegacyUnsafePointer`): Ordered (before, after) padding sizes for each axis.
* â€‹constant ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The constant to pad output with.

</section>

---

## pad_reflect

<section class='mojo-docs'>

`pad_reflect[output_layout: Layout, input_layout: Layout, dtype: DType, paddings_type: DType](output: LayoutTensor[dtype, output_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, input_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], paddings: LegacyUnsafePointer[Scalar[paddings_type]])`

Fill `output` with values from `input`, and edges padded with reflected values from the unpadded region.

Example:
var input = [\[1, 2],
\[3, 4]]
var paddings = [2, 2, 1, 0]

Yields:
output = [\[2, 1, 2],
\[4, 3, 4],
\[2, 1, 2],
\[4, 3, 4],
\[2, 1, 2],
\[4, 3, 4]]

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output buffer.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input buffer.
* â€‹paddings (`LegacyUnsafePointer`): Ordered (before, after) padding sizes for each axis.

</section>

---

## pad_repeat

<section class='mojo-docs'>

`pad_repeat[output_layout: Layout, input_layout: Layout, dtype: DType, paddings_type: DType](output: LayoutTensor[dtype, output_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, input_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], paddings: LegacyUnsafePointer[Scalar[paddings_type]])`

Fill `output` with values from `input`, and edges padded boundary values from the unpadded region.

Example:
var input = [\[1, 2],
\[3, 4]]
var paddings = [2, 2, 1, 0]

Yields:
output = [\[1, 1, 2],
\[1, 1, 2],
\[1, 1, 2],
\[3, 3, 4],
\[3, 3, 4],
\[3, 3, 4]]

**Parameters:**

* â€‹output\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout of the output buffer.
* â€‹input\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): Layout of the input buffer.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType of the input/output buffer.
* â€‹paddings\_type ([`DType`](/mojo/std/builtin/dtype/DType)): DType of the input, output, and padding buffers.

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output buffer.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input buffer.
* â€‹paddings (`LegacyUnsafePointer`): Ordered (before, after) padding sizes for each axis.

</section>

---

## pad_shape

<section class='mojo-docs'>

`pad_shape[input_type: DType, paddings_type: DType, single_thread_blocking_override: Bool](input_buf: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], paddings_buf: LayoutTensor[paddings_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> IndexList[LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

Compute the output shape of a `pad` operation, and assert the inputs are compatible.

**Parameters:**

* â€‹input\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input tensor.
* â€‹paddings\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the padding tensor.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.

**Args:**

* â€‹input\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The tensor to pad.
* â€‹paddings\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The paddings tensor, of shape (input\_rank, 2).

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## get_padding_output_shape

<section class='mojo-docs'>

`get_padding_output_shape[rank: Int](input_shape: IndexList[rank], paddings: LayoutTensor[DType.index, Layout(IntTuple((2 * rank))), origin]) -> IndexList[rank]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## get_row_offset

<section class='mojo-docs'>

`get_row_offset[dtype: DType, tensor_layout: Layout](input_tensor: LayoutTensor[dtype, tensor_layout, MutAnyOrigin], output_tensor: LayoutTensor[dtype, tensor_layout, MutAnyOrigin], row_length: Int, row: Int) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## pad_gpu

<section class='mojo-docs'>

## Functions

* [â€‹`get_padding_output_shape`](./get_padding_output_shape):
* [â€‹`get_row_offset`](./get_row_offset):
* [â€‹`pad_constant`](./pad_constant): Fill `output` with values from `input`, and edges padded with `constant` based on `paddings`.
* [â€‹`padded_copy_kernel`](./padded_copy_kernel):
* [â€‹`scalar_copy_row`](./scalar_copy_row):
* [â€‹`vector_copy_row`](./vector_copy_row):

</section>

---

## pad_constant (Pad_gpu)

<section class='mojo-docs'>

`pad_constant[rank: Int, dtype: DType, padding_type: DType](output: LegacyUnsafePointer[Scalar[dtype]], output_shape: IndexList[rank], input: LegacyUnsafePointer[Scalar[dtype]], input_shape: IndexList[rank], paddings: LegacyUnsafePointer[Scalar[padding_type]], constant: Scalar[dtype], ctx: DeviceContext)`

Fill `output` with values from `input`, and edges padded with `constant` based on `paddings`.

Example:

```mojo
var input_shape = (X, Y, Z)
var paddings = [x0, x1, y0, y1, z0, z1]

out[x, y, z] =
  input[x - x0, y - y0, z - z0] if x âˆˆ [x0, x0 + X] &&
                                   y âˆˆ [y0, y0 + Y] &&
                                   z âˆˆ [z0, z0 + Z]
  else constant
```

**Args:**

* â€‹output (`LegacyUnsafePointer`): The output buffer.
* â€‹output\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The output shape.
* â€‹input (`LegacyUnsafePointer`): The input buffer.
* â€‹input\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The input shape.
* â€‹paddings (`LegacyUnsafePointer`): Ordered (before, after) padding sizes for each axis.
* â€‹constant ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The constant to pad output with.
* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): Device context for participating GPU.

</section>

---

## padded_copy_kernel

<section class='mojo-docs'>

`padded_copy_kernel[dtype: DType, tensor_layout: Layout, simd_width: Int](input_tensor: LayoutTensor[dtype, tensor_layout, MutAnyOrigin], output_tensor: LayoutTensor[dtype, tensor_layout, MutAnyOrigin], rows_per_sm: Int, total_rows: Int, row_length: Int, scaled_row_length: Int)`

</section>

---

## scalar_copy_row

<section class='mojo-docs'>

`scalar_copy_row[dtype: DType](input_ptr: LegacyUnsafePointer[Scalar[dtype]], output_ptr: LegacyUnsafePointer[Scalar[dtype]], row_length: Int, threads_per_row: Int)`

</section>

---

## vector_copy_row

<section class='mojo-docs'>

`vector_copy_row[dtype: DType, simd_width: Int](input_ptr: LegacyUnsafePointer[Scalar[dtype]], output_ptr: LegacyUnsafePointer[Scalar[dtype]], scaled_row_length: Int, row_length: Int, threads_per_row: Int)`

</section>

---

## PoolMethod

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct PoolMethod`

## Fields

* â€‹value (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `AVG`

`comptime AVG = PoolMethod(1)`

### `MAX`

`comptime MAX = PoolMethod(0)`

## Methods

### `__eq__`

`__eq__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `__ne__`

`__ne__(self, rhs: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## avg_pool

<section class='mojo-docs'>

`avg_pool[dtype: DType, int_type: DType, count_boundary: Bool = False, target: StringSlice[StaticConstantOrigin] = "cpu"](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], strides: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dilations: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], paddings: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ceil_mode: Bool = False, ctx_ptr: DeviceContextPtr = DeviceContextPtr())`

</section>

---

## avg_pool_cpu

<section class='mojo-docs'>

`avg_pool_cpu[dtype: DType, int_type: DType, rank: Int = 4, count_boundary: Bool = False](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], strides: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dilations: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], paddings: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ceil_mode: Bool = False)`

Computes the average pool.

Params:
count\_boundary: Whether to count the boundary in the average computation.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Batched image input to the pool2d operator.
* â€‹filter ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Filter size on height and width dimensions with assumed tuple
  def (filter\_h, filter\_w).
* â€‹strides ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Strides on height and width dimensions with assumed
  tuple def (stride\_h, stride\_w).
* â€‹dilations ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Dilations on height and width dimensions with assumed
  tuple def (dilation\_h, dilation\_w).
* â€‹paddings ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Paddings on height and width dimensions with assumed
  tuple def (pad\_h\_before, pad\_h\_after, pad\_w\_before, pad\_w\_after)).
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Pre-allocated output tensor space.
* â€‹ceil\_mode ([`Bool`](/mojo/std/builtin/bool/Bool)): Ceiling mode defines the output shape and implicit padding.

</section>

---

## avg_pool_gpu

<section class='mojo-docs'>

`avg_pool_gpu[dtype: DType, int_type: DType, count_boundary: Bool = False](ctx: DeviceContext, input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], strides: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dilations: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], paddings: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ceil_mode: Bool = False)`

Computes the average pool on GPU.

Params:
count\_boundary: Whether to count the boundary in the average computation.

**Args:**

* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): The DeviceContext to use for GPU execution.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): (On device) Batched image input to the pool2d operator.
* â€‹filter ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): (On host) Filter size on height and width dimensions with assumed tuple
  def (filter\_h, filter\_w).
* â€‹strides ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): (On host) Strides on height and width dimensions with assumed
  tuple def (stride\_h, stride\_w).
* â€‹dilations ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): (On host) Dilations on height and width dimensions with assumed
  tuple def (dilation\_h, dilation\_w).
* â€‹paddings ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): (On host) Paddings on height and width dimensions with assumed
  tuple def (pad\_h\_before, pad\_h\_after, pad\_w\_before, pad\_w\_after)).
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): (On device) Pre-allocated output tensor space.
* â€‹ceil\_mode ([`Bool`](/mojo/std/builtin/bool/Bool)): Ceiling mode defines the output shape and implicit padding.

</section>

---

## pool

<section class='mojo-docs'>

## Structs

* [â€‹`PoolMethod`](./PoolMethod):

## Functions

* [â€‹`avg_pool`](./avg_pool):
* [â€‹`avg_pool_cpu`](./avg_pool_cpu): Computes the average pool.
* [â€‹`avg_pool_gpu`](./avg_pool_gpu): Computes the average pool on GPU.
* [â€‹`max_pool`](./max_pool):
* [â€‹`max_pool_cpu`](./max_pool_cpu): Computes fp32 pooling.
* [â€‹`max_pool_gpu`](./max_pool_gpu): Computes max pooling on GPU.
* [â€‹`pool_shape`](./pool_shape):
* [â€‹`pool_shape_ceil`](./pool_shape_ceil):
* [â€‹`pool_shape_impl`](./pool_shape_impl): Compute the output shape of a pooling operation, and assert the inputs are compatible. Works for 2D pool operations only in the NHWC format.

</section>

---

## max_pool

<section class='mojo-docs'>

`max_pool[dtype: DType, int_type: DType, target: StringSlice[StaticConstantOrigin] = "cpu"](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], strides: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dilations: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], paddings: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ceil_mode: Bool = False, ctx_ptr: DeviceContextPtr = DeviceContextPtr())`

</section>

---

## max_pool_cpu

<section class='mojo-docs'>

`max_pool_cpu[dtype: DType, int_type: DType](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], strides: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dilations: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], paddings: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ceil_mode: Bool = False)`

Computes fp32 pooling.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Batched image input to the pool2d operator.
* â€‹filter ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Filter size on height and width dimensions with assumed tuple
  def (filter\_h, filter\_w).
* â€‹strides ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Strides on height and width dimensions with assumed
  tuple def (stride\_h, stride\_w).
* â€‹dilations ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Dilations on height and width dimensions with assumed
  tuple def (dilation\_h, dilation\_w).
* â€‹paddings ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Paddings on height and width dimensions with assumed
  tuple def (pad\_h\_before, pad\_h\_after, pad\_w\_before, pad\_w\_after)).
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Pre-allocated output tensor space.
* â€‹ceil\_mode ([`Bool`](/mojo/std/builtin/bool/Bool)): Ceiling mode defines the output shape and implicit padding.

</section>

---

## max_pool_gpu

<section class='mojo-docs'>

`max_pool_gpu[dtype: DType, int_type: DType](ctx: DeviceContext, input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], strides: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dilations: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], paddings: LayoutTensor[int_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ceil_mode: Bool = False)`

Computes max pooling on GPU.

**Args:**

* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): The DeviceContext to use for GPU execution.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): (On device) Batched image input to the pool2d operator.
* â€‹filter ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): (On host) Filter size on height and width dimensions with assumed tuple
  def (filter\_h, filter\_w).
* â€‹strides ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): (On host) Strides on height and width dimensions with assumed
  tuple def (stride\_h, stride\_w).
* â€‹dilations ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): (On host) Dilations on height and width dimensions with assumed
  tuple def (dilation\_h, dilation\_w).
* â€‹paddings ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): (On host) Paddings on height and width dimensions with assumed
  tuple def (pad\_h\_before, pad\_h\_after, pad\_w\_before, pad\_w\_after)).
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): (On device) Pre-allocated output tensor space.
* â€‹ceil\_mode ([`Bool`](/mojo/std/builtin/bool/Bool)): Ceiling mode defines the output shape and implicit padding.

</section>

---

## pool_shape

<section class='mojo-docs'>

`pool_shape[input_type: DType, filter_type: DType, strides_type: DType, dilations_type: DType, paddings_type: DType, single_thread_blocking_override: Bool](input_buf: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter_buf: LayoutTensor[filter_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], strides_buf: LayoutTensor[strides_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dilations_buf: LayoutTensor[dilations_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], paddings_buf: LayoutTensor[paddings_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> IndexList[LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## pool_shape_ceil

<section class='mojo-docs'>

`pool_shape_ceil[input_type: DType, filter_type: DType, strides_type: DType, dilations_type: DType, paddings_type: DType, single_thread_blocking_override: Bool](input_buf: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter_buf: LayoutTensor[filter_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], strides_buf: LayoutTensor[strides_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dilations_buf: LayoutTensor[dilations_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], paddings_buf: LayoutTensor[paddings_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> IndexList[LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## pool_shape_impl

<section class='mojo-docs'>

`pool_shape_impl[input_type: DType, filter_type: DType, strides_type: DType, dilations_type: DType, paddings_type: DType, single_thread_blocking_override: Bool, ceil_mode: Bool](input_buf: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], filter_buf: LayoutTensor[filter_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], strides_buf: LayoutTensor[strides_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], dilations_buf: LayoutTensor[dilations_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], paddings_buf: LayoutTensor[paddings_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> IndexList[LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

Compute the output shape of a pooling operation, and assert the inputs are compatible. Works for 2D pool operations only in the NHWC format.

**Parameters:**

* â€‹input\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input tensor.
* â€‹filter\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the filter tensor.
* â€‹strides\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the strides tensor.
* â€‹dilations\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the dilations tensor.
* â€‹paddings\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the paddings tensor.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.
* â€‹ceil\_mode ([`Bool`](/mojo/std/builtin/bool/Bool)): Define rounding mode for shape calculation.

**Args:**

* â€‹input\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹filter\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The filter size buffer.
* â€‹strides\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The strides size buffer.
* â€‹dilations\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The dilations size buffer.
* â€‹paddings\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The paddings size buffer.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## rand_normal

<section class='mojo-docs'>

## Functions

* [â€‹`random_normal`](./random_normal): Call `output_fn` with values generated from a normal distribution with the specified mean and standard deviation.

</section>

---

## random_normal

<section class='mojo-docs'>

`random_normal[dtype: DType, rank: Int, //, output_fn: fn[width: Int, _rank: Int](idx: IndexList[_rank], val: SIMD[dtype, width]) capturing -> None, target: StringSlice[StaticConstantOrigin]](shape: IndexList[rank], mean: Float32, stddev: Float32, seed_value: UInt64, ctx: DeviceContextPtr)`

Call `output_fn` with values generated from a normal distribution with the specified mean and standard deviation.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type to generate.
* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the underlying buffer.
* â€‹output\_fn (`fn[width: Int, _rank: Int](idx: IndexList[_rank], val: SIMD[dtype, width]) capturing -> None`): The function which stores the generated values.
* â€‹target (`StringSlice`): The target to run on.

**Args:**

* â€‹shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The shape of the output being stored into by output\_fn.
* â€‹mean ([`Float32`](/mojo/std/builtin/simd/#float32)): The mean of the normal distribution.
* â€‹stddev ([`Float32`](/mojo/std/builtin/simd/#float32)): The standard deviation of the normal distribution.
* â€‹seed\_value ([`UInt64`](/mojo/std/builtin/simd/#uint64)): Seed value used to initialize the random number generator.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The device context.

</section>

---

## rand_uniform

<section class='mojo-docs'>

## Functions

* [â€‹`random_uniform`](./random_uniform): Call `output_fn` with values generated from a uniform distribution on \[lower\_bound, upper\_bound] for floating-point types or \[lower\_bound, upper\_bound) for integer types.

</section>

---

## random_uniform

<section class='mojo-docs'>

`random_uniform[dtype: DType, rank: Int, //, output_fn: fn[width: Int, _rank: Int](idx: IndexList[_rank], val: SIMD[dtype, width]) capturing -> None, target: StringSlice[StaticConstantOrigin]](shape: IndexList[rank], lower_bound: Scalar[dtype], upper_bound: Scalar[dtype], seed_value: UInt64, ctx: DeviceContextPtr)`

Call `output_fn` with values generated from a uniform distribution on \[lower\_bound, upper\_bound] for floating-point types or \[lower\_bound, upper\_bound) for integer types.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type to generate.
* â€‹rank ([`Int`](/mojo/std/builtin/int/Int)): The rank of the underlying buffer.
* â€‹output\_fn (`fn[width: Int, _rank: Int](idx: IndexList[_rank], val: SIMD[dtype, width]) capturing -> None`): The function which stores the generated values.
* â€‹target (`StringSlice`): The target to run on.

**Args:**

* â€‹shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The shape of the output being stored into by output\_fn.
* â€‹lower\_bound ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The lower bound on the uniform range.
* â€‹upper\_bound ([`Scalar`](/mojo/std/builtin/simd/#scalar)): The upper bound on the uniform range.
* â€‹seed\_value ([`UInt64`](/mojo/std/builtin/simd/#uint64)): Seed value used to initialize the random number generator.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The device context.

</section>

---

## randn

<section class='mojo-docs'>

## Functions

* [â€‹`random_normal`](./random_normal): Fill `output` with values generated from Normal(mean, variance) distribution.

</section>

---

## random_normal (Randn)

<section class='mojo-docs'>

`random_normal[dtype: DType, mean: Float64, variance: Float64](output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Fill `output` with values generated from Normal(mean, variance) distribution.

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output buffer.

</section>

---

## repeat_interleave

<section class='mojo-docs'>

## Functions

* [â€‹`repeat_interleave`](./repeat_interleave): Fill `output` by repeating values from `input` along `axis` based on the values in `repeats` buffer.
* [â€‹`repeat_interleave_shape`](./repeat_interleave_shape):

</section>

---

## repeat_interleave (Repeat_interleave)

<section class='mojo-docs'>

`repeat_interleave[dtype: DType, type_repeats: DType](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], repeats: LayoutTensor[type_repeats, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], axis: Int, output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Fill `output` by repeating values from `input` along `axis` based on the values in `repeats` buffer.

This is intended to implement the same functionality as torch.repeat:
<https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html>

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input buffer.
* â€‹repeats ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The number of repetitions each element in input.
* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis along which to repeat values.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output buffer.

</section>

---

## repeat_interleave_shape

<section class='mojo-docs'>

`repeat_interleave_shape[type_repeats: DType](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], repeats: LayoutTensor[type_repeats, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], axis: Int) -> IndexList[LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## reshape

<section class='mojo-docs'>

## Functions

* [â€‹`layout_tensor_reshape`](./layout_tensor_reshape):
* [â€‹`reshape`](./reshape):
* [â€‹`reshape_shape`](./reshape_shape):

</section>

---

## layout_tensor_reshape

<section class='mojo-docs'>

`layout_tensor_reshape[output_rank: Int, dtype: DType, single_thread_blocking_override: Bool](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], new_shape: IndexList[output_rank]) -> LayoutTensor[dtype, Layout.row_major[output_rank](), origin, address_space=address_space]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

</section>

---

## reshape (Reshape)

<section class='mojo-docs'>

`reshape[dtype: DType, //, output_rank: Int, single_thread_blocking_override: Bool = True](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], new_shape: IndexList[output_rank]) -> LayoutTensor[dtype, Layout.row_major[output_rank](), origin, address_space=address_space]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

</section>

---

## reshape_shape

<section class='mojo-docs'>

`reshape_shape[output_rank: Int, input_type: DType, target_shape_type: DType, single_thread_blocking_override: Bool](input_buf: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], target_shape_buf: LayoutTensor[target_shape_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> IndexList[output_rank]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## CoordinateTransformationMode

<section class='mojo-docs'>

`struct CoordinateTransformationMode`

## Fields

* â€‹value (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `AlignCorners`

`comptime AlignCorners = CoordinateTransformationMode(1)`

### `Asymmetric`

`comptime Asymmetric = CoordinateTransformationMode(2)`

### `HalfPixel`

`comptime HalfPixel = CoordinateTransformationMode(0)`

### `HalfPixel1D`

`comptime HalfPixel1D = CoordinateTransformationMode(3)`

## Methods

### `__init__`

`__init__(out self, value: Int)`

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## InterpolationMode

<section class='mojo-docs'>

`struct InterpolationMode`

## Fields

* â€‹value (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `Linear`

`comptime Linear = InterpolationMode(0)`

## Methods

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## Interpolator

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct Interpolator[mode: InterpolationMode]`

## Fields

* â€‹cubic\_coeff (`Float32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(cubic_coeff: Float32) -> Self`

`__init__() -> Self`

### `filter_length`

`static filter_length() -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `filter`

`filter(self, x: Float32) -> Float32`

**Returns:**

[`Float32`](/mojo/std/builtin/simd/#float32)

</section>

---

## RoundMode

<section class='mojo-docs'>

`struct RoundMode`

## Fields

* â€‹value (`Int`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `Ceil`

`comptime Ceil = RoundMode(3)`

### `Floor`

`comptime Floor = RoundMode(2)`

### `HalfDown`

`comptime HalfDown = RoundMode(0)`

### `HalfUp`

`comptime HalfUp = RoundMode(1)`

## Methods

### `__init__`

`__init__(out self, value: Int)`

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

</section>

---

## coord_transform

<section class='mojo-docs'>

`coord_transform[mode: CoordinateTransformationMode](out_coord: Int, in_dim: Int, out_dim: Int, scale: Float32) -> Float32`

**Returns:**

[`Float32`](/mojo/std/builtin/simd/#float32)

</section>

---

## resize

<section class='mojo-docs'>

## Structs

* [â€‹`CoordinateTransformationMode`](./CoordinateTransformationMode):
* [â€‹`InterpolationMode`](./InterpolationMode):
* [â€‹`Interpolator`](./Interpolator):
* [â€‹`RoundMode`](./RoundMode):

## Functions

* [â€‹`coord_transform`](./coord_transform):
* [â€‹`interpolate_point_1d`](./interpolate_point_1d):
* [â€‹`linear_filter`](./linear_filter): This is a tent filter.
* [â€‹`resize_linear`](./resize_linear): Resizes input to output shape using linear interpolation.
* [â€‹`resize_nearest_neighbor`](./resize_nearest_neighbor):

</section>

---

## interpolate_point_1d

<section class='mojo-docs'>

`interpolate_point_1d[in_layout: Layout, //, coordinate_transformation_mode: CoordinateTransformationMode, antialias: Bool, dtype: DType, interpolation_mode: InterpolationMode](interpolator: Interpolator[interpolation_mode], dim: Int, out_coords: IndexList[in_layout.rank()], scale: Float32, input: LayoutTensor[dtype, in_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## linear_filter

<section class='mojo-docs'>

`linear_filter(x: Float32) -> Float32`

This is a tent filter.

f(x) = 1 + x, x < 0
f(x) = 1 - x, 0 <= x < 1
f(x) = 0, x >= 1

**Returns:**

[`Float32`](/mojo/std/builtin/simd/#float32)

</section>

---

## resize_linear

<section class='mojo-docs'>

`resize_linear[coordinate_transformation_mode: CoordinateTransformationMode, antialias: Bool, dtype: DType](input: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Resizes input to output shape using linear interpolation.

Does not use anti-aliasing filter for downsampling (coming soon).

**Parameters:**

* â€‹coordinate\_transformation\_mode ([`CoordinateTransformationMode`](/mojo/kernels/nn/resize/CoordinateTransformationMode)): How to map a coordinate in output to a coordinate in input.
* â€‹antialias ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether or not to use an antialiasing linear/cubic filter, which when downsampling, uses
  more points to avoid aliasing artifacts. Effectively stretches the filter by a factor of 1 / scale.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Type of input and output.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input to be resized.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output containing the resized input.

</section>

---

## resize_nearest_neighbor

<section class='mojo-docs'>

`resize_nearest_neighbor[coordinate_transformation_mode: CoordinateTransformationMode, round_mode: RoundMode, dtype: DType](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## Weighted2DPoint

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct Weighted2DPoint[dtype: DType]`

Utility class to wrap 2-d point coordinates and floating point weight for bilinear interpolation.

## Fields

* â€‹y (`Int`):
* â€‹x (`Int`):
* â€‹w (`Scalar[dtype]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(y: Int, x: Int, weight: Scalar[dtype]) -> Self`

</section>

---

## roi_align

<section class='mojo-docs'>

## Structs

* [â€‹`Weighted2DPoint`](./Weighted2DPoint): Utility class to wrap 2-d point coordinates and floating point weight for bilinear interpolation.

## Functions

* [â€‹`roi_align_nhwc`](./roi_align_nhwc): Compute ROIAlign a batch of rois of shape \[M, 5] where the first dim is the batch index, followed by region box coordinates (y0, x0) (y1, x1). For inputs of NHWC format. The output shape is \[M, output\_height, output\_width, C].

</section>

---

## roi_align_nhwc

<section class='mojo-docs'>

`roi_align_nhwc[dtype: DType, output_layout: Layout, input_layout: Layout, roi_layout: Layout, //, aligned: Bool, mode: StringSlice[StaticConstantOrigin] = "AVG"](output: LayoutTensor[dtype, output_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, input_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], rois: LayoutTensor[dtype, roi_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output_height: Int, output_width: Int, in_spatial_scale: Scalar[dtype], in_sampling_ratio: Scalar[dtype])`

Compute ROIAlign a batch of rois of shape \[M, 5] where the first dim is the batch index, followed by region box coordinates (y0, x0) (y1, x1). For inputs of NHWC format. The output shape is \[M, output\_height, output\_width, C].

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input tensor.
* â€‹output\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The output layout.
* â€‹input\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The input layout.
* â€‹roi\_layout ([`Layout`](/mojo/kernels/layout/layout/Layout)): The layout of the regions of interests (ROI).
* â€‹aligned ([`Bool`](/mojo/std/builtin/bool/Bool)): If not true offset the ROIs by 0.5.
* â€‹mode (`StringSlice`): The pooling mode "AVG" for average and "MAX" for max pooling.

</section>

---

## apply_rope

<section class='mojo-docs'>

`apply_rope[dtype: DType, freq_dtype: DType, x_layout: Layout, rank: Int, width: Int, //, *, interleaved: Bool, alignment: Int, output_fn: fn[width: Int, alignment: Int](idx: IndexList[rank], val: SIMD[dtype, width]) capturing -> None](x: LayoutTensor[dtype, x_layout, MutAnyOrigin], idx: IndexList[rank], freq_val: SIMD[freq_dtype, width])`

</section>

---

## get_identity_rope_coeff (Rope)

<section class='mojo-docs'>

`get_identity_rope_coeff[width: Int, dtype: DType]() -> SIMD[dtype, width]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## get_safetensors_idx (Rope)

<section class='mojo-docs'>

`get_safetensors_idx(head_dim_idx: Int, head_size: Int) -> Tuple[Int, Int]`

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple)

</section>

---

## rope

<section class='mojo-docs'>

## Functions

* [â€‹`apply_rope`](./apply_rope):
* [â€‹`get_identity_rope_coeff`](./get_identity_rope_coeff):
* [â€‹`get_safetensors_idx`](./get_safetensors_idx):
* [â€‹`rope_ragged`](./rope_ragged):

</section>

---

## rope_ragged

<section class='mojo-docs'>

`rope_ragged[dtype: DType, x_layout: Layout, freq_dtype: DType, input_row_offsets_layout: Layout, start_pos_layout: Layout, freqs_cis_layout: Layout, *, interleaved: Bool, target: StringSlice[StaticConstantOrigin], output_fn: fn[width: Int, alignment: Int](idx: IndexList[3], val: SIMD[dtype, width]) capturing -> None, mrope_section: Optional[IntTuple] = None](x: LayoutTensor[dtype, x_layout, MutAnyOrigin], input_row_offsets: LayoutTensor[DType.uint32, input_row_offsets_layout, MutAnyOrigin], start_pos: LayoutTensor[DType.uint32, start_pos_layout, MutAnyOrigin], freqs_cis: LayoutTensor[freq_dtype, freqs_cis_layout, MutAnyOrigin], context: Optional[DeviceContext], position_ids: OptionalReg[LayoutTensor[DType.uint32, Layout.row_major(-1, -1), MutAnyOrigin]] = None)`

</section>

---

## apply_penalties_to_logits

<section class='mojo-docs'>

`apply_penalties_to_logits[logit_type: DType, penalty_type: DType, //, target: StringSlice[StaticConstantOrigin]](logits: LayoutTensor[logit_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], compressed_frequency_data: LayoutTensor[DType.int32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], frequency_offsets: LayoutTensor[DType.uint32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], frequency_penalty: LayoutTensor[penalty_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], presence_penalty: LayoutTensor[penalty_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], repetition_penalty: LayoutTensor[penalty_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr)`

Apply penalties to the logits based on the frequency of the tokens in the batch.

The frequency data is stored in a CSR format, where the frequency\_offsets is the
starting index of each sequence in the frequency\_data array. The frequency\_data
array is a 2D array, where:

* frequency\_data\[i, 0] is the token id
* frequency\_data\[i, 1] is the frequency of the token in the sequence

</section>

---

## sampling

<section class='mojo-docs'>

## Functions

* [â€‹`apply_penalties_to_logits`](./apply_penalties_to_logits): Apply penalties to the logits based on the frequency of the tokens in the batch.
* [â€‹`update_frequency_data`](./update_frequency_data): Update the frequency data for the given new tokens.
* [â€‹`update_frequency_data_kernel`](./update_frequency_data_kernel): GPU kernel to update token frequency data in CSR format.

</section>

---

## update_frequency_data

<section class='mojo-docs'>

`update_frequency_data[token_type: DType, //, target: StringSlice[StaticConstantOrigin]](compressed_frequency_data: LayoutTensor[DType.int32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], frequency_offsets: LayoutTensor[DType.uint32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], new_tokens: LayoutTensor[token_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr)`

Update the frequency data for the given new tokens.

The frequency data is stored in a CSR format. This kernel expects there will be
enough padding for each sequence to store the new tokens.

</section>

---

## update_frequency_data_kernel

<section class='mojo-docs'>

`update_frequency_data_kernel[token_type: DType, block_size: Int, freq_data_layout: Layout, freq_offsets_layout: Layout, new_tokens_layout: Layout](compressed_frequency_data: LayoutTensor[DType.int32, freq_data_layout, MutAnyOrigin], frequency_offsets: LayoutTensor[DType.uint32, freq_offsets_layout, MutAnyOrigin], new_tokens: LayoutTensor[token_type, new_tokens_layout, MutAnyOrigin])`

GPU kernel to update token frequency data in CSR format.

Searches for new tokens in existing frequency data and either increments
their count or adds them to the first available padding slot.

</section>

---

## get_sliding_window_out_dim

<section class='mojo-docs'>

`get_sliding_window_out_dim[ceil_mode: Bool = False](in_dim: Int, ft_dim: Int, dilation: Int, stride: Int, pad: Int) -> Int`

Return output dimension for a sliding window operation along some dimension.

**Parameters:**

* â€‹ceil\_mode ([`Bool`](/mojo/std/builtin/bool/Bool)): Define rounding mode for shape calculation.

**Args:**

* â€‹in\_dim ([`Int`](/mojo/std/builtin/int/Int)): The size of the input dimension.
* â€‹ft\_dim ([`Int`](/mojo/std/builtin/int/Int)): The size of the corresponding filter dimension.
* â€‹dilation ([`Int`](/mojo/std/builtin/int/Int)): The dilation for the sliding window operation.
* â€‹stride ([`Int`](/mojo/std/builtin/int/Int)): The stride for the sliding window operation.
* â€‹pad ([`Int`](/mojo/std/builtin/int/Int)): The total padding for the sliding window operation.

**Returns:**

[`Int`](/mojo/std/builtin/int/Int): The size of the output dimension.

</section>

---

## shapes

<section class='mojo-docs'>

## Functions

* [â€‹`get_sliding_window_out_dim`](./get_sliding_window_out_dim): Return output dimension for a sliding window operation along some dimension.

</section>

---

## copy_to_slice

<section class='mojo-docs'>

`copy_to_slice[dtype: DType, start_type: DType, end_type: DType, step_type: DType, target: StringSlice[StaticConstantOrigin] = "cpu"](buffer: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], in_slice: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], start: LayoutTensor[start_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], end: LayoutTensor[end_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], step: LayoutTensor[step_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], context: DeviceContextPtr = DeviceContextPtr())`

</section>

---

## slice

<section class='mojo-docs'>

## Functions

* [â€‹`copy_to_slice`](./copy_to_slice):
* [â€‹`slice_as_copy`](./slice_as_copy):
* [â€‹`slice_as_view`](./slice_as_view):
* [â€‹`slice_dim_as_view`](./slice_dim_as_view):
* [â€‹`slice_shape`](./slice_shape):
* [â€‹`sliced_add`](./sliced_add): Adds tensors a and b element-wise for rows < lora\_end\_idx, otherwise copies a.

</section>

---

## slice_as_copy

<section class='mojo-docs'>

`slice_as_copy[dtype: DType, index_type: DType](output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], tensor: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], start: LayoutTensor[index_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], end: LayoutTensor[index_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], step: LayoutTensor[index_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## slice_as_view

<section class='mojo-docs'>

`slice_as_view[dtype: DType, start_type: DType, end_type: DType, step_type: DType](tensor: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], starts: LayoutTensor[start_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ends: LayoutTensor[end_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], steps: LayoutTensor[step_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> LayoutTensor[dtype, Layout.row_major[LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank](), origin, address_space=address_space]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

</section>

---

## slice_dim_as_view

<section class='mojo-docs'>

`slice_dim_as_view[dtype: DType, dim: Int](tensor: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], start: Int, end: Int, step: Int) -> LayoutTensor[dtype, Layout.row_major[LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank](), origin, address_space=address_space]`

**Returns:**

[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)

</section>

---

## slice_shape

<section class='mojo-docs'>

`slice_shape[input_type: DType, start_type: DType, stop_type: DType, step_type: DType, single_thread_blocking_override: Bool](input_buf: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], start_buf: LayoutTensor[start_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], stop_buf: LayoutTensor[stop_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], step_buf: LayoutTensor[step_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> IndexList[LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList)

</section>

---

## sliced_add

<section class='mojo-docs'>

`sliced_add[dtype: DType, //, target: StringSlice[StaticConstantOrigin]](c: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], lora_end_idx: LayoutTensor[DType.int64, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: Optional[DeviceContext])`

Adds tensors a and b element-wise for rows < lora\_end\_idx, otherwise copies a.

This is used for LoRA where only some sequences have LoRA applied.
For rows in \[0, lora\_end\_idx): c = a + b
For rows in \[lora\_end\_idx, batch\_seq\_len): c = a

**Args:**

* â€‹c ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output tensor.
* â€‹a ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): First input tensor.
* â€‹b ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Second input tensor.
* â€‹lora\_end\_idx ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Scalar tensor with end index of LoRA token portion (rows to apply add).
* â€‹ctx ([`Optional`](/mojo/std/collections/optional/Optional)): Device context for GPU operations.

</section>

---

## identity

<section class='mojo-docs'>

`identity(x: SIMD[dtype, size]) -> SIMD[dtype, size]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## softmax (3)

<section class='mojo-docs'>

## Functions

* [â€‹`identity`](./identity):
* [â€‹`logsoftmax`](./logsoftmax):
* [â€‹`mul`](./mul):
* [â€‹`reciprocal`](./reciprocal):
* [â€‹`reduce_add_simd`](./reduce_add_simd): This functions adds val to either the scalar value or the vector value depending on the step\_simd\_width. This is useful when the simd\_width varies between iterations as in vectorize.
* [â€‹`softmax`](./softmax):
* [â€‹`softmax_2_pass`](./softmax_2_pass): Performs an unbatched softmax on an input tensor using the two-pass online algorithm.
* [â€‹`softmax_3_pass`](./softmax_3_pass): Performs an unbatched softmax on an input tensor using the three-pass algorithm.
* [â€‹`softmax_kernel`](./softmax_kernel):
* [â€‹`sub`](./sub):

</section>

---

## logsoftmax

<section class='mojo-docs'>

`logsoftmax[dtype: DType, simd_width: Int, rank: Int, input_fn: fn[_simd_width: Int, _rank: Int](IndexList[_rank]) capturing -> SIMD[dtype, _simd_width], target: StringSlice[StaticConstantOrigin] = "cpu"](shape: IndexList[rank], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], axis: Int, context: DeviceContextPtr = DeviceContextPtr())`

`logsoftmax[dtype: DType, simd_width: Int, rank: Int, target: StringSlice[StaticConstantOrigin] = "cpu"](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], axis: Int, context: DeviceContextPtr = DeviceContextPtr())`

</section>

---

## mul (Softmax)

<section class='mojo-docs'>

`mul(x: SIMD[dtype, size], y: SIMD[dtype, size]) -> SIMD[dtype, size]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## reciprocal

<section class='mojo-docs'>

`reciprocal(x: SIMD[dtype, size]) -> SIMD[dtype, size]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## reduce_add_simd

<section class='mojo-docs'>

`reduce_add_simd[simd_width: Int, step_simd_width: Int, dtype: DType](mut scalar: Scalar[dtype], mut vector: SIMD[dtype, simd_width], val: SIMD[dtype, step_simd_width])`

This functions adds val to either the scalar value or the vector value depending on the step\_simd\_width. This is useful when the simd\_width varies between iterations as in vectorize.

</section>

---

## softmax (4)

<section class='mojo-docs'>

`softmax[dtype: DType, simd_width: Int, rank: Int](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], axis: Int)`

`softmax[dtype: DType, simd_width: Int, rank: Int, input_fn: fn[_simd_width: Int, _rank: Int](IndexList[_rank]) capturing -> SIMD[dtype, _simd_width], target: StringSlice[StaticConstantOrigin] = "cpu", logsoftmax: Bool = False](shape: IndexList[rank], output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], axis: Int, context: DeviceContextPtr = DeviceContextPtr())`

</section>

---

## softmax_2_pass

<section class='mojo-docs'>

`softmax_2_pass[simd_width: Int, dtype: DType](output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Performs an unbatched softmax on an input tensor using the two-pass online algorithm.

The unbatched two-pass online softmax is described in "Online
normalizer calculation for softmax" (<https://arxiv.org/abs/1805.02867>) and
"A full-stack search technique for domain optimized deep learning
accelerators" (<https://dl.acm.org/doi/abs/10.1145/3503222.3507767>) and is
defined as:

```
procedure SoftmaxUnbatched(InputInput)
  runningMax = -âˆž
  runningSum = 0
  STAGE 1:
  for i = 0 to N do
    newMax = max(runningMax, Input[i])
    runningSum = runningSum*exp(runningMax-newMax) + exp(Input[i]-newMax)
    runningMax = newMax
  end for
  for i = 0 to N do
    Output[i] = exp(Input[i] - runningMax) / runningSum
  end for
```

**Parameters:**

* â€‹simd\_width ([`Int`](/mojo/std/builtin/int/Int)): The simd\_width to use in vectorization.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of the input and output buffers.

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output buffer in which to store the softmax values.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input buffer used to compute the softmax.

</section>

---

## softmax_3_pass

<section class='mojo-docs'>

`softmax_3_pass[simd_width: Int, dtype: DType, origins: OriginSet, input_fn_1d: fn[_simd_width: Int](Int) capturing -> SIMD[dtype, _simd_width], logsoftmax: Bool = False](output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Performs an unbatched softmax on an input tensor using the three-pass algorithm.

The unbatched three-pass softmax is defined as:

```
procedure SoftmaxUnbatched(InputInput)
  maxVal = -âˆž
  denom = 0
  STEP 1: find the max value in each batch
  for i = 0 to N do
    maxVal = max(maxVal, Input[b, i])
  end for
  STEP 2: compute the exponential for each batch
  for i = 0 to N do
    Output[b, i] = exp(Input[b, i] - maxVal)
    denom += Output[b, i]
  end for
  STEP 3: normalize each batch
  for i = 0 to N do
    Output[b, i] /= denom
  end for
```

**Parameters:**

* â€‹simd\_width ([`Int`](/mojo/std/builtin/int/Int)): The simd\_width to use in vectorization.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of the input and output buffers.
* â€‹origins ([`OriginSet`](/mojo/std/builtin/type_aliases/#originset)): The OriginSet of captured arguments by the input\_fn\_1d.
* â€‹input\_fn\_1d (`fn[_simd_width: Int](Int) capturing -> SIMD[dtype, _simd_width]`): The elementwise input lambda.
* â€‹logsoftmax ([`Bool`](/mojo/std/builtin/bool/Bool)): Enable to apply elementwise log() to outputs after softmax.

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output buffer in which to store the softmax values.

</section>

---

## softmax_kernel

<section class='mojo-docs'>

`softmax_kernel[BLOCK_SIZE: Int, input_fn: fn[_dtype: DType, _simd_width: Int, _rank: Int](IndexList[_rank]) capturing -> SIMD[_dtype, _simd_width], dtype: DType, layout: Layout, sink_type: DType, rank: Int, accum_type: DType = get_accum_type[dtype](), *, sink: Bool = False, logsoftmax: Bool = False](shape: IndexList[rank], output: LayoutTensor[dtype, layout, MutAnyOrigin], sink_weights: LayoutTensor[sink_type, Layout.row_major(-1), MutAnyOrigin])`

</section>

---

## sub

<section class='mojo-docs'>

`sub(x: SIMD[dtype, size], y: SIMD[dtype, size]) -> SIMD[dtype, size]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## spatial_merge

<section class='mojo-docs'>

## Functions

* [â€‹`spatial_merge`](./spatial_merge):
* [â€‹`spatial_merge_kernel`](./spatial_merge_kernel): Spatial merge kernel.

</section>

---

## spatial_merge (Spatial_merge)

<section class='mojo-docs'>

`spatial_merge[dtype: DType](output: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], grid_thw: LayoutTensor[DType.int64, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], hidden_size: Int, merge_size: Int, ctx: DeviceContext)`

</section>

---

## spatial_merge_kernel

<section class='mojo-docs'>

`spatial_merge_kernel[dtype: DType, input_layout: Layout, output_layout: Layout, grid_thw_layout: Layout](output: LayoutTensor[dtype, output_layout, MutAnyOrigin], input: LayoutTensor[dtype, input_layout, MutAnyOrigin], grid_thw: LayoutTensor[DType.int64, grid_thw_layout, MutAnyOrigin], batch_size: Int, hidden_size: Int, merge_size: Int)`

Spatial merge kernel.

Grid: 1D over all output patches (one block per output patch).
Threads: loop over channels (hidden\_size x merge\_size^2).

**Args:**

* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output tensor.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input tensor.
* â€‹grid\_thw ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Grid dimensions tensor (B, 3) containing \[t, h, w] for each item.
* â€‹batch\_size ([`Int`](/mojo/std/builtin/int/Int)): Number of items in batch.
* â€‹hidden\_size ([`Int`](/mojo/std/builtin/int/Int)): Hidden dimension size.
* â€‹merge\_size ([`Int`](/mojo/std/builtin/int/Int)): Size of spatial merge blocks.

</section>

---

## split

<section class='mojo-docs'>

## Functions

* [â€‹`split`](./split):

</section>

---

## split (Split)

<section class='mojo-docs'>

`split[dtype: DType, num_outputs: Int, target: StringSlice[StaticConstantOrigin], trace_description: StringSlice[StaticConstantOrigin], outputs_origin: MutOrigin, outputs_layout: Layout](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], axis: Int, outputs: StaticTuple[LayoutTensor[dtype, outputs_layout, outputs_origin], num_outputs], ctx: DeviceContext)`

</section>

---

## tile

<section class='mojo-docs'>

## Functions

* [â€‹`tile`](./tile): Implements the `Tile` operator from the ONNX spec. This behaves like Numpy tile, but without broadcast.
* [â€‹`tile_shape`](./tile_shape): Compute the output shape of a `tile` operation, and assert the inputs are compatible.

</section>

---

## tile (Tile)

<section class='mojo-docs'>

`tile[dtype: DType, type_repeats: DType](input: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], repeats: LayoutTensor[type_repeats, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

Implements the `Tile` operator from the ONNX spec. This behaves like Numpy tile, but without broadcast.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input and output tensors.
* â€‹type\_repeats ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the repeats tensor.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor. Currently <= 4 dimensions are supported.
* â€‹repeats ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): One-dimensional tensor that specifies the number of repeated
  copies along each of the input's dimensions. Length equals
  input tensor rank.
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output tensor. Has the same dimensions and type as input.

</section>

---

## tile_shape

<section class='mojo-docs'>

`tile_shape[input_type: DType, repeats_type: DType, single_thread_blocking_override: Bool](input_buf: LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], repeats_buf: LayoutTensor[repeats_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment]) -> IndexList[LayoutTensor[input_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

Compute the output shape of a `tile` operation, and assert the inputs are compatible.

**Parameters:**

* â€‹input\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the input tensor.
* â€‹repeats\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Type of the repeats tensor.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If True, then the operation is run
  synchronously using a single thread.

**Args:**

* â€‹input\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹repeats\_buf ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The repeats tensor.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## TopK_2

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct TopK_2[T: DType, largest: Bool = True]`

## Fields

* â€‹p (`Int`):
* â€‹u (`Scalar[T]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__() -> Self`

### `insert`

`insert(mut self, elem: Scalar[T], elem_id: Int)`

</section>

---

## apply_gumbel_noise_kernel

<section class='mojo-docs'>

`apply_gumbel_noise_kernel[dtype: DType, input_layout: Layout, num_sms: Int, num_threads: Int](output: LayoutTensor[dtype, input_layout, MutAnyOrigin], input: LayoutTensor[dtype, input_layout, MutAnyOrigin], temperature: LegacyUnsafePointer[Float32], seed: LegacyUnsafePointer[UInt64])`

</section>

---

## fused_token_sampling_cpu

<section class='mojo-docs'>

`fused_token_sampling_cpu[dtype: DType, out_idx_type: DType](max_k: Int, input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], out_idxs: LayoutTensor[out_idx_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], k: OptionalReg[LayoutTensor[DType.int64, Layout.row_major(-1), MutAnyOrigin]] = None, temperature: OptionalReg[LayoutTensor[DType.float32, Layout.row_major(-1), MutAnyOrigin]] = None, top_p: OptionalReg[LayoutTensor[DType.float32, Layout.row_major(-1), MutAnyOrigin]] = None, seed: OptionalReg[LayoutTensor[DType.uint64, Layout.row_major(-1), MutAnyOrigin]] = None)`

Generalized implementation of the Top K algorithm with sampling. Returns the sampled index from the innermost dimension of the input tensor for each row/subvolume.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the input buffer.
* â€‹out\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the output indices.

**Args:**

* â€‹max\_k ([`Int`](/mojo/std/builtin/int/Int)): Largest number of top elements.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): NDBuffer\[dtype, rank] (Any shape)- The input tensor.
* â€‹out\_idxs ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): NDBuffer\[out\_idx\_type, rank] (shape of \[input\_shape\[:-1]] + \[1]) - The output indices.
* â€‹k ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional device buffer of top elements to keep for each batch element.
* â€‹temperature ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): The temperature based scaling.
* â€‹top\_p ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Only use the tokens whose cumulative probability exceeds this threshold.
* â€‹seed ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): The seed to use for the random number generator.

</section>

---

## fused_token_sampling_gpu

<section class='mojo-docs'>

`fused_token_sampling_gpu[dtype: DType, out_idx_type: DType, //](ctx: DeviceContext, max_k: Int, min_top_p: Float32, input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], out_idxs: LayoutTensor[out_idx_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], block_size: OptionalReg[Int] = None, num_blocks_per_input: OptionalReg[Int] = None, k: OptionalReg[LayoutTensor[DType.int64, Layout.row_major(-1), MutAnyOrigin]] = None, temperature: OptionalReg[LayoutTensor[DType.float32, Layout.row_major(-1), MutAnyOrigin]] = None, top_p: OptionalReg[LayoutTensor[DType.float32, Layout.row_major(-1), MutAnyOrigin]] = None, seed: OptionalReg[LayoutTensor[DType.uint64, Layout.row_major(-1), MutAnyOrigin]] = None)`

Top K algorithm with fused sampling. Returns the sampled indices from the Top-K of the innermost dimension of the input tensor for each row/subvolume.

</section>

---

## gumbel_sampling_gpu

<section class='mojo-docs'>

`gumbel_sampling_gpu[dtype: DType, out_idx_type: DType, input_layout: Layout, //](ctx: DeviceContext, input: LayoutTensor[dtype, input_layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], out_idxs: LayoutTensor[out_idx_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], temperature: OptionalReg[LayoutTensor[DType.float32, Layout.row_major(-1), MutAnyOrigin]] = None, seed: OptionalReg[LayoutTensor[DType.uint64, Layout.row_major(-1), MutAnyOrigin]] = None)`

Gumbel sampling using the Gumbel-max trick for categorical distributions.

Applies Gumbel(0,1) noise to input logits, then selects the argmax.
This is mathematically equivalent to sampling from softmax(logits/temperature)
but avoids expensive softmax computation.

**Args:**

* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): Device context for GPU operations.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input logits tensor \[batch, vocab\_size].
* â€‹out\_idxs ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output tensor for sampled indices \[batch, 1].
* â€‹temperature ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional per-token temperature scaling \[batch].
* â€‹seed ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional per-token random seeds \[batch] for reproducibility.

</section>

---

## topk

<section class='mojo-docs'>

## Structs

* [â€‹`TopK_2`](./TopK_2):

## Functions

* [â€‹`apply_gumbel_noise_kernel`](./apply_gumbel_noise_kernel):
* [â€‹`fused_token_sampling_cpu`](./fused_token_sampling_cpu): Generalized implementation of the Top K algorithm with sampling. Returns the sampled index from the innermost dimension of the input tensor for each row/subvolume.
* [â€‹`fused_token_sampling_gpu`](./fused_token_sampling_gpu): Top K algorithm with fused sampling. Returns the sampled indices from the Top-K of the innermost dimension of the input tensor for each row/subvolume.
* [â€‹`gumbel_sampling_gpu`](./gumbel_sampling_gpu): Gumbel sampling using the Gumbel-max trick for categorical distributions.
* [â€‹`top_k`](./top_k): Implementation of the Top K algorithm. Returns the top or bottom K elements and their index along a specified axis.
* [â€‹`top_k_shape_impl`](./top_k_shape_impl): Compute the output shape of a top/bottom k operation.
* [â€‹`topk_gpu`](./topk_gpu): Generalized implementation of the Top K algorithm with/without sampling. Returns the sampled index from the innermost dimension of the input tensor for each row/subvolume or the top K values and indices across the tensor.

</section>

---

## top_k

<section class='mojo-docs'>

`top_k[dtype: DType, out_idx_type: DType, //, largest: Bool = True, target: StringSlice[StaticConstantOrigin] = "cpu"](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], max_k: Int, axis: Int, out_vals: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], out_idxs: LayoutTensor[out_idx_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], sorted: Bool, ctx: DeviceContextPtr, k: OptionalReg[LayoutTensor[DType.int64, Layout.row_major(-1), MutAnyOrigin]] = None)`

Implementation of the Top K algorithm. Returns the top or bottom K elements and their index along a specified axis.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the input buffer.
* â€‹out\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The data dtype of the output indices (default is DType.int64).
* â€‹largest ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to find the maximum (top k) or minimum value (bottom k).
* â€‹target (`StringSlice`): The target to run on.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹max\_k ([`Int`](/mojo/std/builtin/int/Int)): The largest number of top elements.
* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis along which to operate.
* â€‹out\_vals ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output values.
* â€‹out\_idxs ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output indices.
* â€‹sorted ([`Bool`](/mojo/std/builtin/bool/Bool)): Indicates if the top/bottom K elements are in (stable) sorted order.
* â€‹ctx ([`DeviceContextPtr`](/mojo/std/runtime/asyncrt/DeviceContextPtr)): The device call context.
* â€‹k ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Per batch element k value.

</section>

---

## top_k_shape_impl

<section class='mojo-docs'>

`top_k_shape_impl[dtype: DType, single_thread_blocking_override: Bool](input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], max_k: Int, axis: Int) -> IndexList[LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank]`

Compute the output shape of a top/bottom k operation.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): Data type of the input buffer.
* â€‹single\_thread\_blocking\_override ([`Bool`](/mojo/std/builtin/bool/Bool)): If this function can block.

**Args:**

* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor.
* â€‹max\_k ([`Int`](/mojo/std/builtin/int/Int)): The maximum K value.
* â€‹axis ([`Int`](/mojo/std/builtin/int/Int)): The axis value in a tensor.

**Returns:**

[`IndexList`](/mojo/std/utils/index_/IndexList): The output shape.

</section>

---

## topk_gpu

<section class='mojo-docs'>

`topk_gpu[dtype: DType, out_idx_type: DType, //, sampling: Bool = True, largest: Bool = True, _force_old_impl: Bool = False](ctx: DeviceContext, max_k: Int, input: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], out_vals: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], out_idxs: LayoutTensor[out_idx_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], block_size: OptionalReg[Int] = None, num_blocks_per_input: OptionalReg[Int] = None, k: OptionalReg[LayoutTensor[DType.int64, Layout.row_major(-1), MutAnyOrigin]] = None, temperature: OptionalReg[LayoutTensor[DType.float32, Layout.row_major(-1), MutAnyOrigin]] = None, top_p: OptionalReg[LayoutTensor[DType.float32, Layout.row_major(-1), MutAnyOrigin]] = None, seed: OptionalReg[LayoutTensor[DType.uint64, Layout.row_major(-1), MutAnyOrigin]] = None)`

Generalized implementation of the Top K algorithm with/without sampling. Returns the sampled index from the innermost dimension of the input tensor for each row/subvolume or the top K values and indices across the tensor.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType - The data dtype of the input tensor.
* â€‹out\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): DType - The data dtype of the output indices (default is DType.int).
* â€‹sampling ([`Bool`](/mojo/std/builtin/bool/Bool)): Bool - Whether to return token samples from topK dist (default is True).
* â€‹largest ([`Bool`](/mojo/std/builtin/bool/Bool)): Bool - Whether to find the maximum or minimum value.
* â€‹\_force\_old\_impl ([`Bool`](/mojo/std/builtin/bool/Bool)): Bool - Whether to force use the old implementation.

**Args:**

* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): DeviceContext
  The context for GPU execution.
* â€‹max\_k ([`Int`](/mojo/std/builtin/int/Int)): Int
  Largest number of top elements to keep for each batch element.
* â€‹input ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): NDBuffer\[dtype, rank]
  Input tensor as a device NDBuffer.
* â€‹out\_vals ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): NDBuffer\[dtype, rank]
  Output buffer on device for the K largest values.
* â€‹out\_idxs ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): NDBuffer\[DType.int, rank]
  Output buffer on device for the indices of the K largest values, or sampled token indices.
  Last dimension is 1 if sampling is True, otherwise K.
* â€‹block\_size ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Int
  The number of threads per block (default is 256 from TRT and empirical testing).
* â€‹num\_blocks\_per\_input ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): OptionalReg\[Int]
  Number of blocks per input (default computed from input size and block size).
  This is the equivalent of "BLOCKS\_PER\_BEAM" in TRT-LLM kernel allowing for much larger
  batch sizes through packing several elements per thread in the first stage.
* â€‹k ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional NDBuffer\[DType.int64, 1, MutAnyOrigin]
  Device buffer of top elements to keep for each batch element.
* â€‹temperature ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): The temperature based scaling.
* â€‹top\_p ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Only use the tokens whose cumulative probability exceeds this threshold.
* â€‹seed ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): The seed to use for the random number generator.

</section>

---

## TopKMaskLogitsKernel

<section class='mojo-docs'>

`TopKMaskLogitsKernel[block_size: Int, vec_size: Int, dtype: DType, out_idx_type: DType, logits_layout: Layout, masked_logits_layout: Layout](logits: LayoutTensor[dtype, logits_layout, MutAnyOrigin], masked_logits: LayoutTensor[dtype, masked_logits_layout, MutAnyOrigin], top_k_arr: LegacyUnsafePointer[Scalar[out_idx_type]], top_k_val: Int, d: Int)`

</section>

---

## TopKSamplingFromProbKernel

<section class='mojo-docs'>

`TopKSamplingFromProbKernel[block_size: Int, vec_size: Int, dtype: DType, out_idx_type: DType, probs_layout: Layout, output_layout: Layout, deterministic: Bool](probs: LayoutTensor[dtype, probs_layout, MutAnyOrigin], output: LayoutTensor[out_idx_type, output_layout, MutAnyOrigin], indices: LegacyUnsafePointer[Scalar[out_idx_type]], top_k_arr: LegacyUnsafePointer[Scalar[out_idx_type]], top_k_val: Int, d: Int, rng_seed: UInt64, rng_offset: UInt64)`

Kernel for top-k sampling from probability distribution.

This kernel performs top-k sampling by:

1. Using ternary search to find a pivot threshold.
2. Rejecting samples iteratively until acceptance criteria is met.
3. Sampling an index using uniform random numbers from Random generator.

**Args:**

* â€‹probs ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input probability distribution \[batch\_size, d].
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output sampled indices \[batch\_size].
* â€‹indices (`LegacyUnsafePointer`): Optional row indices for batch indexing \[batch\_size].
* â€‹top\_k\_arr (`LegacyUnsafePointer`): Optional per-row top\_k values \[batch\_size].
* â€‹top\_k\_val ([`Int`](/mojo/std/builtin/int/Int)): Default top\_k value if top\_k\_arr is null.
* â€‹d ([`Int`](/mojo/std/builtin/int/Int)): Vocabulary size.
* â€‹rng\_seed ([`UInt64`](/mojo/std/builtin/simd/#uint64)): Random seed for Random number generator.
* â€‹rng\_offset ([`UInt64`](/mojo/std/builtin/simd/#uint64)): Random offset for Random number generator.

</section>

---

## TopKSoftmaxSampleKernel

<section class='mojo-docs'>

`TopKSoftmaxSampleKernel[block_size: Int, vec_size: Int, dtype: DType, out_idx_type: DType, logits_layout: Layout, sampled_indices_layout: Layout](logits: LayoutTensor[dtype, logits_layout, MutAnyOrigin], sampled_indices: LayoutTensor[out_idx_type, sampled_indices_layout, MutAnyOrigin], top_k_arr: LegacyUnsafePointer[Scalar[out_idx_type]], top_k_val: Int, temperature_val: Float32, temperature: LegacyUnsafePointer[Float32], seed_val: UInt64, seed: LegacyUnsafePointer[UInt64], d: Int)`

</section>

---

## ValueCount

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ValueCount[T: DType]`

A struct that holds a value and a count, used for block reductions.

This is useful for computing both the sum of values and the count
of elements that satisfy a condition in a single reduction pass.

## Parameters

* â€‹T ([`DType`](/mojo/std/builtin/dtype/DType)): The DType of the value field.

## Fields

* â€‹value (`Scalar[T]`):
* â€‹count (`Int32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(value: Scalar[T], count: Int32) -> Self`

`__init__() -> Self`

### `__add__`

`__add__(self, other: Self) -> Self`

### `__iadd__`

`__iadd__(mut self, other: Self)`

</section>

---

## device_sampling_from_prob

<section class='mojo-docs'>

`device_sampling_from_prob[vec_size: Int, block_size: Int, dtype: DType, deterministic: Bool = False](i: Int, d: Int, low: Float64, u: Float32, prob_vec: SIMD[DType.float32, vec_size], aggregate: Float32, sampled_id_sram: LegacyUnsafePointer[Int, address_space=AddressSpace.SHARED], last_valid_id_sram: LegacyUnsafePointer[Int, address_space=AddressSpace.SHARED]) -> Float32`

Device-level sampling from probability distribution with atomic operations.

**Returns:**

[`Float32`](/mojo/std/builtin/simd/#float32)

</section>

---

## get_min_max_value

<section class='mojo-docs'>

`get_min_max_value[vec_size: Int, block_size: Int, dtype: DType](in_data: LegacyUnsafePointer[Scalar[dtype]], row_idx: Int, d: Int) -> Tuple[Float32, Float32]`

Compute the minimum and maximum values from input data using block reduction.

**Parameters:**

* â€‹vec\_size ([`Int`](/mojo/std/builtin/int/Int)): Number of elements each thread processes per iteration (vectorization width).
* â€‹block\_size ([`Int`](/mojo/std/builtin/int/Int)): Number of threads per block.
* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of the input data.

**Args:**

* â€‹in\_data (`LegacyUnsafePointer`): Pointer to input data buffer.
* â€‹row\_idx ([`Int`](/mojo/std/builtin/int/Int)): Row index for the current block (for 2D data access).
* â€‹d ([`Int`](/mojo/std/builtin/int/Int)): Total number of elements in the row.

**Returns:**

[`Tuple`](/mojo/std/builtin/tuple/Tuple): Tuple containing \[min\_val, max\_val].

</section>

---

## topk_fi

<section class='mojo-docs'>

## Structs

* [â€‹`ValueCount`](./ValueCount): A struct that holds a value and a count, used for block reductions.

## Functions

* [â€‹`device_sampling_from_prob`](./device_sampling_from_prob): Device-level sampling from probability distribution with atomic operations.
* [â€‹`get_min_max_value`](./get_min_max_value): Compute the minimum and maximum values from input data using block reduction.
* [â€‹`topk_mask_logits`](./topk_mask_logits):
* [â€‹`topk_sampling_from_prob`](./topk_sampling_from_prob): Top-K sampling from probability distribution.
* [â€‹`topk_softmax_sample`](./topk_softmax_sample): Samples token indices from top-K logits using softmax probabilities.
* [â€‹`TopKMaskLogitsKernel`](./TopKMaskLogitsKernel):
* [â€‹`TopKSamplingFromProbKernel`](./TopKSamplingFromProbKernel): Kernel for top-k sampling from probability distribution.
* [â€‹`TopKSoftmaxSampleKernel`](./TopKSoftmaxSampleKernel):

</section>

---

## topk_mask_logits

<section class='mojo-docs'>

`topk_mask_logits[dtype: DType, out_idx_type: DType, block_size: Int = 1024](ctx: DeviceContext, logits: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], masked_logits: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], top_k_val: Int, top_k_arr: OptionalReg[LayoutTensor[out_idx_type, Layout.row_major(-1), MutAnyOrigin]] = None)`

</section>

---

## topk_sampling_from_prob

<section class='mojo-docs'>

`topk_sampling_from_prob[dtype: DType, out_idx_type: DType, block_size: Int = 1024](ctx: DeviceContext, probs: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output: LayoutTensor[out_idx_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], top_k_val: Int, deterministic: Bool = False, rng_seed: UInt64 = 0, rng_offset: UInt64 = 0, indices: OptionalReg[LayoutTensor[out_idx_type, Layout.row_major(-1), MutAnyOrigin]] = None, top_k_arr: OptionalReg[LayoutTensor[out_idx_type, Layout.row_major(-1), MutAnyOrigin]] = None)`

Top-K sampling from probability distribution.

Performs stochastic sampling from a probability distribution, considering only
the top-k most probable tokens. Uses rejection sampling with ternary search
to efficiently find appropriate samples.

**Args:**

* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): Device context for kernel execution.
* â€‹probs ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Input probability distribution \[batch\_size, d].
* â€‹output ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): Output sampled indices \[batch\_size].
* â€‹top\_k\_val ([`Int`](/mojo/std/builtin/int/Int)): Default top-k value (number of top tokens to consider).
* â€‹deterministic ([`Bool`](/mojo/std/builtin/bool/Bool)): Whether to use deterministic sampling.
* â€‹rng\_seed ([`UInt64`](/mojo/std/builtin/simd/#uint64)): Random seed for Random number generator.
* â€‹rng\_offset ([`UInt64`](/mojo/std/builtin/simd/#uint64)): Random offset for Random number generator.
* â€‹indices ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional row indices for batch indexing \[batch\_size].
* â€‹top\_k\_arr ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)): Optional per-row top-k values \[batch\_size].

**Raises:**

Error: If tensor ranks or shapes are invalid.

</section>

---

## topk_softmax_sample

<section class='mojo-docs'>

`topk_softmax_sample[dtype: DType, out_idx_type: DType, block_size: Int = 1024](ctx: DeviceContext, logits: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], sampled_indices: LayoutTensor[out_idx_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], top_k_val: Int, temperature_val: Float32 = 1, seed_val: UInt64 = 0, top_k_arr: OptionalReg[LayoutTensor[out_idx_type, Layout.row_major(-1), MutAnyOrigin]] = None, temperature: OptionalReg[LayoutTensor[DType.float32, Layout.row_major(-1), MutAnyOrigin]] = None, seed: OptionalReg[LayoutTensor[DType.uint64, Layout.row_major(-1), MutAnyOrigin]] = None)`

Samples token indices from top-K logits using softmax probabilities.

This kernel performs single-pass top-K selection and categorical sampling:

1. Finds the k-th largest logit via ternary search.
2. Computes softmax over top-K elements and caches them in shared memory.
3. Samples a single token index from the categorical distribution.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the input logits tensor.
* â€‹out\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): The data type of the output sampled indices.
* â€‹block\_size ([`Int`](/mojo/std/builtin/int/Int)): The number of threads per block (default is 1024).

**Args:**

* â€‹ctx ([`DeviceContext`](/mojo/std/gpu/host/device_context/DeviceContext)): DeviceContext
  The context for GPU execution.
* â€‹logits ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)):
  Input logits tensor with shape \[batch\_size, vocab\_size].
* â€‹sampled\_indices ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)):
  Output buffer for sampled token indices with shape \[batch\_size].
* â€‹top\_k\_val ([`Int`](/mojo/std/builtin/int/Int)): Int
  Default number of top elements to sample from for each batch element.
* â€‹temperature\_val ([`Float32`](/mojo/std/builtin/simd/#float32)): Float32
  Temperature for softmax scaling (default is 1.0).
* â€‹seed\_val ([`UInt64`](/mojo/std/builtin/simd/#uint64)): UInt64
  Seed for the random number generator (default is 0).
* â€‹top\_k\_arr ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)):
  Optional per-batch top-K values. If provided, overrides top\_k\_val
  for each batch element.
* â€‹temperature ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)):
  Optional per-batch temperature values. If provided, overrides
  temperature\_val for each batch element.
* â€‹seed ([`OptionalReg`](/mojo/std/collections/optional/OptionalReg)):
  Optional per-batch seed values. If provided, overrides seed\_val
  for each batch element.

</section>

---

## toppminp

<section class='mojo-docs'>

## Functions

* [â€‹`merge`](./merge): Merge two sorted subarrays into one sorted array.
* [â€‹`merge_sort_recursive`](./merge_sort_recursive): Recursive merge sort implementation.
* [â€‹`min_p_sampling`](./min_p_sampling): Naive CPU implementation of Min-P sampling for token selection. This function applies temperature scaling, softmax, a merge sort, and then samples tokens based on the calculated probability threshold (Min-P).
* [â€‹`sort_buf_descending`](./sort_buf_descending): Sort each batch separately in descending order using parallel merge sort.
* [â€‹`top_p_sampling`](./top_p_sampling): Naive CPU implementation of Top-P sampling for token selection. This function applies temperature scaling, softmax, a merge sort, and then samples tokens based on the cumulative probability mass (Top-P).

</section>

---

## merge

<section class='mojo-docs'>

`merge[dtype: DType, out_idx_type: DType](mut buf_keys: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mut buf_ids: LayoutTensor[out_idx_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], start: Int, mid: Int, end: Int)`

Merge two sorted subarrays into one sorted array.

</section>

---

## merge_sort_recursive

<section class='mojo-docs'>

`merge_sort_recursive[dtype: DType, out_idx_type: DType](mut buf_keys: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mut buf_ids: LayoutTensor[out_idx_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], start: Int, end: Int)`

Recursive merge sort implementation.

</section>

---

## min_p_sampling

<section class='mojo-docs'>

`min_p_sampling[dtype: DType, out_idx_type: DType, //, _test_sort: Bool = False](min_ps: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_logits: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], out_token_ids: LayoutTensor[out_idx_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], temperature: Scalar[dtype] = 1)`

Naive CPU implementation of Min-P sampling for token selection. This function applies temperature scaling, softmax, a merge sort, and then samples tokens based on the calculated probability threshold (Min-P).

</section>

---

## sort_buf_descending

<section class='mojo-docs'>

`sort_buf_descending[dtype: DType, out_idx_type: DType](mut buf_keys: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], mut buf_ids: LayoutTensor[out_idx_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], vocab_size: Int)`

Sort each batch separately in descending order using parallel merge sort.

</section>

---

## top_p_sampling

<section class='mojo-docs'>

`top_p_sampling[dtype: DType, out_idx_type: DType, //, _test_sort: Bool = False](top_ps: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_logits: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], out_token_ids: LayoutTensor[out_idx_type, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], temperature: Scalar[dtype] = 1)`

Naive CPU implementation of Top-P sampling for token selection. This function applies temperature scaling, softmax, a merge sort, and then samples tokens based on the cumulative probability mass (Top-P).

</section>

---

## DoubleBuffer

<section class='mojo-docs'>

`struct DoubleBuffer[dtype: DType]`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(out self)`

`__init__(out self, current: LegacyUnsafePointer[Scalar[dtype]], alternate: LegacyUnsafePointer[Scalar[dtype]], size: Int)`

### `current`

`current(self, ctx: DeviceContext) -> DeviceBuffer[dtype]`

**Returns:**

[`DeviceBuffer`](/mojo/std/gpu/host/device_context/DeviceBuffer)

### `alternate`

`alternate(self, ctx: DeviceContext) -> DeviceBuffer[dtype]`

**Returns:**

[`DeviceBuffer`](/mojo/std/gpu/host/device_context/DeviceBuffer)

### `swap`

`swap(mut self)`

</section>

---

## toppminp_gpu

<section class='mojo-docs'>

## `comptime` values

### `DEBUG_FILE`

`comptime DEBUG_FILE = False`

### `SEED`

`comptime SEED = 42`

## Structs

* [â€‹`DoubleBuffer`](./DoubleBuffer):

## Functions

* [â€‹`min_p_sampling_gpu`](./min_p_sampling_gpu): GPU implementation of Min-P sampling for token selection. This function applies temperature scaling, softmax, a radix sort, and then samples tokens based on the calculated probability threshold (Min-P).
* [â€‹`normalize`](./normalize):
* [â€‹`normalize_u32`](./normalize_u32):
* [â€‹`radix_sort_pairs_kernel`](./radix_sort_pairs_kernel): Radix pair sort kernel for (default) descending order.
* [â€‹`run_radix_sort_pairs_gpu`](./run_radix_sort_pairs_gpu):
* [â€‹`top_p_sampling_gpu`](./top_p_sampling_gpu): GPU implementation of Top-P sampling for token selection. This function applies temperature scaling, softmax, a radix sort, and then samples tokens based on the cumulative probability mass (Top-P).
* [â€‹`topk_wrapper`](./topk_wrapper): Copy of `Kernels/mojo/nn/topk.mojo:_topk_stage1` with the addition of max\_vals and p\_threshold arguments to determine if sorting is needed for top-p/min-p sampling.
* [â€‹`topp_minp_sampling_kernel`](./topp_minp_sampling_kernel): Top P-Min P sampling kernel.

</section>

---

## min_p_sampling_gpu

<section class='mojo-docs'>

`min_p_sampling_gpu[dtype: DType, out_idx_type: DType, //, _test_sort: Bool = False](ctx: DeviceContext, min_ps: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_logits: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], out_token_ids: LayoutTensor[out_idx_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], temperature: Scalar[dtype] = 1)`

GPU implementation of Min-P sampling for token selection. This function applies temperature scaling, softmax, a radix sort, and then samples tokens based on the calculated probability threshold (Min-P).

</section>

---

## normalize

<section class='mojo-docs'>

`normalize(value: BFloat16) -> UInt16`

**Returns:**

[`UInt16`](/mojo/std/builtin/simd/#uint16)

`normalize(value: Int32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

`normalize(value: UInt16) -> UInt16`

**Returns:**

[`UInt16`](/mojo/std/builtin/simd/#uint16)

`normalize(value: Float32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

`normalize(value: Scalar[dtype]) -> Scalar[_uint_type_of_width[bit_width_of[dtype]()]()]`

Normalize the value to the appropriate unsigned integer type. This is needed for radix sort to work correctly.

**Returns:**

[`Scalar`](/mojo/std/builtin/simd/#scalar)

</section>

---

## normalize_u32

<section class='mojo-docs'>

`normalize_u32(value: UInt32) -> UInt32`

**Returns:**

[`UInt32`](/mojo/std/builtin/simd/#uint32)

</section>

---

## radix_sort_pairs_kernel

<section class='mojo-docs'>

`radix_sort_pairs_kernel[dtype: DType, out_idx_type: DType, current_bit: Int, ascending: Bool = False, BLOCK_SIZE: Int = 256, NUM_BITS_PER_PASS: Int = 4](input_keys_: LegacyUnsafePointer[Scalar[dtype]], output_keys_: LegacyUnsafePointer[Scalar[dtype]], input_key_ids_: LegacyUnsafePointer[Scalar[out_idx_type]], output_key_ids_: LegacyUnsafePointer[Scalar[out_idx_type]], num_keys: Int, skip_sort: LegacyUnsafePointer[Scalar[DType.bool]])`

Radix pair sort kernel for (default) descending order.

Implementation based on:
AMD. Introduction to GPU Radix Sort. GPUOpen, 2017. Available at:
<https://gpuopen.com/download/publications/Introduction_to_GPU_Radix_Sort.pdf>.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType - Data type.
* â€‹out\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): DType - Output index type.
* â€‹current\_bit ([`Int`](/mojo/std/builtin/int/Int)): Int - Current bit to start sorting NUM\_BITS\_PER\_PASS bits at.
* â€‹ascending ([`Bool`](/mojo/std/builtin/bool/Bool)): Bool - Whether to sort in ascending order.
* â€‹BLOCK\_SIZE ([`Int`](/mojo/std/builtin/int/Int)): Int - Block size.
* â€‹NUM\_BITS\_PER\_PASS ([`Int`](/mojo/std/builtin/int/Int)): Int - Number of bits per pass.

**Args:**

* â€‹input\_keys\_ (`LegacyUnsafePointer`): Input tensor values to sort.
* â€‹output\_keys\_ (`LegacyUnsafePointer`): Output tensor values sorted in (default) descending order.
* â€‹input\_key\_ids\_ (`LegacyUnsafePointer`): Input tensor indices.
* â€‹output\_key\_ids\_ (`LegacyUnsafePointer`): Output tensor indices sorted in (default) descending order.
* â€‹num\_keys ([`Int`](/mojo/std/builtin/int/Int)): Number of keys to sort per batch.
* â€‹skip\_sort (`LegacyUnsafePointer`): Whether sorting is skipped for this batch.

</section>

---

## run_radix_sort_pairs_gpu

<section class='mojo-docs'>

`run_radix_sort_pairs_gpu[dtype: DType, out_idx_type: DType, ascending: Bool = False, BLOCK_SIZE: Int = 256, NUM_BITS_PER_PASS: Int = 4](ctx: DeviceContext, mut keys: DoubleBuffer[dtype], mut key_ids: DoubleBuffer[out_idx_type], skip_sort: LegacyUnsafePointer[Scalar[DType.bool]], in_shape: IndexList[size, element_type=element_type])`

</section>

---

## top_p_sampling_gpu

<section class='mojo-docs'>

`top_p_sampling_gpu[dtype: DType, out_idx_type: DType, //, _test_sort: Bool = False](ctx: DeviceContext, top_ps: LayoutTensor[dtype, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_logits: LayoutTensor[dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], out_token_ids: LayoutTensor[out_idx_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], temperature: Scalar[dtype] = 1)`

GPU implementation of Top-P sampling for token selection. This function applies temperature scaling, softmax, a radix sort, and then samples tokens based on the cumulative probability mass (Top-P).

</section>

---

## topk_wrapper

<section class='mojo-docs'>

`topk_wrapper[T: DType, out_idx_type: DType, is_top_p: Bool, largest: Bool = True, _test_sort: Bool = False](K: Int, num_elements: Int, num_blocks_per_input: Int, in_buffer: LegacyUnsafePointer[Scalar[T]], local_topk_vals: LegacyUnsafePointer[Scalar[T]], local_topk_idxs: LegacyUnsafePointer[Scalar[out_idx_type]], p_threshold: LegacyUnsafePointer[Scalar[T]], skip_sort: LegacyUnsafePointer[Scalar[DType.bool]])`

Copy of `Kernels/mojo/nn/topk.mojo:_topk_stage1` with the addition of max\_vals and p\_threshold arguments to determine if sorting is needed for top-p/min-p sampling.

Arguments:
K: Int - Number of top elements to select per block
num\_elements: Int - Size of last dimension of input buffer (vocab size)
num\_blocks\_per\_input: Int - Number of blocks used to process the input data
in\_buffer: UnsafePointer\[Scalar\[T]] - Input buffer containing the elements to process
local\_topk\_vals: UnsafePointer\[Scalar\[T]] - Output buffer to store the local top-K values
local\_topk\_idxs: UnsafePointer\[Scalar\[out\_idx\_type]] - Output buffer to store the indices of local top-K elements
p\_threshold: UnsafePointer\[Scalar\[T]] - Threshold for top-p sampling if is\_top\_p is True else min-p coefficient
skip\_sort: UnsafePointer\[Scalar\[DType.bool]] - Output buffer to store whether sorting is needed

**Parameters:**

* â€‹T ([`DType`](/mojo/std/builtin/dtype/DType)): DType - The data type of the elements.
* â€‹out\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): DType - The data type of the output indices.
* â€‹is\_top\_p ([`Bool`](/mojo/std/builtin/bool/Bool)): Bool - Whether this if for top-p sampling or min-p sampling.
* â€‹largest ([`Bool`](/mojo/std/builtin/bool/Bool)): Bool - Whether to find the maximum or minimum value.
* â€‹\_test\_sort ([`Bool`](/mojo/std/builtin/bool/Bool)): Bool - An internal test flag to not skip sort if testing.

</section>

---

## topp_minp_sampling_kernel

<section class='mojo-docs'>

`topp_minp_sampling_kernel[dtype: DType, out_idx_type: DType, is_top_p: Bool](p_thresholds_: LegacyUnsafePointer[Scalar[dtype]], sorted_probs_: LegacyUnsafePointer[Scalar[dtype]], sorted_ids_: LegacyUnsafePointer[Scalar[out_idx_type]], out_token_ids: LegacyUnsafePointer[Scalar[out_idx_type]], skip_sort: LegacyUnsafePointer[Scalar[DType.bool]], vocab_size: Int)`

Top P-Min P sampling kernel.

**Parameters:**

* â€‹dtype ([`DType`](/mojo/std/builtin/dtype/DType)): DType - scalar values dtype.
* â€‹out\_idx\_type ([`DType`](/mojo/std/builtin/dtype/DType)): DType - output index type.
* â€‹is\_top\_p ([`Bool`](/mojo/std/builtin/bool/Bool)): Bool - Whether to use Top-P (True) or Min-P (False) sampling.

</section>

---

## nvml

<section class='mojo-docs'>

Implements wrappers around the NVIDIA Management Library (nvml).

## Modules

* [â€‹`nvml`](./nvml/): Implements wrappers around the NVIDIA Management Library (nvml).

</section>

---

## ClockType

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct ClockType`

## Fields

* â€‹code (`Int32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Equatable`](/mojo/std/builtin/comparable/Equatable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `GRAPHICS`

`comptime GRAPHICS = ClockType(0)`

Graphics clock domain.

### `MEM`

`comptime MEM = ClockType(2)`

Memory clock domain.

### `SM`

`comptime SM = ClockType(1)`

SM clock domain.

### `VIDEO`

`comptime VIDEO = ClockType(2)`

Video clock domain.

## Methods

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

`Bool`

</section>

---

## Device

<section class='mojo-docs'>

`struct Device`

## Fields

* â€‹idx (`Int`):
* â€‹device (`_DeviceImpl`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

## Methods

### `__init__`

`__init__(out self, idx: Int = 0)`

### `get_driver_version`

`get_driver_version(self) -> DriverVersion`

Returns NVIDIA driver version.

**Returns:**

`DriverVersion`

### `max_mem_clock`

`max_mem_clock(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `max_graphics_clock`

`max_graphics_clock(self) -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

### `mem_clocks`

`mem_clocks(self) -> List[Int]`

**Returns:**

[`List`](/mojo/std/collections/list/List)

### `graphics_clocks`

`graphics_clocks(self, memory_clock_mhz: Int) -> List[Int]`

**Returns:**

[`List`](/mojo/std/collections/list/List)

### `set_clock`

`set_clock(self, mem_clock: Int, graphics_clock: Int)`

### `gpu_turbo_enabled`

`gpu_turbo_enabled(self) -> Bool`

Returns True if the gpu turbo is enabled.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `set_gpu_turbo`

`set_gpu_turbo(self, enabled: Bool = True)`

Sets the GPU turbo state.

### `get_persistence_mode`

`get_persistence_mode(self) -> Bool`

Returns True if the gpu persistence mode is enabled.

**Returns:**

[`Bool`](/mojo/std/builtin/bool/Bool)

### `set_persistence_mode`

`set_persistence_mode(self, enabled: Bool = True)`

Sets the persistence mode.

### `set_max_gpu_clocks`

`set_max_gpu_clocks(device)`

### `__str__`

`__str__(self) -> String`

**Returns:**

`String`

### `write_to`

`write_to(self, mut writer: T)`

### `__repr__`

`__repr__(self) -> String`

**Returns:**

`String`

</section>

---

## DriverVersion

<section class='mojo-docs'>

`struct DriverVersion`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`StringableRaising`](/mojo/std/builtin/str/StringableRaising)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = False`

### `__del__is_trivial`

`comptime __del__is_trivial = False`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

## Methods

### `__init__`

`__init__(out self, var value: List[String])`

### `__copyinit__`

`__copyinit__(out self, other: Self)`

### `major`

`major(self) -> Int`

**Returns:**

`Int`

### `minor`

`minor(self) -> Int`

**Returns:**

`Int`

### `patch`

`patch(self) -> Int`

**Returns:**

`Int`

### `__str__`

`__str__(self) -> String`

**Returns:**

`String`

</section>

---

## EnableState

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct EnableState`

## Fields

* â€‹code (`Int32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Equatable`](/mojo/std/builtin/comparable/Equatable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `DISABLED`

`comptime DISABLED = EnableState(0)`

Feature disabled.

### `ENABLED`

`comptime ENABLED = EnableState(1)`

Feature enabled.

## Methods

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

`Bool`

</section>

---

## Result

<section class='mojo-docs'>

`@register_passable(trivial)`
`struct Result`

## Fields

* â€‹code (`Int32`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Copyable`](/mojo/std/builtin/value/Copyable),
[`Equatable`](/mojo/std/builtin/comparable/Equatable),
[`ImplicitlyCopyable`](/mojo/std/builtin/value/ImplicitlyCopyable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible),
[`Movable`](/mojo/std/builtin/value/Movable),
[`Stringable`](/mojo/std/builtin/str/Stringable),
[`Writable`](/mojo/std/io/write/Writable)

## `comptime` members

### `__copyinit__is_trivial`

`comptime __copyinit__is_trivial = True`

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `__moveinit__is_trivial`

`comptime __moveinit__is_trivial = True`

### `ALREADY_INITIALIZED`

`comptime ALREADY_INITIALIZED = Result(5)`

Deprecated: Multiple initializations are now allowed through ref counting.

### `ARGUMENT_VERSION_MISMATCH`

`comptime ARGUMENT_VERSION_MISMATCH = Result(25)`

The provided version is invalid/unsupported.

### `CORRUPTED_INFOROM`

`comptime CORRUPTED_INFOROM = Result(14)`

The infoROM is corrupted.

### `DEPRECATED`

`comptime DEPRECATED = Result(26)`

The requested functionality has been deprecated.

### `DRIVER_NOT_LOADED`

`comptime DRIVER_NOT_LOADED = Result(9)`

NVIDIA driver is not loaded.

### `FREQ_NOT_SUPPORTED`

`comptime FREQ_NOT_SUPPORTED = Result(24)`

Ran out of critical resources, other than memory.

### `FUNCTION_NOT_FOUND`

`comptime FUNCTION_NOT_FOUND = Result(13)`

Local version of NVML doesn't implement this function.

### `GPU_IS_LOST`

`comptime GPU_IS_LOST = Result(15)`

The GPU has fallen off the bus or has otherwise become inaccessible.

### `GPU_NOT_FOUND`

`comptime GPU_NOT_FOUND = Result(28)`

No GPUs were found.

### `IN_USE`

`comptime IN_USE = Result(19)`

An operation cannot be performed because the GPU is currently in use.

### `INSUFFICIENT_POWER`

`comptime INSUFFICIENT_POWER = Result(8)`

A device's external power cables are not properly attached.

### `INSUFFICIENT_RESOURCES`

`comptime INSUFFICIENT_RESOURCES = Result(23)`

Ran out of critical resources, other than memory.

### `INSUFFICIENT_SIZE`

`comptime INSUFFICIENT_SIZE = Result(7)`

An input argument is not large enough.

### `INVALID_ARGUMENT`

`comptime INVALID_ARGUMENT = Result(2)`

A supplied argument is invalid.

### `IRQ_ISSUE`

`comptime IRQ_ISSUE = Result(11)`

NVIDIA Kernel detected an interrupt issue with a GPU.

### `LIB_RM_VERSION_MISMATCH`

`comptime LIB_RM_VERSION_MISMATCH = Result(18)`

RM detects a driver/library version mismatch.

### `LIBRARY_NOT_FOUND`

`comptime LIBRARY_NOT_FOUND = Result(12)`

NVML Shared Library couldn't be found or loaded.

### `MEMORY`

`comptime MEMORY = Result(20)`

Insufficient memory.

### `NO_DATA`

`comptime NO_DATA = Result(21)`

No data.

### `NO_PERMISSION`

`comptime NO_PERMISSION = Result(4)`

The current user does not have permission for operation.

### `NOT_FOUND`

`comptime NOT_FOUND = Result(6)`

A query to find an object was unsuccessful.

### `NOT_READY`

`comptime NOT_READY = Result(27)`

The system is not ready for the request.

### `NOT_SUPPORTED`

`comptime NOT_SUPPORTED = Result(3)`

The requested operation is not available on target device.

### `OPERATING_SYSTEM`

`comptime OPERATING_SYSTEM = Result(17)`

The GPU control device has been blocked by the operating system/cgroups.

### `RESET_REQUIRED`

`comptime RESET_REQUIRED = Result(16)`

The GPU requires a reset before it can be used again.

### `SUCCESS`

`comptime SUCCESS = Result(0)`

The operation was successful.

### `TIMEOUT`

`comptime TIMEOUT = Result(10)`

User provided timeout passed.

### `UNINITIALIZED`

`comptime UNINITIALIZED = Result(1)`

NVML was not first initialized with `nvmlInit()`.

### `UNKNOWN`

`comptime UNKNOWN = Result(999)`

An internal driver error occurred.

### `VGPU_ECC_NOT_SUPPORTED`

`comptime VGPU_ECC_NOT_SUPPORTED = Result(22)`

The requested vgpu operation is not available on target device, because ECC is enabled.

## Methods

### `__eq__`

`__eq__(self, other: Self) -> Bool`

**Returns:**

`Bool`

### `write_to`

`write_to(self, mut writer: T)`

### `__str__`

`__str__(self) -> String`

**Returns:**

`String`

</section>

---

## nvml (Nvml)

<section class='mojo-docs'>

Implements wrappers around the NVIDIA Management Library (nvml).

## `comptime` values

### `CUDA_NVML_LIBRARY`

`comptime CUDA_NVML_LIBRARY = _Global["CUDA_NVML_LIBRARY", _init_dylib]`

### `CUDA_NVML_LIBRARY_BASE_NAME`

`comptime CUDA_NVML_LIBRARY_BASE_NAME = "libnvidia-ml"`

### `CUDA_NVML_LIBRARY_DIR`

`comptime CUDA_NVML_LIBRARY_DIR = "/usr/lib/x86_64-linux-gnu"`

### `CUDA_NVML_LIBRARY_EXT`

`comptime CUDA_NVML_LIBRARY_EXT = ".so"`

## Structs

* [â€‹`ClockType`](./ClockType):
* [â€‹`Device`](./Device):
* [â€‹`DriverVersion`](./DriverVersion):
* [â€‹`EnableState`](./EnableState):
* [â€‹`Result`](./Result):

</section>

---

## quantization

<section class='mojo-docs'>

This package contains a set of APIs for quantizing tensor data.

Quantization is a technique used to reduce the precision of floating-point
numbers, which are used in most neural networks. Quantization is a type of
lossy compression, which means that some precision is lost, but the resulting
tensors take less memory and computations are faster.

## Modules

* [â€‹`per_channel_grouped_4bit`](./per_channel_grouped_4bit/):
* [â€‹`qmatmul`](./qmatmul/):
* [â€‹`qmatmul_gpu`](./qmatmul_gpu/):
* [â€‹`qmatmul_k`](./qmatmul_k/):

</section>

---

## Q4sym

<section class='mojo-docs'>

`struct Q4sym[group_size: Int, float_dtype: DType = DType.float32]`

Q4sym: compresses values of type `float_dtype` to 4bit unsigned integers which have been dynamically symmetrically quantized with the given scale factor.

`group_size` determines the number of elements which share quantization
parameters.

We store things in a strided fashion:
Example:

Assume `group_size = 8` and we want to process uint4 numbers:
A, B, C, D, E, F, G, H which have associated bits aaaa, bbbb, cccc, ....

eeeeaaaa|ffffbbbb|ggggcccc|hhhhdddd

To uncompress to floating point, take the decoded uint4 value, subtract
the implicit zero-point of 2^4=8, and multiply by the scale factor.

## Parameters

* â€‹group\_size ([`Int`](/mojo/std/builtin/int/Int)): The number of encoded numbers stored in this struct.
* â€‹float\_dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The floating point dtype this struct works with.

## Fields

* â€‹scale (`StaticTuple[UInt8, 2]`): The FP16 scale of the group, stored as individual bytes.
* â€‹bits (`StaticTuple[UInt8, (group_size // 2)]`): The bits of the encoded uint4 numbers.

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`Defaultable`](/mojo/std/builtin/value/Defaultable),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

## Methods

### `__init__`

`__init__(out self)`

Construct a default initialized Q4sym.

`__init__(out self, data: SIMD[float_dtype, group_size])`

Construct an encoded Q4sym from data.

**Args:**

* â€‹data ([`SIMD`](/mojo/std/builtin/simd/SIMD)): The floating point data to encode and store.

### `decode_scale`

`decode_scale(mut self) -> Float16`

Obtain the scale factor.

**Returns:**

`Float16`: The decoded scale factor.

### `decode_unsigned`

`decode_unsigned(mut self) -> SIMD[DType.uint8, group_size]`

Decode the stored uint4 numbers to uint8.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): The decoded stored numbers as uint8 numbers. These have an implicit
zero-point of 8.

### `decode_signed`

`decode_signed(mut self) -> SIMD[DType.int8, group_size]`

Decode the stored uint4 numbers to requantized int4 numbers.

This is done by simply subtracting an implicit zp of 8 from the
unsigned decoding.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): The decoded stored numbers as int8 numbers. These have a zero-point of
0\.

### `decode_fully`

`decode_fully(mut self) -> SIMD[float_dtype, group_size]`

Decode the stored numbers into floating point representation.

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD): The decoded numbers.

### `quantize_and_write_to_tensor`

`static quantize_and_write_to_tensor(input_tensor: LayoutTensor[float_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output_tensor: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], input_shape: IndexList[LayoutTensor[float_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank])`

Encodes the floating point numbers in `input_tensor` along the inner-most dimension and writes the result to output\_tensor.

**Args:**

* â€‹input\_tensor ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor we are encoding.
* â€‹output\_tensor ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output tensor containing the encoded input.
  The shape of the output should be the same as the input
  except along the inner dimension where if the original inner
  dimension was `d`, the corresponding output dimension should be:
  ceil(`d` / group\_size) \* size\_of(self).
* â€‹input\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The shape of the input tensor.

### `dequantize_and_write_to_tensor`

`static dequantize_and_write_to_tensor(input_tensor: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output_tensor: LayoutTensor[float_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output_shape: IndexList[LayoutTensor[float_dtype, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank])`

Encodes the floating point numbers in `input_tensor` along the inner-most dimension and writes the result to output\_tensor.

**Args:**

* â€‹input\_tensor ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The input tensor we are decoding.
* â€‹output\_tensor ([`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor)): The output tensor containing the decoded input.
* â€‹output\_shape ([`IndexList`](/mojo/std/utils/index_/IndexList)): The shape of the output tensor.

</section>

---

## block_Q4_K

<section class='mojo-docs'>

`struct block_Q4_K`

## Fields

* â€‹base\_scale (`Float16`):
* â€‹base\_min (`Float16`):
* â€‹q\_scales\_and\_mins (`InlineArray[UInt8, 12]`):
* â€‹q\_bits (`InlineArray[UInt8, 128]`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `group_count`

`comptime group_count = 8`

### `group_size`

`comptime group_size = 32`

</section>

---

## block_Q6_K

<section class='mojo-docs'>

`struct block_Q6_K`

## Fields

* â€‹q\_bits\_lo (`InlineArray[UInt8, 128]`):
* â€‹q\_bits\_hi (`InlineArray[UInt8, 64]`):
* â€‹q\_scales (`InlineArray[Int8, 16]`):
* â€‹base\_scale (`Float16`):

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `group_count`

`comptime group_count = 16`

### `group_size`

`comptime group_size = 16`

</section>

---

## block_QK_K

<section class='mojo-docs'>

`struct block_QK_K`

## Implemented traits

[`AnyType`](/mojo/std/builtin/anytype/AnyType),
[`ImplicitlyDestructible`](/mojo/std/builtin/anytype/ImplicitlyDestructible)

## `comptime` members

### `__del__is_trivial`

`comptime __del__is_trivial = True`

### `quantized_k`

`comptime quantized_k = 256`

</section>

---

## calculate_symmetric_vector

<section class='mojo-docs'>

`calculate_symmetric_vector[input_dtype: DType, simd_width: Int, output_bits: Int](data: SIMD[input_dtype, simd_width]) -> Tuple[SIMD[DType.uint8, simd_width], Scalar[input_dtype]]`

Symmetrically quantizes the given SIMD vector `data` with input type `input_dtype` and `simd_width` elements, assuming we want the results to fit in an unsigned integer of size `output_bits`.

**Parameters:**

* â€‹input\_dtype ([`DType`](/mojo/std/builtin/dtype/DType)): The dtype of the input tensor.
* â€‹simd\_width ([`Int`](/mojo/std/builtin/int/Int)): The width of the SIMD input.
* â€‹output\_bits ([`Int`](/mojo/std/builtin/int/Int)): The bits we want to fit the unsigned integral result in.

**Args:**

* â€‹data ([`SIMD`](/mojo/std/builtin/simd/SIMD)): The input SIMD we want to quantize.

**Returns:**

`Tuple`: A vector of the quantized values.
The associated scale factor.

</section>

---

## per_channel_grouped_4bit

<section class='mojo-docs'>

## Structs

* [â€‹`block_Q4_K`](./block_Q4_K):
* [â€‹`block_Q6_K`](./block_Q6_K):
* [â€‹`block_QK_K`](./block_QK_K):
* [â€‹`Q4sym`](./Q4sym): Q4sym: compresses values of type `float_dtype` to 4bit unsigned integers which have been dynamically symmetrically quantized with the given scale factor.

## Functions

* [â€‹`calculate_symmetric_vector`](./calculate_symmetric_vector): Symmetrically quantizes the given SIMD vector `data` with input type `input_dtype` and `simd_width` elements, assuming we want the results to fit in an unsigned integer of size `output_bits`.
* [â€‹`q4_k_dequantize_impl`](./q4_k_dequantize_impl):
* [â€‹`q6_k_dequantize_impl`](./q6_k_dequantize_impl):
* [â€‹`scale_min_k4`](./scale_min_k4):

</section>

---

## q4_k_dequantize_impl

<section class='mojo-docs'>

`q4_k_dequantize_impl(input_tensor: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output_tensor: LayoutTensor[DType.float32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## q6_k_dequantize_impl

<section class='mojo-docs'>

`q6_k_dequantize_impl(input_tensor: LayoutTensor[DType.uint8, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output_tensor: LayoutTensor[DType.float32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], output_shape: IndexList[LayoutTensor[DType.float32, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment].rank])`

</section>

---

## scale_min_k4

<section class='mojo-docs'>

`scale_min_k4(src_ptr: LegacyUnsafePointer[block_Q4_K, mut=mut, origin=origin], g: Int) -> Tuple[Float32, Float32]`

**Returns:**

`Tuple`

</section>

---

## qmatmul

<section class='mojo-docs'>

## `comptime` values

### `K_BATCH_SIZE`

`comptime K_BATCH_SIZE = 512`

Defines the batch size of K used to pack A and unpack B weights.

## Functions

* [â€‹`matmul_qint4`](./matmul_qint4):
* [â€‹`matmul_qint4_pack_b`](./matmul_qint4_pack_b):

</section>

---

## matmul_qint4

<section class='mojo-docs'>

`matmul_qint4[group_size: Int, b_layout: Layout = Layout.row_major[2](), elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](a: LayoutTensor[DType.float32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[DType.uint8, b_layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c: LayoutTensor[DType.float32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## matmul_qint4_pack_b

<section class='mojo-docs'>

`matmul_qint4_pack_b[group_size: Int](b: LayoutTensor[DType.uint8, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_rot: LayoutTensor[DType.uint8, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## args_to_tuple

<section class='mojo-docs'>

`args_to_tuple[swap: Bool](arg_0: Int, arg_1: Int) -> Tuple[Int, Int]`

**Returns:**

`Tuple`

</section>

---

## gpu_qint4_repack_GPTQ

<section class='mojo-docs'>

`gpu_qint4_repack_GPTQ[group_size: Int, target: StringSlice[StaticConstantOrigin]](b: LayoutTensor[DType.uint8, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_packed: LayoutTensor[DType.uint8, layout, origin, address_space=address_space, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], perm_idx: OptionalReg[LayoutTensor[DType.int32, Layout.row_major(-1), MutAnyOrigin]] = None, ctx: DeviceContextPtr = DeviceContextPtr())`

</section>

---

## gpu_qint4_repack_Q4_0

<section class='mojo-docs'>

`gpu_qint4_repack_Q4_0[b_shape: DimList, //, target: StringSlice[StaticConstantOrigin]](b: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_packed: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr = DeviceContextPtr())`

</section>

---

## qmatmul_gpu

<section class='mojo-docs'>

## Functions

* [â€‹`args_to_tuple`](./args_to_tuple):
* [â€‹`gpu_qint4_repack_GPTQ`](./gpu_qint4_repack_GPTQ):
* [â€‹`gpu_qint4_repack_Q4_0`](./gpu_qint4_repack_Q4_0):
* [â€‹`matmul_gpu_qint4`](./matmul_gpu_qint4):
* [â€‹`matmul_gpu_qint4_impl`](./matmul_gpu_qint4_impl):
* [â€‹`multistage_gemm_q`](./multistage_gemm_q):
* [â€‹`multistage_mma_q`](./multistage_mma_q):
* [â€‹`multistage_qgemm_kernel`](./multistage_qgemm_kernel):
* [â€‹`pack_Q_tile`](./pack_Q_tile):
* [â€‹`q_smem_usage`](./q_smem_usage):
* [â€‹`repack_GPTQ_for_sm8x`](./repack_GPTQ_for_sm8x):
* [â€‹`repack_Q4_0_for_sm8x`](./repack_Q4_0_for_sm8x):
* [â€‹`unpack_4bit_int`](./unpack_4bit_int):

</section>

---

## matmul_gpu_qint4

<section class='mojo-docs'>

`matmul_gpu_qint4[c_type: DType, a_type: DType, //, group_size: Int, target: StringSlice[StaticConstantOrigin], elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[a_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: DeviceContextPtr = DeviceContextPtr())`

</section>

---

## matmul_gpu_qint4_impl

<section class='mojo-docs'>

`matmul_gpu_qint4_impl[c_type: DType, a_type: DType, //, group_size: Int, target: StringSlice[StaticConstantOrigin], elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[a_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], ctx: Optional[DeviceContext])`

</section>

---

## multistage_gemm_q

<section class='mojo-docs'>

`multistage_gemm_q[c_type: DType, a_type: DType, b_type: DType, //, *, group_size: Int, pack_factor: Int, config: MatmulConfig[a_type, b_type, c_type, True], elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], a: LayoutTensor[a_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[b_type, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], runtime_config: MatmulConfig[a_type, b_type, c_type, True], ctx: DeviceContext)`

</section>

---

## multistage_mma_q

<section class='mojo-docs'>

`multistage_mma_q[BM: Int, BN: Int, BK: Int, WM: Int, WN: Int, num_threads: Int, num_pipeline_stages: Int, transpose_b: Bool, group_size: Int, pack_factor: Int, c_type: DType, c_layout: Layout, a_type: DType, a_layout: Layout, a_smem_layout: Layout, b_type: DType, b_layout: Layout, b_smem_layout: Layout, scales_type: DType, scales_layout: Layout, scales_smem_layout: Layout, /, *, swizzle_a: Bool = True, static_num_iters: Int = -1, prefetch_init: Bool = True, continue_prefetch_b: Bool = False, transpose_b_next: Bool = False, b_next_gmem_layout: Layout = Layout(), b_next_smem_layout: Layout = Layout(), next_op_b_iter_alignment: Int = align_of[b_type]()](c: LayoutTensor[c_type, c_layout, origin, address_space=AddressSpace.LOCAL], a_iter_arg: LayoutTensorIter[dtype, a_layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], b_iter_arg: LayoutTensorIter[b_type, b_layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], a_smem_iter_arg: LayoutTensorIter[a_type, a_smem_layout, origin, address_space=AddressSpace.SHARED, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], mut b_smem_iter: LayoutTensorIter[b_type, b_smem_layout, origin, address_space=AddressSpace.SHARED, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], scales_smem_iter_arg: LayoutTensorIter[scales_type, scales_smem_layout, origin, address_space=AddressSpace.SHARED, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], scales_iter_arg: LayoutTensorIter[scales_type, scales_layout, origin, address_space=address_space, alignment=alignment, circular=circular, axis=axis, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked], num_iters: Int, /, *, num_b_rows: OptionalReg[Int] = None)`

</section>

---

## multistage_qgemm_kernel

<section class='mojo-docs'>

`multistage_qgemm_kernel[c_type: DType, c_layout: Layout, a_type: DType, a_layout: Layout, b_packed_type: DType, b_layout: Layout, group_size: Int, pack_factor: Int, transpose_b: Bool, config: MatmulConfig[a_type, b_packed_type, c_type, transpose_b], elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](c: LayoutTensor[c_type, c_layout, MutAnyOrigin], a: LayoutTensor[a_type, a_layout, MutAnyOrigin], b_packed: LayoutTensor[b_packed_type, b_layout, MutAnyOrigin])`

</section>

---

## pack_Q_tile

<section class='mojo-docs'>

`pack_Q_tile(input: SIMD[DType.uint8, 16]) -> SIMD[DType.uint32, 4]`

**Returns:**

[`SIMD`](/mojo/std/builtin/simd/SIMD)

</section>

---

## q_smem_usage

<section class='mojo-docs'>

`q_smem_usage[config: MatmulConfig[a_type, b_type, c_type, transpose_b], group_size: Int]() -> Int`

**Returns:**

[`Int`](/mojo/std/builtin/int/Int)

</section>

---

## repack_GPTQ_for_sm8x

<section class='mojo-docs'>

`repack_GPTQ_for_sm8x[in_layout: Layout, out_layout: Layout, scales_type: DType, group_size: Int, has_perm: Bool, *, perm_layout: Layout = Layout()](in_tensor: LayoutTensor[DType.uint8, in_layout, MutAnyOrigin], out_tensor: LayoutTensor[DType.uint8, out_layout, MutAnyOrigin], perm_idx: LayoutTensor[DType.int32, perm_layout, MutAnyOrigin])`

</section>

---

## repack_Q4_0_for_sm8x

<section class='mojo-docs'>

`repack_Q4_0_for_sm8x[q_layout: Layout, repack_layout: Layout, scales_type: DType](q_weight: LayoutTensor[DType.uint8, q_layout, MutAnyOrigin], q_packed_weight: LayoutTensor[DType.uint8, repack_layout, MutAnyOrigin])`

</section>

---

## unpack_4bit_int

<section class='mojo-docs'>

`unpack_4bit_int(val: SIMD[DType.uint32, size], idx: Int) -> UInt8`

**Returns:**

`UInt8`

</section>

---

## qmatmul_k

<section class='mojo-docs'>

## Functions

* [â€‹`matmul_Q4_K`](./matmul_Q4_K):
* [â€‹`matmul_Q4_K_pack_b`](./matmul_Q4_K_pack_b):
* [â€‹`matmul_Q6_K`](./matmul_Q6_K):
* [â€‹`matmul_Q6_K_pack_b`](./matmul_Q6_K_pack_b):

</section>

---

## matmul_Q4_K

<section class='mojo-docs'>

`matmul_Q4_K[elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](a: LayoutTensor[DType.float32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c: LayoutTensor[DType.float32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## matmul_Q4_K_pack_b

<section class='mojo-docs'>

`matmul_Q4_K_pack_b(b: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_packed: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## matmul_Q6_K

<section class='mojo-docs'>

`matmul_Q6_K[elementwise_lambda_fn: OptionalReg[fn[dtype: DType, width: Int, *, alignment: Int = 1](IndexList[2], SIMD[dtype, width]) capturing -> None] = None](a: LayoutTensor[DType.float32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], c: LayoutTensor[DType.float32, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>

---

## matmul_Q6_K_pack_b

<section class='mojo-docs'>

`matmul_Q6_K_pack_b(b: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment], b_packed: LayoutTensor[DType.uint8, layout, origin, element_layout=element_layout, layout_int_type=layout_int_type, linear_idx_type=linear_idx_type, masked=masked, alignment=alignment])`

</section>